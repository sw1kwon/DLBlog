{
 "cells": [
  {
   "cell_type": "raw",
   "id": "20ee4531-eaa2-4d6d-acb8-4cd9edf06ac9",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"13wk-1: (강화학습) -- 강화학습 Intro, Bandit 게임 설명, Bandit 환경 설계 및 풀이\"\n",
    "author: \"최규빈\"\n",
    "date: \"05/28/2025\"\n",
    "draft: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dbdb4a-2b04-405d-9c7d-3f12efb0d756",
   "metadata": {},
   "source": [
    "📘 **Note Format Guide**\n",
    "\n",
    "This format serves as a structured guide for organizing lecture content, personal interpretation, experiments, and study-related questions.\n",
    "\n",
    "| Type | What It Means | When I Use It |\n",
    "|------|----------------|----------------|\n",
    "| 📝 Lecture | Original material from the professor’s notes | When I’m referencing core concepts or provided code |\n",
    "| 🗣️ In-Class Note | Verbal explanations shared during the lecture | When I want to record something the professor said in class but didn’t include in the official notes |\n",
    "| ✍️ My Note | My thoughts, interpretations, or additional explanations | When I reflect on or explain something in my own words |\n",
    "| 🔬 Experiment | Code I tried out or changed to explore further | When I test variations or go beyond the original example |\n",
    "| ❓ Question | Questions I had while studying | When I want to revisit or research something more deeply |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0327ab2-b48d-4e5f-840e-8ea0d0f60fd9",
   "metadata": {},
   "source": [
    "📝\n",
    "🗣️\n",
    "✍️\n",
    "🔬\n",
    "❓"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0235c145-6f02-41a6-8ba4-b014a49ce9b7",
   "metadata": {
    "id": "4d47a7c9"
   },
   "source": [
    "# 1. 강의노트 원본 및 영상 링크"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d434fce-a1e9-45a0-882e-22b056be41e3",
   "metadata": {},
   "source": [
    "[https://guebin.github.io/DL2025/posts/13wk-1.html](https://guebin.github.io/DL2025/posts/13wk-1/.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e54afc5-3248-4e78-a6cd-e3f397c1bc03",
   "metadata": {},
   "source": [
    "# 2. Imports 📝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e67c2fbc-b914-41de-ba7c-d1ca82bbe3e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7647998-5c41-471b-88a9-a04b022dfeaa",
   "metadata": {},
   "source": [
    "# 3. 강화학습 Intro 📝"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2294a5d7-09da-484d-97e9-587b8e4f9628",
   "metadata": {},
   "source": [
    "`-` 강화학습(대충설명): 어떠한 \"(게임)환경\"이 있을때 거기서 \"뭘 할지\"를 학습하는 과업"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e6959084-9697-4a84-96ad-6276be7e553e",
   "metadata": {},
   "source": [
    "![그림1: 셔튼(@sutton1998reinforcement)의 교재에서 발췌한 그림, 되게 유명한 그림이에요](https://github.com/guebin/DL2025/blob/main/posts/13wk-1-fig1.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b496219-1194-48a2-9a70-3c2be6311558",
   "metadata": {},
   "source": [
    "`-` 딥마인드: breakout $\\to$ 알파고 \n",
    "\n",
    "- <https://www.youtube.com/watch?v=TmPfTpjtdgg>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "38ccaff6-78c7-47ef-afd1-bdb4eb6e83ed",
   "metadata": {},
   "source": [
    "![그림2: 벽돌깨기](https://github.com/guebin/DL2025/blob/main/posts/13wk-1-fig2.png?raw=true)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f3d2bd6-146b-4656-99cc-528085d87b81",
   "metadata": {},
   "source": [
    "`-` 강화학습에서 \"강화\"는 뭘 강화한다는것일까? \n",
    "\n",
    "- <https://k9connoisseur.com/blogs/news/positive-reinforcement-dog-training> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd25bb7-2776-423c-8b3b-5fa7cd591621",
   "metadata": {},
   "source": [
    "`-` 강화학습 미래? (이거 잘하면 먹고 살 수 있을까?) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c651b5af-9328-4df2-b8ea-604d50528be3",
   "metadata": {},
   "source": [
    "# 4. Bandit 게임 설명 📝"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae247d25-c760-43de-8079-f961f7d93011",
   "metadata": {},
   "source": [
    "`-` 문제설명: 두 개의 버튼이 있다. `버튼0`을 누르면 1의 보상을, `버튼1`을 누르면 10의 보상을 준다고 가정 \n",
    "\n",
    "- Agent: 버튼0을 누르거나,버튼1을 누르는 존재 \n",
    "- Env: Agent의 Action을 바탕으로 Reward를 주는 존재\n",
    "\n",
    "> 주의: 이 문제 상황에서 state는 없음 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e481d6-e2f9-41db-be55-d079c84dfb0e",
   "metadata": {},
   "source": [
    "`-` 생성형AI로 위의 상황을 설명한것 (왼쪽부터 챗지피티, 제미나이, 퍼플렉시티의 결과)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248f1aee-7a9d-45a5-9469-efeac0fba8b4",
   "metadata": {},
   "source": [
    "::: {layout-ncol=3}\n",
    "![](https://github.com/guebin/DL2025/blob/main/posts/13wk-1-fig3-gpt.png?raw=true)\n",
    "\n",
    "![](https://github.com/guebin/DL2025/blob/main/posts/13wk-1-fig3-gemini.png?raw=true)\n",
    "\n",
    "![](https://github.com/guebin/DL2025/blob/main/posts/13wk-1-fig3-perplexity.png?raw=true)\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29634314-350f-4c95-9237-a3b0da263698",
   "metadata": {},
   "source": [
    "- 클로드로 생성: <https://claude.ai/public/artifacts/1f52fcb2-ef08-4af1-8cf8-4a497d7bcc5f>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19c58b3-c091-4510-802d-8a97310af06b",
   "metadata": {},
   "source": [
    "`-` 게임진행양상\n",
    "\n",
    "- 처음에는 아는게 없음. 일단 \"아무거나\" 눌러보자. (\"에이전트가 랜덤액션을 한다\" 고 표현함 )\n",
    "- 한 20번 정도 눌러보면서 결과를 관찰함 (\"에이전트가 경험을 축적한다\"고 표현함) \n",
    "- 버튼0을 누를때는 1점, 버튼1을 누를때는 10점을 준다는 사실을 깨달음. (\"에이전트가 환경을 이해했다\"고 표현함)\n",
    "- 버튼1을 누르는게 나한테 이득이 라는 사실을 깨달음. (\"에이전트가 최적의 정책을 학습했다\" 고 표현함)\n",
    "- 이제부터 무조건 버튼1만 누름 $\\to$ 게임 클리어 (\"강화학습 성공\"이라 표현할 수 있음)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea0051f-eefb-4207-ac46-7f95eee8d81b",
   "metadata": {},
   "source": [
    "`-` 어떻게 버튼1을 누르는게 이득이라는 사실을 아는거지? $\\to$ 아래와 같은 테이블을 만들면 된다. (`q_table`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69913a6d-27c5-4760-81ed-aa29f6d22744",
   "metadata": {},
   "source": [
    "|| Action0 | Action1 | \n",
    "|:-:|:-:|:-:|\n",
    "|State0 | mean(Reward \\| State0, Action0) |mean(Reward \\| State0, Action1)|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c26c21-2d24-4161-9060-879392784e00",
   "metadata": {},
   "source": [
    "- 🗣️\n",
    "    - mean(Reward \\| State0, Action0): State0에서 Action0을 했을 때 얻는 Reward의 mean\n",
    "    - mean(Reward \\| State0, Action1): State0에서 Action1을 했을 때 얻는 Reward의 mean\n",
    "    - q_table을 갖고 있으면 환경에 대응하여 어떠한 행동을 해야할 지 알 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649ce8a9-6299-4616-9d18-c97d7e6aa816",
   "metadata": {},
   "source": [
    "# 5.  Bandit 환경 설계 및 풀이 📝"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f600a5db-10a6-41ad-9dba-6880d33d874d",
   "metadata": {},
   "source": [
    "## A. 대충 개념만 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bbe57e-2c62-479b-a082-cc2b22356355",
   "metadata": {},
   "source": [
    "🗣️("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22fc740b-4843-47eb-9179-ba9c4a9877d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_space = [0,1]\n",
    "action = np.random.choice(action_space)\n",
    "action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef6305d-3649-45f7-8a7f-59a3f1a88000",
   "metadata": {},
   "source": [
    "- action space: agent가 할 수 있는 action들의 집합\n",
    "- 처음 action은 random으로 뽑음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ddec849-25ff-41ac-a6bb-0b7c3f6bb267",
   "metadata": {},
   "outputs": [],
   "source": [
    "if action == 1:\n",
    "    reward = 10\n",
    "elif action == 0:\n",
    "    reward = 1\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbd9e8df-8c4f-486f-b39b-a228baf0b311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeff5e40-4ed5-4b95-8c87-0e0330113d3d",
   "metadata": {},
   "source": [
    "```python\n",
    "if action == 1:\n",
    "    reward = 10\n",
    "else:\n",
    "    reward = 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0cb359-3488-4692-96ba-1efb5c34c5e4",
   "metadata": {},
   "source": [
    "- 동일 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e399522f-17ca-4aee-9c77-5a96d673a96d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.int64(0), 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_space = [0,1]\n",
    "action = np.random.choice(action_space)\n",
    "if action == 1:\n",
    "    reward = 10\n",
    "else:\n",
    "    reward = 1\n",
    "(action, reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9882e2f4-3d35-41b6-96a3-d7b0cc3658e8",
   "metadata": {},
   "source": [
    "- action과 reward의 history를 컴퓨터에 저장할 공간이 필요함\n",
    "    - 게임을 오래하면 필요한 메모리 공간이 매우 커지므로 다음과 같은 자료형 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f88aa8f-11fd-4ab6-8d34-38de91cdc8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = collections.deque(maxlen=5) # list? numpy? 비슷한 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1d95cb7-db89-424a-bae5-d2f8b3950cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([], maxlen=5)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "422892c7-7835-4781-a1cf-b02b5269427b",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions.append(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6782170c-92c5-4719-8d4c-0a8161addda6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([np.int64(0)], maxlen=5)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78ea5e90-4a63-4292-8488-3e3c606ff116",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = [0,1]\n",
    "actions = collections.deque(maxlen=5)\n",
    "#---#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "09a9dfa1-32fe-4e79-be90-768902ff12f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = np.random.choice(action_space)\n",
    "if action == 1:\n",
    "    reward = 10\n",
    "else:\n",
    "    reward = 1\n",
    "actions.append(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "78977d11-e1e2-488a-89a3-97585dad664d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1)],\n",
       "      maxlen=5)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef660c1-d19d-4a4c-8e83-6b008fa83a24",
   "metadata": {},
   "source": [
    "- 위의 두 코드를 5번 실행하니 deque([np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1)],\n",
    "      maxlen=5) 과 같은 결과를 얻었음\n",
    "- 한 번 더 실행한다면 이전 결과가 밀리면서 사라짐\n",
    "    - deque([np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1)],\n",
    "      maxlen=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "66a7d678-d8ae-4081-83f9-7897b61cefe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = [0,1]\n",
    "actions_deque = collections.deque(maxlen=500)\n",
    "rewards_deque = collections.deque(maxlen=500)\n",
    "#---#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9560b724-0dd4-4da7-8827-2b261504bffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    action = np.random.choice(action_space)\n",
    "    if action == 1:\n",
    "        reward = 10\n",
    "    else:\n",
    "        reward = 1\n",
    "    actions_deque.append(action)\n",
    "    rewards_deque.append(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2ddc1fcf-0466-4da4-b15a-2c1865200b87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([np.int64(0),\n",
       "       np.int64(0),\n",
       "       np.int64(1),\n",
       "       np.int64(1),\n",
       "       np.int64(0),\n",
       "       np.int64(0),\n",
       "       np.int64(0),\n",
       "       np.int64(1),\n",
       "       np.int64(0),\n",
       "       np.int64(1)],\n",
       "      maxlen=500)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions_deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f05d548c-5a40-46ab-a9c4-7f0c4a82d8a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([1, 1, 10, 10, 1, 1, 1, 10, 1, 10], maxlen=500)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards_deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c563dcf6-417a-4c9a-98aa-82448e129c94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False,  True,  True, False, False, False,  True, False,\n",
       "        True])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(actions_deque) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af42d723-605a-4890-9c7b-b1e80f09e311",
   "metadata": {},
   "source": [
    "- 브로드캐스팅을 하기 위해 numpy 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "348aacd0-cf71-4fe0-bf88-42d1b666ac56",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_numpy = np.array(actions_deque)\n",
    "rewards_numpy = np.array(rewards_deque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d9250be6-06a8-4b68-8ecf-939d177862ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 0, 0, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "23793cfb-058d-4cc8-b753-9a75e69f8af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  1, 10, 10,  1,  1,  1, 10,  1, 10])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c5c3f8b3-b4df-442d-8b12-4798f6d78943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, 10, 10, 10])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards_numpy[actions_numpy == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c2b60503-3702-47d1-8605-5345831a7bc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(10.0)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards_numpy[actions_numpy == 1].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec06ee3d-450e-4eba-ac55-f6cc90e97592",
   "metadata": {},
   "source": [
    "- 여기서는 reward가 일정하지만 random하게 주는 경우도 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54e6c91-6d41-459d-920d-e54df5592d5c",
   "metadata": {},
   "source": [
    "```\n",
    "q0 = 0을 눌렀을때 받는 보상의 평균\n",
    "q1 = 1을 눌렀을때 받는 보상의 평균\n",
    "q_table = [q0, q1]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8c872789-7895-422f-a37f-e96ce00409fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1., 10.])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q0 = rewards_numpy[actions_numpy == 0].mean()\n",
    "q1 = rewards_numpy[actions_numpy == 1].mean()\n",
    "q_table = np.array([q0,q1])\n",
    "q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e8e098-4a38-4ef9-8f73-a91928e6f5ad",
   "metadata": {},
   "source": [
    "- q_table을 보고 action 하는 것을 코드로 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "43264c38-04ac-4209-ae37-0cd10d32aba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(1)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "89947c60-67eb-474c-82e5-25666e48c922",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = q_table.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4328b893-9c41-4735-b724-fbb689915f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    action = q_table.argmax() # 이제는 action을 random으로 하는 것이 아니라 q_table을 보고 함\n",
    "    if action == 1:\n",
    "        reward = 10\n",
    "    else:\n",
    "        reward = 1\n",
    "    actions_deque.append(action)\n",
    "    rewards_deque.append(reward)\n",
    "    actions_numpy = np.array(actions_deque)\n",
    "    rewards_numpy = np.array(rewards_deque)\n",
    "    q0 = rewards_numpy[actions_numpy == 0].mean()\n",
    "    q1 = rewards_numpy[actions_numpy == 1].mean()\n",
    "    q_table = np.array([q0,q1]) # q_table update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9903c6b2-48b1-45f6-aeeb-a4e74087f5a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  1, 10, 10,  1,  1,  1, 10,  1, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "       10, 10, 10])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards_numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3804b1-b539-43e1-ab37-a172cd27c9a3",
   "metadata": {},
   "source": [
    "- 처음에는 random이다가 끝에는 10점을 받는 것을 알 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c2192cfa-17c3-4882-a4fb-3e4340008036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions_numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e58504-1dcf-499e-8e64-e247d15f2d86",
   "metadata": {},
   "source": [
    ")🗣️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "3f99cc1b-e046-46d6-be53-2b2f93a4ca41",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = [0,1] \n",
    "actions_deque = collections.deque(maxlen=500)\n",
    "rewards_deque =  collections.deque(maxlen=500)\n",
    "#---#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "13146de5-a6ab-4d02-9527-9c09084f374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    action = np.random.choice(action_space)\n",
    "    if action == 1:\n",
    "        reward = 10 \n",
    "    else:\n",
    "        reward = 1\n",
    "    actions_deque.append(action)\n",
    "    rewards_deque.append(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "67ff5b6a-38aa-4523-ba3f-cca68585be2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([0, 1, 0, 0, 0, 1, 0, 1, 1, 0], maxlen=500)"
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions_deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "6d92dca2-4f94-4468-913f-9df41b190d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([1, 10, 1, 1, 1, 10, 1, 10, 10, 1], maxlen=500)"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards_deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "id": "8a83ec00-abff-4f00-9f91-fa2e31c7882d",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_numpy = np.array(actions_deque)\n",
    "rewards_numpy = np.array(rewards_deque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "d1d75161-4d30-46bd-9847-f43f0c3fe61c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1., 10.])"
      ]
     },
     "execution_count": 522,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q0 = rewards_numpy[actions_numpy == 0].mean()\n",
    "q1 = rewards_numpy[actions_numpy == 1].mean()\n",
    "q_table = np.array([q0,q1])\n",
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "057398c0-de67-44a3-9b8c-107fdc803857",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = q_table.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "d6f136e0-be0a-4d68-bd95-2de46c107415",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    action = q_table.argmax()\n",
    "    if action == 1:\n",
    "        reward = 10 \n",
    "    else:\n",
    "        reward = 1\n",
    "    actions_deque.append(action)\n",
    "    rewards_deque.append(reward)\n",
    "    actions_numpy = np.array(actions_deque)\n",
    "    rewards_numpy = np.array(rewards_deque)    \n",
    "    q0 = rewards_numpy[actions_numpy == 0].mean()\n",
    "    q1 = rewards_numpy[actions_numpy == 1].mean()\n",
    "    q_table = np.array([q0,q1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "9cd0dfdb-8cd1-4559-beec-35741a29d3f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 526,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "dd121c26-6028-4a88-b167-f2292ba69786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, 10,  1,  1,  1, 10,  1, 10, 10,  1, 10, 10, 10, 10, 10])"
      ]
     },
     "execution_count": 525,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards_numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b1168d-2f31-4279-9917-d902a5c59c43",
   "metadata": {},
   "source": [
    "## B. 클래스를 이용한 구현 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979755ba-b73b-42b3-8c4a-9211742c06a4",
   "metadata": {},
   "source": [
    "🗣️("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21adf9fb-3aea-4205-b369-f08f69648ce4",
   "metadata": {},
   "source": [
    "- 강화학습에서 객체라고 불릴 수 있는 것: Agent, Environment\n",
    "    - Agent가 하는 행동: action, 저장(Environment의 reward)\n",
    "    - Environment가 하는 행동: reward(action을 받아서)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8373c787-d66f-4ab8-90f2-e36b16275e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bandit:\n",
    "    def __init__(self):\n",
    "        pass # 초기값 패스\n",
    "    def step(self, action):\n",
    "        # action --> reward\n",
    "        if action == 0:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = 10\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "41f54461-071c-4dcf-9d7f-26db44ea1435",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Bandit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c5faeb0f-1b31-4d79-82f8-a09ba5c774c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8c417e23-afb6-4161-8629-c879fefaad98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf49a079-ac0d-400e-9655-13e55b89686c",
   "metadata": {},
   "source": [
    "- env의 reward도 저장하게 하고 싶다면"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "55a20ad6-5964-4ead-84b2-719e99f35b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bandit:\n",
    "    def __init__(self):\n",
    "        self.reward = None\n",
    "    def step(self, action):\n",
    "        # action --> reward\n",
    "        if action == 0:\n",
    "            self.reward = 1\n",
    "        else:\n",
    "            self.reward = 10\n",
    "        return self.reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fc646463-f5c6-4ce9-a219-84d22164b4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Bandit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "80c4cf4b-3ab8-4544-a045-1ca71407cc54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d1384d7b-7b0a-4f3a-bc88-71c592f330c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1ca683d5-948b-44eb-957b-ef5ac3fee6d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "64c02788-39cd-4181-95aa-1aa5a34128d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c62faba3-20ef-4def-811d-9e8a996332af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def act(self):\n",
    "        # 만약에 경험이 20보다 작음 --> 랜덤 액션\n",
    "        # 경험이 20보다 크면 --> action = q_table.argmax()\n",
    "        pass\n",
    "    def save_experience(self):\n",
    "        # 데이터\n",
    "        pass\n",
    "    def learn(self):\n",
    "        # q_table을 업데이트하는 과정\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8960b8-b30e-43ef-881c-df595f039469",
   "metadata": {},
   "source": [
    "- 다음 시간에 이어서"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508b2169-7c2c-4279-ad1c-8c4ce97cc67b",
   "metadata": {},
   "source": [
    ")🗣️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "4ee96e9f-587c-4358-b728-23718a57d8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bandit:\n",
    "    def __init__(self):\n",
    "        self.reward = None \n",
    "    def step(self,action):\n",
    "        if action == 0:\n",
    "            self.reward = 1\n",
    "        else: \n",
    "            self.reward = 10 \n",
    "        return self.reward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "74991c83-f4ad-4dbb-8e44-4043b557e8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Bandit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "id": "aa4e545e-50fc-4ede-9ba8-0ddc540e1d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        pass \n",
    "    def act(self):\n",
    "        # 만약에 경험이 20보다 작음 --> 랜덤액션 \n",
    "        # 경험이 20보다 크면 --> action = q_tabel.argmax()\n",
    "        pass \n",
    "    def save_experience(self):\n",
    "        # 데이터 저장 \n",
    "        pass \n",
    "    def learn(self):\n",
    "        # q_table 을 업데이트하는 과정 \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce16de50-2ae1-4172-beb1-7acf73fb0fbf",
   "metadata": {},
   "source": [
    ":::{.callout-important}\n",
    "앞으로의 수업에서는 아래에 해당하는 클래스의 기본 개념을 숙지하셔야 합니다. (`13wk-2` 주차 강의듣기전까지 꼭!) \n",
    "\n",
    "1. 클래스와 인스턴스의 개념, `__init__`, `self`, 메소드\n",
    "2. 클래스의 상속 \n",
    "\n",
    "관련하여 제가 작년에 수업한 자료는 아래와 같습니다\n",
    "\n",
    "1. <https://guebin.github.io/PP2024/posts/11wk-2.html> 에서 1-7까지.. \n",
    "2. <https://guebin.github.io/PP2024/posts/14wk-2.html> 에서 8-A\n",
    "\n",
    "물론, 꼭 제 강의노트로만 공부하셔야하는것은 아닙니다. 제 수업 외에도 클래스를 잘 설명하는 다양한 자료들이 많이 있으니 자유롭게 참고하여 학습하시기 바랍니다.\n",
    ":::"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
