[
  {
    "objectID": "posts/exercise1.html",
    "href": "posts/exercise1.html",
    "title": "A1: Exercise â€“ ver. 0503-1",
    "section": "",
    "text": "ë¬¸ì œí’€ì´ì— í•„ìš”í•œ ëª¨ë“ˆì€ ìŠ¤ìŠ¤ë¡œ import í•  ê²ƒ\nimport torch"
  },
  {
    "objectID": "posts/exercise1.html#ë²¡í„°ì™€-í–‰ë ¬",
    "href": "posts/exercise1.html#ë²¡í„°ì™€-í–‰ë ¬",
    "title": "A1: Exercise â€“ ver. 0503-1",
    "section": "$. ë²¡í„°ì™€ í–‰ë ¬",
    "text": "$. ë²¡í„°ì™€ í–‰ë ¬\n(1) ì•„ë˜ì™€ ê°™ì´ length 5 ì¸ vectorë¥¼ torch.tensorë¡œ ì„ ì–¸í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•˜ë¼.\n\\[{\\bf x} = [1,2,3,4,5]\\]\n(í’€ì´)\n\nx = torch.tensor([1,2,3,4,5])\nx\n\ntensor([1, 2, 3, 4, 5])\n\n\n(2) ì•„ë˜ì™€ ê°™ì€ 2x2 matrix ë¥¼ torch.tensorë¡œ ì„ ì–¸í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•˜ë¼.\n\\[{\\bf A} = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\\]\n(í’€ì´?)\n\nA = torch.tensor([[1,2],[3,4]])\nA\n\ntensor([[1, 2],\n        [3, 4]])\n\n\n(3) ì•„ë˜ì™€ ê°™ì€ matrix ë¥¼ torch.tensorë¡œ ì„ ì–¸í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•˜ë¼.\n\\[{\\bf W} = \\begin{bmatrix} 2.5  \\\\  4 \\end{bmatrix}\\]\n(í’€ì´?)\n\nW = torch.tensor([[2.5],[4]])\nW\n\ntensor([[2.5000],\n        [4.0000]])\n\n\n(4) ì•„ë˜ì™€ ê°™ì€ matrix ë¥¼ torch.tensorë¡œ ì„ ì–¸í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•˜ë¼.\n\\[{\\bf x} = \\begin{bmatrix} 2.5  & 4 \\end{bmatrix}\\]\n(í’€ì´?)\n\nx = torch.tensor([[2.5, 4]])\nx\n\ntensor([[2.5000, 4.0000]])"
  },
  {
    "objectID": "posts/exercise1.html#concat-stack",
    "href": "posts/exercise1.html#concat-stack",
    "title": "A1: Exercise â€“ ver. 0503-1",
    "section": "$. concat, stack",
    "text": "$. concat, stack\na,bê°€ ì•„ë˜ì™€ ê°™ì´ ì£¼ì–´ì¡Œë‹¤ê³  í•˜ì.\n\na = torch.tensor([1]*10)\nb = torch.tensor([2]*10)\n\n(ê´€ì°°?)\n\na\n\ntensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n\n\n\na.shape\n\ntorch.Size([10])\n\n\n\nb\n\ntensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n\n\n(ì‹¤í—˜?)\n\ntorch.tensor([[1]*10])\n\ntensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n\n\n\ntorch.tensor([[1]*10]).shape\n\ntorch.Size([1, 10])\n\n\nì•„ë˜ë¥¼ ì˜ ì½ê³  ë¬¼ìŒì— ë‹µí•˜ë¼.\n(1) ì£¼ì–´ì§„ a,bì™€ torch.concatë¥¼ ì´ìš©í•˜ì—¬ ì•„ë˜ì™€ ê°™ì€ ë°°ì—´ì„ ë§Œë“¤ì–´ë¼.\ntensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n(í’€ì´?)\n\ntorch.concat([a,b])\n\ntensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n\n\n(2) ì£¼ì–´ì§„ a,b ì™€ torch.concat,.reshapeë¥¼ ì´ìš©í•˜ì—¬ ì•„ë˜ì™€ ê°™ì€ ë°°ì—´ì„ ë§Œë“¤ì–´ë¼.\ntensor([[1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [2],\n        [2],\n        [2],\n        [2],\n        [2],\n        [2],\n        [2],\n        [2],\n        [2],\n        [2]])\n(í’€ì´)\n\ntorch.concat([a.reshape(-1,1), b.reshape(-1,1)])\n\ntensor([[1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [2],\n        [2],\n        [2],\n        [2],\n        [2],\n        [2],\n        [2],\n        [2],\n        [2],\n        [2]])\n\n\n(ê´€ì°°?)\n\na.reshape(-1,1)\n\ntensor([[1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1]])\n\n\n\na\n\ntensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n\n\n(3) ì£¼ì–´ì§„ a,b ì™€ torch.concat,.reshapeë¥¼ ì´ìš©í•˜ì—¬ ì•„ë˜ì™€ ê°™ì€ ë°°ì—´ì„ ë§Œë“¤ì–´ë¼.\ntensor([[1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2]])\n(í’€ì´?)\n\ntorch.concat([a,b])\n\ntensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n\n\n\ntorch.concat([a,b]).reshape(-1,2)\n\ntensor([[1, 1],\n        [1, 1],\n        [1, 1],\n        [1, 1],\n        [1, 1],\n        [2, 2],\n        [2, 2],\n        [2, 2],\n        [2, 2],\n        [2, 2]])\n\n\n\ntorch.concat([a.reshape(-1,1),b.reshape(-1,1)], axis=1)\n\ntensor([[1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2]])\n\n\n(4) ì£¼ì–´ì§„ a,bì™€ torch.stack ì„ ì´ìš©í•˜ì—¬ ì•„ë˜ì™€ ê°™ì€ ë°°ì—´ì„ ë§Œë“¤ì–´ë¼.\ntensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n        [2, 2, 2, 2, 2, 2, 2, 2, 2, 2]]\n(í’€ì´?)\n\ntorch.stack([a,b])\n\ntensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n        [2, 2, 2, 2, 2, 2, 2, 2, 2, 2]])\n\n\n(ê´€ì°°?)\n\ntorch.stack([a,b], axis=1)\n\ntensor([[1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2]])\n\n\n(5) ì£¼ì–´ì§„ a,bì™€ torch.stackì„ ì´ìš©í•˜ì—¬ ì•„ë˜ì™€ ê°™ì€ ë°°ì—´ì„ ë§Œë“¤ì–´ë¼.\ntensor([[1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2]])\n(í’€ì´?)\n\ntorch.stack([a,b], axis=1)\n\ntensor([[1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2]])"
  },
  {
    "objectID": "posts/exercise1.html#í–‰ë ¬ê³±",
    "href": "posts/exercise1.html#í–‰ë ¬ê³±",
    "title": "A1: Exercise â€“ ver. 0503-1",
    "section": "$. í–‰ë ¬ê³±",
    "text": "$. í–‰ë ¬ê³±\n(1) ì•„ë˜ì™€ ê°™ì€ í…ì„œë¥¼ ê³ ë ¤í•˜ì.\n\na = torch.tensor([1,2,3,4,5]).reshape(-1,1)\nb = torch.tensor([3,2,1,1,2]).reshape(-1,1)\n\n@ ì—°ì‚°ìë¥¼ ì´ìš©í•˜ì—¬ \\(\\sum_{i=1}^{5}a_ib_i\\)ë¥¼ ê³„ì‚°í•˜ë¼.\n(í’€ì´)\n\na.T @ b\n\ntensor([[24]])\n\n\n(2) ì•„ë˜ì™€ ê°™ì€ í…ì„œë¥¼ ê³ ë ¤í•˜ì.\n\ntorch.manual_seed(0)\nx = torch.randn(100).reshape(-1,1)\n\n@ì—°ì‚°ìë¥¼ ì´ìš©í•˜ì—¬ \\(\\sum_{i=1}^{100}x_i^2\\)ì„ ê³„ì‚°í•˜ë¼.\n(í’€ì´?)\n\nx.T @ x\n\ntensor([[105.0856]])"
  },
  {
    "objectID": "posts/exercise1.html#ì¸ë±ì‹±",
    "href": "posts/exercise1.html#ì¸ë±ì‹±",
    "title": "A1: Exercise â€“ ver. 0503-1",
    "section": "$. ì¸ë±ì‹±",
    "text": "$. ì¸ë±ì‹±\nì•„ë˜ì™€ ê°™ì€ ë°°ì—´ì„ ì„ ì–¸í•˜ë¼.\n\ntorch.manual_seed(1)\nx = torch.randn(12).reshape(3,4)\nx\n\ntensor([[ 0.6614,  0.2669,  0.0617,  0.6213],\n        [-0.4519, -0.1661, -1.5228,  0.3817],\n        [-1.0276, -0.5631, -0.8923, -0.0583]])\n\n\n(1) 1ì—´ì„ ì¶”ì¶œí•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•˜ë¼. ì¦‰ ê²°ê³¼ê°€ ì•„ë˜ì™€ ê°™ì´ ë‚˜ì˜¤ë„ë¡ í•˜ë¼.\ntensor([[ 0.6614],\n        [-0.4519],\n        [-1.0276]])\n(í’€ì´?)\n\nx[:,0]\n\ntensor([ 0.6614, -0.4519, -1.0276])\n\n\n\nx[:,0].reshape(-1,1)\n\ntensor([[ 0.6614],\n        [-0.4519],\n        [-1.0276]])\n\n\n\nx[:,0].unsqueeze(1)\n\ntensor([[ 0.6614],\n        [-0.4519],\n        [-1.0276]])\n\n\n(2) 2-3ì—´ì„ ì¶”ì¶œí•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•˜ë¼. ì¦‰ ê²°ê³¼ê°€ ì•„ë˜ì™€ ê°™ì´ ë‚˜ì˜¤ë„ë¡ í•˜ë¼.\ntensor([[ 0.2669,  0.0617],\n        [-0.1661, -1.5228],\n        [-0.5631, -0.8923]])\n(í’€ì´?)\n\nx[:,1:3]\n\ntensor([[ 0.2669,  0.0617],\n        [-0.1661, -1.5228],\n        [-0.5631, -0.8923]])\n\n\n\nx[:,[1,2]]\n\ntensor([[ 0.2669,  0.0617],\n        [-0.1661, -1.5228],\n        [-0.5631, -0.8923]])\n\n\n(3) 2-3í–‰ì„ ì¶”ì¶œí•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•˜ë¼. ì¦‰ ê²°ê³¼ê°€ ì•„ë˜ì™€ ê°™ì´ ë‚˜ì˜¤ë„ë¡ í•˜ë¼.\ntensor([[-0.4519, -0.1661, -1.5228,  0.3817],\n        [-1.0276, -0.5631, -0.8923, -0.0583]])\n\n(í’€ì´?)\n\nx[1:,:]\n\ntensor([[-0.4519, -0.1661, -1.5228,  0.3817],\n        [-1.0276, -0.5631, -0.8923, -0.0583]])\n\n\n\nx[1:3,:]\n\ntensor([[-0.4519, -0.1661, -1.5228,  0.3817],\n        [-1.0276, -0.5631, -0.8923, -0.0583]])\n\n\n\nx[[1,2],:]\n\ntensor([[-0.4519, -0.1661, -1.5228,  0.3817],\n        [-1.0276, -0.5631, -0.8923, -0.0583]])\n\n\n(ê´€ì°°?)\n\nx[1:2,:]\n\ntensor([[-0.4519, -0.1661, -1.5228,  0.3817]])"
  },
  {
    "objectID": "posts/exercise1.html#torch.einsum",
    "href": "posts/exercise1.html#torch.einsum",
    "title": "A1: Exercise â€“ ver. 0503-1",
    "section": "$. torch.einsum",
    "text": "$. torch.einsum\n\nX = torch.randn(5,2)\nX\n\ntensor([[-0.1955, -0.9656],\n        [ 0.4224,  0.2673],\n        [-0.4212, -0.5107],\n        [-1.5727, -0.1232],\n        [ 3.5870, -1.8313]])\n\n\n(1) ì•„ë˜ì— ëŒ€ì‘í•˜ëŠ” ì½”ë“œë¥¼ torch.einsumìœ¼ë¡œ êµ¬í˜„í•˜ë¼.\n\nX.t()\n\ntensor([[-0.1955,  0.4224, -0.4212, -1.5727,  3.5870],\n        [-0.9656,  0.2673, -0.5107, -0.1232, -1.8313]])\n\n\n(í’€ì´?)\n\ntorch.einsum('ij-&gt;ji', X)\n\ntensor([[-0.1955,  0.4224, -0.4212, -1.5727,  3.5870],\n        [-0.9656,  0.2673, -0.5107, -0.1232, -1.8313]])"
  },
  {
    "objectID": "posts/exercise1.html#q1",
    "href": "posts/exercise1.html#q1",
    "title": "A1: Exercise â€“ ver. 0503-1",
    "section": "Q1",
    "text": "Q1\n\nunsqueezeë¥¼ ì“°ì§€ ì•Šê³  ì—´ ë²¡í„°ë¥¼ ë½‘ì•˜ì„ ë•Œ .shape ì°¨ì´ì ì€?\n\n\nì˜ˆì œ ë¹„êµ\nx[:, 0]           # unsqueeze ì—†ì´\nx[:, 0].unsqueeze(1)  # unsqueeze ì‚¬ìš©\n\n1. x[:, 0]\n\nê²°ê³¼ í…ì„œ shape: torch.Size([3])\nì„¤ëª…: 1ì—´ì˜ ê°’ì„ 1ì°¨ì› ë²¡í„°ë¡œ ë°˜í™˜í•œë‹¤. ì˜ˆ: [0.6614, -0.4519, -1.0276]\n\n\n\n2. x[:, 0].unsqueeze(1)\n\nê²°ê³¼ í…ì„œ shape: torch.Size([3, 1])\nì„¤ëª…: ì—´ ë°©í–¥ìœ¼ë¡œ 2ì°¨ì› í…ì„œë¡œ ë§Œë“ ë‹¤. ì˜ˆ: [[0.6614], [-0.4519], [-1.0276]]\n\n\n\n\nì°¨ì´ì  ìš”ì•½\n\n\n\n\n\n\n\n\ní•­ëª©\nx[:, 0]\nx[:, 0].unsqueeze(1)\n\n\n\n\nshape\n[3]\n[3, 1]\n\n\nì°¨ì› ìˆ˜\n1ì°¨ì› (ë²¡í„°)\n2ì°¨ì› (í–‰ë ¬)\n\n\në¸Œë¡œë“œìºìŠ¤íŒ…\nìë™ìœ¼ë¡œ í™•ì¥ ì–´ë ¤ì›€\në‹¤ë¥¸ í–‰ë ¬ê³¼ ê³§ë°”ë¡œ ì—°ì‚° ê°€ëŠ¥\n\n\n\n\nì–¸ì œ unsqueeze(1)ë¥¼ ì¨ì•¼ í•˜ë‚˜?\n\nëª¨ë¸ì— ì…ë ¥í•  ë•Œ (ì˜ˆ: [batch, features] í˜•íƒœ)\ní–‰ë ¬ ì—°ì‚°ì—ì„œ ì°¨ì› ì¼ì¹˜ë¥¼ ë§ì¶°ì•¼ í•  ë•Œ\nì‹œê°í™” ì‹œ ì—´ ë²¡í„° í˜•íƒœë¡œ ìœ ì§€í•˜ê³  ì‹¶ì„ ë•Œ"
  },
  {
    "objectID": "posts/exercise1.html#q2",
    "href": "posts/exercise1.html#q2",
    "title": "A1: Exercise â€“ ver. 0503-1",
    "section": "Q2",
    "text": "Q2\n\nx[:, 1:3] vs.Â x[:, [1, 2]] ì°¨ì´ì \n\n\n1. x[:, 1:3] â€” ìŠ¬ë¼ì´ì‹±\n\nì—°ì†ëœ êµ¬ê°„ì„ slice ê°ì²´ë¡œ ì¶”ì¶œ\nê²°ê³¼ëŠ” ì—°ì†ëœ ë©”ëª¨ë¦¬ ë¸”ë¡ì´ë¯€ë¡œ ë·°(view)ê°€ ë¨ â†’ ì›ë³¸ ë³€ê²½ ì‹œ ë°˜ì˜ ê°€ëŠ¥\në©”ëª¨ë¦¬ íš¨ìœ¨ì \nì¸ë±ìŠ¤ëŠ” start:endë¡œ, endëŠ” í¬í•¨í•˜ì§€ ì•ŠìŒ\n\nx[:, 1:3]  # ì—´ 1ë²ˆê³¼ 2ë²ˆ\n\n\n2. x[:, [1, 2]] â€” Fancy Indexing\n\në¦¬ìŠ¤íŠ¸ë¡œ ì§€ì •í•œ ì¸ë±ìŠ¤ë“¤ì„ ì§ì ‘ ì„ íƒ\në³µì‚¬ë³¸(copy)ì´ ìƒì„±ë˜ë¯€ë¡œ ì›ë³¸ ë³€ê²½ê³¼ ì—°ê²°ë˜ì§€ ì•ŠìŒ\nì—°ì†ëœ ì—´ë¿ ì•„ë‹ˆë¼ ì„ì˜ì˜ ìˆœì„œ, ì¤‘ë³µ, ë¶ˆì—°ì† ì¸ë±ìŠ¤ë„ ê°€ëŠ¥\n\nx[:, [1, 2]]       # ì—´ 1ë²ˆê³¼ 2ë²ˆ (ë³µì‚¬ë³¸)\nx[:, [2, 1, 2]]    # ì—´ ìˆœì„œ, ì¤‘ë³µë„ ê°€ëŠ¥\n\n\nì°¨ì´ ì˜ˆì‹œ\n\n\n\ní•­ëª©\nx[:, 1:3]\nx[:, [1, 2]]\n\n\n\n\nì—°ì†ëœ ì—´ë§Œ ì„ íƒ ê°€ëŠ¥\nâœ…\nâœ…\n\n\në¶ˆì—°ì† ì—´ ì„ íƒ\nâŒ\nâœ…\n\n\nì¤‘ë³µ ì—´ ì„ íƒ\nâŒ\nâœ… ([2, 1, 2] ë“±)\n\n\në°˜í™˜ê°’ì´ ë·°(view)?\nâœ…\nâŒ (copy)\n\n\në©”ëª¨ë¦¬ íš¨ìœ¨\në†’ìŒ\në‚®ìŒ (ë³µì‚¬ ë°œìƒ)"
  },
  {
    "objectID": "posts/exercise1.html#q3",
    "href": "posts/exercise1.html#q3",
    "title": "A1: Exercise â€“ ver. 0503-1",
    "section": "Q3",
    "text": "Q3\n\nx[1:3, :] vs.Â x[-2:, :] ê´€ê³„\n\n\në™ì¼í•œ ê²°ê³¼ë¥¼ ë§Œë“¦\në‘˜ ë‹¤ 2~3í–‰(ì¸ë±ìŠ¤ 1, 2)ì„ ì„ íƒí•˜ëŠ” ì½”ë“œë‹¤.\nx[1:3, :]    # í–‰ ì¸ë±ìŠ¤ 1~2\nx[-2:, :]    # ëì—ì„œ ë‘ ë²ˆì§¸ë¶€í„° ëê¹Œì§€ = ì¸ë±ìŠ¤ 1~2\n\n\n\ní‘œí˜„\nì˜ë¯¸\ní–‰ ì¶”ì¶œ ê²°ê³¼\n\n\n\n\nx[1:3, :]\n1ë²ˆì§¸ë¶€í„° 2ë²ˆì§¸ê¹Œì§€ ìŠ¬ë¼ì´ì‹±\n[1, 2]í–‰\n\n\nx[-2:, :]\në’¤ì—ì„œ 2ê°œ ìŠ¬ë¼ì´ì‹±\n[1, 2]í–‰\n\n\n\n\n\nì°¨ì´ì \n\nx[1:3, :]ì€ ì •ë°©í–¥ ì¸ë±ìŠ¤ë¡œ ìœ„ì¹˜ë¥¼ ê¸°ì¤€\nx[-2:, :]ì€ ì—­ë°©í–¥ ì¸ë±ìŠ¤ë¡œ ë ê¸°ì¤€ â†’ ê°€ë³€ ê¸¸ì´ í…ì„œì—ì„œ ìœ ìš©"
  },
  {
    "objectID": "posts/exercise1.html#q2-q3-ì •ë¦¬-í¬ì¸íŠ¸",
    "href": "posts/exercise1.html#q2-q3-ì •ë¦¬-í¬ì¸íŠ¸",
    "title": "A1: Exercise â€“ ver. 0503-1",
    "section": "Q2 Q3 ì •ë¦¬ í¬ì¸íŠ¸",
    "text": "Q2 Q3 ì •ë¦¬ í¬ì¸íŠ¸\n\n\n\n\n\n\n\në¹„êµ ëŒ€ìƒ\nì°¨ì´ì  ìš”ì•½\n\n\n\n\n[:, 1:3] vs [:, [1, 2]]\nì „ìëŠ” ì—°ì† êµ¬ê°„ ìŠ¬ë¼ì´ì‹±, í›„ìëŠ” ì§€ì •ëœ ì—´ ë³µì‚¬\n\n\n[1:3, :] vs [-2:, :]\nê²°êµ­ ê°™ì€ í–‰ ì¶”ì¶œ, ì ‘ê·¼ ë°©ì‹(ì• vs.Â ë’¤ ê¸°ì¤€)ë§Œ ë‹¤ë¦„"
  },
  {
    "objectID": "posts/exercise1.html#q4",
    "href": "posts/exercise1.html#q4",
    "title": "A1: Exercise â€“ ver. 0503-1",
    "section": "Q4",
    "text": "Q4\n\nunsqueezeì— ëŒ€í•´ ì„¤ëª…\n\n\nunsqueezeëŠ” í…ì„œ ì°¨ì›ì„ ì¸ìœ„ì ìœ¼ë¡œ ì¶”ê°€í•˜ëŠ” í•¨ìˆ˜ë¡œ, PyTorchì—ì„œ ë°ì´í„°ë¥¼ ë‹¤ë£° ë•Œ ë§¤ìš° ìì£¼ ì“°ì¸ë‹¤.\nì°¨ì›ì´ ë§ì§€ ì•Šì•„ ì˜¤ë¥˜ê°€ ë°œìƒí•  ë•Œ, unsqueezeë¥¼ ì´ìš©í•˜ë©´ ì´ë¥¼ ê°„ë‹¨í•˜ê²Œ í•´ê²°í•  ìˆ˜ ìˆë‹¤.\n\n\ntorch.unsqueeze(tensor, dim) ë˜ëŠ” tensor.unsqueeze(dim)\n\nì—­í• :\n\nì§€ì •í•œ ìœ„ì¹˜ dimì— í¬ê¸° 1ì§œë¦¬ ì°¨ì›ì„ ì¶”ê°€í•œë‹¤.\nê²°ê³¼ í…ì„œì˜ shapeì€ ì›ë˜ë³´ë‹¤ ì°¨ì›ì´ 1 ì¦ê°€í•œë‹¤.\n\n\n\nì˜ˆì‹œ 1: ë²¡í„° â†’ ì—´ ë²¡í„°\nx = torch.tensor([1, 2, 3])       # shape: [3]\nx_unsq = x.unsqueeze(1)           # shape: [3, 1]\n\n# ê²°ê³¼:\n# tensor([[1],\n#         [2],\n#         [3]])\n\n\nì˜ˆì‹œ 2: ë²¡í„° â†’ ë°°ì¹˜ í…ì„œ\nx = torch.tensor([1, 2, 3])       # shape: [3]\nx_unsq = x.unsqueeze(0)           # shape: [1, 3]\n\n# ê²°ê³¼:\n# tensor([[1, 2, 3]])\n\n\nShape ë³€í™” ì •ë¦¬:\n\n\n\nì›ë˜ shape\nunsqueeze(0)\nunsqueeze(1)\nunsqueeze(-1)\n\n\n\n\n[3]\n[1, 3]\n[3, 1]\n[3, 1]\n\n\n[3, 4]\n[1, 3, 4]\n[3, 1, 4]\n[3, 4, 1]\n\n\n\n\n\n\nì£¼ë¡œ ì‚¬ìš©í•˜ëŠ” ìƒí™©\n\n1. ëª¨ë¸ ì…ë ¥ ì°¨ì› ë§ì¶”ê¸°\nx = torch.tensor([1.0, 2.0, 3.0])  # shape [3]\nx = x.unsqueeze(0)  # shape [1, 3] â†’ batch ì°¨ì› ì¶”ê°€\n\n\n2. ë¸Œë¡œë“œìºìŠ¤íŒ… ìš©ë„\na = torch.tensor([1, 2, 3])         # shape: [3]\nb = a.unsqueeze(1)                  # shape: [3, 1]\nâ†’ ì´í›„ ì—°ì‚° ì‹œ ìë™ìœ¼ë¡œ [3, 1]ê³¼ [3, 4] ê°™ì€ í…ì„œê°€ ë§ì¶°ì§\n\n\n\nì£¼ì˜\n\nunsqueezeëŠ” ì‹¤ì œ ë°ì´í„°ë¥¼ ë°”ê¾¸ì§€ ì•Šê³  shapeë§Œ ë³€ê²½í•œë‹¤.\nì›ë˜ì˜ í…ì„œì™€ ë™ì¼í•œ ë©”ëª¨ë¦¬ ì°¸ì¡°ë¥¼ ê°€ì§ˆ ìˆ˜ë„ ìˆìŒ (ì¦‰, viewì¼ ìˆ˜ ìˆìŒ)\n\n\n\nê´€ë ¨ í•¨ìˆ˜\n\n\n\ní•¨ìˆ˜\nê¸°ëŠ¥\n\n\n\n\nunsqueeze\nì°¨ì› ì¶”ê°€\n\n\nsqueeze\ní¬ê¸° 1ì¸ ì°¨ì› ì œê±°\n\n\nreshape\nì „ì²´ ì°¨ì›ì„ ì›í•˜ëŠ” í˜•íƒœë¡œ ì¬êµ¬ì„±\n\n\nview\nreshapeê³¼ ê±°ì˜ ë™ì¼, ë©”ëª¨ë¦¬ ì—°ì† ìš”êµ¬\n\n\n\n\ntorch.unsqueeze?\n\n\nDocstring:\nunsqueeze(input, dim) -&gt; Tensor\nReturns a new tensor with a dimension of size one inserted at the\nspecified position.\nThe returned tensor shares the same underlying data with this tensor.\nA :attr:`dim` value within the range ``[-input.dim() - 1, input.dim() + 1)``\ncan be used. Negative :attr:`dim` will correspond to :meth:`unsqueeze`\napplied at :attr:`dim` = ``dim + input.dim() + 1``.\nArgs:\n    input (Tensor): the input tensor.\n    dim (int): the index at which to insert the singleton dimension\nExample::\n    &gt;&gt;&gt; x = torch.tensor([1, 2, 3, 4])\n    &gt;&gt;&gt; torch.unsqueeze(x, 0)\n    tensor([[ 1,  2,  3,  4]])\n    &gt;&gt;&gt; torch.unsqueeze(x, 1)\n    tensor([[ 1],\n            [ 2],\n            [ 3],\n            [ 4]])\nType:      builtin_function_or_method"
  },
  {
    "objectID": "posts/exercise1.html#q5",
    "href": "posts/exercise1.html#q5",
    "title": "A1: Exercise â€“ ver. 0503-1",
    "section": "Q5",
    "text": "Q5\n\ntorch.einsumì— ëŒ€í•œ ì„¤ëª…\n\n\ntorch.einsumì€ Einstein summation (ì•„ì¸ìŠˆíƒ€ì¸ í‘œê¸°ë²•)ì„ ì‚¬ìš©í•´ í…ì„œ ì—°ì‚°ì„ ê°„ê²°í•˜ê³  ì§ê´€ì ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆëŠ” ê°•ë ¥í•œ ë„êµ¬\n\n\ntorch.einsumì´ë€?\n\neinsumì€ â€œì–´ë–¤ ì°¨ì›ì„ ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°í•˜ê³ , ì–´ë–¤ ì°¨ì›ì„ ë‚¨ê¸¸ì§€â€ë¥¼ ì§ì ‘ ì§€ì •í•˜ëŠ” ë°©ì‹\në¶ˆí•„ìš”í•œ reshape, transpose, sum ë“±ì„ ë”°ë¡œ í•˜ì§€ ì•Šì•„ë„ í•œ ì¤„ë¡œ ì—°ì‚° ê°€ëŠ¥\n\n\n\nê¸°ë³¸ ë¬¸ë²•\ntorch.einsum('input_indices-&gt;output_indices', operands)\nì˜ˆ:\ntorch.einsum('ij-&gt;ji', X)       # ì „ì¹˜ transpose\ntorch.einsum('ij,jk-&gt;ik', A, B) # í–‰ë ¬ ê³± A @ B\n\n\nê¸°ë³¸ ì˜ˆì œ ëª¨ìŒ\n\nì „ì¹˜: X.t()\ntorch.einsum('ij-&gt;ji', X)\n\n\ní–‰ë ¬ ê³±: A @ B\ntorch.einsum('ik,kj-&gt;ij', A, B)\nì„¤ëª…:\n\ni: Aì˜ í–‰ ì¸ë±ìŠ¤\nk: Aì˜ ì—´ = Bì˜ í–‰ (sum ëŒ€ìƒ)\nj: Bì˜ ì—´\n\n\n\në²¡í„° ë‚´ì : torch.dot(a, b)\ntorch.einsum('i,i-&gt;', a, b)  # ê²°ê³¼: ìŠ¤ì¹¼ë¼\n\n\në°°ì¹˜ í–‰ë ¬ ê³±: torch.bmm(A, B)\ntorch.einsum('bij,bjk-&gt;bik', A, B)\n\n\nì—´ë³„ í•©ê³„: torch.sum(X, dim=0)\ntorch.einsum('ij-&gt;j', X)\n\n\n\nì¥ì  ìš”ì•½\n\n\n\n\n\n\n\nì¥ì \nì„¤ëª…\n\n\n\n\nì§ê´€ì ì¸ ì¸ë±ìŠ¤ ì§€ì •\nì›í•˜ëŠ” ì°¨ì›ì„ ë‚¨ê¸°ê±°ë‚˜ ë”í•  ìˆ˜ ìˆìŒ\n\n\në³µì¡í•œ ì—°ì‚° ë‹¨ìˆœí™”\nmatmul, sum, permute, reshape ë“±ì„ í•œ ì¤„ë¡œ ì²˜ë¦¬\n\n\në©”ëª¨ë¦¬ íš¨ìœ¨ì \në¶ˆí•„ìš”í•œ ì¤‘ê°„ í…ì„œ ìƒì„± ì—†ìŒ\n\n\n\n\n\nì‹œê°ì ìœ¼ë¡œ ì´í•´\n# ì˜ˆ: í–‰ë ¬ ê³± A @ B\n\nA.shape = (2, 3)     =&gt; 'ik'\nB.shape = (3, 4)     =&gt; 'kj'\nê²°ê³¼: torch.einsum('ik,kj-&gt;ij', A, B)  â†’ shape (2, 4)\n\n\nìœ ì˜ì‚¬í•­\n\n'ij-&gt;ji'ì²˜ëŸ¼ í™”ì‚´í‘œ(-&gt;)ëŠ” â€œì´ ì¸ë±ìŠ¤ë“¤ì„ ì–´ë–»ê²Œ ì¬êµ¬ì„±í•  ê²ƒì¸ì§€â€ë¥¼ ëœ»í•¨\nê°™ì€ ë¬¸ìê°€ ë‘ ë²ˆ ì´ìƒ ë“±ì¥í•˜ë©´ ê·¸ ì°¨ì›ì„ summation (ì¶•ì†Œ) ëŒ€ìƒìœ¼ë¡œ ë´„\nì—†ëŠ” ì°¨ì›ì— ëŒ€í•œ ë¬¸ìëŠ” ì˜¤ë¥˜ë¥¼ ë°œìƒì‹œí‚´\n\n\n# torch.einsum?"
  },
  {
    "objectID": "posts/exercise1.html#q6",
    "href": "posts/exercise1.html#q6",
    "title": "A1: Exercise â€“ ver. 0503-1",
    "section": "Q6",
    "text": "Q6\n\neinsumì„ ì‚¬ìš©í•  ë•Œ, ë¸Œë¡œë“œìºìŠ¤íŒ…ê³¼ ì°¨ì› ì¶•ì†ŒëŠ” ì–´ë–»ê²Œ ë™ì‘?\n\n\nì´ê±´ einsumì˜ í•µì‹¬ ì‘ë™ ì›ë¦¬ì´ì í° ê°•ì  ì¤‘ í•˜ë‚˜\n\n\nì°¨ì› ì¶•ì†Œ (Reduction)\neinsumì—ì„œ ê°™ì€ ë¬¸ìê°€ ë‘ ë²ˆ ë“±ì¥í•˜ë©´, ê·¸ ì°¨ì›ì€ sum (í•©ì‚°) ì²˜ë¦¬ë¨\n\nì˜ˆì‹œ: ë²¡í„° ë‚´ì \na = torch.tensor([1.0, 2.0, 3.0])\nb = torch.tensor([4.0, 5.0, 6.0])\ntorch.einsum('i,i-&gt;', a, b)\n\n'i,i-&gt;'ëŠ” ië¼ë¦¬ ê³±í•˜ê³  â†’ ëª¨ë“  iì— ëŒ€í•´ í•©.\nê²°ê³¼: 1Ã—4 + 2Ã—5 + 3Ã—6 = 32\n\n=&gt; ê²°ê³¼ëŠ” ìŠ¤ì¹¼ë¼ (0ì°¨ì› í…ì„œ)\n\n\n\në¸Œë¡œë“œìºìŠ¤íŒ… (Broadcasting)\n\neinsumì€ ìë™ ë¸Œë¡œë“œìºìŠ¤íŒ…ì„ ì§€ì›í•˜ì§€ ì•ŠìŒ\ní•˜ì§€ë§Œ ì¸ë±ìŠ¤ ì°¨ì›ì„ ëª…ì‹œí•˜ë©´, ëª…ì‹œì  ë¸Œë¡œë“œìºìŠ¤íŒ…ì²˜ëŸ¼ ì‘ë™ì‹œí‚¬ ìˆ˜ ìˆìŒ\n\n\nì˜ˆì‹œ: ì™¸ì  (outer product)\na = torch.tensor([1.0, 2.0, 3.0])   # shape [3]\nb = torch.tensor([4.0, 5.0])        # shape [2]\n\ntorch.einsum('i,j-&gt;ij', a, b)\n\nê²°ê³¼: shape [3, 2]\niëŠ” aì˜ ì°¨ì›, jëŠ” bì˜ ì°¨ì› â†’ ê°ê° ë¸Œë¡œë“œìºìŠ¤íŠ¸í•´ì„œ ê³±í•¨\n\n[[1*4, 1*5],\n [2*4, 2*5],\n [3*4, 3*5]]\n\n\n\nì˜ˆì œ í•˜ë‚˜ë¡œ ì¶•ì†Œ vs ë¸Œë¡œë“œìºìŠ¤íŒ… ë¹„êµ\nX = torch.randn(3, 4)\n\n# ì—´ë³„ í•©ê³„ (ì¶•ì†Œ)\ntorch.einsum('ij-&gt;j', X)   # shape: [4]\n\n# í–‰ë³„ í•©ê³„ (ì¶•ì†Œ)\ntorch.einsum('ij-&gt;i', X)   # shape: [3]\n\n# ìê¸° ìì‹ ê³¼ì˜ ì™¸ì  (ë¸Œë¡œë“œìºìŠ¤íŒ…)\ntorch.einsum('i,j-&gt;ij', X[0], X[1])\n\n\ní•µì‹¬ ìš”ì•½í‘œ\n\n\n\ní‘œí˜„\nì˜ë¯¸\n\n\n\n\n'ij-&gt;j'\ni ì°¨ì›ì„ sum â†’ ì—´ í•©ê³„\n\n\n'i,i-&gt;'\ni ì°¨ì›ì„ sum â†’ ë‚´ì  (ìŠ¤ì¹¼ë¼)\n\n\n'i,j-&gt;ij'\në¸Œë¡œë“œìºìŠ¤íŠ¸ ê³± â†’ ì™¸ì  (Outer Product)\n\n\n\n\n\nì •ë¦¬\n\neinsumì—ì„œ ê°™ì€ ë¬¸ìê°€ ë‘ ë²ˆ ë“±ì¥í•˜ë©´ â†’ ì¶•ì†Œ(sum)\në‹¤ë¥¸ ë¬¸ìë¼ë¦¬ëŠ” â†’ ë¸Œë¡œë“œìºìŠ¤íŠ¸\në¸Œë¡œë“œìºìŠ¤íŠ¸ëŠ” ìë™ì´ ì•„ë‹ˆë¼, ëª…ì‹œëœ ì¸ë±ìŠ¤ë¡œ ì˜ë„ì ìœ¼ë¡œ ì„¤ê³„"
  },
  {
    "objectID": "posts/exercise1.html#ì¶”ê°€-ì§ˆë¬¸ë“¤",
    "href": "posts/exercise1.html#ì¶”ê°€-ì§ˆë¬¸ë“¤",
    "title": "A1: Exercise â€“ ver. 0503-1",
    "section": "ì¶”ê°€ ì§ˆë¬¸ë“¤",
    "text": "ì¶”ê°€ ì§ˆë¬¸ë“¤\n\neinsum('bij,bjk-&gt;bik')ì€ ì–´ë–¤ ì°¨ì› ì—°ì‚°ì„ ìˆ˜í–‰í•˜ëŠ”ê°€? ì§ì ‘ shape ì˜ˆì‹œë¡œ ì„¤ëª…í•´ë´.\neinsumì„ ì‚¬ìš©í•´ MSE (Mean Squared Error)ë¥¼ í•œ ì¤„ë¡œ ê³„ì‚°í•˜ë ¤ë©´ ì–´ë–¤ ì‹ì´ ë ê¹Œ?\neinsumì˜ í‘œí˜„ë ¥ì„ ì´ìš©í•´ CNN í•„í„° ì ìš© ì „ dot productë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì‘ì„±í•´ë³¼ ìˆ˜ ìˆì„ê¹Œ?"
  },
  {
    "objectID": "posts/03wk-2.html",
    "href": "posts/03wk-2.html",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "",
    "text": "ğŸ“˜ Note Format Guide\nThis format serves as a structured guide for organizing lecture content, personal interpretation, experiments, and study-related questions.\nğŸ“ ğŸ—£ï¸ âœï¸ ğŸ”¬ â“"
  },
  {
    "objectID": "posts/03wk-2.html#a.-ë¡œì§€ìŠ¤í‹±-ëª¨í˜•",
    "href": "posts/03wk-2.html#a.-ë¡œì§€ìŠ¤í‹±-ëª¨í˜•",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "A. ë¡œì§€ìŠ¤í‹± ëª¨í˜•",
    "text": "A. ë¡œì§€ìŠ¤í‹± ëª¨í˜•\n- \\(x\\)ê°€ ì»¤ì§ˆìˆ˜ë¡ (í˜¹ì€ ì‘ì•„ì§ˆìˆ˜ë¡) \\(y=1\\)ì´ ì˜ë‚˜ì˜¤ëŠ” ëª¨í˜•ì€ ì•„ë˜ì™€ ê°™ì´ ì„¤ê³„í•  ìˆ˜ ìˆìŒ &lt;â€” ì™¸ìš°ì„¸ìš”!!!\n\n\\(y_i \\sim {\\cal B}(\\pi_i),\\quad\\) where \\(\\pi_i = \\frac{\\exp(w_0+w_1x_i)}{1+\\exp(w_0+w_1x_i)} = \\frac{1}{1+\\exp(-w_0-w_1x_i)}\\)\n\\(\\hat{y}_i= \\frac{\\exp(\\hat{w}_0+\\hat{w}_1x_i)}{1+\\exp(\\hat{w}_0+\\hat{w}_1x_i)}=\\frac{1}{1+\\exp(-\\hat{w}_0-\\hat{w}_1x_i)}\\)\n\n- íšŒê·€ëª¨í˜•ê³¼ ë¡œì§€ìŠ¤í‹± ëª¨í˜•ì˜ ë¹„êµ\n\níšŒê·€ëª¨í˜•: \\(y_i \\sim {\\cal N}(w_0+w_1x_i, \\sigma^2)\\)1\në¡œì§€ìŠ¤í‹±: \\(y_i \\sim {\\cal B}\\big(\\frac{\\exp(w_0+w_1x_i)}{1+\\exp(w_0+w_1x_i)}\\big)\\)\n\n- ìš°ë¦¬ê°€ ì˜ˆì¸¡í•˜ê³  ì‹¶ì€ê²ƒ\n\níšŒê·€ëª¨í˜•: ì •ê·œë¶„í¬ì˜ í‰ê· ì„ ì˜ˆì¸¡í•˜ê³  ì‹¶ìŒ. ì¦‰ \\(w_0+w_1x_i\\)ë¥¼ ì˜ˆì¸¡í•˜ê³  ì‹¶ìŒ. ì˜ˆì¸¡ê°’ìœ¼ë¡œëŠ” \\(\\hat{w}_0 + \\hat{w}_1x_i\\)ë¥¼ ì‚¬ìš©!\në¡œì§€ìŠ¤í‹±: ë² ë¥´ëˆ„ì´ì˜ í‰ê· ì„ ì˜ˆì¸¡í•˜ê³  ì‹¶ìŒ. ì¦‰ \\(\\frac{\\exp(w_0+w_1x_i)}{1+\\exp(w_0+w_1x_i)}\\)ë¥¼ ì˜ˆì¸¡í•˜ê³  ì‹¶ìŒ. ì˜ˆì¸¡ê°’ìœ¼ë¡œëŠ” \\(\\frac{\\exp(\\hat{w}_0+\\hat{w}_1x_i)}{1+\\exp(\\hat{w}_0+\\hat{w}_1x_i)}\\)ë¥¼ ì‚¬ìš©!"
  },
  {
    "objectID": "posts/03wk-2.html#b.-ë°ì´í„°-ìŠ¤í™ê³¼-ì·¨ì—…",
    "href": "posts/03wk-2.html#b.-ë°ì´í„°-ìŠ¤í™ê³¼-ì·¨ì—…",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "B. ë°ì´í„° â€“ ìŠ¤í™ê³¼ ì·¨ì—…",
    "text": "B. ë°ì´í„° â€“ ìŠ¤í™ê³¼ ì·¨ì—…\nğŸ—£ï¸(\n\në°ì´í„° ë§Œë“¤ê¸°\n\n\ntorch.linspace(-1,1,2000)\n\ntensor([-1.0000, -0.9990, -0.9980,  ...,  0.9980,  0.9990,  1.0000])\n\n\n\nlen(torch.linspace(-1,1,2000))\n\n2000\n\n\n\nx = torch.linspace(-1,1,2000).reshape(2000,1)\nx\n\ntensor([[-1.0000],\n        [-0.9990],\n        [-0.9980],\n        ...,\n        [ 0.9980],\n        [ 0.9990],\n        [ 1.0000]])\n\n\n\nì´ ìƒíƒœì—ì„œ ì„ í˜• ë³€í™˜ì„ í•œë‹¤ë©´\n\n-1 + x*5 : ì„ í˜• ëª¨ë¸\në¡œì§€ìŠ¤í‹± ëª¨í˜•ì€\n\n\n\nx = torch.linspace(-1,1,2000).reshape(2000,1)\nprob = torch.exp(-1 + x*5) / (1+ torch.exp(-1 + x*5))\nplt.plot(x,prob)\n\n\n\n\n\n\n\n\n\në‹¤ë¥¸ ë°©ë²• (ë³´ê¸° ì¢‹ê²Œ)\n\n\nx = torch.linspace(-1,1,2000).reshape(2000,1)\nw0, w1 = -1, 5\nprob = torch.exp(w0 + x*w1) / (1+ torch.exp(w0 + x*w1))\nplt.plot(x,prob)\n\n\n\n\n\n\n\n\n\nprob\n\ntensor([[0.0025],\n        [0.0025],\n        [0.0025],\n        ...,\n        [0.9818],\n        [0.9819],\n        [0.9820]])\n\n\n\nprob.shape\n\ntorch.Size([2000, 1])\n\n\n\ny ë§Œë“¤ê¸° (probë¡œ ë² ë¥´ëˆ„ì´ ì‹œí–‰)\n\n\ntorch.bernoulli(prob)\n\ntensor([[0.],\n        [0.],\n        [0.],\n        ...,\n        [1.],\n        [1.],\n        [1.]])\n\n\n\ntorch.bernoulli(prob).shape\n\ntorch.Size([2000, 1])\n\n\n\nseed ê³ ì • í›„ ì‹œê°í™”\n\n\ntorch.manual_seed(43052)\nx = torch.linspace(-1,1,2000).reshape(2000,1)\nw0, w1 = -1, 5\nprob = torch.exp(w0 + x*w1) / (1+ torch.exp(w0 + x*w1))\ny = torch.bernoulli(prob)\n\n\nplt.plot(x,y) # ë³´ê¸° ì‰½ì§€ëŠ” ì•ŠìŒ \n\n\n\n\n\n\n\n\n\nplt.plot(x,y, 'o') # ì ë“¤ì´ ë„ˆë¬´ ë§ì´ ê²¹ì¹¨\n\n\n\n\n\n\n\n\n\nplt.plot(x,y,'.',alpha=0.03) # íˆ¬ëª…ë„ ì¡°ì ˆ\n\n\n\n\n\n\n\n\n\nxê°€ ì¦ê°€í• ìˆ˜ë¡ yëŠ” 1ì´ ë‚˜ì˜¬ ê°€ëŠ¥ì„±ì´ ë†’ì•„ì§€ê³ \nxê°€ ê°ì†Œí• ìˆ˜ë¡ yëŠ” 0ì´ ë‚˜ì˜¬ ê°€ëŠ¥ì„±ì´ ë†’ì•„ì§\n\n\nplt.plot(x,y,'.',alpha=0.03) # ê´€ì¸¡(error í¬í•¨)\nplt.plot(x,prob,'--') # ì‹¤ì²´ ë°ì´í„°ì—ì„œëŠ” ê´€ì¸¡ ë¶ˆê°€ëŠ¥ (error-free structure)\n\n\n\n\n\n\n\n\n)ğŸ—£ï¸\n\ntorch.manual_seed(43052)\nx = torch.linspace(-1,1,2000).reshape(2000,1)\nw0,w1 = -1, 5\nprob = torch.exp(w0+w1*x) / (1+torch.exp(w0+w1*x)) \ny = torch.bernoulli(prob)\n\n\nplt.plot(x,y,'.',alpha=0.03)\nplt.plot(x[0],y[0],'.',label=r\"$(x_i,y_i)$\",color=\"C0\")\nplt.plot(x,prob,'--r',label=r\"prob (true, unknown) = $\\frac{exp(-1+5x)}{1+exp(-1+5x)}$\")\nplt.legend()\n\n\n\n\n\n\n\n\n\nğŸ—£ï¸\n\nprob: í™•ë¥ \níŒŒë€ìƒ‰ ì : ê´€ì¸¡ê°’\nëª©í‘œ: ë¹¨ê°„ìƒ‰ ì„  ì˜ ë§ì¶”ê¸°\n\në°©ë²•: ìµœì´ˆì˜ ê³¡ì„ ì„ ê·¸ë¦¬ê³  update"
  },
  {
    "objectID": "posts/03wk-2.html#c.-step1-net-ì„¤ê³„-ëª¨ë¸ë§",
    "href": "posts/03wk-2.html#c.-step1-net-ì„¤ê³„-ëª¨ë¸ë§",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "C. Step1: net ì„¤ê³„ (ëª¨ë¸ë§)",
    "text": "C. Step1: net ì„¤ê³„ (ëª¨ë¸ë§)\n- ìµœì´ˆì˜ ê³¡ì„ ì„ ê·¸ë ¤ë³´ì.\n\nìµœì´ˆì˜ì§ì„ : \\(\\hat{y}_i= \\hat{w}_0+\\hat{w}_1x_i\\) ì—ì„œ ì•„ë¬´ \\(\\hat{w}_0\\), \\(\\hat{w}_1\\) ì„ ì„¤ì •í•˜ë©´ ëœë‹¤.\nìµœì´ˆì˜ê³¡ì„ : \\(\\hat{y}_i= \\frac{\\exp(\\hat{w}_0+\\hat{w}_1x_i)}{1+\\exp(\\hat{w}_0+\\hat{w}_1x_i)}=\\frac{1}{1+\\exp(-\\hat{w}_0-\\hat{w}_1x_i)}\\) ì—ì„œ ì•„ë¬´ \\(\\hat{w}_0\\), \\(\\hat{w}_1\\) ì„ ì„¤ì •í•˜ë©´ ëœë‹¤.\n\n\n\n\n\n\n\nNote\n\n\n\nì¼ë‹¨ì€ ì´ˆê¸° ì„¤ì •ê°’ì„ \\(\\hat{w}_0 = -0.8\\), \\(\\hat{w}_1 = -0.3\\) ìœ¼ë¡œ í•˜ì. (ì‹¤ì œê°’ì€ \\(w_0=-1\\), \\(w_1=5\\) ì´ë‹¤)\n\n\n# ë°©ë²•1 â€“ l1, sigmoid\nğŸ—£ï¸(\n\nw0hat = -4\nw1hat = 10\nyhat = torch.exp(w0hat + w1hat*x) / (1+ torch.exp(w0hat + w1hat*x))\n\n\nyhat\n\ntensor([[8.3153e-07],\n        [8.3989e-07],\n        [8.4833e-07],\n        ...,\n        [9.9748e-01],\n        [9.9750e-01],\n        [9.9753e-01]])\n\n\n\nplt.plot(x,y,'.',alpha=0.03)\nplt.plot(x, yhat, '--')\n\n\n\n\n\n\n\n\n\nyhatì„ ë‹¤ìŒê³¼ ê°™ì´ í•  ìˆ˜ë„ ìˆìŒ\n\n\nlinr = torch.nn.Linear(1,1)\n# linr(x)\n\n\ndef sigmoid(x):\n    return torch.exp(x) / (1+ torch.exp(x)) #  í¸ì˜ìƒ linr(x) ëŒ€ì‹  xë¡œ ì‘ì„±\n\n\nlinr(x)\n\ntensor([[ 0.6311],\n        [ 0.6304],\n        [ 0.6297],\n        ...,\n        [-0.6902],\n        [-0.6909],\n        [-0.6916]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\n\nyhat = sigmoid(linr(x))\nplt.plot(x, yhat.data) # ê³¡ì„  ì¤‘ ì¼ë¶€ë§Œ ê·¸ë ¤ì ¸ ì§ì„ ì²˜ëŸ¼ ë³´ì„\n\n\n\n\n\n\n\n\n\nlinr(x)ê°€ ê³„ì‚°ë˜ëŠ” ê³¼ì •\n\n\nlinr.weight, linr.bias\n\n(Parameter containing:\n tensor([[-0.6613]], requires_grad=True),\n Parameter containing:\n tensor([-0.0303], requires_grad=True))\n\n\n\n-0.6613*x + -0.0303\n\ntensor([[ 0.6310],\n        [ 0.6303],\n        [ 0.6297],\n        ...,\n        [-0.6903],\n        [-0.6909],\n        [-0.6916]])\n\n\n\nê°’ì„ ì•„ê¹Œì²˜ëŸ¼ ì§€ì •í•´ì£¼ë©´\n\n\nlinr.weight.data = torch.tensor([[10.0]])\nlinr.bias.data = torch.tensor([-4.0])\n\nâ“ biasëŠ” [[-4.0]]ì´ ì•„ë‹ˆë¼ [-4.0]\nğŸ”¬(\n\nlinr.weight.data = torch.tensor([[10.0]])\nlinr.bias.data = torch.tensor([[-4.0]])\n\n\nlinr(x)\n\ntensor([[-14.0000],\n        [-13.9900],\n        [-13.9800],\n        ...,\n        [  5.9800],\n        [  5.9900],\n        [  6.0000]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\nâ“ ìƒê´€ì—†ëŠ”ë“¯?\n\nlinr.weight.data = torch.tensor([10.0])\nlinr.bias.data = torch.tensor([[-4.0]])\n\n\n# linr(x) # error: RuntimeError: mat2 must be a matrix, got 1-D tensor\n\n)ğŸ”¬\nğŸ”¬ ì°¸ê³ ) -4.0ì´ ì•„ë‹ˆë¼ -4ë¥¼ ì“°ë©´ error\n\nlinr.weight.data = torch.tensor([[10.0]])\nlinr.bias.data = torch.tensor([-4.0])\n\n\nlinr(x)\n\ntensor([[-14.0000],\n        [-13.9900],\n        [-13.9800],\n        ...,\n        [  5.9800],\n        [  5.9900],\n        [  6.0000]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\n\nw0hat + w1hat*x # ìœ„ì™€ ë™ì¼\n\ntensor([[-14.0000],\n        [-13.9900],\n        [-13.9800],\n        ...,\n        [  5.9800],\n        [  5.9900],\n        [  6.0000]])\n\n\n\n-4*x + 10 # ì´ê²ƒë„ ë™ì¼\n\ntensor([[14.0000],\n        [13.9960],\n        [13.9920],\n        ...,\n        [ 6.0080],\n        [ 6.0040],\n        [ 6.0000]])\n\n\n\në‹¤ì‹œ ì •ë¦¬í•˜ë©´\n\n\ndef sigmoid(x):\n    return torch.exp(x) / (1+ torch.exp(x)) #  í¸ì˜ìƒ linr(x) ëŒ€ì‹  xë¡œ ì‘ì„±\n\n\nl1 = torch.nn.Linear(1,1)\nl1.weight.data = torch.tensor([[10.0]])\nl1.bias.data = torch.tensor([-4.0])\n#yhat = torch.exp(l1(x)) / (1+ torch.exp(l1(x)))\nyhat = sigmoid(l1(x))\n\n\nplt.plot(x,yhat.data)\n\n\n\n\n\n\n\n\n\nê°’ì„ ë°”ê¾¸ê³  ì‹¶ìœ¼ë©´\n\n\nl1 = torch.nn.Linear(1,1)\nl1.weight.data = torch.tensor([[-0.3]])\nl1.bias.data = torch.tensor([-0.8])\n#yhat = torch.exp(l1(x)) / (1+ torch.exp(l1(x)))\nyhat = sigmoid(l1(x))\n\n\nplt.plot(x,yhat.data)\n\n\n\n\n\n\n\n\n\nplt.plot(x,y,'.',alpha=0.03)\nplt.plot(x, yhat.data, '--')\n\n\n\n\n\n\n\n\n)ğŸ—£ï¸\n\nl1 = torch.nn.Linear(1,1)\nl1(x) # w0hat + w1hat*x \n\ntensor([[ 0.4735],\n        [ 0.4728],\n        [ 0.4721],\n        ...,\n        [-0.9890],\n        [-0.9897],\n        [-0.9905]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\nğŸ—£ï¸ ì‹¤í–‰í•  ë•Œë§ˆë‹¤ ë‹¬ë¼ì§€ë¯€ë¡œ ì•„ë˜ì™€ ê°™ì´ ê³ ì •\n\nl1.weight.data = torch.tensor([[-0.3]])\nl1.bias.data = torch.tensor([-0.8])\n\n\ndef sigmoid(x):\n    return torch.exp(x)/(1+torch.exp(x))\n\n\nplt.plot(x,y,'.',alpha=0.03)\nplt.plot(x[0],y[0],'o',label=r\"$(x_i,y_i)$\",color=\"C0\")\nplt.plot(x,prob,'--r',label=r\"prob (true, unknown) = $\\frac{exp(-1+5x)}{1+exp(-1+5x)}$\")\nplt.plot(x,sigmoid(l1(x)).data,'--b', label=r\"prob (estimated) = $(x_i,\\hat{y}_i)$ -- first curve\")\nplt.legend()\n\n\n\n\n\n\n\n\n#\n# ë°©ë²•2 â€“ l1, a1\nğŸ—£ï¸(\nx -&gt; w0hat + w1hat*x  # ìµœì´ˆì˜ ê³¡ì„ ì„ ê·¸ë¦¬ê¸° ìœ„í•œ ì„ í˜• ë³€í™˜\nu = w0hat + w1hat*x  # ê²°ê³¼ë¥¼ uë¡œ ì €ì¥\nfirst_curve = yhat = prob_hat = sigmoid(u)\nu = w0hat + w1hat*x = l1(x) # l1ì„ ë§Œë“ ë‹¤ë©´ ì´ë ‡ê²Œë„ ì“¸ ìˆ˜ ìˆìŒ\nl1 = torch.nn.Linear(1,1)\nl1.weight.data = torch.tensor([[-0.3]])\nl1.bias.data = torch.tensor([-0.8])\nu = l1(x)\nyhat = sigmoid(u)\n\nsigmoidëŠ” ì§ì ‘ ë§Œë“¤ì—ˆìŒ\n\n\nsigmoid?\n\n\nSignature: sigmoid(x)\nDocstring: &lt;no docstring&gt;\nFile:      /tmp/ipykernel_30452/3273882758.py\nType:      function\n\n\n\n\nsigmoid??\n\n\nSignature: sigmoid(x)\nDocstring: &lt;no docstring&gt;\nSource:   \ndef sigmoid(x):\n    return torch.exp(x)/(1+torch.exp(x))\nFile:      /tmp/ipykernel_30452/3273882758.py\nType:      function\n\n\n\n\në‹¤ìŒê³¼ ê°™ì´ë„ í•  ìˆ˜ ìˆìŒ (torch.nnì˜ í´ë˜ìŠ¤ ì´ìš©)\n\n\nsig = torch.nn.Sigmoid()\nsig\n\nSigmoid()\n\n\n\nl1 = torch.nn.Linear(1,1)\nl1.weight.data = torch.tensor([[-0.3]])\nl1.bias.data = torch.tensor([-0.8])\nu = l1(x)\nyhat = sig(u)\n\n\nyhat\n\ntensor([[0.3775],\n        [0.3775],\n        [0.3774],\n        ...,\n        [0.2499],\n        [0.2498],\n        [0.2497]], grad_fn=&lt;SigmoidBackward0&gt;)\n\n\n\nl1 = torch.nn.Linear(1,1)\nl1.weight.data = torch.tensor([[-0.3]])\nl1.bias.data = torch.tensor([-0.8])\nyhat = sig(l1(x)) # x --&gt; l1 --&gt; sig ë¡œ ì´í•´\n\n\nplt.plot(x,y,'.',alpha=0.03)\nplt.plot(x[0],y[0],'o',label=r\"$(x_i,y_i)$\",color=\"C0\")\nplt.plot(x,prob,'--r',label=r\"prob (true, unknown) = $\\frac{exp(-1+5x)}{1+exp(-1+5x)}$\")\nplt.plot(x,sigmoid(l1(x)).data,'--b', label=r\"prob (estimated) = $(x_i,\\hat{y}_i)$ -- first curve\")\nplt.legend()\n\n\n\n\n\n\n\n\n\në°©ë²•1ê³¼ ë™ì¼í•œ ê²°ê³¼\n\n)ğŸ—£ï¸\n\nl1 = torch.nn.Linear(1,1)\nl1.weight.data = torch.tensor([[-0.3]])\nl1.bias.data = torch.tensor([-0.8])\n\n\na1 = torch.nn.Sigmoid()\n\n\nsigmoid(l1(x)), a1(l1(x)) # ë˜‘ê°™ì•„ìš”\n\n(tensor([[0.3775],\n         [0.3775],\n         [0.3774],\n         ...,\n         [0.2499],\n         [0.2498],\n         [0.2497]], grad_fn=&lt;DivBackward0&gt;),\n tensor([[0.3775],\n         [0.3775],\n         [0.3774],\n         ...,\n         [0.2499],\n         [0.2498],\n         [0.2497]], grad_fn=&lt;SigmoidBackward0&gt;))\n\n\n- ì§€ê¸ˆê¹Œì§€ì˜ êµ¬í˜„ í™•ì¸\n\nplt.plot(x,y,'.',alpha=0.03)\nplt.plot(x[0],y[0],'o',label=r\"$(x_i,y_i)$\",color=\"C0\")\nplt.plot(x,prob,'--r',label=r\"prob (true, unknown) = $\\frac{exp(-1+5x)}{1+exp(-1+5x)}$\")\nplt.plot(x,a1(l1(x)).data,'--b', label=r\"prob (estimated) = $(x_i,\\hat{y}_i)$ -- first curve with $(a_1 \\circ l_1)(x)$\")\nplt.legend()\n\n\n\n\n\n\n\n\n#\n# ë°©ë²•3 - l1, a1 ë§Œë“¤ê³  \\(\\to\\) net\nğŸ—£ï¸(\n\nyhat\n\ntensor([[0.3775],\n        [0.3775],\n        [0.3774],\n        ...,\n        [0.2499],\n        [0.2498],\n        [0.2497]], grad_fn=&lt;SigmoidBackward0&gt;)\n\n\n\na1(l1(x))\n\ntensor([[0.3775],\n        [0.3775],\n        [0.3774],\n        ...,\n        [0.2499],\n        [0.2498],\n        [0.2497]], grad_fn=&lt;SigmoidBackward0&gt;)\n\n\n\nnet = al \\(\\circ\\) l1 ì„ ì •ì˜í•˜ì—¬ net(x)ë„ ê°™ì€ ê²°ê³¼ë¥¼ ë‚˜ì˜¤ê²Œ í•˜ê³  ì‹¶ìŒ\n\n\ntorch.nn.Sequential(l1,a1)\n\nSequential(\n  (0): Linear(in_features=1, out_features=1, bias=True)\n  (1): Sigmoid()\n)\n\n\n\nnet = torch.nn.Sequential(l1,a1)\nnet(x) # a1(l1(x))\n\ntensor([[0.3775],\n        [0.3775],\n        [0.3774],\n        ...,\n        [0.2499],\n        [0.2498],\n        [0.2497]], grad_fn=&lt;SigmoidBackward0&gt;)\n\n\n\nì´ë ‡ê²Œ í•œ ì´ìœ : parameters()ë¥¼ ì´ìš©í•˜ì—¬ optimizerë¥¼ ë§Œë“¤ ìˆ˜ ìˆìŒ\n\n\nnet.parameters()\n\n&lt;generator object Module.parameters at 0x7f34682a17b0&gt;\n\n\n)ğŸ—£ï¸\n- ê´€ì°°: ì§€ê¸ˆ ì•„ë˜ì˜ êµ¬ì¡°ì´ë‹¤.\n\\[{\\bf x} \\overset{l_1}{\\to} {\\bf u} \\overset{a_1}{\\to} {\\bf v} = \\hat{\\bf y}\\]\n- ì†Œë§: í•¨ìˆ˜ \\(l_1, a_1\\) ì˜ í•©ì„±ì„ í•˜ë‚˜ë¡œ ë¬¶ì–´ì„œ\n\\[(a_1\\circ l_1)({\\bf x}) := net({\\bf x})\\]\nì´ëŸ¬í•œ ê¸°ëŠ¥ì„ í•˜ëŠ” í•˜ë‚˜ì˜ í•¨ìˆ˜ \\(net\\)ì„ ë§Œë“¤ ìˆ˜ ì—†ì„ê¹Œ?\n\nl1 = torch.nn.Linear(1,1)\nl1.weight.data = torch.tensor([[-0.3]])\nl1.bias.data = torch.tensor([-0.8])\na1 = torch.nn.Sigmoid()\n\n\nnet = torch.nn.Sequential(l1,a1) #l1ì„ ì·¨í•˜ê³  ê·¸ë‹¤ìŒì— a1ì„ ì·¨í•˜ë¼ëŠ” ì˜ë¯¸\n\n\nnet(x), a1(l1(x)), sigmoid(l1(x))\n\n(tensor([[0.3775],\n         [0.3775],\n         [0.3774],\n         ...,\n         [0.2499],\n         [0.2498],\n         [0.2497]], grad_fn=&lt;SigmoidBackward0&gt;),\n tensor([[0.3775],\n         [0.3775],\n         [0.3774],\n         ...,\n         [0.2499],\n         [0.2498],\n         [0.2497]], grad_fn=&lt;SigmoidBackward0&gt;),\n tensor([[0.3775],\n         [0.3775],\n         [0.3774],\n         ...,\n         [0.2499],\n         [0.2498],\n         [0.2497]], grad_fn=&lt;DivBackward0&gt;))\n\n\n* net êµ¬ì¡° ì ê¹ ì‚´í´ë³´ê¸°\nğŸ—£ï¸(\n\nnet\n\nSequential(\n  (0): Linear(in_features=1, out_features=1, bias=True)\n  (1): Sigmoid()\n)\n\n\n\nnet[0]\n\nLinear(in_features=1, out_features=1, bias=True)\n\n\n\nl1\n\nLinear(in_features=1, out_features=1, bias=True)\n\n\n\nnet[1]\n\nSigmoid()\n\n\n\naaa = torch.nn.Sigmoid()\naaa\n\nSigmoid()\n\n\n\nnetê°€ ë¦¬ìŠ¤íŠ¸ì²˜ëŸ¼ ë˜ì–´ ìˆì–´ ì²«ë²ˆì§¸ ì›ì†Œ net[0]ì€ l1 ì´ê³  ë‘ë²ˆì§¸ ì›ì†Œ net[1]ì€ aaaì¸ë“¯\ní™•ì¸ ë°©ë²•: ì•„ë˜\n\n\nl1 is net[0]\n\nTrue\n\n\n\na1 is net[1]\n\nTrue\n\n\n\në‹¤ë¥¸ í™•ì¸ ë°©ë²•\n\nì˜¤ë¸Œì íŠ¸: ë©”ëª¨ë¦¬ì— ì €ì¥\nì €ì¥ë˜ì–´ ìˆëŠ” ì£¼ì†Œê°€ ë™ì¼í•˜ë©´ ê°™ì€ ì˜¤ë¸Œì íŠ¸\n\n\n\nid(net[0]), id(l1)\n\n(139863062109248, 139863062109248)\n\n\n\nid(net[1]), id(a1)\n\n(139863062108960, 139863062108960)\n\n\n\nnet(x), a1(l1(x))\n\n(tensor([[0.3775],\n         [0.3775],\n         [0.3774],\n         ...,\n         [0.2499],\n         [0.2498],\n         [0.2497]], grad_fn=&lt;SigmoidBackward0&gt;),\n tensor([[0.3775],\n         [0.3775],\n         [0.3774],\n         ...,\n         [0.2499],\n         [0.2498],\n         [0.2497]], grad_fn=&lt;SigmoidBackward0&gt;))\n\n\n\nnet(x), net[1](net[0](x)) # ì´ê²ƒë„ ë™ì¼\n\n(tensor([[0.3775],\n         [0.3775],\n         [0.3774],\n         ...,\n         [0.2499],\n         [0.2498],\n         [0.2497]], grad_fn=&lt;SigmoidBackward0&gt;),\n tensor([[0.3775],\n         [0.3775],\n         [0.3774],\n         ...,\n         [0.2499],\n         [0.2498],\n         [0.2497]], grad_fn=&lt;SigmoidBackward0&gt;))\n\n\n)ğŸ—£ï¸\n\nnet[0], net[1]\n\n(Linear(in_features=1, out_features=1, bias=True), Sigmoid())\n\n\n\nl1 is net[0]\n\nTrue\n\n\n\na1 is net[1]\n\nTrue\n\n\n#\n# ë°©ë²•4 â€“ netì„ ë°”ë¡œ ë§Œë“¤ê¸°\nğŸ—£ï¸(\n# x --&gt; yhat: íšŒê·€ë¶„ì„ì—ì„œ ìµœì´ˆì˜ ì§ì„  ë°”ë¡œ ë§Œë“œëŠ” ë°©ë²•\nnet = torch.nn.Linear(1,1)\nyhat - net(x)\n# x --&gt; yhat: ë¡œì§€ìŠ¤í‹±ì—ì„œ ìµœì´ˆì˜ ê³¡ì„  ë°”ë¡œ ë§Œë“œëŠ” ë°©ë²•\nnet = torch.nn.Sequential(\n    l1,\n    a1\n)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n)\nyhat = net(x)\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n)\nyhat = net(x)\n\n\nnet[0].weight # ì•„ë¬´ parameterê°€ ë“¤ì–´ê°€ ìˆìŒ\n\nParameter containing:\ntensor([[0.4945]], requires_grad=True)\n\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n)\nnet[0].weight.data = torch.tensor([[-0.3]])\nnet[0].bias.data = torch.tensor([-0.8])\nyhat = net(x)\n\n\nnet(x)\n\ntensor([[0.3775],\n        [0.3775],\n        [0.3774],\n        ...,\n        [0.2499],\n        [0.2498],\n        [0.2497]], grad_fn=&lt;SigmoidBackward0&gt;)\n\n\n)ğŸ—£ï¸\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n)\nnet[0].weight.data = torch.tensor([[-0.3]])\nnet[0].bias.data = torch.tensor([-0.8])\nyhat = net(x)\n\n\nnet(x)\n\ntensor([[0.3775],\n        [0.3775],\n        [0.3774],\n        ...,\n        [0.2499],\n        [0.2498],\n        [0.2497]], grad_fn=&lt;SigmoidBackward0&gt;)\n\n\nğŸ—£ï¸ ê²°ë¡ : ìœ„ì˜ ë°©ë²•ìœ¼ë¡œ ì‚¬ìš©í•˜ë©´ ë¨\n#"
  },
  {
    "objectID": "posts/03wk-2.html#d.-step14",
    "href": "posts/03wk-2.html#d.-step14",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "D. Step1~4",
    "text": "D. Step1~4\nğŸ—£ï¸(\n\ní•™ìŠµ ì‹œì‘\n\n\nplt.plot(x,y,'.',alpha=0.03) # given data\n\n\n\n\n\n\n\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n)\nnet[0].weight.data = torch.tensor([[-0.3]])\nnet[0].bias.data = torch.tensor([-0.8])\nyhat = net(x)\n\n\nplt.plot(x,y,'.',alpha=0.03) # given data\nplt.plot(x,net(x).data, '--') # ìµœì´ˆì˜ ê³¡ì„  ê·¸ë¦¬ê¸°\n\n\n\n\n\n\n\n\n\nìµœì´ˆì˜ ê³¡ì„ ë³´ë‹¤ ë‚˜ì€ ê³¡ì„ ì„ ì°¾ìœ¼ë©° updateí•˜ë©´ ë¨\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n)\nnet[0].weight.data = torch.tensor([[-0.3]])\nnet[0].bias.data = torch.tensor([-0.8])\nyhat = net(x)\nloss = torch.mean((y-yhat)**2) # loss í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ ì¤Œ\nloss\n\ntensor(0.2747, grad_fn=&lt;MeanBackward0&gt;)\n\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n)\nnet[0].weight.data = torch.tensor([[-0.3]])\nnet[0].bias.data = torch.tensor([-0.8])\noptimizr = torch.optim.SGD(net.parameters(), lr=0.25)\nfor epoc in range(200):\n    yhat = net(x)\n    loss = torch.mean((y-yhat)**2)\n    loss.backward() # ë¯¸ë¶„\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'.',alpha=0.03)\nplt.plot(x,net(x).data, '--')\n\n\n\n\n\n\n\n\n\nìµœì´ˆì˜ ê³¡ì„ ë³´ë‹¤ëŠ” ê·¸ëŸ´ë“¯í•´ì§\nì•Œê³  ìˆëŠ” True ê°’ê³¼ ë¹„êµí•´ë³´ë©´\n\nì£¼í™©ìƒ‰ ì„ : True\n200ë²ˆ ì •ë„ ë°˜ë³µí•˜ë‹ˆ ì–´ëŠ ì •ë„ ì˜¨ ê²ƒ ê°™ì§€ë§Œ ë”± ë§ë‹¤ê³  ë³´ê¸°ëŠ” ì–´ë ¤ì›€\n\n\n\nplt.plot(x,y,'.',alpha=0.03)\nplt.plot(x,prob,'--')\nplt.plot(x,net(x).data, '--')\n\n\n\n\n\n\n\n\n\n200ë²ˆ ë” (ì´ 400ë²ˆ)\n\n\nfor epoc in range(200):\n    yhat = net(x)\n    loss = torch.mean((y-yhat)**2)\n    loss.backward() # ë¯¸ë¶„\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'.',alpha=0.03)\nplt.plot(x,prob,'--')\nplt.plot(x,net(x).data, '--')\n\n\n\n\n\n\n\n\n\n200ë²ˆ ë” (ì´ 600ë²ˆ)\n\n\nfor epoc in range(200):\n    yhat = net(x)\n    loss = torch.mean((y-yhat)**2)\n    loss.backward() # ë¯¸ë¶„\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'.',alpha=0.03)\nplt.plot(x,prob,'--')\nplt.plot(x,net(x).data, '--')\n\n\n\n\n\n\n\n\n\n200ë²ˆ ë” (ì´ 800ë²ˆ)\n\n\nfor epoc in range(200):\n    yhat = net(x)\n    loss = torch.mean((y-yhat)**2)\n    loss.backward() # ë¯¸ë¶„\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'.',alpha=0.03)\nplt.plot(x,prob,'--')\nplt.plot(x,net(x).data, '--')\n\n\n\n\n\n\n\n\n\nëŒë¦´ìˆ˜ë¡ ê°€ê¹Œì›Œì§ˆ ê²ƒ ê°™ìŒ\n\n)ğŸ—£ï¸\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1, out_features=1),\n    torch.nn.Sigmoid()\n)\nl1, a1 = net \nl1.weight.data = torch.tensor([[-0.3]])\nl1.bias.data = torch.tensor([-0.8])\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25)\n#---#\nfor epoc in range(100):\n    ## 1\n    yhat = net(x) \n    ## 2 \n    loss = torch.mean((y-yhat)**2)\n    ## 3\n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'.',alpha=0.05)\nplt.plot(x,prob,'--r')\nplt.plot(x,yhat.data,'--b')\nplt.title('after 100 epochs')\n\nText(0.5, 1.0, 'after 100 epochs')\n\n\n\n\n\n\n\n\n\n\nfor epoc in range(4900):\n    ## 1\n    yhat = net(x) \n    ## 2 \n    loss = torch.mean((y-yhat)**2)\n    ## 3\n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'.',alpha=0.05)\nplt.plot(x,prob,'--r')\nplt.plot(x,yhat.data,'--b')\nplt.title('after 5000 epochs')\n\nText(0.5, 1.0, 'after 5000 epochs')\n\n\n\n\n\n\n\n\n\nğŸ—£ï¸ ë¡œì§€ìŠ¤í‹±ì´ í•´ê²°ëœ ê²ƒì²˜ëŸ¼ ë³´ì„\nğŸ—£ï¸(\n\në‹¤ìŒê³¼ ê°™ì´ í•´ë„ ë§ˆì°¬ê°€ì§€ (ì´ˆê¸°ê°’ì€ í¬ê²Œ ì¤‘ìš”í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ)\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1, out_features=1),\n    torch.nn.Sigmoid()\n)\n# l1, a1 = net \n# l1.weight.data = torch.tensor([[-0.3]])\n# l1.bias.data = torch.tensor([-0.8])\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25)\n#---#\nfor epoc in range(100):\n    ## 1\n    yhat = net(x) \n    ## 2 \n    loss = torch.mean((y-yhat)**2)\n    ## 3\n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'.',alpha=0.05)\nplt.plot(x,prob,'--r')\nplt.plot(x,yhat.data,'--b')\nplt.title('after 100 epochs')\n\nText(0.5, 1.0, 'after 100 epochs')\n\n\n\n\n\n\n\n\n\n\nfor epoc in range(4900):\n    ## 1\n    yhat = net(x) \n    ## 2 \n    loss = torch.mean((y-yhat)**2)\n    ## 3\n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'.',alpha=0.05)\nplt.plot(x,prob,'--r')\nplt.plot(x,yhat.data,'--b')\nplt.title('after 5000 epochs')\n\nText(0.5, 1.0, 'after 5000 epochs')\n\n\n\n\n\n\n\n\n\n\nì„±ê³µí•œ ê²ƒ ê°™ì§€ë§Œ ì‹¤ìƒì€ ê·¸ë ‡ì§€ ì•ŠìŒ\n\nfor epoc in range(4900):\n    ## 1\n    yhat = net(x) \n    ## 2 \n    loss = torch.mean((y-yhat)**2) # ì´ ë¶€ë¶„ì— ë¬¸ì œê°€ ìˆì–´ ì„¤ëª…í•  ì˜ˆì •\n    ## 3\n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n)ğŸ—£ï¸"
  },
  {
    "objectID": "posts/03wk-2.html#a.-ì‹œê°í™”ë¥¼-ìœ„í•œ-ì¤€ë¹„",
    "href": "posts/03wk-2.html#a.-ì‹œê°í™”ë¥¼-ìœ„í•œ-ì¤€ë¹„",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "A. ì‹œê°í™”ë¥¼ ìœ„í•œ ì¤€ë¹„",
    "text": "A. ì‹œê°í™”ë¥¼ ìœ„í•œ ì¤€ë¹„\n\ndef plot_loss(loss_fn, ax=None, Wstar=[-1,5]):\n    w0hat,w1hat =torch.meshgrid(torch.arange(-10,3,0.1),torch.arange(-1,10,0.1),indexing='ij')\n    w0hat = w0hat.reshape(-1)\n    w1hat = w1hat.reshape(-1)\n    def l(w0hat,w1hat):\n        yhat = torch.exp(w0hat+w1hat*x)/(1+torch.exp(w0hat+w1hat*x))\n        return loss_fn(yhat,y) \n    loss = list(map(l,w0hat,w1hat))\n    #---#\n    if ax is None: \n        fig = plt.figure()\n        ax = fig.add_subplot(1,1,1,projection='3d')\n    ax.scatter(w0hat,w1hat,loss,s=0.001) \n    ax.scatter(w0hat[::20],w1hat[::20],loss[::20],s=0.1,color='C0') \n    w0star,w1star = np.array(Wstar).reshape(-1)\n    ax.scatter(w0star,w1star,l(w0star,w1star),s=200,marker='*',color='red',label=f\"W=[{w0star:.1f},{w1star:.1f}]\")\n    #---#\n    ax.elev = 15\n    ax.dist = -20\n    ax.azim = 75    \n    ax.legend()\n    ax.set_xlabel(r'$w_0$')  # xì¶• ë ˆì´ë¸” ì„¤ì •\n    ax.set_ylabel(r'$w_1$')  # yì¶• ë ˆì´ë¸” ì„¤ì •\n    ax.set_xticks([-10,-5,0])  # xì¶• í‹± ê°„ê²© ì„¤ì •\n    ax.set_yticks([-10,0,10])  # yì¶• í‹± ê°„ê²© ì„¤ì •\n\n\ndef _learn_and_record(net, loss_fn, optimizr):\n    yhat_history = [] \n    loss_history = []\n    What_history = []\n    Whatgrad_history = []\n    What_history.append([net[0].bias.data.item(), net[0].weight.data.item()])\n    for epoc in range(100): \n        ## step1 \n        yhat = net(x)\n        ## step2 \n        loss = loss_fn(yhat,y)\n        ## step3\n        loss.backward() \n        ## step4 \n        optimizr.step()\n        ## record \n        if epoc % 5 ==0: \n            yhat_history.append(yhat.reshape(-1).data.tolist())\n            loss_history.append(loss.item())\n            What_history.append([net[0].bias.data.item(), net[0].weight.data.item()])\n            Whatgrad_history.append([net[0].bias.grad.item(), net[0].weight.grad.item()])\n        optimizr.zero_grad() \n        \n    return yhat_history, loss_history, What_history, Whatgrad_history\n    \ndef show_animation(net, loss_fn, optimizr):\n    yhat_history,loss_history,What_history,Whatgrad_history = _learn_and_record(net,loss_fn,optimizr)\n    \n    fig = plt.figure(figsize=(7.5,3.5))\n    ax1 = fig.add_subplot(1, 2, 1)\n    ax2 = fig.add_subplot(1, 2, 2, projection='3d')\n    ## ax1: ì™¼ìª½ê·¸ë¦¼ \n    ax1.scatter(x,y,alpha=0.01)\n    ax1.scatter(x[0],y[0],color='C0',label=r\"observed data = $(x_i,y_i)$\")\n    ax1.plot(x,prob,'--',label=r\"prob (true) = $(x_i,\\frac{exp(-1+5x_i)}{1+exp(-1+5x_i)})$\")    \n    line, = ax1.plot(x,yhat_history[0],'--',label=r\"prob (estimated) = $(x_i,\\hat{y}_i)$\") \n    ax1.legend()\n    ## ax2: ì˜¤ë¥¸ìª½ê·¸ë¦¼ \n    plot_loss(loss_fn,ax2)\n    ax2.scatter(np.array(What_history)[0,0],np.array(What_history)[0,1],loss_history[0],color='blue',s=200,marker='*')    \n    def animate(epoc):\n        line.set_ydata(yhat_history[epoc])\n        w0hat = np.array(What_history)[epoc,0]\n        w1hat = np.array(What_history)[epoc,1]\n        w0hatgrad = np.array(Whatgrad_history)[epoc,0]\n        w1hatgrad = np.array(Whatgrad_history)[epoc,1]\n        ax2.scatter(w0hat,w1hat,loss_history[epoc],color='grey')\n        ax2.set_title(f\"What.grad=[{w0hatgrad:.4f},{w1hatgrad:.4f}]\",y=0.8)\n        fig.suptitle(f\"epoch={epoc*5} // What=[{w0hat:.2f},{w1hat:.2f}] // Loss={loss_fn.__class__.__name__} // Opt={optimizr.__class__.__name__}\")\n        return line\n    ani = animation.FuncAnimation(fig, animate, frames=20)    \n    plt.close()\n    return ani\n\n\nfrom matplotlib import animation\nplt.rcParams[\"animation.html\"] = \"jshtml\"\n\ní•¨ìˆ˜ì‚¬ìš©ë²•\n\nloss_fn = torch.nn.MSELoss()\nplot_loss(loss_fn)\n\n\n\n\n\n\n\n\nğŸ—£ï¸(\n\ndef loss_fn2(yhat,y):\n    return loss_fn(yhat,y)*2\n\n\nplot_loss(loss_fn2)\n\n\n\n\n\n\n\n\n\nzì¶•ë§Œ 2ë°° ì¦ê°€ (í•¨ìˆ˜: ê³¡ë©´ì„ ê·¸ë ¤ì£¼ëŠ” ì—­í• )\n\n\n# show_animation??\n\n\nSignature: show_animation(net, loss_fn, optimizr)\n\nnet: ì´ˆê¸° ì„¤ì • ê°’ (w0, w1)\nloss_fn: ê·¸ë¦¼\noptimizr: í•™ìŠµ ê³¼ì •\n\në°‘ ì½”ë“œ: ì–´ë– í•œ ì´ˆê¸°ê°’ì„ ë°›ì•„ í•™ìŠµí•˜ëŠ” ê³¼ì •ì„ ê·¸ë ¤ì¤Œ\n\nì‹¤í–‰í•  ë•Œë§ˆë‹¤ ì´ˆê¸°ê°’ ë‹¬ë¼ì§\n\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25)\nshow_animation(net,loss_fn,optimizr)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\nì´ˆê¸°ê°’ ê³ ì •\n\në§Œì•½ í•™ìŠµë¥ ì´ 2.5ë©´ ë” ë¹¨ë¦¬ ë–¨ì–´ì§\n\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n)\nnet[0].weight.data = torch.tensor([[-0.8]])\nnet[0].bias.data = torch.tensor([-0.3])\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25)\nshow_animation(net,loss_fn,optimizr)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n)ğŸ—£ï¸\n\ntorch.manual_seed(42)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25) \nshow_animation(net,loss_fn,optimizr)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\nğŸ—£ï¸\n\nì´ˆê¸°ê°’ì— ë”°ë¼ í•™ìŠµì´ ë‹¬ë¼ì§\në§Œì•½ ì´ˆê¸° ê°’ì´ ìš°ì¸¡ ìƒë‹¨ì´ë¼ë©´ í‰í‰í•˜ê¸° ë•Œë¬¸ì— updateê°€ ì•„ì£¼ ì¡°ê¸ˆì”© ì¼ì–´ë‚¨"
  },
  {
    "objectID": "posts/03wk-2.html#b.-ì¢‹ì€-ì´ˆê¸°ê°’",
    "href": "posts/03wk-2.html#b.-ì¢‹ì€-ì´ˆê¸°ê°’",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "B. ì¢‹ì€ ì´ˆê¸°ê°’",
    "text": "B. ì¢‹ì€ ì´ˆê¸°ê°’\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nnet[0].bias.data = torch.tensor([-0.8])\nnet[0].weight.data = torch.tensor([[-0.3]])\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25) \n#---#\nshow_animation(net,loss_fn,optimizr)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\nğŸ—£ï¸\n\nì´ ê²½ìš°ëŠ” ê¸°ë‹¤ë¦¬ë©´ í•™ìŠµì´ ì˜ ë  ê±° ê°™ìŒ"
  },
  {
    "objectID": "posts/03wk-2.html#c.-ê°€ëŠ¥ì„±-ìˆëŠ”-ì´ˆê¸°ê°’",
    "href": "posts/03wk-2.html#c.-ê°€ëŠ¥ì„±-ìˆëŠ”-ì´ˆê¸°ê°’",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "C. ê°€ëŠ¥ì„± ìˆëŠ” ì´ˆê¸°ê°’",
    "text": "C. ê°€ëŠ¥ì„± ìˆëŠ” ì´ˆê¸°ê°’\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nnet[0].bias.data = torch.tensor([-3.0])\nnet[0].weight.data = torch.tensor([[-1.0]])\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25) \n#---#\nshow_animation(net,loss_fn,optimizr)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\nğŸ—£ï¸\n\në§ˆì§€ë§‰ì— ì•½ê°„ í¬ë§ì´ ë³´ì„\në§ˆìŒ ë¨¹ê³  20,000ë²ˆ ì •ë„ ëŒë¦¬ë©´ ë  ê±° ê°™ìŒ"
  },
  {
    "objectID": "posts/03wk-2.html#d.-ìµœì•…ì˜-ì´ˆê¸°ê°’",
    "href": "posts/03wk-2.html#d.-ìµœì•…ì˜-ì´ˆê¸°ê°’",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "D. ìµœì•…ì˜ ì´ˆê¸°ê°’",
    "text": "D. ìµœì•…ì˜ ì´ˆê¸°ê°’\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nnet[0].bias.data = torch.tensor([-10.0])\nnet[0].weight.data = torch.tensor([[-1.0]])\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25) \n#---#\nshow_animation(net,loss_fn,optimizr)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\nğŸ—£ï¸\n\nì´ ê²½ìš°ëŠ” í¬ë§ì´ ì—†ìŒ\nê³¡ì„ ì´ ì•„ë˜ë¡œ ë³¼ë¡í•œ 2ì°¨ í•¨ìˆ˜ê°€ ì•„ë‹ˆê³  4ì°¨ í•¨ìˆ˜ë¼ë©´ í•™ìŠµë¥ ê³¼ ì´ˆê¸° ê°’ì— ë”°ë¼ ê°–í˜€ë²„ë¦´ ìˆ˜ë„ ìˆê³  ìš´ì— ë”°ë¼ ë‹¬ë¼ì§\n\n\ní•´ê²°í•˜ëŠ” ì ‘ê·¼ë²•:\n\nì»´ê³µìŠ¤íƒ€ì¼: ì—í­ì„ ëŠ˜ë ¤ë³¼ê¹Œ?\nì‚°ê³µìŠ¤íƒ€ì¼: ì˜µí‹°ë§ˆì´ì €ë¥¼ ë°”ê¿”ë³¼ê¹Œ?\ní†µê³„ìŠ¤íƒ€ì¼: Lossë¥¼ ë°”ê¿”ë³¼ê¹Œ?\nğŸ—£ï¸\n\nì´ˆê¸° ê°’ì„ ë°”ê¿”ê°€ë©° ë¬´ìˆ˜íˆ ì‹¤í–‰í•˜ë©° ì°¾ìŒ\nì´ ì–´ë ¤ìš´ ê³¡ë©´ì— ëŒ€í•´ ì˜µí‹°ë§ˆì´ì €ë¥¼ ìˆ˜ì •\nê³¡ë©´ ìì²´ë¥¼ ìµœì í™”ê°€ ì˜ ë˜ê²Œ ë°”ê¿ˆ (loss í•¨ìˆ˜ë¥¼ ë°”ê¿ˆ: MSE Loss ë§ê³  ë‹¤ë¥¸ Loss?)"
  },
  {
    "objectID": "posts/03wk-2.html#a.-bce-lossë¥¼-ì‚¬ìš©í•˜ì—¬-í•™ìŠµ",
    "href": "posts/03wk-2.html#a.-bce-lossë¥¼-ì‚¬ìš©í•˜ì—¬-í•™ìŠµ",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "A. BCE Lossë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ",
    "text": "A. BCE Lossë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ\n- BCE lossë¼ëŠ”ê²Œ ìˆìŒ.\n\n\\(loss= - \\sum_{i=1}^{n} \\big(y_i\\log(\\hat{y}_i)+(1-y_i)\\log(1-\\hat{y}_i)\\big)\\)\nhttps://en.wikipedia.org/wiki/Cross-entropy\n\nğŸ—£ï¸(\nyi = 0\nyi_hat = 0.001\nlog(1) = 0\nloss = 0\nyi = 0\nyi_hat = 0.9999\nlog(1-0.999) = log(0) = -ë¬´í•œëŒ€\nloss = ë¬´í•œëŒ€\nyi = 1\nyi_hat = 1\nloss = 0\nyi = 1\nyi_hat = 0.0001\nloss = ë¬´í•œëŒ€\n\në¹„ìŠ·í• ìˆ˜ë¡ 0, ë‹¤ë¥¼ìˆ˜ë¡ ë¬´í•œëŒ€ê¹Œì§€ ê° -&gt; lossì˜ ì—­í• ì€ í•¨\nì›ë¦¬: - log likelihoood\n\n)ğŸ—£ï¸\n\nğŸ—£ï¸\n\nnet[0] = torch.nn.Linear(in_features=1, out_features=1)\nnet[1] = torch.nn.Sigmoid()\nnet = [net[0], net[1]] ëŠë‚Œ\nl1, a1 = [net[0], net[1]] ëŠë‚Œ\n\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1, out_features=1),\n    torch.nn.Sigmoid()\n)\nl1, a1 = net \nl1.weight.data = torch.tensor([[-0.3]])\nl1.bias.data = torch.tensor([-0.8])\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25)\n#---#\nfor epoc in range(100):\n    ## 1\n    yhat = net(x) \n    ## 2 \n    #loss = torch.mean((y-yhat)**2) # loss_fn(yhat,y)\n    loss = -torch.mean(y*torch.log(yhat) + (1-y)*torch.log(1-yhat))\n    ## 3\n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'.',alpha=0.05)\nplt.plot(x,prob,'--r')\nplt.plot(x,yhat.data,'--b')\nplt.title('after 100 epochs')\n\nText(0.5, 1.0, 'after 100 epochs')\n\n\n\n\n\n\n\n\n\nê°™ì€ 100 ì—í­ì¸ë° í›¨ì”¬ ì˜ë§ì¶¤..\nğŸ—£ï¸ ë™ì¼í•œ ì´ˆê¸° ê°’\n- lossìˆ˜ì‹ì„ ëª»ì™¸ìš°ê² ë‹¤ë©´?\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1, out_features=1),\n    torch.nn.Sigmoid()\n)\nl1, a1 = net \nl1.weight.data = torch.tensor([[-0.3]])\nl1.bias.data = torch.tensor([-0.8])\nloss_fn = torch.nn.BCELoss()\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25)\n#---#\nfor epoc in range(100):\n    ## 1\n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y) # yhatë¶€í„° ì¨ì•¼í•¨\n    ## 3\n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'.',alpha=0.05)\nplt.plot(x,prob,'--r')\nplt.plot(x,yhat.data,'--b')\nplt.title('after 100 epochs')\n\nText(0.5, 1.0, 'after 100 epochs')"
  },
  {
    "objectID": "posts/03wk-2.html#b.-loss-function-ì‹œê°í™”",
    "href": "posts/03wk-2.html#b.-loss-function-ì‹œê°í™”",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "B. Loss Function ì‹œê°í™”",
    "text": "B. Loss Function ì‹œê°í™”\n\nplot_loss(torch.nn.MSELoss())\n\n\n\n\n\n\n\n\nğŸ—£ï¸ MSELossëŠ” ìš°ì¸¡ ìƒë‹¨ì— ìˆìœ¼ë©´ ì•ˆ ë  ê²ƒ ê°™ìŒ\n\nplot_loss(torch.nn.BCELoss())\n\n\n\n\n\n\n\n\n- ë¹„êµí•´ë³´ì.\n\nfig = plt.figure()\nax1 = fig.add_subplot(1,2,1,projection='3d')\nax2 = fig.add_subplot(1,2,2,projection='3d')\nplot_loss(torch.nn.MSELoss(),ax1)\nplot_loss(torch.nn.BCELoss(),ax2)\n\n\n\n\n\n\n\n\n\nğŸ—£ï¸\n\nì˜¤ë¥¸ìª½ê³¼ ê°™ì€ ê²½ìš°ë¥¼ ì–´ë ¤ìš´ ë§ë¡œ convex functionì´ë¼ê³  í•¨\nloss í•¨ìˆ˜ê°€ convex functionì´ë©´ ìˆ˜ë ´ì‹œí‚¤ê¸° ì‰¬ì›€"
  },
  {
    "objectID": "posts/03wk-2.html#c.-í•™ìŠµê³¼ì •-ì‹œê°í™”-ì¢‹ì€-ì´ˆê¸°ê°’",
    "href": "posts/03wk-2.html#c.-í•™ìŠµê³¼ì •-ì‹œê°í™”-ì¢‹ì€-ì´ˆê¸°ê°’",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "C. í•™ìŠµê³¼ì • ì‹œê°í™” â€“ ì¢‹ì€ ì´ˆê¸°ê°’",
    "text": "C. í•™ìŠµê³¼ì • ì‹œê°í™” â€“ ì¢‹ì€ ì´ˆê¸°ê°’\n- MSELoss\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nnet[0].bias.data = torch.tensor([-0.8])\nnet[0].weight.data = torch.tensor([[-0.3]])\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25) \n#---#\nshow_animation(net,loss_fn,optimizr)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n- BCELoss\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nnet[0].bias.data = torch.tensor([-0.8])\nnet[0].weight.data = torch.tensor([[-0.3]])\nloss_fn = torch.nn.BCELoss()\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25) \n#---#\nshow_animation(net,loss_fn,optimizr)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nğŸ—£ï¸ ê°™ì€ ì´ˆê¸°ê°’ì¸ë° BCELossê°€ ë” ìˆ˜ë ´ì„ ì˜ í•  ê²ƒ ê°™ìŒ"
  },
  {
    "objectID": "posts/03wk-2.html#d.-í•™ìŠµê³¼ì •-ì‹œê°í™”-ê°€ëŠ¥ì„±-ìˆëŠ”-ì´ˆê¸°ê°’",
    "href": "posts/03wk-2.html#d.-í•™ìŠµê³¼ì •-ì‹œê°í™”-ê°€ëŠ¥ì„±-ìˆëŠ”-ì´ˆê¸°ê°’",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "D. í•™ìŠµê³¼ì • ì‹œê°í™” â€“ ê°€ëŠ¥ì„± ìˆëŠ” ì´ˆê¸°ê°’",
    "text": "D. í•™ìŠµê³¼ì • ì‹œê°í™” â€“ ê°€ëŠ¥ì„± ìˆëŠ” ì´ˆê¸°ê°’\n- MSELoss\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nnet[0].bias.data = torch.tensor([-3.0])\nnet[0].weight.data = torch.tensor([[-1.0]])\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25) \n#---#\nshow_animation(net,loss_fn,optimizr)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n- BCELoss\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nnet[0].bias.data = torch.tensor([-3.0])\nnet[0].weight.data = torch.tensor([[-1.0]])\nloss_fn = torch.nn.BCELoss()\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25) \n#---#\nshow_animation(net,loss_fn,optimizr)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nğŸ—£ï¸ BCELossëŠ” ì²˜ìŒë¶€í„° ì˜ ë–¨ì–´ì§"
  },
  {
    "objectID": "posts/03wk-2.html#e.-í•™ìŠµê³¼ì •-ì‹œê°í™”-ìµœì•…ì˜-ì´ˆê¸°ê°’",
    "href": "posts/03wk-2.html#e.-í•™ìŠµê³¼ì •-ì‹œê°í™”-ìµœì•…ì˜-ì´ˆê¸°ê°’",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "E. í•™ìŠµê³¼ì • ì‹œê°í™” â€“ ìµœì•…ì˜ ì´ˆê¸°ê°’",
    "text": "E. í•™ìŠµê³¼ì • ì‹œê°í™” â€“ ìµœì•…ì˜ ì´ˆê¸°ê°’\n- MSELoss\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nnet[0].bias.data = torch.tensor([-10.0])\nnet[0].weight.data = torch.tensor([[-1.0]])\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25) \n#---#\nshow_animation(net,loss_fn,optimizr)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n- BCELoss\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nnet[0].bias.data = torch.tensor([-10.0])\nnet[0].weight.data = torch.tensor([[-1.0]])\nloss_fn = torch.nn.BCELoss()\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25) \n#---#\nshow_animation(net,loss_fn,optimizr)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nğŸ—£ï¸ BCELossëŠ” ì²˜ìŒë¶€í„° ì˜ ë–¨ì–´ì§"
  },
  {
    "objectID": "posts/03wk-2.html#a.-í•™ìŠµê³¼ì •-ì‹œê°í™”-ì¢‹ì€-ì´ˆê¸°ê°’",
    "href": "posts/03wk-2.html#a.-í•™ìŠµê³¼ì •-ì‹œê°í™”-ì¢‹ì€-ì´ˆê¸°ê°’",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "A. í•™ìŠµê³¼ì • ì‹œê°í™” â€“ ì¢‹ì€ ì´ˆê¸°ê°’",
    "text": "A. í•™ìŠµê³¼ì • ì‹œê°í™” â€“ ì¢‹ì€ ì´ˆê¸°ê°’\n- MSELoss + SGD\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nnet[0].bias.data = torch.tensor([-0.8470])\nnet[0].weight.data = torch.tensor([[-0.3467]])\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25) \n#---#\nshow_animation(net,loss_fn,optimizr)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n- MSELoss + Adam\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nnet[0].bias.data = torch.tensor([-0.8])\nnet[0].weight.data = torch.tensor([[-0.3]])\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters(),lr=0.25) \n#---#\nshow_animation(net,loss_fn,optimizr)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nğŸ—£ï¸ Adamì„ ì‚¬ìš©í•˜ë‹ˆ ë¹¨ë¦¬ ë–¨ì–´ì§€ë©´ì„œ ì˜ ìˆ˜ë ´í•¨ (í˜ìœ¼ë¡œ ë¯¸ëŠ” ëŠë‚Œ)"
  },
  {
    "objectID": "posts/03wk-2.html#b.-í•™ìŠµê³¼ì •-ì‹œê°í™”-ê°€ëŠ¥ì„±-ìˆëŠ”-ì´ˆê¸°ê°’",
    "href": "posts/03wk-2.html#b.-í•™ìŠµê³¼ì •-ì‹œê°í™”-ê°€ëŠ¥ì„±-ìˆëŠ”-ì´ˆê¸°ê°’",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "B. í•™ìŠµê³¼ì • ì‹œê°í™” â€“ ê°€ëŠ¥ì„± ìˆëŠ” ì´ˆê¸°ê°’",
    "text": "B. í•™ìŠµê³¼ì • ì‹œê°í™” â€“ ê°€ëŠ¥ì„± ìˆëŠ” ì´ˆê¸°ê°’\n- MSELoss + SGD\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nnet[0].bias.data = torch.tensor([-3.0])\nnet[0].weight.data = torch.tensor([[-1.0]])\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25) \n#---#\nshow_animation(net,loss_fn,optimizr)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n- MSELoss + Adam\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nnet[0].bias.data = torch.tensor([-3.0])\nnet[0].weight.data = torch.tensor([[-1.0]])\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters(),lr=0.25) \n#---#\nshow_animation(net,loss_fn,optimizr)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nğŸ—£ï¸ Adamì„ ì‚¬ìš©í•˜ë‹ˆ ë¹¨ë¦¬ ë–¨ì–´ì§€ë©´ì„œ ì˜ ìˆ˜ë ´í•¨ (ë§ˆì§€ë§‰ì€ ì‚´ì§ ëŒì•„ê°€ëŠ” ëŠë‚Œ)"
  },
  {
    "objectID": "posts/03wk-2.html#c.-í•™ìŠµê³¼ì •-ì‹œê°í™”-ìµœì•…ì˜-ì´ˆê¸°ê°’",
    "href": "posts/03wk-2.html#c.-í•™ìŠµê³¼ì •-ì‹œê°í™”-ìµœì•…ì˜-ì´ˆê¸°ê°’",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "C. í•™ìŠµê³¼ì • ì‹œê°í™” â€“ ìµœì•…ì˜ ì´ˆê¸°ê°’",
    "text": "C. í•™ìŠµê³¼ì • ì‹œê°í™” â€“ ìµœì•…ì˜ ì´ˆê¸°ê°’\n- MSELoss + SGD\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nnet[0].bias.data = torch.tensor([-10.0])\nnet[0].weight.data = torch.tensor([[-1.0]])\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD(net.parameters(),lr=0.05) \n#---#\nshow_animation(net,loss_fn,optimizr)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n- MSELoss + Adam\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nnet[0].bias.data = torch.tensor([-10.0])\nnet[0].weight.data = torch.tensor([[-1.0]])\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters(),lr=0.25) \n#---#\nshow_animation(net,loss_fn,optimizr)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nğŸ—£ï¸ Adamì„ ì‚¬ìš©í•˜ë‹ˆ ë¹¨ë¦¬ ë–¨ì–´ì§€ë©´ì„œ ì˜ ìˆ˜ë ´í•¨ (ë‚´ë ¤ì˜¤ëŠ” í˜ì´ ê°•í•´ì„œ ê·¸ëŸ°ì§€ ë§ˆì§€ë§‰ì€ ì‚´ì§ ëŒë‹¤ê°€ ê°€ëŠ” ëŠë‚Œ)\nğŸ—£ï¸ í˜„ì¬ ìµœì í™”ë¥¼ ì˜í•˜ê³  ì‹¶ìœ¼ë©´ Adamì„ ì‚¬ìš©í•˜ë©´ ë¨"
  },
  {
    "objectID": "posts/03wk-2.html#d.-ì°¸ê³ ìë£Œ",
    "href": "posts/03wk-2.html#d.-ì°¸ê³ ìë£Œ",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "D. ì°¸ê³ ìë£Œ",
    "text": "D. ì°¸ê³ ìë£Œ\nhttps://www.youtube.com/watch?v=MD2fYip6QsQ\n\n11:50 â€“ Momentum\n12:30 â€“ RMSprop\n15:55 â€“ Adam\nğŸ—£ï¸ local minê³¼ global minì´ ë”°ë¡œ ìˆì„ ë•Œ\n\nì¼ë°˜ì ì¸ ê²½ì‚¬í•˜ê°•ë²•ì€ ë³´í†µ local minì— ë¹ ì§\nAdamì€ local minì„ ì˜ íƒˆì¶œí•¨ (í•­ìƒì€ X)"
  },
  {
    "objectID": "posts/03wk-2.html#a.-ì‹ ë¬¸ê¸°ì‚¬-ë°ì´í„°ì˜-ëª¨í‹°ë¸Œ",
    "href": "posts/03wk-2.html#a.-ì‹ ë¬¸ê¸°ì‚¬-ë°ì´í„°ì˜-ëª¨í‹°ë¸Œ",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "A. ì‹ ë¬¸ê¸°ì‚¬ (ë°ì´í„°ì˜ ëª¨í‹°ë¸Œ)",
    "text": "A. ì‹ ë¬¸ê¸°ì‚¬ (ë°ì´í„°ì˜ ëª¨í‹°ë¸Œ)\n- ìŠ¤í™ì´ ë†’ì•„ë„ ì·¨ì—…ì´ ì•ˆëœë‹¤ê³  í•©ë‹ˆë‹¤..\nì¤‘ì†ŒÂ·ì§€ë°© ê¸°ì—… â€œë½‘ì•„ë´¤ì ê·¸ë§Œë‘ë‹ˆê¹Œâ€\nì¤‘ì†Œê¸°ì—… ê´€ê³„ìë“¤ì€ ê³ ìŠ¤í™ ì§€ì›ìë¥¼ êº¼ë¦¬ëŠ” ì´ìœ ë¡œ ë†’ì€ í‡´ì§ë¥ ì„ ê¼½ëŠ”ë‹¤. ì—¬ê±´ì´ ì¢‹ì€ ëŒ€ê¸°ì—…ìœ¼ë¡œ ì´ì§í•˜ê±°ë‚˜ íšŒì‚¬ë¥¼ ê´€ë‘ëŠ” ê²½ìš°ê°€ ë§ë‹¤ëŠ” í•˜ì†Œì—°ì´ë‹¤. ê³ ìš©ì •ë³´ì›ì´ ì§€ë‚œ 3ì¼ ê³µê°œí•œ ìë£Œì— ë”°ë¥´ë©´ ì¤‘ì†Œê¸°ì—… ì²­ë…„ì·¨ì—…ì ê°€ìš´ë° 49.5%ê°€ 2ë…„ ë‚´ì— íšŒì‚¬ë¥¼ ê·¸ë§Œë‘ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ë‹¤.\nì¤‘ì†Œ ITì—…ì²´ ê´€ê³„ìëŠ” â€œê¸°ì—… ì…ì¥ì—ì„œ ê°€ì¥ ë¼ˆì•„í”ˆ ê²Œ ì‹ ì…ì‚¬ì›ì´ ê·¸ë§Œë‘¬ì„œ ìƒˆë¡œ ë½‘ëŠ” ì¼â€ì´ë¼ë©° â€œëª…ë¬¸ëŒ€ ë‚˜ì˜¨ ìŠ¤í™ ì¢‹ì€ ì§€ì›ìë¥¼ ë½‘ì•„ë†”ë„ 1ë…„ì„ ì±„ìš°ì§€ ì•Šê³  ê·¸ë§Œë‘ëŠ” ì‚¬ì›ì´ ëŒ€ë¶€ë¶„ì´ë¼ ìš°ë¦¬ë„ ëˆˆì„ ë‚®ì¶° ì‚¬ëŒì„ ë½‘ëŠ”ë‹¤â€ê³  ë§í–ˆë‹¤."
  },
  {
    "objectID": "posts/03wk-2.html#b.-ê°€ì§œë°ì´í„°-ìŠ¤í™ì˜-ì—­ì„¤",
    "href": "posts/03wk-2.html#b.-ê°€ì§œë°ì´í„°-ìŠ¤í™ì˜-ì—­ì„¤",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "B. ê°€ì§œë°ì´í„° â€“ ìŠ¤í™ì˜ ì—­ì„¤",
    "text": "B. ê°€ì§œë°ì´í„° â€“ ìŠ¤í™ì˜ ì—­ì„¤\nğŸ—£ï¸ x: ìŠ¤í™, prob: í•©ê²©í•  í™•ë¥ \n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DL2025/main/posts/ironyofspec.csv\")\ndf\n\n\n\n\n\n\n\n\nx\nprob\ny\n\n\n\n\n0\n-1.000000\n0.000045\n0.0\n\n\n1\n-0.998999\n0.000046\n0.0\n\n\n2\n-0.997999\n0.000047\n0.0\n\n\n3\n-0.996998\n0.000047\n0.0\n\n\n4\n-0.995998\n0.000048\n0.0\n\n\n...\n...\n...\n...\n\n\n1995\n0.995998\n0.505002\n0.0\n\n\n1996\n0.996998\n0.503752\n0.0\n\n\n1997\n0.997999\n0.502501\n0.0\n\n\n1998\n0.998999\n0.501251\n1.0\n\n\n1999\n1.000000\n0.500000\n1.0\n\n\n\n\n2000 rows Ã— 3 columns\n\n\n\n\nx = torch.tensor(df.x).float().reshape(-1,1)\ny = torch.tensor(df.y).float().reshape(-1,1)\nprob = torch.tensor(df.prob).float().reshape(-1,1)\n\n\nplt.plot(x,y,'o',alpha=0.02)\nplt.plot(x[0],y[0],'o',label= r\"observed data = $(x_i,y_i)$\",color=\"C0\")\nplt.plot(x,prob,'--b',label= r\"prob (true, unknown)\")\nplt.legend()\n\n\n\n\n\n\n\n\nğŸ—£ï¸ ìŠ¤í™ì´ ë„ˆë¬´ ë†’ìœ¼ë©´ ì˜¤íˆë ¤ ë–¨ì–´ì§"
  },
  {
    "objectID": "posts/03wk-2.html#c.-ë¡œì§€ìŠ¤í‹±ìœ¼ë¡œ-ì í•©",
    "href": "posts/03wk-2.html#c.-ë¡œì§€ìŠ¤í‹±ìœ¼ë¡œ-ì í•©",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "C. ë¡œì§€ìŠ¤í‹±ìœ¼ë¡œ ì í•©",
    "text": "C. ë¡œì§€ìŠ¤í‹±ìœ¼ë¡œ ì í•©\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n)\nloss_fn = torch.nn.BCELoss()\noptimizr = torch.optim.Adam(net.parameters())\n#---# \nfor epoc in range(5000):\n    ## 1 \n    yhat = net(x)\n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'o',alpha=0.02)\nplt.plot(x[0],y[0],'o',label= r\"observed data = $(x_i,y_i)$\",color=\"C0\")\nplt.plot(x,prob,'--b',label= r\"prob (true, unknown)\")\nplt.plot(x,net(x).data, '--', label= r\"prob (estimated) = $(x_i,\\hat{y}_i)$\")\nplt.legend()\n\n\n\n\n\n\n\n\n- Epochì„ 10ì–µë²ˆìœ¼ë¡œ ì„¤ì •í•´ë„ ì´ê±´ ëª» ë§ì¶œê²ƒ ê°™ìŒ.\n\nğŸ—£ï¸\n\nì£¼í™©ìƒ‰ ì„ (model)ì´ ì˜¬ë¼ê°€ë‹¤ê°€ ë‚´ë ¤ì˜¤ëŠ” ê²ƒì€ ìµœì´ˆì˜ ê³¡ì„ ì´ ë°”ë€” ìˆ˜ ìˆëŠ” ë²”ìœ„ë¥¼ ë²—ì–´ë‚¨ (ìˆ˜ì‹ì ìœ¼ë¡œ)\nì´ëŸ° ê²½ìš° ëª¨í˜•ì˜ í‘œí˜„ë ¥ì´ ë‚®ë‹¤ê³  í‘œí˜„í•¨"
  },
  {
    "objectID": "posts/03wk-2.html#d.-ë¡œì§€ìŠ¤í‹±-í•œê³„ê·¹ë³µ-ì•„ì´ë””ì–´ë§Œ",
    "href": "posts/03wk-2.html#d.-ë¡œì§€ìŠ¤í‹±-í•œê³„ê·¹ë³µ-ì•„ì´ë””ì–´ë§Œ",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "D. ë¡œì§€ìŠ¤í‹± í•œê³„ê·¹ë³µ â€“ ì•„ì´ë””ì–´ë§Œ",
    "text": "D. ë¡œì§€ìŠ¤í‹± í•œê³„ê·¹ë³µ â€“ ì•„ì´ë””ì–´ë§Œ\nğŸ—£ï¸ ë°˜ë°˜ ì˜ë¼ì„œ í•˜ë©´ ë  ê²ƒ ê°™ìŒ\n- sigmoidë¥¼ ë„£ê¸° ì „ì˜ ìƒíƒœê°€ ì§ì„ ì´ ì•„ë‹ˆë¼ êº½ì´ëŠ” ì§ì„ ì´ì•¼ í•œë‹¤.\n\na = torch.nn.Sigmoid()\n\n\nfig,ax = plt.subplots(4,2,figsize=(8,8))\nu1 = torch.tensor([-6,-4,-2,0,2,4,6])\nu2 = torch.tensor([6,4,2,0,-2,-4,-6])\nu3 = torch.tensor([-6,-2,2,6,2,-2,-6])\nu4 = torch.tensor([-6,-2,2,6,4,2,0])\nax[0,0].plot(u1,'--o',color='C0',label = r\"$u_1$\")\nax[0,0].legend()\nax[0,1].plot(a(u1),'--o',color='C0',label = r\"$a(u_1)=\\frac{exp(u_1)}{exp(u_1)+1}$\")\nax[0,1].legend()\nax[1,0].plot(u2,'--o',color='C1',label = r\"$u_2$\")\nax[1,0].legend()\nax[1,1].plot(a(u2),'--o',color='C1',label = r\"$a(u_2)=\\frac{exp(u_2)}{exp(u_2)+1}$\")\nax[1,1].legend()\nax[2,0].plot(u3,'--o',color='C2', label = r\"$u_3$\")\nax[2,0].legend()\nax[2,1].plot(a(u3),'--o',color='C2', label = r\"$a(u_3)=\\frac{exp(u_3)}{exp(u_3)+1}$\")\nax[2,1].legend()\nax[3,0].plot(u4,'--o',color='C3', label = r\"$u_4$\")\nax[3,0].legend()\nax[3,1].plot(a(u4),'--o',color='C3', label = r\"$a(u_4)=\\frac{exp(u_4)}{exp(u_4)+1}$\")\nax[3,1].legend()"
  },
  {
    "objectID": "posts/03wk-2.html#footnotes",
    "href": "posts/03wk-2.html#footnotes",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nì›ë˜ëŠ” ì´ë ‡ê²Œ ì¼ì—ˆì§€.. \\(y_i = w_0 + w_1x_i + \\epsilon_i \\quad \\epsilon_i \\sim {\\cal N}(0,\\sigma^2)\\)â†©ï¸"
  },
  {
    "objectID": "posts/01wk-2.html",
    "href": "posts/01wk-2.html",
    "title": "01wk-2, 02wk-1: (íšŒê·€) â€“ íšŒê·€ëª¨í˜•, ì†ì‹¤í•¨ìˆ˜, íŒŒì´í† ì¹˜ë¥¼ ì´ìš©í•œ ì¶”ì •",
    "section": "",
    "text": "ğŸ“˜ Note Format Guide\nThis format serves as a structured guide for organizing lecture content, personal interpretation, experiments, and study-related questions.\nğŸ“ ğŸ—£ï¸ âœï¸ ğŸ”¬ â“"
  },
  {
    "objectID": "posts/01wk-2.html#a.-ì•„ì´ìŠ¤-ì•„ë©”ë¦¬ì¹´ë…¸-ê°€ì§œìë£Œ",
    "href": "posts/01wk-2.html#a.-ì•„ì´ìŠ¤-ì•„ë©”ë¦¬ì¹´ë…¸-ê°€ì§œìë£Œ",
    "title": "01wk-2, 02wk-1: (íšŒê·€) â€“ íšŒê·€ëª¨í˜•, ì†ì‹¤í•¨ìˆ˜, íŒŒì´í† ì¹˜ë¥¼ ì´ìš©í•œ ì¶”ì •",
    "section": "A. ì•„ì´ìŠ¤ ì•„ë©”ë¦¬ì¹´ë…¸ (ê°€ì§œìë£Œ)",
    "text": "A. ì•„ì´ìŠ¤ ì•„ë©”ë¦¬ì¹´ë…¸ (ê°€ì§œìë£Œ)\n- ì¹´í˜ì£¼ì¸ì¸ ë°•í˜œì›ì”¨ëŠ” ì˜¨ë„ì™€ ì•„ì´ìŠ¤ì•„ë©”ë¦¬ì¹´ë…¸ íŒë§¤ëŸ‰ì´ ê´€ê³„ê°€ ìˆë‹¤ëŠ” ê²ƒì„ ì•Œì•˜ë‹¤. êµ¬ì²´ì ìœ¼ë¡œëŠ”\n\nâ€œì˜¨ë„ê°€ ë†’ì•„ì§ˆ ìˆ˜ë¡ (=ë‚ ì”¨ê°€ ë”ìš¸ìˆ˜ë¡) ì•„ì´ìŠ¤ì•„ë©”ë¦¬ì¹´ë…¸ì˜ íŒë§¤ëŸ‰ì´ ì¦ê°€â€\n\ní•œë‹¤ëŠ” ì‚¬ì‹¤ì„ ì•Œê²Œ ë˜ì—ˆë‹¤. ì´ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•´ì„œ ì•„ë˜ì™€ ê°™ì´ 100ê°œì˜ ë°ì´í„°ë¥¼ ëª¨ì•˜ë‹¤.\n\ntemp = [-2.4821, -2.3621, -1.9973, -1.6239, -1.4792, -1.4635, -1.4509, -1.4435,\n        -1.3722, -1.3079, -1.1904, -1.1092, -1.1054, -1.0875, -0.9469, -0.9319,\n        -0.8643, -0.7858, -0.7549, -0.7421, -0.6948, -0.6103, -0.5830, -0.5621,\n        -0.5506, -0.5058, -0.4806, -0.4738, -0.4710, -0.4676, -0.3874, -0.3719,\n        -0.3688, -0.3159, -0.2775, -0.2772, -0.2734, -0.2721, -0.2668, -0.2155,\n        -0.2000, -0.1816, -0.1708, -0.1565, -0.1448, -0.1361, -0.1057, -0.0603,\n        -0.0559, -0.0214,  0.0655,  0.0684,  0.1195,  0.1420,  0.1521,  0.1568,\n         0.2646,  0.2656,  0.3157,  0.3220,  0.3461,  0.3984,  0.4190,  0.5443,\n         0.5579,  0.5913,  0.6148,  0.6469,  0.6469,  0.6523,  0.6674,  0.7059,\n         0.7141,  0.7822,  0.8154,  0.8668,  0.9291,  0.9804,  0.9853,  0.9941,\n         1.0376,  1.0393,  1.0697,  1.1024,  1.1126,  1.1532,  1.2289,  1.3403,\n         1.3494,  1.4279,  1.4994,  1.5031,  1.5437,  1.6789,  2.0832,  2.2444,\n         2.3935,  2.6056,  2.6057,  2.6632]\n\n\nsales= [-8.5420, -6.5767, -5.9496, -4.4794, -4.2516, -3.1326, -4.0239, -4.1862,\n        -3.3403, -2.2027, -2.0262, -2.5619, -1.3353, -2.0466, -0.4664, -1.3513,\n        -1.6472, -0.1089, -0.3071, -0.6299, -0.0438,  0.4163,  0.4166, -0.0943,\n         0.2662,  0.4591,  0.8905,  0.8998,  0.6314,  1.3845,  0.8085,  1.2594,\n         1.1211,  1.9232,  1.0619,  1.3552,  2.1161,  1.1437,  1.6245,  1.7639,\n         1.6022,  1.7465,  0.9830,  1.7824,  2.1116,  2.8621,  2.1165,  1.5226,\n         2.5572,  2.8361,  3.3956,  2.0679,  2.8140,  3.4852,  3.6059,  2.5966,\n         2.8854,  3.9173,  3.6527,  4.1029,  4.3125,  3.4026,  3.2180,  4.5686,\n         4.3772,  4.3075,  4.4895,  4.4827,  5.3170,  5.4987,  5.4632,  6.0328,\n         5.2842,  5.0539,  5.4538,  6.0337,  5.7250,  5.7587,  6.2020,  6.5992,\n         6.4621,  6.5140,  6.6846,  7.3497,  8.0909,  7.0794,  6.8667,  7.4229,\n         7.2544,  7.1967,  9.5006,  9.0339,  7.4887,  9.0759, 11.0946, 10.3260,\n        12.2665, 13.0983, 12.5468, 13.8340]\n\nğŸ—£ï¸ ìŒìˆ˜ íŒë§¤ëŸ‰ì€ ì¼ë‹¨ ë¬´ì‹œ\nì—¬ê¸°ì—ì„œ tempëŠ” í‰ê· ê¸°ì˜¨ì´ê³ , salesëŠ” ì•„ì´ìŠ¤ì•„ë©”ë¦¬ì¹´ë…¸ íŒë§¤ëŸ‰ì´ë‹¤. í‰ê· ê¸°ì˜¨ê³¼ íŒë§¤ëŸ‰ì˜ ê·¸ë˜í”„ë¥¼ ê·¸ë ¤ë³´ë©´ ì•„ë˜ì™€ ê°™ë‹¤.\n\nplt.plot(temp,sales,'o')\n\n\n\n\n\n\n\n\nğŸ—£ï¸ ì•½ê°„ì˜ ì˜¤ì°¨ëŠ” ìˆì§€ë§Œ ì„ ìœ¼ë¡œ ë³´ì„\nì˜¤ëŠ˜ ë°”ê¹¥ì˜ ì˜¨ë„ëŠ” 0.5ë„ ì´ë‹¤. ì•„ì´ìŠ¤ ì•„ë©”ë¼ì¹´ë…¸ë¥¼ ëª‡ì”ì •ë„ ë§Œë“¤ì–´ ë‘ë©´ ì¢‹ì„ê¹Œ?\nğŸ—£ï¸ ì´ ê·¸ë˜í”„ë¥¼ ë³´ê³  4.5ì” ì •ë„ë¡œ ì§ì‘ ê°€ëŠ¥"
  },
  {
    "objectID": "posts/01wk-2.html#b.-ê°€ì§œìë£Œë¥¼-ë§Œë“ -ë°©ë²•",
    "href": "posts/01wk-2.html#b.-ê°€ì§œìë£Œë¥¼-ë§Œë“ -ë°©ë²•",
    "title": "01wk-2, 02wk-1: (íšŒê·€) â€“ íšŒê·€ëª¨í˜•, ì†ì‹¤í•¨ìˆ˜, íŒŒì´í† ì¹˜ë¥¼ ì´ìš©í•œ ì¶”ì •",
    "section": "B. ê°€ì§œìë£Œë¥¼ ë§Œë“  ë°©ë²•",
    "text": "B. ê°€ì§œìë£Œë¥¼ ë§Œë“  ë°©ë²•\n- ë°©ë²•1: \\(y_i= w_0+w_1 x_i +\\epsilon_i = 2.5 + 4x_i +\\epsilon_i, \\quad i=1,2,\\dots,n\\)\nğŸ—£ï¸(\nxi = ì˜¨ë„ = temp\nyi = íŒë§¤ëŸ‰ = sales\níŒë§¤ëŸ‰ = 2.5 + 4*ì˜¨ë„ + ì˜¤ì°¨\n\ntorch.randn(10) # í‘œì¤€ì •ê·œë¶„í¬ì—ì„œ 10ê°œ ê°’ ì¶”ì¶œ, ê¸¸ì´ê°€ 10ì¸ vector (column vectorì¸ì§€ row vectorì¸ì§€ëŠ” ëª¨ë¦„)\n\ntensor([-0.4351, -0.4066,  1.2577, -1.1443,  0.3941, -0.2229, -0.4337,  0.8736,\n         0.6216,  1.0963])\n\n\n\ntorch.randn(100).sort() # 100ê°œ ê°’ì„ ì •ë ¬ / ì•ì€ ì •ë ¬ëœ ê°’, ë’¤ëŠ” ì¸ë±ìŠ¤\n\ntorch.return_types.sort(\nvalues=tensor([-3.3450e+00, -2.3363e+00, -1.7533e+00, -1.6534e+00, -1.4996e+00,\n        -1.4218e+00, -1.3757e+00, -1.3314e+00, -1.1898e+00, -1.1594e+00,\n        -1.1386e+00, -1.0975e+00, -1.0961e+00, -1.0899e+00, -1.0250e+00,\n        -9.7851e-01, -9.1254e-01, -8.8307e-01, -8.7845e-01, -8.4915e-01,\n        -7.4344e-01, -7.0972e-01, -7.0845e-01, -6.8746e-01, -6.7488e-01,\n        -6.6512e-01, -6.0503e-01, -5.8921e-01, -5.4838e-01, -5.1363e-01,\n        -5.0996e-01, -4.7537e-01, -4.3955e-01, -3.5707e-01, -3.4237e-01,\n        -3.4013e-01, -3.2890e-01, -3.2078e-01, -3.0216e-01, -2.9112e-01,\n        -2.8083e-01, -2.4387e-01, -2.4171e-01, -2.0109e-01, -1.9779e-01,\n        -1.9549e-01, -5.8397e-02, -2.5842e-02, -2.2056e-02,  2.0055e-03,\n         1.0348e-02,  2.2201e-02,  2.5445e-02,  2.6868e-02,  6.2116e-02,\n         1.3408e-01,  1.5172e-01,  2.0091e-01,  2.3218e-01,  2.5000e-01,\n         2.7442e-01,  2.8144e-01,  3.4857e-01,  3.7494e-01,  4.4520e-01,\n         4.8013e-01,  4.9466e-01,  5.0311e-01,  5.7595e-01,  6.2995e-01,\n         6.3221e-01,  6.5666e-01,  6.5788e-01,  6.6027e-01,  6.7909e-01,\n         7.1635e-01,  7.1752e-01,  7.2141e-01,  8.0059e-01,  8.0419e-01,\n         8.0801e-01,  8.1830e-01,  8.9444e-01,  9.6222e-01,  9.9973e-01,\n         1.1303e+00,  1.1527e+00,  1.2046e+00,  1.2086e+00,  1.2469e+00,\n         1.2752e+00,  1.2872e+00,  1.3125e+00,  1.4296e+00,  1.4390e+00,\n         1.5448e+00,  1.6129e+00,  1.6454e+00,  1.6769e+00,  1.7580e+00]),\nindices=tensor([81, 19, 56, 18, 89, 54, 27, 31, 65, 85, 94, 47,  0,  7,  8, 57, 14, 92,\n         3, 12, 86, 48,  9, 82, 62, 78,  1, 28, 32, 67, 21, 53, 10, 30, 23,  5,\n        88, 24, 63, 40, 20, 77, 34, 87, 99, 80, 41,  4, 69, 90, 35, 72, 58, 11,\n        22, 42, 76, 95, 74, 38, 46, 59, 91, 68, 43, 44, 50, 96, 51,  6, 29, 13,\n        66, 49, 73,  2, 70, 93, 97, 16, 15, 98, 55, 33, 39, 84, 25, 61, 17, 64,\n        45, 26, 75, 71, 79, 37, 60, 83, 36, 52]))\n\n\n\na = torch.randn(100).sort()\ntype(a)\n\ntorch.return_types.sort\n\n\n\na[0]\n\ntensor([-2.8188e+00, -2.7746e+00, -2.5355e+00, -2.4374e+00, -2.2716e+00,\n        -2.1492e+00, -1.8555e+00, -1.8281e+00, -1.6228e+00, -1.6164e+00,\n        -1.5151e+00, -1.5046e+00, -1.4989e+00, -1.4708e+00, -1.4605e+00,\n        -1.3748e+00, -1.3521e+00, -1.3183e+00, -1.2710e+00, -1.2416e+00,\n        -1.1459e+00, -1.0949e+00, -1.0907e+00, -1.0903e+00, -1.0481e+00,\n        -1.0313e+00, -1.0079e+00, -1.0003e+00, -9.9874e-01, -9.9081e-01,\n        -9.8943e-01, -9.7448e-01, -9.4772e-01, -9.4282e-01, -9.1282e-01,\n        -8.8605e-01, -8.6893e-01, -8.5283e-01, -7.8566e-01, -7.7867e-01,\n        -7.6961e-01, -7.4827e-01, -6.6928e-01, -6.3990e-01, -5.9842e-01,\n        -5.8057e-01, -5.5388e-01, -5.1941e-01, -5.1005e-01, -4.9040e-01,\n        -4.7796e-01, -3.9862e-01, -3.9854e-01, -3.8835e-01, -3.7719e-01,\n        -3.6587e-01, -3.0923e-01, -3.0278e-01, -2.5337e-01, -2.1358e-01,\n        -1.7441e-01, -1.4875e-01, -5.6163e-02, -3.3250e-02, -2.6646e-02,\n         2.1082e-03,  1.3442e-02,  9.5665e-02,  1.0434e-01,  1.2852e-01,\n         1.8255e-01,  2.2326e-01,  2.3160e-01,  2.5853e-01,  2.6803e-01,\n         3.3640e-01,  3.6288e-01,  3.7120e-01,  3.8451e-01,  4.0117e-01,\n         4.3763e-01,  4.5193e-01,  5.2404e-01,  6.1333e-01,  6.7461e-01,\n         6.8081e-01,  8.0477e-01,  9.1538e-01,  9.5395e-01,  1.0907e+00,\n         1.1139e+00,  1.1281e+00,  1.2559e+00,  1.2686e+00,  1.3258e+00,\n         1.3563e+00,  1.3864e+00,  1.5558e+00,  1.6258e+00,  2.1654e+00])\n\n\n\nx,_ = torch.randn(100).sort() # ì–¸íŒ¨í‚¹\nx\n\ntensor([-2.8984, -2.6607, -2.2449, -2.2072, -2.1918, -2.1538, -1.9428, -1.9416,\n        -1.8612, -1.6956, -1.6357, -1.4785, -1.4322, -1.2127, -1.1737, -0.9456,\n        -0.9244, -0.8456, -0.8190, -0.7925, -0.7609, -0.7305, -0.7011, -0.6806,\n        -0.6442, -0.6117, -0.6059, -0.5994, -0.4920, -0.4066, -0.3879, -0.3867,\n        -0.3612, -0.3604, -0.3142, -0.3112, -0.2940, -0.2812, -0.2753, -0.2665,\n        -0.2145, -0.2106, -0.1864, -0.1633, -0.1470, -0.1331, -0.1316, -0.0994,\n        -0.0954, -0.0717, -0.0586, -0.0329,  0.0095,  0.0182,  0.0214,  0.0915,\n         0.0952,  0.1077,  0.1124,  0.1612,  0.1614,  0.1969,  0.2003,  0.3242,\n         0.3424,  0.3925,  0.4078,  0.4468,  0.4536,  0.5199,  0.5238,  0.5563,\n         0.5595,  0.6236,  0.6372,  0.6451,  0.6630,  0.7122,  0.7335,  0.7569,\n         0.7589,  0.8969,  0.9318,  0.9552,  1.0023,  1.0198,  1.1083,  1.1978,\n         1.2752,  1.2928,  1.3265,  1.3825,  1.4325,  1.5292,  1.6095,  1.6239,\n         1.7316,  2.0886,  2.3070,  3.2682])\n\n\n\ntorch.manual_seed(43052) # ê°’ ê³ ì •\nx,_ = torch.randn(100).sort()\nx\n\ntensor([-2.4821, -2.3621, -1.9973, -1.6239, -1.4792, -1.4635, -1.4509, -1.4435,\n        -1.3722, -1.3079, -1.1904, -1.1092, -1.1054, -1.0875, -0.9469, -0.9319,\n        -0.8643, -0.7858, -0.7549, -0.7421, -0.6948, -0.6103, -0.5830, -0.5621,\n        -0.5506, -0.5058, -0.4806, -0.4738, -0.4710, -0.4676, -0.3874, -0.3719,\n        -0.3688, -0.3159, -0.2775, -0.2772, -0.2734, -0.2721, -0.2668, -0.2155,\n        -0.2000, -0.1816, -0.1708, -0.1565, -0.1448, -0.1361, -0.1057, -0.0603,\n        -0.0559, -0.0214,  0.0655,  0.0684,  0.1195,  0.1420,  0.1521,  0.1568,\n         0.2646,  0.2656,  0.3157,  0.3220,  0.3461,  0.3984,  0.4190,  0.5443,\n         0.5579,  0.5913,  0.6148,  0.6469,  0.6469,  0.6523,  0.6674,  0.7059,\n         0.7141,  0.7822,  0.8154,  0.8668,  0.9291,  0.9804,  0.9853,  0.9941,\n         1.0376,  1.0393,  1.0697,  1.1024,  1.1126,  1.1532,  1.2289,  1.3403,\n         1.3494,  1.4279,  1.4994,  1.5031,  1.5437,  1.6789,  2.0832,  2.2444,\n         2.3935,  2.6056,  2.6057,  2.6632])\n\n\n\n# temp # ìœ„ì˜ tempì™€ xëŠ” ë™ì¼\n\n\nsales[0] # -2.4821 * 4 + 2.5 + ì˜¤ì°¨\n\n-8.542\n\n\n\ntorch.manual_seed(43052)\nx,_ = torch.randn(100).sort()\neps = torch.randn(100)*0.5 # ì˜¤ì°¨ ë§Œë“¤ê¸° (ë¶„ì‚° ì‘ê²Œí•˜ë ¤ê³  0.5ë¥¼ ê³±í•¨)\n\n\n-2.4821 * 4 + 2.5 + eps[0] # sales[0]ê³¼ ë™ì¼\n\ntensor(-8.5420)\n\n\n\nx[1] * 4 + 2.5 + eps[1] # ë‘ ë²ˆì§¸ ê°’\n\ntensor(-6.5767)\n\n\n\nsales[1]\n\n-6.5767\n\n\n\ntorch.manual_seed(43052)\nx,_ = torch.randn(100).sort()\neps = torch.randn(100)*0.5\ny = x * 4 + 2.5 + eps # ë¸Œë¡œë“œìºìŠ¤íŒ… ì´ìš©\n\n\ntemp[:5],sales[:5]\n\n([-2.4821, -2.3621, -1.9973, -1.6239, -1.4792],\n [-8.542, -6.5767, -5.9496, -4.4794, -4.2516])\n\n\n\nx[:5], y[:5] # ìœ„ì™€ ë™ì¼\n\n(tensor([-2.4821, -2.3621, -1.9973, -1.6239, -1.4792]),\n tensor([-8.5420, -6.5767, -5.9496, -4.4794, -4.2516]))\n\n\n)ğŸ—£ï¸\n\ntorch.manual_seed(43052)\nx,_ = torch.randn(100).sort()\neps = torch.randn(100)*0.5\ny = x * 4 + 2.5 + eps\n\n\nx[:5], y[:5]\n\n(tensor([-2.4821, -2.3621, -1.9973, -1.6239, -1.4792]),\n tensor([-8.5420, -6.5767, -5.9496, -4.4794, -4.2516]))\n\n\n- ë°©ë²•2: \\({\\bf y}={\\bf X}{\\bf W} +\\boldsymbol{\\epsilon}\\)\n\n\\({\\bf y}=\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\dots \\\\ y_n\\end{bmatrix}, \\quad {\\bf X}=\\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\dots \\\\ 1 & x_n\\end{bmatrix}, \\quad {\\bf W}=\\begin{bmatrix} 2.5 \\\\ 4 \\end{bmatrix}, \\quad \\boldsymbol{\\epsilon}= \\begin{bmatrix} \\epsilon_1 \\\\ \\dots \\\\ \\epsilon_n\\end{bmatrix}\\)\n\nğŸ—£ï¸(\n\\(y_1 = 2.5 + 4x_1 + \\epsilon_1\\)\n\\(y_2 = 2.5 + 4x_2 + \\epsilon_2\\)\n\\(y_3 = 2.5 + 4x_3 + \\epsilon_3\\) â€¦ ì„ ìœ„ì™€ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆìŒ\në°©ë²•1ì€ scalarë¡œ í‘œí˜„, ë°©ë²•2ëŠ” matrixë¡œ í‘œí˜„\n\ny # ê¸¸ì´ê°€ 100ì¸ vector (ë°©ë²•1) / ë°©ë²•2ëŠ” (100,1) matrixë¡œ í‘œí˜„ë˜ì–´ì•¼ í•¨\n\ntensor([-8.5420, -6.5767, -5.9496, -4.4794, -4.2516, -3.1326, -4.0239, -4.1862,\n        -3.3403, -2.2027, -2.0262, -2.5619, -1.3353, -2.0466, -0.4664, -1.3513,\n        -1.6472, -0.1089, -0.3071, -0.6299, -0.0438,  0.4163,  0.4166, -0.0943,\n         0.2662,  0.4591,  0.8905,  0.8998,  0.6314,  1.3845,  0.8085,  1.2594,\n         1.1211,  1.9232,  1.0619,  1.3552,  2.1161,  1.1437,  1.6245,  1.7639,\n         1.6022,  1.7465,  0.9830,  1.7824,  2.1116,  2.8621,  2.1165,  1.5226,\n         2.5572,  2.8361,  3.3956,  2.0679,  2.8140,  3.4852,  3.6059,  2.5966,\n         2.8854,  3.9173,  3.6527,  4.1029,  4.3125,  3.4026,  3.2180,  4.5686,\n         4.3772,  4.3075,  4.4895,  4.4827,  5.3170,  5.4987,  5.4632,  6.0328,\n         5.2842,  5.0539,  5.4538,  6.0337,  5.7250,  5.7587,  6.2020,  6.5992,\n         6.4621,  6.5140,  6.6846,  7.3497,  8.0909,  7.0794,  6.8667,  7.4229,\n         7.2544,  7.1967,  9.5006,  9.0339,  7.4887,  9.0759, 11.0946, 10.3260,\n        12.2665, 13.0983, 12.5468, 13.8340])\n\n\n\nx # ê¸¸ì´ê°€ 100ì¸ vector (ë°©ë²•1) / ë°©ë²•2ëŠ” [1 x] ì´ëŸ°ì‹ìœ¼ë¡œ í‘œí˜„ë˜ì–´ì•¼ í•¨\n\ntensor([-2.4821, -2.3621, -1.9973, -1.6239, -1.4792, -1.4635, -1.4509, -1.4435,\n        -1.3722, -1.3079, -1.1904, -1.1092, -1.1054, -1.0875, -0.9469, -0.9319,\n        -0.8643, -0.7858, -0.7549, -0.7421, -0.6948, -0.6103, -0.5830, -0.5621,\n        -0.5506, -0.5058, -0.4806, -0.4738, -0.4710, -0.4676, -0.3874, -0.3719,\n        -0.3688, -0.3159, -0.2775, -0.2772, -0.2734, -0.2721, -0.2668, -0.2155,\n        -0.2000, -0.1816, -0.1708, -0.1565, -0.1448, -0.1361, -0.1057, -0.0603,\n        -0.0559, -0.0214,  0.0655,  0.0684,  0.1195,  0.1420,  0.1521,  0.1568,\n         0.2646,  0.2656,  0.3157,  0.3220,  0.3461,  0.3984,  0.4190,  0.5443,\n         0.5579,  0.5913,  0.6148,  0.6469,  0.6469,  0.6523,  0.6674,  0.7059,\n         0.7141,  0.7822,  0.8154,  0.8668,  0.9291,  0.9804,  0.9853,  0.9941,\n         1.0376,  1.0393,  1.0697,  1.1024,  1.1126,  1.1532,  1.2289,  1.3403,\n         1.3494,  1.4279,  1.4994,  1.5031,  1.5437,  1.6789,  2.0832,  2.2444,\n         2.3935,  2.6056,  2.6057,  2.6632])\n\n\n[1 x] ë§Œë“¤ê¸°\n\ntorch.ones(100) , x # ê¸¸ì´ê°€ 100ì¸ vector\n\n(tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n tensor([-2.4821, -2.3621, -1.9973, -1.6239, -1.4792, -1.4635, -1.4509, -1.4435,\n         -1.3722, -1.3079, -1.1904, -1.1092, -1.1054, -1.0875, -0.9469, -0.9319,\n         -0.8643, -0.7858, -0.7549, -0.7421, -0.6948, -0.6103, -0.5830, -0.5621,\n         -0.5506, -0.5058, -0.4806, -0.4738, -0.4710, -0.4676, -0.3874, -0.3719,\n         -0.3688, -0.3159, -0.2775, -0.2772, -0.2734, -0.2721, -0.2668, -0.2155,\n         -0.2000, -0.1816, -0.1708, -0.1565, -0.1448, -0.1361, -0.1057, -0.0603,\n         -0.0559, -0.0214,  0.0655,  0.0684,  0.1195,  0.1420,  0.1521,  0.1568,\n          0.2646,  0.2656,  0.3157,  0.3220,  0.3461,  0.3984,  0.4190,  0.5443,\n          0.5579,  0.5913,  0.6148,  0.6469,  0.6469,  0.6523,  0.6674,  0.7059,\n          0.7141,  0.7822,  0.8154,  0.8668,  0.9291,  0.9804,  0.9853,  0.9941,\n          1.0376,  1.0393,  1.0697,  1.1024,  1.1126,  1.1532,  1.2289,  1.3403,\n          1.3494,  1.4279,  1.4994,  1.5031,  1.5437,  1.6789,  2.0832,  2.2444,\n          2.3935,  2.6056,  2.6057,  2.6632]))\n\n\n\n# torch.stack([torch.ones(100) , x]) # ì¢Œìš°ë¡œ í•©ì¹˜ê¸° ìœ„í•´ stack ì‚¬ìš©\nprint(torch.stack([torch.ones(100) , x]).shape)\n# torch.stack([torch.ones(100) , x], axis=1) # ì›í–ˆë˜ ê²°ê³¼\nprint(torch.stack([torch.ones(100) , x], axis=1).shape)\n\n# torch.stack([torch.ones(100) , x]).T # ë‹¤ë¥¸ ë°©ë²•\nprint(torch.stack([torch.ones(100) , x]).T.shape)\n\ntorch.Size([2, 100])\ntorch.Size([100, 2])\ntorch.Size([100, 2])\n\n\n\nX = torch.stack([torch.ones(100) , x], axis=1)\nW = torch.tensor([[2.5],[4.0]])\ny = X@W + eps.reshape(100,1)\ny.shape\n\ntorch.Size([100, 1])\n\n\n\nsales[:5]\n\n[-8.542, -6.5767, -5.9496, -4.4794, -4.2516]\n\n\n\ny[:5,0]\n\ntensor([-8.5420, -6.5767, -5.9496, -4.4794, -4.2516])\n\n\nsalesì™€ y ë™ì¼\nğŸ”¬ğŸ—£ï¸(\n\n(ì°¸ê³ ) ì¸ë±ì‹± ê´€ë ¨ ì„¤ëª…\n\n\ny[:5]\n\ntensor([[-8.5420],\n        [-6.5767],\n        [-5.9496],\n        [-4.4794],\n        [-4.2516]])\n\n\nyëŠ” matrix ì´ë¯€ë¡œ\n\ny[:5,[0]] # column vectorì²˜ëŸ¼ ë¨\n\ntensor([[-8.5420],\n        [-6.5767],\n        [-5.9496],\n        [-4.4794],\n        [-4.2516]])\n\n\n\n# y[:,:] # yê°€ ê·¸ëŒ€ë¡œ ë‚˜ì˜´\n\n\ny[:5,:] # ê·¸ ì¤‘ 5ê°œë§Œ\n\ntensor([[-8.5420],\n        [-6.5767],\n        [-5.9496],\n        [-4.4794],\n        [-4.2516]])\n\n\në‚˜ì—´ ë°©ì‹ë§Œ ë‹¤ë¥´ê³  ê°’ì€ salesì™€ ë˜‘ê°™ìŒ\n)ğŸ”¬ğŸ—£ï¸\n\nX = torch.stack([torch.ones(100) , x], axis=1) # (100, 2)\nW = torch.tensor([[2.5],[4.0]]) # (2, 1)\ny = X@W + eps.reshape(100,1) # (100, 1)\nx # ì•„ë§ˆë„ (100,) \n\ntensor([-2.4821, -2.3621, -1.9973, -1.6239, -1.4792, -1.4635, -1.4509, -1.4435,\n        -1.3722, -1.3079, -1.1904, -1.1092, -1.1054, -1.0875, -0.9469, -0.9319,\n        -0.8643, -0.7858, -0.7549, -0.7421, -0.6948, -0.6103, -0.5830, -0.5621,\n        -0.5506, -0.5058, -0.4806, -0.4738, -0.4710, -0.4676, -0.3874, -0.3719,\n        -0.3688, -0.3159, -0.2775, -0.2772, -0.2734, -0.2721, -0.2668, -0.2155,\n        -0.2000, -0.1816, -0.1708, -0.1565, -0.1448, -0.1361, -0.1057, -0.0603,\n        -0.0559, -0.0214,  0.0655,  0.0684,  0.1195,  0.1420,  0.1521,  0.1568,\n         0.2646,  0.2656,  0.3157,  0.3220,  0.3461,  0.3984,  0.4190,  0.5443,\n         0.5579,  0.5913,  0.6148,  0.6469,  0.6469,  0.6523,  0.6674,  0.7059,\n         0.7141,  0.7822,  0.8154,  0.8668,  0.9291,  0.9804,  0.9853,  0.9941,\n         1.0376,  1.0393,  1.0697,  1.1024,  1.1126,  1.1532,  1.2289,  1.3403,\n         1.3494,  1.4279,  1.4994,  1.5031,  1.5437,  1.6789,  2.0832,  2.2444,\n         2.3935,  2.6056,  2.6057,  2.6632])\n\n\n(100,)ì„ (100,1)ë¡œ ë°”ê¾¸ê³  ì‹¶ìŒ\n\ntorch.manual_seed(43052)\nx,_ = torch.randn(100).sort()\neps = torch.randn(100)*0.5\ny = x * 4 + 2.5 + eps\n\nX = torch.stack([torch.ones(100) , x], axis=1) # (100, 2)\nW = torch.tensor([[2.5],[4.0]]) # (2, 1)\ny = X@W + eps.reshape(100,1) # (100, 1)\nx = X[:,[1]]\n\n\nx[:5], y[:5]\n\n(tensor([[-2.4821],\n         [-2.3621],\n         [-1.9973],\n         [-1.6239],\n         [-1.4792]]),\n tensor([[-8.5420],\n         [-6.5767],\n         [-5.9496],\n         [-4.4794],\n         [-4.2516]]))\n\n\n\ntemp[:5], sales[:5]\n\n([-2.4821, -2.3621, -1.9973, -1.6239, -1.4792],\n [-8.542, -6.5767, -5.9496, -4.4794, -4.2516])\n\n\në°©ë²• 2ì²˜ëŸ¼ matrixë¡œë„ ê°€ëŠ¥í•˜ë‹¤ëŠ” ê²ƒì„ í™•ì¸\n)ğŸ—£ï¸\nğŸ“(\nX = torch.stack([torch.ones(100),x],axis=1)\nW = torch.tensor([[2.5],[4.0]])\ny = X@W + eps.reshape(100,1)\nx = X[:,[1]]\nâœï¸ í¸ì˜ìƒ ìœ„ì˜ ì½”ë“œëŠ” ì‹¤í–‰ì‹œí‚¤ì§€ ì•ŠìŒ\n\nX[:5,:], y[:5,:]\n\n(tensor([[ 1.0000, -2.4821],\n         [ 1.0000, -2.3621],\n         [ 1.0000, -1.9973],\n         [ 1.0000, -1.6239],\n         [ 1.0000, -1.4792]]),\n tensor([[-8.5420],\n         [-6.5767],\n         [-5.9496],\n         [-4.4794],\n         [-4.2516]]))\n\n\n)ğŸ“\n- tureì™€ observed dataë¥¼ ë™ì‹œì— ì‹œê°í™”\nğŸ—£ï¸(\n\nplt.plot(temp, sales) # ì´ëŸ¬í•œ ë°ì´í„°ë¥¼ ê´€ì¸¡í–ˆë‹¤ê³  ìƒê°\n\n\n\n\n\n\n\n\n\nplt.plot(temp, sales, 'o') # scatter plot\n\n\n\n\n\n\n\n\n\nplt.plot(x, y, 'o') # ìœ„ì™€ ë™ì¼\n\n\n\n\n\n\n\n\nxì—ì„œ yë¡œ ê°€ëŠ” íŒ¨í„´ì„ ì°¾ê³  ì‹¶ìŒ\n\nplt.plot(x, y, 'o', label=\"observed data\") # ê´€ì¸¡í•œ ê°’\nplt.legend()\n\n\n\n\n\n\n\n\n\nplt.plot(x, y, 'o', label=\"observed data\") # ì ì„  + epsilon(í†µê³„ì ìœ¼ë¡œ ì„¤ëª…í•  ìˆ˜ ì—†ëŠ” í˜„ìƒ, random)\nplt.plot(x, 2.5 + 4*x, '--', label=\"true\") # ì›ë˜ ê´€ì¸¡ë˜ì–´ì•¼ í–ˆë˜ ê°’\nplt.legend()\n\n\n\n\n\n\n\n\n\ní•˜ê³  ì‹¶ì€ ê²ƒ\n\nì¹´í˜ ì£¼ì¸: ì˜¨ë„ê°€ 0.5ì¼ ë•Œ ì–¼ë§ˆë‚˜ íŒ”ë¦´ì§€ ì•Œê³  ì‹¶ìŒ\nê°€ì¥ ê°„ë‹¨: 0.5ë¥¼ ì ì„  ìœ„ì— ì˜¬ë¦° í›„ y ê°’ì„ ì˜ˆì¸¡ (0.5 * 4 + 2.5 = 4.5)\ní•˜ì§€ë§Œ ì‹¤ì œë¡œëŠ” íŒŒë€ìƒ‰ë§Œ ì•Œê³  ìˆìœ¼ë¯€ë¡œ ìœ„ì˜ ë°©ë²•ì€ cheating\n\n\n\nplt.plot(x,y,'o',label=r\"observed data: $(x_i,y_i)$\")\n#plt.plot(x,2.5+4*x,'--',label=r\"true: $(x_i, 4x_i+2.5)$ // $y=4x+2.5$ \")\nplt.legend()\n\n\n\n\n\n\n\n\n\ní•˜ê³  ì‹¶ì€ ê²ƒ\n\nìœ„ì˜ ìƒíƒœì—ì„œ ì ë‹¹í•œ ì¶”ì„¸ì„ ì„ ê·¸ë ¤ì„œ ì¶”ì •\n\n\n)ğŸ—£ï¸\n\nplt.plot(x,y,'o',label=r\"observed data: $(x_i,y_i)$\")\n#plt.plot(x,2.5+4*x,'--',label=r\"true: $(x_i, 4x_i+2.5)$ // $y=4x+2.5$ \")\nplt.legend()"
  },
  {
    "objectID": "posts/01wk-2.html#c.-íšŒê·€ë¶„ì„ì´ë€",
    "href": "posts/01wk-2.html#c.-íšŒê·€ë¶„ì„ì´ë€",
    "title": "01wk-2, 02wk-1: (íšŒê·€) â€“ íšŒê·€ëª¨í˜•, ì†ì‹¤í•¨ìˆ˜, íŒŒì´í† ì¹˜ë¥¼ ì´ìš©í•œ ì¶”ì •",
    "section": "C. íšŒê·€ë¶„ì„ì´ë€?",
    "text": "C. íšŒê·€ë¶„ì„ì´ë€?\n- í´ë¦¬ì…°: ê´€ì¸¡í•œ ìë£Œ \\((x_i,y_i)\\) ê°€ ìˆìŒ \\(\\to\\) ìš°ë¦¬ëŠ” \\((x_i,y_i)\\)ì˜ ê´€ê³„ë¥¼ íŒŒì•…í•˜ì—¬ ìƒˆë¡œìš´ \\(x\\)ê°€ ì™”ì„ë•Œ ê·¸ê²ƒì— ëŒ€í•œ ì˜ˆì¸¡ê°’(predicted value) \\(\\hat{y}\\)ì„ ì•Œì•„ë‚´ëŠ” ë²•ì¹™ì„ ì•Œê³  ì‹¶ìŒ \\(\\to\\) ê´€ê³„ë¥¼ íŒŒì•…í•˜ê¸° ìœ„í•´ì„œ \\((x_i, y_i)\\)ì˜ ì‚°ì ë„ë¥¼ ê·¸ë ¤ë³´ë‹ˆ \\(x_i\\)ì™€ \\(y_i\\)ëŠ” ì„ í˜•ì„±ì„ ê°€ì§€ê³  ìˆë‹¤ëŠ” ê²ƒì´ íŒŒì•…ë¨ \\(\\to\\) ì˜¤ì°¨í•­ì´ ë“±ë¶„ì‚°ì„±ì„ ê°€ì§€ê³  ì–´ì©Œê³  ì €ì©Œê³ â€¦ \\(\\to\\) í•˜ì—¬íŠ¼ \\((x_i,y_i)\\) ë¥¼ â€œì ë‹¹íˆ ì˜ ê´€í†µí•˜ëŠ”â€ ì–´ë– í•œ í•˜ë‚˜ì˜ ì¶”ì„¸ì„ ì„ ì˜ ì¶”ì •í•˜ë©´ ëœë‹¤.\n- íšŒê·€ë¶„ì„ì´ë€ ì‚°ì ë„ë¥¼ ë³´ê³  ì ë‹¹í•œ ì¶”ì„¸ì„ ì„ ì°¾ëŠ” ê²ƒì´ë‹¤. ì¢€ ë” ì •í™•í•˜ê²Œ ë§í•˜ë©´ \\((x_1,y_1) \\dots (x_n,y_n)\\) ìœ¼ë¡œ \\(\\begin{bmatrix} \\hat{w}_0 \\\\ \\hat{w}_1 \\end{bmatrix}\\) ë¥¼ ìµœëŒ€í•œ \\(\\begin{bmatrix} 2.5 \\\\ 4 \\end{bmatrix}\\)ì™€ ë¹„ìŠ·í•˜ê²Œ ì°¾ëŠ” ê²ƒ.\n\ngiven data : \\(\\big\\{(x_i,y_i) \\big\\}_{i=1}^{n}\\)\nparameter: \\({\\bf W}=\\begin{bmatrix} w_0 \\\\ w_1 \\end{bmatrix}\\)\nestimated parameter: \\({\\bf \\hat{W}}=\\begin{bmatrix} \\hat{w}_0 \\\\ \\hat{w}_1 \\end{bmatrix}\\)\n\nğŸ—£ï¸ y = ax + b ê¼´ì—ì„œ a, bë¥¼ ì •í•¨\n- ë” ì‰½ê²Œ ë§í•˜ë©´ ì•„ë˜ì˜ ê·¸ë¦¼ì„ ë³´ê³  â€œì ë‹¹í•œâ€ ì¶”ì„¸ì„ ì„ ì°¾ëŠ” ê²ƒì´ë‹¤.\n\nplt.plot(x,y,'o',label=r\"observed data: $(x_i,y_i)$\")\nplt.legend()\n\n\n\n\n\n\n\n\n- ì¶”ì„¸ì„ ì„ ê·¸ë¦¬ëŠ” í–‰ìœ„ = \\((w_0,w_1)\\)ì„ ì„ íƒí•˜ëŠ”ì¼"
  },
  {
    "objectID": "posts/01wk-2.html#a.-1ë‹¨ê³„-ìµœì´ˆì˜-ì ì„ ",
    "href": "posts/01wk-2.html#a.-1ë‹¨ê³„-ìµœì´ˆì˜-ì ì„ ",
    "title": "01wk-2, 02wk-1: (íšŒê·€) â€“ íšŒê·€ëª¨í˜•, ì†ì‹¤í•¨ìˆ˜, íŒŒì´í† ì¹˜ë¥¼ ì´ìš©í•œ ì¶”ì •",
    "section": "A. 1ë‹¨ê³„ â€“ ìµœì´ˆì˜ ì ì„ ",
    "text": "A. 1ë‹¨ê³„ â€“ ìµœì´ˆì˜ ì ì„ \nğŸ—£ï¸(\n\nWhat = torch.tensor([[-5.0],[10.0]], requires_grad=True)\nWhat\n\ntensor([[-5.],\n        [10.]], requires_grad=True)\n\n\n\nyhat = X@What\n\n\n# plt.plot(x, y, 'o')\n# plt.plot(x, yhat, '--')\n\n\nì‹¤í–‰ì‹œí‚¤ë©´ error\nrequires_grad=Trueë¥¼ ì—†ì• ë©´ error ë°œìƒ X\nrequires_grad=True\n\në¯¸ë¶„ì´ í•„ìš”í•¨ì„ ë‚˜íƒ€ë‚´ëŠ” ì˜µì…˜\nì§€ê¸ˆì€ ì˜ë¯¸ë¥¼ ì •í™•í•˜ê²Œ ì•Œ ìˆ˜ ì—†ì§€ë§Œ í¸ì˜ìƒ ì´ë¦„ì„ ë¯¸ë¶„ê¼¬ë¦¬í‘œë¼ê³  ë¶€ë¥´ê² ìŒ\n\n\n\nWhat+1\n\ntensor([[-4.],\n        [11.]], grad_fn=&lt;AddBackward0&gt;)\n\n\n\nê¼¬ë¦¬í‘œê°€ ë°”ë€Œê¸´ í•˜ë‚˜ í° ì§€ì¥ì€ ì—†ìŒ\n\n\n# yhat\n\n\nyhatì„ ì‹¤í–‰ì‹œì¼œë„ ê³„ì‚°ì„ ì˜ ë˜ë‚˜ ê¼¬ë¦¬í‘œê°€ ìˆìŒ\nê¼¬ë¦¬í‘œ ë•Œë¬¸ì— ê·¸ë˜í”„ë¥¼ ê·¸ë¦¬ë©´ errorê°€ ë°œìƒ\ní•´ê²°ì±… (ê¼¬ë¦¬í‘œë¥¼ ì œê±°í•œë‹¤ê³  ìƒê°, ê¼¬ë¦¬í‘œê°€ ìˆìœ¼ë©´ ê³„ì‚°ì€ ê°€ëŠ¥í•˜ë‚˜ ê·¸ë˜í”„ ê·¸ë¦¬ê¸° ë¶ˆê°€ëŠ¥)\n\nRuntimeError: Canâ€™t call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.\n.data\n\n\n\n# yhat.detach()\n\n\n# yhat.data\n\n\nplt.plot(x, y, 'o')\nplt.plot(x, yhat.detach(), '--') # ê·¸ë¦¼ì„ ê·¸ë¦¬ê¸° ìœ„í•´ì„œ yhatì˜ ë¯¸ë¶„ê¼¬ë¦¬í‘œë¥¼ ì œê±°\n\n\n\n\n\n\n\n\n)ğŸ—£ï¸\nğŸ—£ï¸ ê·¸ëƒ¥ ì•„ë¬´ ì§ì„ ì„ ê·¸ìŒ (2ë‹¨ê³„ë§Œ ì˜ ë˜ë©´ ìƒê´€ X)\n\nWhat = torch.tensor([[-5.0],[10.0]])\nWhat\n\ntensor([[-5.],\n        [10.]])\n\n\n\nyhat = X@What \n\n\nplt.plot(x,y,'o')\nplt.plot(x,yhat.data,'--')"
  },
  {
    "objectID": "posts/01wk-2.html#b.-2ë‹¨ê³„-update",
    "href": "posts/01wk-2.html#b.-2ë‹¨ê³„-update",
    "title": "01wk-2, 02wk-1: (íšŒê·€) â€“ íšŒê·€ëª¨í˜•, ì†ì‹¤í•¨ìˆ˜, íŒŒì´í† ì¹˜ë¥¼ ì´ìš©í•œ ì¶”ì •",
    "section": "B. 2ë‹¨ê³„ â€“ update",
    "text": "B. 2ë‹¨ê³„ â€“ update\n- â€™ì ë‹¹í•œ ì •ë„â€™ë¥¼ íŒë‹¨í•˜ê¸° ìœ„í•œ ì¥ì¹˜: loss function ë„ì…!\n\\[loss=\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2=\\sum_{i=1}^{n}(y_i-(\\hat{w}_0+\\hat{w}_1x_i))^2=({\\bf y}-{\\bf\\hat{y}})^\\top({\\bf y}-{\\bf\\hat{y}})=({\\bf y}-{\\bf X}{\\bf \\hat{W}})^\\top({\\bf y}-{\\bf X}{\\bf \\hat{W}})\\]\nğŸ—£ï¸ lossëŠ” \\((\\hat{w}_0, \\hat{w}_1)\\)ì„ ì…ë ¥ìœ¼ë¡œ ë°›ìŒ. loss ê°’ì„ ìµœì†Œë¡œ ë§Œë“œëŠ” \\((\\hat{w}_0, \\hat{w}_1)\\)ì„ ì°¾ìœ¼ë©´ ë¨.\n- loss í•¨ìˆ˜ì˜ íŠ¹ì§•: ìœ„ ê·¸ë¦¼ì˜ ì£¼í™©ìƒ‰ ì ì„ ì´ â€˜ì ë‹¹í•  ìˆ˜ë¡â€™ lossê°’ì´ ì‘ë‹¤.\n\nplt.plot(x,y,'o')\nplt.plot(x,yhat)\n\n\n\n\n\n\n\n\n\nloss = torch.sum((y-yhat)**2)\nloss\n\ntensor(8587.6875)\n\n\n- ìš°ë¦¬ì˜ ëª©í‘œ: ì´ loss(=8587.6275)ì„ ë” ì¤„ì´ì.\n\nê¶ê·¹ì ìœ¼ë¡œëŠ” ì•„ì˜ˆ ëª¨ë“  ì¡°í•© \\((\\hat{w}_0,\\hat{w}_1)\\)ì— ëŒ€í•˜ì—¬ ê°€ì¥ ì‘ì€ lossë¥¼ ì°¾ìœ¼ë©´ ì¢‹ê² ë‹¤.\n\n- ë¬¸ì œì˜ ì¹˜í™˜: ìƒê°í•´ë³´ë‹ˆê¹Œ ìš°ë¦¬ì˜ ë¬¸ì œëŠ” ì•„ë˜ì™€ ê°™ì´ ìˆ˜í•™ì ìœ¼ë¡œ ë‹¨ìˆœí™” ë˜ì—ˆë‹¤.\n\nê°€ì¥ ì ë‹¹í•œ ì£¼í™©ìƒ‰ ì„ ì„ ì°¾ì \\(\\to\\) \\(loss(\\hat{w}_0,\\hat{w}_1)\\)ë¥¼ ìµœì†Œë¡œí•˜ëŠ” \\((\\hat{w}_0,\\hat{w}_1)\\)ì˜ ê°’ì„ ì°¾ì.\n\n- ìˆ˜ì •ëœ ëª©í‘œ: \\(loss(\\hat{w}_0,\\hat{w}_1)\\)ë¥¼ ìµœì†Œë¡œ í•˜ëŠ” \\((\\hat{w}_0,\\hat{w}_1)\\)ì„ êµ¬í•˜ë¼.\n\në‹¨ìˆœí•œ ìˆ˜í•™ë¬¸ì œê°€ ë˜ì—ˆë‹¤. ì´ê²ƒì€ ë§ˆì¹˜ \\(f(x,y)\\)ë¥¼ ìµœì†Œí™”í•˜ëŠ” \\((x,y)\\)ë¥¼ ì°¾ìœ¼ë¼ëŠ” ê²ƒì„.\ní•¨ìˆ˜ì˜ ìµœëŒ€ê°’ í˜¹ì€ ìµœì†Œê°’ì„ ì»´í“¨í„°ë¥¼ ì´ìš©í•˜ì—¬ ì°¾ëŠ”ê²ƒì„ â€œìµœì í™”â€ë¼ê³  í•˜ë©° ì´ëŠ” ì‚°ê³µêµìˆ˜ë‹˜ë“¤ì´ ê°€ì¥ ì˜í•˜ëŠ” ë¶„ì•¼ì„. (ì‚°ê³µêµìˆ˜ë‹˜ë“¤ì—ê²Œ ë¶€íƒí•˜ë©´ ì˜í•´ì¤Œ, ì‚°ê³µêµìˆ˜ë‹˜ë“¤ì€ ë³´í†µ ìµœì í™”í•´ì„œ ì–´ë””ì— ì“¸ì§€ë³´ë‹¤ ìµœì í™” ìì²´ì— ë” ê´€ì‹¬ì„ ê°€ì§€ê³  ì—°êµ¬í•˜ì‹¬)\nìµœì í™”ë¥¼ í•˜ëŠ” ë°©ë²•? ê²½ì‚¬í•˜ê°•ë²•\n\n# ê²½ì‚¬í•˜ê°•ë²• ì•„ì´ë””ì–´ (1ì°¨ì›)\n\nì„ì˜ì˜ ì ì„ ì°ëŠ”ë‹¤.\nê·¸ ì ì—ì„œ ìˆœê°„ê¸°ìš¸ê¸°ë¥¼ êµ¬í•œë‹¤. (ì ‘ì„ ) &lt;â€“ ë¯¸ë¶„\nìˆœê°„ê¸°ìš¸ê¸°(=ë¯¸ë¶„ê³„ìˆ˜)ì˜ ë¶€í˜¸ë¥¼ ì‚´í´ë³´ê³  ë¶€í˜¸ì™€ ë°˜ëŒ€ë°©í–¥ìœ¼ë¡œ ì›€ì§ì¸ë‹¤.\n\n\níŒ: ê¸°ìš¸ê¸°ì˜ ì ˆëŒ€ê°’ í¬ê¸°ì™€ ë¹„ë¡€í•˜ì—¬ ë³´í­(=ì›€ì§ì´ëŠ” ì •ë„)ì„ ì¡°ì ˆí•œë‹¤. \\(\\to\\) \\(\\alpha\\)ë¥¼ ë„ì…\n\n\nìµœì¢…ìˆ˜ì‹: \\(\\hat{w} \\leftarrow \\hat{w} - \\alpha \\times \\frac{\\partial}{\\partial w}loss(w)\\)\n\n#\nğŸ—£ï¸(\n\në³´í­: step size\ní•¨ìˆ˜ë¥¼ ìµœê³ ì°¨í•­ì´ ì–‘ìˆ˜ì¸ 2ì°¨ í•¨ìˆ˜ë¡œ ìƒê°í•˜ë©´ ì´í•´í•˜ê¸° ì‰¬ì›€\n\nxì—ì„œ aë§Œí¼ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì´ë™: x + a\nxì—ì„œ aë§Œí¼ ì™¼ìª½ìœ¼ë¡œ ì´ë™: x - a\në¯¸ë¶„ê³„ìˆ˜ê°€ 0ì¸ìª½ìœ¼ë¡œ ì›€ì§ì¼ ë•Œ\n\nxê°€ ì˜¤ë¥¸ìª½ì— ìˆìœ¼ë©´ ë¯¸ë¶„ê³„ìˆ˜ &gt; 0\nxê°€ ì™¼ìª½ì— ìˆìœ¼ë©´ ë¯¸ë¶„ê³„ìˆ˜ &lt; 0\n\në¯¸ë¶„ê³„ìˆ˜ê°€ 0ì¸ìª½ê³¼ ê°€ê¹Œìš¸ìˆ˜ë¡ ì ‘ì„  ê¸°ìš¸ê¸°ì˜ ì ˆëŒ€ê°’ì´ ì‘ì•„ì§ -&gt; \\(\\alpha\\)ë¡œ ì¡°ì ˆ\n\n\\(\\alpha\\)ê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ ìˆ˜ë ´ ì†ë„ê°€ ëŠë¦´ ìˆ˜ ìˆê³ , ë„ˆë¬´ í¬ë©´ ìˆ˜ë ´ì„ ì•ˆí•  ìˆ˜ ìˆìŒ\n\nì˜ˆì‹œ) \\(f(x) = x^2\\) ì—ì„œ \\(x=2\\)ì¼ ë•Œ \\(\\alpha = 1\\)ì´ë©´ \\(x\\)ëŠ” \\(-2\\)ì™€ \\(2\\)ë§Œ ì™”ë‹¤ê°”ë‹¤ í•¨\n\n\n)ğŸ—£ï¸\n# ê²½ì‚¬í•˜ê°•ë²• ì•„ì´ë””ì–´ (2ì°¨ì›)\n\n\nì„ì˜ì˜ ì ì„ ì°ëŠ”ë‹¤.\nê·¸ ì ì—ì„œ ìˆœê°„ê¸°ìš¸ê¸°ë¥¼ êµ¬í•œë‹¤. (ì ‘í‰ë©´) &lt;â€“ í¸ë¯¸ë¶„\nìˆœê°„ê¸°ìš¸ê¸°(=ë¯¸ë¶„ê³„ìˆ˜)ì˜ ë¶€í˜¸ë¥¼ ì‚´í´ë³´ê³  ë¶€í˜¸ì™€ ë°˜ëŒ€ë°©í–¥ìœ¼ë¡œ ê°ê° ì›€ì§ì¸ë‹¤.\n\n\níŒ: ì—¬ê¸°ì„œë„ ê¸°ìš¸ê¸°ì˜ ì ˆëŒ€ê°’ í¬ê¸°ì™€ ë¹„ë¡€í•˜ì—¬ ë³´í­(=ì›€ì§ì´ëŠ” ì •ë„)ì„ ê°ê° ì¡°ì ˆí•œë‹¤. \\(\\to\\) \\(\\alpha\\)ë¥¼ ë„ì….\n\n#\nğŸ—£ï¸(\n\nì—¬ê¸°ì„œ ì„ì˜ì˜ ì ì€ 2ì°¨ì›\ní¸ë¯¸ë¶„: í•˜ë‚˜ë§Œ ë³€ìˆ˜ë¡œ ë³´ê³  ë‚˜ë¨¸ì§€ ê³ ì •\n\nì´í›„ 1ì°¨ì› ë°©ì‹ê³¼ ë™ì¼\nì–´ë–¤ ë°©í–¥(ì™¼ìª½, ì˜¤ë¥¸ìª½)ìœ¼ë¡œ ì–¼ë§ˆë‚˜ ê°ˆ ì§€(\\(\\alpha\\))\n\n\n)ğŸ—£ï¸\n- ê²½ì‚¬í•˜ê°•ë²• = lossë¥¼ ì¤„ì´ë„ë¡ \\({\\bf \\hat{W}}\\)ë¥¼ ê°œì„ í•˜ëŠ” ë°©ë²•\n\nì—…ë°ì´íŠ¸ ê³µì‹: ìˆ˜ì •ê°’ = ì›ë˜ê°’ - \\(\\alpha\\) \\(\\times\\) ê¸°ìš¸ì–´ì§„í¬ê¸°(=ë¯¸ë¶„ê³„ìˆ˜)\nì—¬ê¸°ì—ì„œ \\(\\alpha\\)ëŠ” ì „ì²´ì ì¸ ë³´í­ì˜ í¬ê¸°ë¥¼ ê²°ì •í•œë‹¤. ì¦‰ \\(\\alpha\\)ê°’ì´ í´ìˆ˜ë¡ í•œë²ˆì˜ updateì— ì›€ì§ì´ëŠ” ì–‘ì´ í¬ë‹¤.\n\nğŸ—£ï¸ \\(\\alpha\\)ë¥¼ MLì—ì„œëŠ” í•™ìŠµë¥ ì´ë¼ê³  í•¨\n- lossëŠ” \\(\\hat{\\bf W} =\\begin{bmatrix} \\hat{w}_0 \\\\ \\hat{w}_1 \\end{bmatrix}\\) ì— ë”°ë¼ì„œ ê°’ì´ ë°”ë€ŒëŠ” í•¨ìˆ˜ë¡œ í•´ì„ê°€ëŠ¥í•˜ê³  êµ¬ì²´ì ì¸ í˜•íƒœëŠ” ì•„ë˜ì™€ ê°™ìŒ.\n\\[ loss(\\hat{w}_0,\\hat{w}_1) := loss(\\hat{\\bf W})=\\sum_{i=1}^{n}(y_i-(\\hat{w}_0+\\hat{w}_1x_i))^2=({\\bf y}-{\\bf X}{\\bf \\hat{W}})^\\top({\\bf y}-{\\bf X}{\\bf \\hat{W}})\\]\në”°ë¼ì„œ êµ¬í•˜ê³  ì‹¶ì€ê²ƒì€ ì•„ë˜ì™€ ê°™ìŒ\n\\[\\hat{\\bf W}^{LSE} = \\underset{\\bf \\hat{W}}{\\operatorname{argmin}} ~ loss(\\hat{\\bf W})\\]\n\n\n\n\n\n\nWarning\n\n\n\nì•„ë˜ì˜ ìˆ˜ì‹\n\\[\\hat{\\bf W}^{LSE} = \\underset{\\bf \\hat{W}}{\\operatorname{argmin}} ~ loss(\\hat{\\bf W})\\]\nì€ ì•„ë˜ì™€ ê°™ì´ í‘œí˜„í•´ë„ ë¬´ë°©í•©ë‹ˆë‹¤.\n\\[\\hat{\\bf W} = \\underset{\\bf W}{\\operatorname{argmin}} ~ loss({\\bf W})\\]\në§ˆì¹˜ í•¨ìˆ˜ \\(f(\\hat{x})=({\\hat x}-1)^2\\) ì„ \\(f(x)=(x-1)^2\\) ì´ë¼ê³  í‘œí˜„í•  ìˆ˜ ìˆëŠ” ê²ƒ ì²˜ëŸ¼ìš”..\n\n\nì—¬ê¸°ê¹Œì§€ 01wk-2ì—ì„œ ìˆ˜ì—…í–ˆìŠµë‹ˆë‹¤~\n\nì—¬ê¸°ë¶€í„°ëŠ” 02wk-1ì—ì„œ..\n# ì§€ë‚œì‹œê°„ ë³µìŠµ\n\n# x,X,W,y // X = [1 x], W = [w0, w1]' # íšŒê·€ë¶„ì„ì—ì„œëŠ” W=Î²\n# íšŒê·€ëª¨í˜•: y=X@W+Ïµ = X@Î²+Ïµ\n# true: E(y)=X@W\n# observed: (x,y)\n# estimated W = What = [w0hat, w1hat]' &lt;-- ì•„ë¬´ê°’ì´ë‚˜ë„£ì—ˆìŒ.. \n# estimated y = yhat = X@What = X@Î²Ì‚ \n# loss = yhatì´ë‘ yë‘ ì–¼ë§ˆë‚˜ ë¹„ìŠ·í•œì§€ = sum((y-yhat)^2)\n# (x,y) ë³´ê³  ìµœì ì˜ ì„ ë¶„ì„ ê·¸ë¦¬ëŠ”ê²ƒ = lossë¥¼ ê°€ì¥ ì‘ê²Œ ë§Œë“œëŠ” What = [w0hat, w1hat] ë¥¼ ì°¾ëŠ”ê²ƒ\n# ì „ëµ: (1) ì•„ë¬´ Whatë‚˜ ì°ëŠ”ë‹¤ (2) ê·¸ê±°ë³´ë‹¤ ë” ë‚˜ì€ Whatì„ ì°¾ëŠ”ë‹¤. (3) 1-2ë¥¼ ë°˜ë³µí•œë‹¤. \n# ì „ëµ2ê°€ ì–´ë ¤ìš´ë°, ì´ë¥¼ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì´ ê²½ì‚¬í•˜ê°•ë²• \n# ê²½ì‚¬í•˜ê°•ë²• ì•Œê³ ë¦¬ì¦˜: ë”ë‚˜ì€What = ì›ë˜What - 0.1*ë¯¸ë¶„ê°’\n\n\nWhat = torch.tensor([[-5.0],[10.0]])\nWhat\n\ntensor([[-5.],\n        [10.]])\n\n\n\nyhat = X@What \nplt.plot(x,y,'o')\nplt.plot(x,yhat,'--')\n\n\n\n\n\n\n\n\n\nloss = torch.sum((y-yhat)**2)\nloss\n\ntensor(8587.6875)\n\n\në³µìŠµë~\n#\n- ë” ë‚˜ì€ ì„ ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•˜ê¸° ìœ„í•´ì„œëŠ” ê³µì‹ â€œë”ë‚˜ì€What = ì›ë˜What - 0.1*ë¯¸ë¶„ê°’â€ ë¥¼ ì ìš©í•´ì•¼í•˜ê³  ì´ë¥¼ ìœ„í•´ì„œëŠ” ë¯¸ë¶„ê°’ì„ ê³„ì‚°í•  ìˆ˜ ìˆì–´ì•¼ í•¨.\n\n\n\n\n\n\nImportant\n\n\n\nê²½ì‚¬í•˜ê°•ë²•ì„ ì¢€ ë” ì—„ë°€í•˜ê²Œ ì¨ë³´ì. ê²½ì‚¬í•˜ê°•ë²•ì€ \\(loss(\\hat{\\bf W})\\)ë¥¼ ìµœì†Œë¡œ ë§Œë“œëŠ” \\(\\hat{\\bf W}\\)ë¥¼ ì»´í“¨í„°ë¡œ êµ¬í•˜ëŠ” ë°©ë²•ì¸ë°, êµ¬ì²´ì ìœ¼ë¡œëŠ” ì•„ë˜ì™€ ê°™ë‹¤.\n1. ì„ì˜ì˜ ì  \\(\\hat{\\bf W}\\)ë¥¼ ì°ëŠ”ë‹¤.\n2. ê·¸ ì ì—ì„œ ìˆœê°„ê¸°ìš¸ê¸°ë¥¼ êµ¬í•œë‹¤. ì¦‰ \\(\\left.\\frac{\\partial}{\\partial {\\bf W}}loss({\\bf W})\\right|_{{\\bf W}=\\hat{\\bf W}}\\) ë¥¼ ê³„ì‚°í•œë‹¤.\n3. \\(\\hat{\\bf W}\\)ì—ì„œì˜ ìˆœê°„ê¸°ìš¸ê¸°ì˜ ë¶€í˜¸ë¥¼ ì‚´í´ë³´ê³  ë¶€í˜¸ì™€ ë°˜ëŒ€ë°©í–¥ìœ¼ë¡œ ì›€ì§ì¸ë‹¤. ì´ë•Œ ê¸°ìš¸ê¸°ì˜ ì ˆëŒ€ê°’ í¬ê¸°ì™€ ë¹„ë¡€í•˜ì—¬ ë³´í­(=ì›€ì§ì´ëŠ” ì •ë„)ì„ ê°ê° ì¡°ì ˆí•œë‹¤. ì¦‰ ì•„ë˜ì˜ ìˆ˜ì‹ì— ë”°ë¼ ì—…ë°ì´íŠ¸ í•œë‹¤.\n\\[\\hat{\\bf W} \\leftarrow \\hat{\\bf W} - \\alpha \\times \\left.\\frac{\\partial}{\\partial {\\bf W}}loss({\\bf W})\\right|_{{\\bf W}=\\hat{\\bf W}}\\]\nì—¬ê¸°ì—ì„œ ë§¨ ë§ˆì§€ë§‰ ìˆ˜ì‹ì„ ê°„ë‹¨í•˜ê²Œ ì“´ ê²ƒì´ ë”ë‚˜ì€What = ì›ë˜What - 0.1*ë¯¸ë¶„ê°’ ì´ë‹¤.\n\n\n- ë¯¸ë¶„ê°’ì„ ê³„ì‚°í•˜ëŠ” ë°©ë²•1\n\n# ì†ì‹¤ 8587.6875 ë¥¼ ê³„ì‚°í•˜ëŠ” ë˜ ë‹¤ë¥¸ ë°©ì‹\ndef l(w0,w1):\n    yhat = w0 + w1*x\n    return torch.sum((y-yhat)**2)\n\n\nl(-5,10)\n\ntensor(8587.6875)\n\n\nğŸ—£ï¸(\n\nêµ³ì´ í•¨ìˆ˜ë¥¼ ë§Œë“  ì´ìœ : ë¯¸ë¶„í•˜ë ¤ê³ \ní¸ë¯¸ë¶„ êµ¬í˜„\n\nl(-5,10)\n(l(w0+h,w1) - l(w0,w1))/h: ë„í•¨ìˆ˜\n\n\n)ğŸ—£ï¸\n\nh=0.001\nprint((l(-5+h,10) - l(-5,10))/h)\nprint((l(-5,10+h) - l(-5,10))/h)\n\ntensor(-1341.7968)\ntensor(1190.4297)\n\n\nì¼ë‹¨ ì´ê±°ë¡œ ì—…ë°ì´íŠ¸í•´ë³¼ê¹Œ?\n\n# ë”ë‚˜ì€What = ì›ë˜What - 0.1*ë¯¸ë¶„ê°’\n# [-5,10] - 0.001 * [-1341.7968,1190.4297]\n\n\nsssss = What - 0.001 * torch.tensor([[-1341.7968],[1190.4297]])\nsssss\n\ntensor([[-3.6582],\n        [ 8.8096]])\n\n\n\nplt.plot(x,y,'o')\nplt.plot(x,X@What,'-') # ì›ë˜What: ì£¼í™©ìƒ‰\nplt.plot(x,X@sssss,'-') # ë”ë‚˜ì€What: ì´ˆë¡ìƒ‰\n\n\n\n\n\n\n\n\n\nì˜ ëœ ê²ƒ ê°™ê¸´í•œë°..\në¯¸ë¶„êµ¬í•˜ëŠ”ê²Œ ë„ˆë¬´ ì–´ë ¤ì›Œ..\në‹¤ë¥¸ ë°©ë²• ì—†ì„ê¹Œ?\n\n\n\n\n\n\n\nImportant\n\n\n\nì‚¬ì‹¤ ì´ ë°©ë²•ì€\n\n\\(\\frac{\\partial}{\\partial w_0}loss(w_0,w_1) \\approx \\frac{loss(w_0+h,w_1)-loss(w_0,w_1)}{h}\\)\n\\(\\frac{\\partial}{\\partial w_1}loss(w_0,w_1) \\approx \\frac{loss(w_0,w_1+h)-loss(w_0,w_1)}{h}\\)\n\nì´ ê³„ì‚°ì„ ì´ìš©í•˜ì—¬\n\\[\\frac{\\partial}{\\partial {\\bf W}}loss({\\bf W}):= \\begin{bmatrix} \\frac{\\partial}{\\partial w_0} \\\\ \\frac{\\partial}{\\partial w_1}\\end{bmatrix}loss({\\bf W}) =  \\begin{bmatrix} \\frac{\\partial}{\\partial w_0}loss({\\bf W}) \\\\ \\frac{\\partial}{\\partial w_1}loss({\\bf W})\\end{bmatrix}  =  \\begin{bmatrix} \\frac{\\partial}{\\partial w_0}loss(w_0,w_1) \\\\ \\frac{\\partial}{\\partial w_1}loss(w_0,w_1)\\end{bmatrix}\\]\në¥¼ ê³„ì‚°í•œ ê²ƒì´ë¼ ë³¼ ìˆ˜ ìˆì£ \n\n\n- ë¯¸ë¶„ê°’ì„ ê³„ì‚°í•˜ëŠ” ë°©ë²•2\n\n## ì•½ê°„ì˜ ì§€ì‹ì´ í•„ìš”í•¨. \n# loss = (y-XWhat)'(y-XWhat)\n# = (y'-What'X')(y-XWhat)\n# = y'y-y'XWhat -What'X'y + What'X'XWhat \n# lossë¥¼ Whatìœ¼ë¡œ ë¯¸ë¶„\n# loss' = -X'y - X'y + 2X'XWhat\n\nâ“ í–‰ë ¬ ë¯¸ë¶„ ë³µìŠµ í•„ìš”\n\n-2*X.T@y + 2*X.T@X@What\n\ntensor([[-1342.2524],\n        [ 1188.9302]])\n\n\nğŸ—£ï¸ ì•½ê°„ì˜ ì˜¤ì°¨ëŠ” ìˆì§€ë§Œ ìœ„ì™€ ë¹„ìŠ· (ê·¸ëŸ¬ë‚˜ ë°©ë²•1, ë°©ë²•2 ë§ê³  ë‹¤ë¥¸ ë°©ë²•ì„ ì“°ê³  ì‹¶ìŒ)\n\n\n\n\n\n\nImportant\n\n\n\nì´ ë°©ë²•ì€ \\(loss({\\bf W})\\)ì˜ ë¯¸ë¶„ì„ êµ¬í• ìˆ˜ ìˆì–´ì•¼ ì‚¬ìš©ê°€ëŠ¥í•©ë‹ˆë‹¤. ì¦‰\n\\[\\frac{\\partial}{\\partial {\\bf W}}loss({\\bf W})= -2{\\bf X}^\\top {\\bf y} + 2{\\bf X}^\\top {\\bf X}{\\bf W}\\]\në¥¼ ê³„ì‚°í•  ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.\n\n\n- ë¯¸ë¶„ê°’ì„ ê³„ì‚°í•˜ëŠ” ë°©ë²•3 â€“ ì´ íŒ¨í„´ì„ ì™¸ìš°ì„¸ì—¬\n\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad=True)\nWhat\n\ntensor([[-5.],\n        [10.]], requires_grad=True)\n\n\n\nyhat = X@What\nloss = torch.sum((y-yhat)**2)\nloss\n\ntensor(8587.6875, grad_fn=&lt;SumBackward0&gt;)\n\n\nğŸ—£ï¸ ê¼¬ë¦¬í‘œê°€ ìˆê¸´í•˜ì§€ë§Œ ê²°ê³¼ëŠ” ìœ„ì™€ ë™ì¼\n\nloss.backward() # lossë¥¼ ë¯¸ë¶„í•˜ë¼.. ê¼¬ë¦¬í‘œê°€ ìˆê²Œ í•œ Whatìœ¼ë¡œ.. \n\nğŸ—£ï¸(\n\nlossë¥¼ Whatìœ¼ë¡œ ë¯¸ë¶„\nì¼ë°˜ì ìœ¼ë¡œ ë¯¸ë¶„ì„ í•˜ë©´ ë„í•¨ìˆ˜ê°€ ë‚˜ì˜¤ì§€ë§Œ, ì´ ê²½ìš°ëŠ” ë„í•¨ìˆ˜ì—ì„œ í˜„ì¬ Whatê°’ì„ ëŒ€ì…í•œ ê²°ê³¼ê°€ ë‚˜ì˜´\nì •í™•íˆ ë§í•˜ë©´ Whatì— í•´ë‹¹í•˜ëŠ” ì ‘ì„ ì˜ ê¸°ìš¸ê¸°\nì‹¤í–‰í•´ë„ ì‹¤í–‰ê²°ê³¼ëŠ” ë‚˜ì˜¤ì§€ ì•ŠìŒ. ê²°ê³¼ëŠ” What.gradì— ì €ì¥ë˜ì–´ ìˆìŒ\n\n)ğŸ—£ï¸\n\nWhat.grad\n\ntensor([[-1342.2524],\n        [ 1188.9305]])\n\n\n- ìœ„ì˜ ì½”ë“œë¥¼ ë‹¤ì‹œ ë³µìŠµí•´ë³´ì.\nâ€“ loss.backward()ì‹¤í–‰ì „ â€“\n\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad=True)\nyhat = X@What\nloss = torch.sum((y-yhat)**2)\n\n\nWhat.data, What.grad\n\n(tensor([[-5.],\n         [10.]]),\n None)\n\n\nğŸ—£ï¸ .backward()ë¥¼ ì‹¤í–‰í•˜ì§€ ì•Šì•„ì„œ .gradì— ì•„ë¬´ ê°’ë„ ì—†ìŒ(Noneìœ¼ë¡œ ì´ˆê¸°í™” ë¨)\nâ€“ loss.backward()ì‹¤í–‰í›„ â€“\n\nloss.backward()\n\n\nWhat.data, What.grad\n\n(tensor([[-5.],\n         [10.]]),\n tensor([[-1342.2524],\n         [ 1188.9305]]))\n\n\nğŸ—£ï¸(\n\n.backward()ë¥¼ ì‹¤í–‰í•˜ë‹ˆ .gradì— ê¸°ìš¸ê¸° ê°’ì´ ê³„ì‚°ë˜ì–´ ì—…ë°ì´íŠ¸ ë¨\nloss.backward(): What.grad &lt;- Whatì—ì„œ ë¯¸ë¶„ê°’ ì¸ì¤„ ì•Œì•˜ìœ¼ë‚˜ ì‚¬ì‹¤ì€\nloss.backward(): What.grad &lt;- What.grad + Whatì—ì„œ ë¯¸ë¶„ê°’ (ì¦‰, ëˆ„ì ì„ ì‹œì¼œì„œ ë”í•¨)\n\n\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad=True)\n\n\nyhat = X@What\nloss = torch.sum((y-yhat)**2)\nloss.backward()\n\n\nWhat.data, What.grad\n\n(tensor([[-5.],\n         [10.]]),\n tensor([[-1342.2524],\n         [ 1188.9305]]))\n\n\n\nyhat = X@What\nloss = torch.sum((y-yhat)**2)\nloss.backward()\n\n\nWhat.data, What.grad\n\n(tensor([[-5.],\n         [10.]]),\n tensor([[-2684.5049],\n         [ 2377.8611]]))\n\n\n\në‘ ë°°ê°€ ë¨\nì™œ?\n\nì‚°ê³µ: ì•Œê³ ë¦¬ì¦˜ ìƒì—ì„œëŠ” What.gradì˜ ê°’ì€ loss.backward()ë¥¼ í• ë•Œë§ˆë‹¤ ì´ˆê¸°í™”ê°€ ë§ìŒ (ì´ë¡ ì ìœ¼ë¡œëŠ” ì´ê²Œ ë§ìŒ)\nì»´ê³µ: ê·¸ëŸ¬ë©´ ë‚˜ì¤‘ì— ê³„ì‚° íš¨ìœ¨ì´ ì•ˆ ì¢‹ì•„ì§ (ì›¬ë§Œí•˜ë©´ ê³„ì‚°í•œ ë¯¸ë¶„ê°’ì„ ê°–ê³  ìˆê³  ì‹¶ìŒ, í•„ìš” ì—†ìœ¼ë©´ ë”°ë¡œ ì´ˆê¸°í™”í•˜ë©´ ë¨)\ní†µê³„: ìµœì í™”ì™€ ë¯¸ë¶„ ë¹¨ë¦¬í•˜ëŠ” ê²ƒì— ê´€ì‹¬ X\n\n\n)ğŸ—£ï¸\nâœï¸ ì´í›„ ì›í™œí•œ ì½”ë“œ ì‹¤í–‰ì„ ìœ„í•œ ì½”ë“œ (ì˜ë¯¸X)\n\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad=True)\nyhat = X@What\nloss = torch.sum((y-yhat)**2)\n\nWhat.data, What.grad\n\nloss.backward()\n\nWhat.data, What.grad\n\n(tensor([[-5.],\n         [10.]]),\n tensor([[-1342.2524],\n         [ 1188.9305]]))\n\n\n# 1íšŒ ì—…ë°ì´íŠ¸ ê³¼ì •ì„ ì°¨ê·¼ì°¨ê·¼ ì‹œê°í™”í•˜ë©° ì •ë¦¬í•´ë³´ì.\n\nalpha = 0.001 \nprint(f\"{What.data} -- ìˆ˜ì •ì „\")\nprint(f\"{-alpha*What.grad} -- ìˆ˜ì •í•˜ëŠ”í­\")\nprint(f\"{What.data-alpha*What.grad} -- ìˆ˜ì •í›„\")\nprint(f\"{torch.tensor([[2.5],[4]])} -- ì°¸ê°’(ì´ê±´ ë¹„ë°€~~)\")\n\ntensor([[-5.],\n        [10.]]) -- ìˆ˜ì •ì „\ntensor([[ 1.3423],\n        [-1.1889]]) -- ìˆ˜ì •í•˜ëŠ”í­\ntensor([[-3.6577],\n        [ 8.8111]]) -- ìˆ˜ì •í›„\ntensor([[2.5000],\n        [4.0000]]) -- ì°¸ê°’(ì´ê±´ ë¹„ë°€~~)\n\n\nğŸ—£ï¸(\n\n\\(\\alpha\\)ë¥¼ 0.001ë¡œ ì¡ì€ ì´ìœ : ë¯¸ë¶„ê°’ì´ 1000 ë‹¨ìœ„ë¡œ ë‚˜ì™€ì„œ ê·¸ëŒ€ë¡œ ë„£ìœ¼ë©´ ì›í•˜ëŠ” ê²°ê³¼ê°€ ì•ˆ ë‚˜ì˜¬ ê²ƒ ê°™ìŒ\n\nì˜ ìˆ˜ë ´ë ë•Œê¹Œì§€ ì‹œí–‰ì°©ì˜¤ë¥¼ ê²ªìœ¼ë©° í•´ë´ì•¼ í•¨\n\nìˆ˜ì •í•˜ëŠ” í­: ìœ„ ê·¸ë˜í”„ì—ì„œ ì£¼í™©ìƒ‰ ì„ \nìˆ˜ì • í›„: ìœ„ ê·¸ë˜í”„ì—ì„œ ì´ˆë¡ìƒ‰ ì„ \nìˆ˜ì • ì „ë³´ë‹¤ ìˆ˜ì • í›„ê°€ ì°¸ê°’ì— ê°€ê¹Œìš°ë¯€ë¡œ ì˜¬ë°”ë¥¸ ë°©í–¥ì„ ì§„í–‰ë˜ê³  ìˆìŒì„ ì•Œ ìˆ˜ ìˆìŒ\n\n)ğŸ—£ï¸\n\nWbefore = What.data\nWafter = What.data - alpha * What.grad \nWbefore, Wafter\n\n(tensor([[-5.],\n         [10.]]),\n tensor([[-3.6577],\n         [ 8.8111]]))\n\n\n\nplt.plot(x,y,'o',label=r'observed data')\nplt.plot(x,X@Wbefore,'--', label=r\"$\\hat{\\bf y}_{before}={\\bf X}@\\hat{\\bf W}_{before}$\")\nplt.plot(x,X@Wafter,'--', label=r\"$\\hat{\\bf y}_{after}={\\bf X}@\\hat{\\bf W}_{after}$\")\nplt.legend()\n\n\n\n\n\n\n\n\n#"
  },
  {
    "objectID": "posts/01wk-2.html#c.-3ë‹¨ê³„-iteration-learn-estimate-bfhat-w",
    "href": "posts/01wk-2.html#c.-3ë‹¨ê³„-iteration-learn-estimate-bfhat-w",
    "title": "01wk-2, 02wk-1: (íšŒê·€) â€“ íšŒê·€ëª¨í˜•, ì†ì‹¤í•¨ìˆ˜, íŒŒì´í† ì¹˜ë¥¼ ì´ìš©í•œ ì¶”ì •",
    "section": "C. 3ë‹¨ê³„ â€“ iteration (=learn = estimate \\(\\bf{\\hat W}\\))",
    "text": "C. 3ë‹¨ê³„ â€“ iteration (=learn = estimate \\(\\bf{\\hat W}\\))\n- ì´ì œ 1ë‹¨ê³„ì™€ 2ë‹¨ê³„ë¥¼ ë°˜ë³µë§Œí•˜ë©´ëœë‹¤. ê·¸ë˜ì„œ ì•„ë˜ì™€ ê°™ì€ ì½”ë“œë¥¼ ì‘ì„±í•˜ë©´ ë  ê²ƒ ê°™ì€ë°â€¦\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad=True) # ìµœì´ˆì˜ ì§ì„ ì„ ë§Œë“œëŠ” ê°’\nfor epoc in range(30):\n    yhat = X@What \n    loss = torch.sum((y-yhat)**2)\n    loss.backward()\n    What.data = What.data - 0.001 * What.grad\nëŒë ¤ë³´ë©´ ì˜ ì•ˆëœë‹¤.\nğŸ—£ï¸ ì›ë˜ ì² ìëŠ” epochì´ì§€ë§Œ í¸ì˜ìƒ epocìœ¼ë¡œ ì‘ì„±, ì˜ ë˜ê¸° ìœ„í•´ì„œëŠ” ë§ˆì§€ë§‰ì— ì´ˆê¸°í™”ë¥¼ í•´ì¤˜ì•¼ í•¨\n- ì•„ë˜ì™€ ê°™ì´ í•´ì•¼í•œë‹¤.\n\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad=True) # ìµœì´ˆì˜ ì§ì„ ì„ ë§Œë“œëŠ” ê°’\nfor epoc in range(30):\n    yhat = X@What \n    loss = torch.sum((y-yhat)**2)\n    loss.backward()\n    What.data = What.data - 0.001 * What.grad\n    What.grad = None \n\n\nplt.plot(x,y,'o',label=r\"observed: $(x_i,y_i)$\")\nplt.plot(x,X@What.data,'--o', label=r\"estimated: $(x_i,\\hat{y}_i)$ -- after 30 iterations (=epochs)\", alpha=0.4 )\nplt.legend()\n\n\n\n\n\n\n\n\n- ì™œ? loss.backward() ëŠ” ì•„ë˜ì˜ ì—­í• ì„ í•˜ëŠ”ê²ƒ ì²˜ëŸ¼ ì´í•´ë˜ì—ˆì§€ë§Œ\n\nWhat.grad \\(\\leftarrow\\) Whatì—ì„œë¯¸ë¶„ê°’\n\nì‹¤ì œë¡œëŠ” ì•„ë˜ì˜ ì—­í• ì„ ìˆ˜í–‰í•˜ê¸° ë•Œë¬¸ì´ë‹¤. (ì»´í“¨í„°ê³µí•™ì ì¸ ì´ìœ ë¡œ..)\n\nWhat.grad \\(\\leftarrow\\) What.grad + Whatì—ì„œë¯¸ë¶„ê°’\n\n\n\n\n\n\n\nNote\n\n\n\nWhat.grad \\(\\leftarrow\\) What.grad + Whatì—ì„œë¯¸ë¶„ê°’ ì„ì„ í™•ì¸í•˜ê¸° ìœ„í•´ì„œ.. ì•½ê°„ì˜ í…ŒìŠ¤íŠ¸ë¥¼ í–ˆìŠµë‹ˆë‹¤.\në¨¼ì €\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad=True) # ìµœì´ˆì˜ ì§ì„ ì„ ë§Œë“œëŠ” ê°’\nprint(What.data)\nprint(What.grad)\në¥¼ í™•ì¸í•œë’¤ ì•„ë˜ë¥¼ ë°˜ë³µì‹¤í–‰í•´ë´¤ì„ë•Œ\nyhat = X@What \nloss = torch.sum((y-yhat)**2)\nloss.backward() # \nprint(What.data)\nprint(What.grad)\nWhat.dataì™€ What.grad ê°’ì´ ê³„ì† ì¼ì •í•˜ê²Œ ë‚˜ì˜¨ë‹¤ë©´\n\nWhat.grad \\(\\leftarrow\\) Whatì—ì„œë¯¸ë¶„ê°’\n\nì´ì™€ ê°™ì€ ê³„ì‚°ì´ ì§„í–‰ë˜ëŠ” ê²ƒì´ê² ê³ , What.gradì˜ ê°’ì´ ìê¾¸ ì»¤ì§„ë‹¤ë©´\n\nWhat.grad \\(\\leftarrow\\) What.grad + Whatì—ì„œë¯¸ë¶„ê°’\n\nì´ì™€ ê°™ì€ ê³„ì‚°ì´ ì§„í–‰ë˜ëŠ” ê²ƒì´ê² ì£ ?"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deep Learning",
    "section": "",
    "text": "Based on: https://guebin.github.io/DL2025/\n\n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nMar 24, 2025\n\n\n03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„\n\n\nsw1kwon \n\n\n\n\nMar 19, 2025\n\n\n03wk-1: (íšŒê·€, ë¡œì§€ìŠ¤í‹±) â€“ íŒŒì´í† ì¹˜ì‹ ì½”ë”©íŒ¨í„´ (2), ë¡œì§€ìŠ¤í‹± ëª¨í˜•\n\n\nsw1kwon \n\n\n\n\nMar 17, 2025\n\n\n02wk-2: (íšŒê·€) â€“ íŒŒë¼ë©”í„°ì˜ í•™ìŠµê³¼ì • ìŒë¯¸, MSE, íŒŒì´í† ì¹˜ì‹ ì½”ë”©íŒ¨í„´ (1)\n\n\nsw1kwon \n\n\n\n\nMar 10, 2025\n\n\n01wk-2, 02wk-1: (íšŒê·€) â€“ íšŒê·€ëª¨í˜•, ì†ì‹¤í•¨ìˆ˜, íŒŒì´í† ì¹˜ë¥¼ ì´ìš©í•œ ì¶”ì •\n\n\nsw1kwon \n\n\n\n\nMar 5, 2025\n\n\n01wk-1: (í† ì¹˜) â€“ ê°•ì˜ì†Œê°œ, íŒŒì´í† ì¹˜ ê¸°ë³¸\n\n\nsw1kwon \n\n\n\n\nJan 1, 2025\n\n\nA1: Exercise â€“ ver. 0503-1\n\n\nsw1kwon \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/03wk-1.html",
    "href": "posts/03wk-1.html",
    "title": "03wk-1: (íšŒê·€, ë¡œì§€ìŠ¤í‹±) â€“ íŒŒì´í† ì¹˜ì‹ ì½”ë”©íŒ¨í„´ (2), ë¡œì§€ìŠ¤í‹± ëª¨í˜•",
    "section": "",
    "text": "ğŸ“˜ Note Format Guide\nThis format serves as a structured guide for organizing lecture content, personal interpretation, experiments, and study-related questions.\nğŸ“ ğŸ—£ï¸ âœï¸ ğŸ”¬ â“"
  },
  {
    "objectID": "posts/03wk-1.html#a.-biasì˜-ì‚¬ìš©",
    "href": "posts/03wk-1.html#a.-biasì˜-ì‚¬ìš©",
    "title": "03wk-1: (íšŒê·€, ë¡œì§€ìŠ¤í‹±) â€“ íŒŒì´í† ì¹˜ì‹ ì½”ë”©íŒ¨í„´ (2), ë¡œì§€ìŠ¤í‹± ëª¨í˜•",
    "section": "A. biasì˜ ì‚¬ìš©",
    "text": "A. biasì˜ ì‚¬ìš©\nğŸ—£ï¸(\n\nì €ë²ˆ ì‹œê°„ ì½”ë“œ\n\n\nnet = torch.nn.Linear(2, 1, bias=False)\nnet.weight.data = torch.tensor([[-5.0, 10.0]])\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD(net.parameters(), lr=0.1) # lr: learning rate\n\n# step 1~4\nfor epoc in range(30):\n    # 1\n    yhat = net(X)\n    # 2\n    loss = loss_fn(yhat,y)\n    # 3\n    loss.backward()\n    # 4\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nnet.weight # ì§€ë‚œ ì‹œê°„ ê²°ê³¼ì™€ ë™ì¼\n\nParameter containing:\ntensor([[2.4290, 4.0144]], requires_grad=True)\n\n\n\nì´ì œ bias=True\n\n\n# net(X) = X@net.weight.T # í˜„ì¬ ì´ë ‡ê²Œ ì•Œê³  ìˆìœ¼ë‚˜ ì‚¬ì‹¤ì€ ì•„ë‹˜\n\n\nnet.weight\n\nParameter containing:\ntensor([[2.4290, 4.0144]], requires_grad=True)\n\n\n\nprint(net.bias) # í˜„ì¬ëŠ” bias=False\n\nNone\n\n\n\n# net(X) = X@net.weight.T + net.bias # ì‚¬ì‹¤ì€ ì´ê²Œ ë§ìŒ\n\n\në‘˜ì€ ë™ì¼\n\ny = X@W + Ïµ # y = net(X) + Ïµ\ny = w0hat + x*w1hat + Ïµ # y = net(x) + Ïµ\n\nnet(X) = X@net.weight.T + net.bias ì—ì„œ Xê°€ xë¡œ ë°”ë€Œë©´\n\nnet(x) = x@net.weight.T + net.bias\nnet(x) = w0hat + x*w1hat ì´ë¯€ë¡œ\nnet.biasì— í•´ë‹¹í•˜ëŠ” ê²ƒì€ w0hat\nnet.weight.Tì— í•´ë‹¹í•˜ëŠ” ê²ƒì€ w1hat ìœ¼ë¡œ ìƒê° ê°€ëŠ¥\n\nìœ„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ net(x)ë¥¼ ë§Œë“¤ë©´\n\nxëŠ” (n,1)ì´ë¯€ë¡œ input ì°¨ì›ì€ 1\n\n\n\nnet = torch.nn.Linear(1,1,bias=True)\nnet\n\nLinear(in_features=1, out_features=1, bias=True)\n\n\n\nnet.weight # 1x1 matrix\n\nParameter containing:\ntensor([[0.3480]], requires_grad=True)\n\n\n\nnet.bias # length 1ì¸ vector\n\nParameter containing:\ntensor([0.7757], requires_grad=True)\n\n\n\nnet.weight.T # net(x) = x@net.weight.T + net.bias ì—ì„œ net.weight.TëŠ” w1hat\n\ntensor([[0.3480]], grad_fn=&lt;PermuteBackward0&gt;)\n\n\n\nnet.weight.data = torch.tensor([[10.0]])\nnet.weight.data\n\ntensor([[10.]])\n\n\n\nnet.bias.data = torch.tensor([[-5.0]]) # net(x) = x@net.weight.T + net.bias ì—ì„œ net.biasëŠ” w0hat\nnet.bias.data\n\ntensor([[-5.]])\n\n\n\nìœ„ì˜ ë‚´ìš©ì„ ì €ë²ˆ ì‹œê°„ ì½”ë“œì— ë°˜ì˜í•˜ë©´\n\nnet ìˆ˜ì •, weight ë° bias ê°’ ìˆ˜ì •\nnet(X) -&gt; net(x)\n\n\n\nnet = torch.nn.Linear(1, 1, bias=True)\nnet.weight.data = torch.tensor([[10.0]])\nnet.bias.data = torch.tensor([[-5.0]])\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD(net.parameters(), lr=0.1) # lr: learning rate\n\n# step 1~4\nfor epoc in range(30):\n    # 1\n    yhat = net(x)\n    # 2\n    loss = loss_fn(yhat,y)\n    # 3\n    loss.backward()\n    # 4\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nnet.weight\n\nParameter containing:\ntensor([[4.0144]], requires_grad=True)\n\n\n\nnet.bias\n\nParameter containing:\ntensor([[2.4290]], requires_grad=True)\n\n\n\nì €ë²ˆ ì‹œê°„ ê²°ê³¼ì™€ ë™ì¼\n\n)ğŸ—£ï¸\nnetì—ì„œ biasë¥¼ ì‚¬ìš©\n\n# step1ì„ ìœ„í•œ ì‚¬ì „ì¤€ë¹„\nnet = torch.nn.Linear(\n    in_features=1,\n    out_features=1,\n    bias=True\n) # net(x) = x@net.weight.T + net.bias \nnet.bias.data = torch.tensor([-5.0])\nnet.weight.data = torch.tensor([[10.0]])\n# step2ë¥¼ ìœ„í•œ ì‚¬ì „ì¤€ë¹„\nloss_fn = torch.nn.MSELoss()\n# step4ë¥¼ ìœ„í•œ ì‚¬ì „ì¤€ë¹„ \noptimizr = torch.optim.SGD(net.parameters(),lr=0.1)\nfor epoc in range(30):\n    # step1: yhat \n    yhat = net(x)\n    # step2: loss\n    loss = loss_fn(yhat,y)\n    # step3: ë¯¸ë¶„\n    loss.backward()\n    # step4: update\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nnet.bias.data, net.weight.data\n\n(tensor([2.4290]), tensor([[4.0144]]))\n\n\n#"
  },
  {
    "objectID": "posts/03wk-1.html#b.-ì˜ëª»ëœ-ì½”ë“œ",
    "href": "posts/03wk-1.html#b.-ì˜ëª»ëœ-ì½”ë“œ",
    "title": "03wk-1: (íšŒê·€, ë¡œì§€ìŠ¤í‹±) â€“ íŒŒì´í† ì¹˜ì‹ ì½”ë”©íŒ¨í„´ (2), ë¡œì§€ìŠ¤í‹± ëª¨í˜•",
    "section": "B. ì˜ëª»ëœ(?) ì½”ë“œ",
    "text": "B. ì˜ëª»ëœ(?) ì½”ë“œ\nğŸ—£ï¸ biasì˜ defaultëŠ” Trueì´ë¯€ë¡œ ì €ë²ˆ ì‹œê°„ ì½”ë“œì—ì„œ biasë¥¼ ì§€ìš°ë©´ bias=Trueê°€ ë¨\n\n# step1ì„ ìœ„í•œ ì‚¬ì „ì¤€ë¹„\nnet = torch.nn.Linear(\n    in_features=2,\n    out_features=1,\n)\nnet.weight.data = torch.tensor([[-5.0,  10.0]])\n# step2ë¥¼ ìœ„í•œ ì‚¬ì „ì¤€ë¹„\nloss_fn = torch.nn.MSELoss()\n# step4ë¥¼ ìœ„í•œ ì‚¬ì „ì¤€ë¹„ \noptimizr = torch.optim.SGD(net.parameters(),lr=0.1)\nfor epoc in range(30):\n    # step1: yhat \n    yhat = net(X)\n    # step2: loss\n    loss = loss_fn(yhat,y)\n    # step3: ë¯¸ë¶„\n    loss.backward()\n    # step4: update\n    optimizr.step()\n    optimizr.zero_grad()\n\nğŸ—£ï¸(\n\nnet.weight # ê²°ê³¼ê°€ ë§ì´ ë‹¬ë¼ì§\n\nParameter containing:\ntensor([[-1.1114,  4.0080]], requires_grad=True)\n\n\n\nplt.plot(x,y,'o')\nplt.plot(x,net(X).data, '--')\n\n\n\n\n\n\n\n\n\nê·¸ëŸ°ë° ê²°ê³¼ë¥¼ ì‹œê°í™”í•´ë³´ë©´ ë‚˜ì˜ì§€ ì•ŠìŒ\n\n)ğŸ—£ï¸\n- ê²°ê³¼ì‹œê°í™”\n\nplt.plot(x,y,'o')\nplt.plot(x,yhat.data,'--')\nplt.title(f'net.weight={net.weight.data.reshape(-1)}');\n\n\n\n\n\n\n\n\n- ë‚˜ì˜ì§€ ì•Šì€ ì´ìœ ?\nâœï¸ ë°”ë¡œ ë°‘ì˜ ì½”ë“œëŠ” í¸ì˜ìƒ ì‹¤í–‰ X\n# step1ì„ ìœ„í•œ ì‚¬ì „ì¤€ë¹„\nnet = torch.nn.Linear(\n    in_features=2,\n    out_features=1,\n)\nyhat = net(X) = X@net.weight.T + net.bias\n\nnet.weight\n\nParameter containing:\ntensor([[-1.1114,  4.0080]], requires_grad=True)\n\n\n\nnet.bias\n\nParameter containing:\ntensor([3.5562], requires_grad=True)\n\n\nğŸ—£ï¸(\n\nì›ë˜ëŒ€ë¡œë¼ë©´ ì ˆí¸, ê¸°ìš¸ê¸° ì´ 2ê°œì˜ parameterë§Œ í•™ìŠµí•´ì•¼í•˜ëŠ”ë° ìœ„ì˜ ê²°ê³¼ëŠ” 3ê°œë¥¼ í•™ìŠµí•¨\nyhat ê³„ì‚° ê³¼ì •ì„ ì‚´í´ë³´ë©´\n\n\nX[[0],:] # nx2 martixì—ì„œ ì²« ë²ˆì§¸ observationë§Œ ë½‘ìŒ\n\ntensor([[ 1.0000, -2.4821]])\n\n\n\nyhat[:1] # ì´ yhatì´ ì–´ë–»ê²Œ ë‚˜ì™”ëŠ”ì§€ ë³´ë©´\n\ntensor([[-7.5063]], grad_fn=&lt;SliceBackward0&gt;)\n\n\n\nX[[0],:] @ net.weight.T + net.bias\n\n\n-1.1114 * 1.0000 + 4.0080 * (-2.4821) + 3.5562 # ì•½ê°„ì˜ ì°¨ì´ëŠ” ì†Œìˆ˜ì  ì°¨ì´\n\n-7.503456799999999\n\n\n\n-2.4821ì€ x, ë‹¤ìŒê³¼ ê°™ì´ ì •ë¦¬í•˜ë©´\n\n\n-1.1114 * 1.0000 + 3.5562\n\n2.4448\n\n\n\nì ˆí¸ì— ëŒ€í•œ True ê°’: 2.5, ê¸°ìš¸ê¸°ì— ëŒ€í•œ True ê°’: 4\n\nì¦‰, ì ˆí¸ì„ 2ê°œë¡œ ë‚˜ëˆ ì„œ í•™ìŠµí•¨ (ë¹„íš¨ìœ¨ì )\n\nê·¸ëŸ¬ë©´ ì´ê²Œ í‹€ë¦° ê²ƒì¸ê°€?\n\níšŒê·€ë¶„ì„ì—ì„œ ì´ë ‡ê²Œ ëª¨ë¸ë§í•˜ë©´ í‹€ë¦¼ (í†µê³„í•™ì  ê´€ì )\ní•˜ì§€ë§Œ í•™ìŠµ ê²°ê³¼ ìì²´ëŠ” ë§ìŒ (ë¹„íš¨ìœ¨ì ì¼ë¿)\nAIë‚˜ DL ê´€ì ì—ì„œëŠ” ìµœì ì˜ parameter ê°œìˆ˜ê°€ ì •í•´ì§€ì§€ ì•Šì€ ê²½ìš°ê°€ ë§ì•„ì„œ\në¹„íš¨ìœ¨ì ì´ê¸´í•´ë„ ì˜ëª»ìœ¼ë¡œ ê¹Œì§€ëŠ” ìƒê° X\n\n\n)ğŸ—£ï¸"
  },
  {
    "objectID": "posts/03wk-1.html#a.-hatbf-y",
    "href": "posts/03wk-1.html#a.-hatbf-y",
    "title": "03wk-1: (íšŒê·€, ë¡œì§€ìŠ¤í‹±) â€“ íŒŒì´í† ì¹˜ì‹ ì½”ë”©íŒ¨í„´ (2), ë¡œì§€ìŠ¤í‹± ëª¨í˜•",
    "section": "A. \\(\\hat{\\bf y} = ??\\)",
    "text": "A. \\(\\hat{\\bf y} = ??\\)\nğŸ—£ï¸(\n\nì¼ë°˜ì ìœ¼ë¡œ íšŒê·€ë¶„ì„ì—ì„œ ì„¤ëª… ë³€ìˆ˜, ë°˜ì‘ ë³€ìˆ˜ ëª¨ë‘ ì—°ì†í˜• ë³€ìˆ˜ì´ì§€ë§Œ,\nyê°€ ìƒíƒœë¥¼ ì˜ë¯¸í•  ë•Œê°€ ìˆìŒ (ex. X = ì ìˆ˜, y = í•©ê²©/ë¶ˆí•©ê²©)\n\ní•©ê²©ì„ 1, ë¶ˆí•©ê²©ì„ 0ìœ¼ë¡œ ìˆ«ìí™”í•˜ë©´\nyëŠ” 0 ë˜ëŠ” 1ë§Œ ê°€ì§\n\nì´ëŸ¬í•œ ìë£ŒëŠ” ë§¤ìš° ë§ìŒ\n\n)ğŸ—£ï¸\n- \\({\\bf X}\\)ë¥¼ ê°€ì§€ê³  \\({\\bf y}\\)ë¥¼ ë§ì¶”ëŠ” ì•„ë˜ì™€ ê°™ì€ ë¬¸ì œ\n\nx = torch.tensor([-6,-5,-4,-3,-2,-1, 0, 1, 2, 3, 4, 5, 6.0]).reshape(-1,1)\ny = torch.tensor([ 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1]).reshape(-1,1)\nplt.plot(x,y,'o')\n\n\n\n\n\n\n\n\nğŸ—£ï¸(\n\nxê°€ ì¦ê°€í•¨ì— ë”°ë¼ yê°€ 1ì´ ë  ê²ƒ ê°™ê³ , xê°€ ê°ì†Œí•¨ì— ë”°ë¼ yê°€ 0ë  ê²ƒ ê°™ìŒ\nëª¨ë¸ë§ì„ ì–´ë–»ê²Œ?\nëª¨ë¸ë§: observed dataë¥¼ ë³´ê³  error-freeí•œ structureë¥¼ ì°¾ëŠ” ê²ƒ\nì—¬ê¸°ì„œ error-freeí•œ structureëŠ”?\n\nerror-free: ìš´ì ì¸ ìš”ì†Œê°€ ì—†ìŒ\n\nìš´ì ì¸ ìš”ì†Œ?\n\nì´ ê²½ìš° 0ì ì¸ë° í•©ê²©, 1ì ì¸ë° ë¶ˆí•©ê²©\në„ì €íˆ ë°›ì•„ë“¤ì´ì§€ ëª»í•  ìˆ˜ ìˆìŒ\n(ì´ë ‡ê²Œ ê²½ê³„ì— ìˆëŠ”ë° ìš´ì ì¸ ìš”ì†Œë¡œ ê²°ì •ë˜ëŠ” ê²½ìš°)\n\nì´ê²ƒì„ ì¼ë°˜ì ì¸ íšŒê·€ë¶„ì„ì²˜ëŸ¼ underlying(error-free)ì´ ìˆê³  ì˜¤ì°¨í•­ì„ ì •ê·œë¶„í¬ì—ì„œ errorë¥¼ ë½‘ì€ ê²ƒìœ¼ë¡œ ì„¤ëª…í•˜ë©´ X\n\nì´ì „ì˜ cafe ë°ì´í„°ëŠ” ì´ë ‡ê²Œ ì„¤ëª… ê°€ëŠ¥\n\nì°¨ë¼ë¦¬ underlyingì—ì„œ xê°’ì— ëŒ€ì‘í•˜ëŠ” yê°’ì„ ì„±ê³µ í™•ë¥ ë¡œ í•˜ëŠ” ë² ë¥´ëˆ„ì´ ì‹œí–‰ìœ¼ë¡œ ì„¤ëª…í•˜ë©´ ê·¸ëŸ´ë“¯ í•¨\n\nunderlying: ì—¬ê¸°ì„œëŠ” ê´€ì¸¡ê°’ì´ ì•„ë‹ˆê³  í™•ë¥ ì„ ì˜ë¯¸í•˜ëŠ” ê³¡ì„ ìœ¼ë¡œ í•´ì„\nì„±ê³µ í™•ë¥ ì´ 0.9ì¸ ë² ë¥´ëˆ„ì´ ì‹œí–‰ì„ í–ˆëŠ”ë° 0.1ì¸ í™•ë¥ ì˜ ê²°ê³¼ê°€ ë‚˜ì™€ë„ ì–´ì©” ìˆ˜ ì—†ìŒ (ìš´ì ì¸ ìš”ì†Œ)\nì˜¤ì°¨: ë² ë¥´ëˆ„ì´ ì‹œí–‰ì— ì˜í•´ ìƒì„±ë˜ëŠ” ëœë¤ì„±\n\ní†µê³„í•™ê³¼ì‹ ëª¨ë¸ë§\n\nstructure(error-free)ë¿ë§Œ ì•„ë‹ˆë¼ (ì´ê²ƒë„ ì–´ë ¤ì›€, ì—¬ê¸°ê¹Œì§€ëŠ” ë¹„í†µê³„í•™ê³¼ì‹)\nê´€ì¸¡ì¹˜ë¥¼ error termì„ ì´ìš©í•´ ì„¤ëª… (ìš´ì ì¸ ìš”ì†Œê°€ ì–´ë–»ê²Œ ì‘ìš©í•˜ëŠ”ì§€)\n\nyhat\n\nunderlying\nyê°€ 0 ë˜ëŠ” 1ë§Œ ê°€ì§€ë¯€ë¡œ yhatë„ ê·¸ë˜ì•¼í•˜ë‚˜ ì‹¶ì§€ë§Œ X (íšŒê·€ë¶„ì„ì—ì„œ ì˜¤ì°¨í•­ì´ í¬í•¨ëœ ê´€ì¸¡ì¹˜ë¥¼ ë”°ë¼ê°€ëŠ” ê²ƒê³¼ ë™ì¼)\nyhatì€ 0ê³¼ 1ì‚¬ì´ì˜ ìˆ«ì (ëª¨ë¸ë§ ëŒ€ìƒ: ê´€ì¸¡ì¹˜ê°€ ì•„ë‹ˆë¼ ì¶”ì„¸ì„ )\n\n\në‹¤ìŒê³¼ ê°™ì´ ëª¨ë¸ë§ì„ í•´ë³´ë©´\n\n\nprob = torch.exp(x) / (torch.exp(x) + 1)\nplt.plot(x,y,'o')\nplt.plot(x,prob,'--')\n\n\n\n\n\n\n\n\n\n\\(\\frac{e^x}{e^x + 1}\\)\n\n\\(x\\)ê°€ ì»¤ì§€ë©´ 1ì— ê°€ê¹Œì›Œì§€ê³ \n\\(x=0\\)ì´ë©´ 1/2\n\\(x\\)ê°€ ì‘ì•„ì§€ë©´ 0ì— ê°€ê¹Œì›Œì§\n\ní•˜ì§€ë§Œ ì´ ìˆ˜ì‹ì€ ì´ ê²½ìš°ì—ë§Œ ë§ê³  í™•ì¥ì„±ì´ ë–¨ì–´ì§\n\n)ğŸ—£ï¸\n- ì•„ë˜ì™€ ê°™ì´ ëª¨í˜•í™” í•˜ë©´?\n\nplt.plot(x,y,'o', label=r\"observed data (with error) = $(x_i,y_i)$\")\nplt.plot(x,torch.exp(x)/(1+torch.exp(x)),'o--', label = \"underlying (without error)\")\nplt.legend()"
  },
  {
    "objectID": "posts/03wk-1.html#b.-hatbf-y-fracexptextlinrbf-x1exptextlinrbf-x",
    "href": "posts/03wk-1.html#b.-hatbf-y-fracexptextlinrbf-x1exptextlinrbf-x",
    "title": "03wk-1: (íšŒê·€, ë¡œì§€ìŠ¤í‹±) â€“ íŒŒì´í† ì¹˜ì‹ ì½”ë”©íŒ¨í„´ (2), ë¡œì§€ìŠ¤í‹± ëª¨í˜•",
    "section": "B. \\(\\hat{\\bf y} = \\frac{\\exp(\\text{linr}({\\bf X}))}{1+\\exp(\\text{linr}({\\bf X}))}\\)",
    "text": "B. \\(\\hat{\\bf y} = \\frac{\\exp(\\text{linr}({\\bf X}))}{1+\\exp(\\text{linr}({\\bf X}))}\\)\n- ê±±ì •: ì‚°ì ë„ê°€ ê¼­ ì•„ë˜ì™€ ê°™ì€ ë°©ì‹ì´ ì•„ë‹ˆë¼ë©´ ì–´ì©Œì§€?\n\nplt.plot(x,y,'o')\n\n\n\n\n\n\n\n\n\n\\(x\\)ê°€ ì¦ê°€í• ìˆ˜ë¡ \\(y\\)ê°€ 0ì´ ëœë‹¤ë©´?\n0ê·¼ì²˜ì—ì„œ ë³€í™”ê°€ ì¼ì–´ë‚˜ì§€ ì•Šê³  2ê·¼ì²˜ì—ì„œ ë³€í™”ê°€ ì¼ì–´ë‚œë‹¤ë©´?\në³€í™”ê°€ ì¢€ ë” ê¸‰í•˜ê²Œ (í˜¹ì€ ì™„ë§Œí•˜ê²Œ ì¼ì–´ë‚œë‹¤ë©´?)\n\nğŸ—£ï¸(\n\n\\(\\frac{e^{-x}}{e^{-x} + 1}\\)\ní•©ê²©ë¥ ì´ ë‚®ì€ ê²½ìš°\nstrictí•˜ê²Œ ê²°ê³¼ê°€ ë‚˜ë‰˜ëŠ” ê²½ìš°(ex. ì¥í•™ê¸ˆ)\n\n\nplt.plot(x,y,'o', label=r\"observed data (with error) = $(x_i,y_i)$\")\nplt.plot(x,torch.exp(-x)/(1+torch.exp(-x)),'o--', label = \"underlying (without error)\")\nplt.legend()\n\n\n\n\n\n\n\n\n\nplt.plot(x,y,'o', label=r\"observed data (with error) = $(x_i,y_i)$\")\nplt.plot(x,torch.exp(-x+3)/(1+torch.exp(-x+3)),'o--', label = \"underlying (without error)\")\nplt.legend()\n\n\n\n\n\n\n\n\n\nplt.plot(x,y,'o', label=r\"observed data (with error) = $(x_i,y_i)$\")\nplt.plot(x,torch.exp(5*x+3)/(1+torch.exp(5*x+3)),'o--', label = \"underlying (without error)\")\nplt.legend()\n\n\n\n\n\n\n\n\n\nì´ëŸ¬í•œ 5*x+3 ë“±ì„ ì¼ë°˜í™”í•˜ë©´\n\n5x+3 = w0hat + w1hat  x : íšŒê·€ë¶„ì„ ì„ í˜• ëª¨í˜•\n= w0hat + w1hat * x = linr(x) # xë¥¼ linear transformì‹œí‚´\n\n\nğŸ”¬ 0ê·¼ì²˜ì—ì„œ ë³€í™”ê°€ ì¼ì–´ë‚˜ì§€ ì•Šê³  2ê·¼ì²˜ì—ì„œ ë³€í™”ê°€ ì¼ì–´ë‚œë‹¤ë©´?\n\nplt.plot(x,y,'o', label=r\"observed data (with error) = $(x_i,y_i)$\")\nplt.plot(x,torch.exp(x-2)/(1+torch.exp(x-2)),'o--', label = \"underlying (without error)\")\nplt.legend()\n\n\n\n\n\n\n\n\nğŸ”¬ ë³€í™”ê°€ ì¢€ ë” ê¸‰í•˜ê²Œ ì¼ì–´ë‚œë‹¤ë©´?\n\nplt.plot(x,y,'o', label=r\"observed data (with error) = $(x_i,y_i)$\")\nplt.plot(x,torch.exp(3*x)/(1+torch.exp(3*x)),'o--', label = \"underlying (without error)\")\nplt.legend()\n\n\n\n\n\n\n\n\nğŸ”¬ ë³€í™”ê°€ ì¢€ ë” ì™„ë§Œí•˜ê²Œ ì¼ì–´ë‚œë‹¤ë©´?\n\nplt.plot(x,y,'o', label=r\"observed data (with error) = $(x_i,y_i)$\")\nplt.plot(x,torch.exp(x/3)/(1+torch.exp(x/3)),'o--', label = \"underlying (without error)\")\nplt.legend()\n\n\n\n\n\n\n\n\n)ğŸ—£ï¸\n\nplt.plot(x,y,'o', label=r\"observed data (with error) = $(x_i,y_i)$\")\nplt.plot(x,torch.exp(5*x+3)/(1+torch.exp(5*x+3)),'o--', label = \"underlying (without error)\")\nplt.legend()\n\n\n\n\n\n\n\n\n- ê±±ì •í•´ê²°\n\n#plt.plot(x,y,'o', label=r\"observed data (with error) = $(x_i,y_i)$\")\nplt.plot(x,torch.exp(x)/(1+torch.exp(x)),'o--', label = \"underlying type1 (without error)\", color=\"C1\")\nplt.plot(x,torch.exp(5*x)/(1+torch.exp(5*x)),'o--', label = \"underlying type2 (without error)\", color=\"C2\")\nplt.legend()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\níšŒê·€ vs ë¡œì§€ìŠ¤í‹±\n\n\\({\\bf X} \\to {\\bf y}\\) ì— ëŒ€í•œ íŒ¨í„´ì´ \\(\\text{linr}({\\bf X}) \\approx {\\bf y}\\) ì´ë¼ë©´ íšŒê·€!\n\\({\\bf X} \\to {\\bf y}\\) ì— ëŒ€í•œ íŒ¨í„´ì´ \\(\\frac{\\exp(\\text{linr}({\\bf X}))}{1+\\exp(\\text{linr}({\\bf X}))} \\approx {\\bf y}\\) ì´ë¼ë©´ ë¡œì§€ìŠ¤í‹±!\n\n\n\nğŸ—£ï¸(\n\nXë¥¼ linear transformí–ˆë”ë‹ˆ ì„  ìì²´ê°€ yì™€ ë¹„ìŠ· =&gt; íšŒê·€\nìœ„ì˜ ê·¸ë˜í”„ë¥¼ ê·¸ë¦¬ëŠ” ì‹ìœ¼ë¡œ í–ˆë”ë‹ˆ yì™€ ë¹„ìŠ· =&gt; ë¡œì§€ìŠ¤í‹±\n\nì •í™•íˆëŠ” í™•ë¥ ì´ yì™€ ë¹„ìŠ·í•˜ë‹¤ë©´ (y ìì²´ëŠ” 0 ë˜ëŠ” 1)\n\n\n)ğŸ—£ï¸"
  },
  {
    "objectID": "posts/03wk-1.html#c.-ë¡œì§€ìŠ¤í‹±-ëª¨í˜•",
    "href": "posts/03wk-1.html#c.-ë¡œì§€ìŠ¤í‹±-ëª¨í˜•",
    "title": "03wk-1: (íšŒê·€, ë¡œì§€ìŠ¤í‹±) â€“ íŒŒì´í† ì¹˜ì‹ ì½”ë”©íŒ¨í„´ (2), ë¡œì§€ìŠ¤í‹± ëª¨í˜•",
    "section": "C. ë¡œì§€ìŠ¤í‹± ëª¨í˜•",
    "text": "C. ë¡œì§€ìŠ¤í‹± ëª¨í˜•\n- \\(x\\)ê°€ ì»¤ì§ˆìˆ˜ë¡ (í˜¹ì€ ì‘ì•„ì§ˆìˆ˜ë¡) \\(y=1\\)ì´ ì˜ë‚˜ì˜¤ëŠ” ëª¨í˜•ì€ ì•„ë˜ì™€ ê°™ì´ ì„¤ê³„í•  ìˆ˜ ìˆìŒ &lt;â€” ì™¸ìš°ì„¸ìš”!!!\n\n\\(y_i \\sim {\\cal B}(\\pi_i),\\quad\\) where \\(\\pi_i = \\frac{\\exp(w_0+w_1x_i)}{1+\\exp(w_0+w_1x_i)} = \\frac{1}{1+\\exp(-w_0-w_1x_i)}\\)\n\\(\\hat{y}_i= \\frac{\\exp(\\hat{w}_0+\\hat{w}_1x_i)}{1+\\exp(\\hat{w}_0+\\hat{w}_1x_i)}=\\frac{1}{1+\\exp(-\\hat{w}_0-\\hat{w}_1x_i)}\\)\nğŸ—£ï¸\n\n\\(\\pi_i\\)ëŠ” í™•ë¥ ì„ ì˜ë¯¸\n\\(\\frac{e^{x}}{1 + e^{x}}\\) = \\(\\frac{1}{e^{-x} + 1}\\) ì—ì„œ \\(x\\) ëŒ€ì‹  \\(w_0+w_1x_i\\)\nì±… ë§ˆë‹¤ ë‹¤ë¥´ì§€ë§Œ ì˜¤ë¥¸ìª½ì²˜ëŸ¼ ë§ì´ ì”€\n\n\n- íšŒê·€ëª¨í˜•ê³¼ ë¡œì§€ìŠ¤í‹± ëª¨í˜•ì˜ ë¹„êµ\n\níšŒê·€ëª¨í˜•: \\(y_i \\sim {\\cal N}(w_0+w_1x_i, \\sigma^2)\\)1\në¡œì§€ìŠ¤í‹±: \\(y_i \\sim {\\cal B}\\big(\\frac{\\exp(w_0+w_1x_i)}{1+\\exp(w_0+w_1x_i)}\\big)\\)\nğŸ—£ï¸\n\níšŒê·€ëª¨í˜•: ì˜¤ì°¨í•­ì˜ ê´€ì ì—ì„œ í•´ì„\në¡œì§€ìŠ¤í‹±(yê°€ 0 ë˜ëŠ” 1): ìœ„ì˜ ê³¡ì„ ì„ ë‚˜íƒ€ë‚´ëŠ” ì¼ë°˜ì ì¸ ìˆ˜ì‹\n\n=&gt; ì´ ìˆ˜ì‹ê°’ì„ í† ëŒ€ë¡œ ë² ë¥´ëˆ„ì´ ì‹œí–‰ì„ í•˜ë©´ ì˜¤ì°¨í•­ê¹Œì§€ ì„¤ëª… ê°€ëŠ¥í•œ ëª¨ë¸ì´ ë¨\n\n\n\n- ìš°ë¦¬ê°€ ì˜ˆì¸¡í•˜ê³  ì‹¶ì€ê²ƒ\n\níšŒê·€ëª¨í˜•: ì •ê·œë¶„í¬ì˜ í‰ê· ì„ ì˜ˆì¸¡í•˜ê³  ì‹¶ìŒ. ì¦‰ \\(w_0+w_1x_i\\)ë¥¼ ì˜ˆì¸¡í•˜ê³  ì‹¶ìŒ. ì˜ˆì¸¡ê°’ìœ¼ë¡œëŠ” \\(\\hat{w}_0 + \\hat{w}_1x_i\\)ë¥¼ ì‚¬ìš©!\në¡œì§€ìŠ¤í‹±: ë² ë¥´ëˆ„ì´ì˜ í‰ê· ì„ ì˜ˆì¸¡í•˜ê³  ì‹¶ìŒ. ì¦‰ \\(\\frac{\\exp(w_0+w_1x_i)}{1+\\exp(w_0+w_1x_i)}\\)ë¥¼ ì˜ˆì¸¡í•˜ê³  ì‹¶ìŒ. ì˜ˆì¸¡ê°’ìœ¼ë¡œëŠ” \\(\\frac{\\exp(\\hat{w}_0+\\hat{w}_1x_i)}{1+\\exp(\\hat{w}_0+\\hat{w}_1x_i)}\\)ë¥¼ ì‚¬ìš©!\nğŸ—£ï¸\n\në‘˜ ë‹¤ \\(\\hat{w}_0\\), \\(\\hat{w}_1\\)ë¥¼ ì¶”ì •í•˜ë©´ ê°ê° ì§ì„ ê³¼ ê³¡ì„ ì´ ê²°ì •ë¨\në² ë¥´ëˆ„ì´ì˜ í‰ê· ì€ \\(p\\)\n\nì¦‰, í™•ë¥ ì„ ì˜ˆì¸¡í•˜ê³  ì‹¶ìŒ"
  },
  {
    "objectID": "posts/03wk-1.html#footnotes",
    "href": "posts/03wk-1.html#footnotes",
    "title": "03wk-1: (íšŒê·€, ë¡œì§€ìŠ¤í‹±) â€“ íŒŒì´í† ì¹˜ì‹ ì½”ë”©íŒ¨í„´ (2), ë¡œì§€ìŠ¤í‹± ëª¨í˜•",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nì›ë˜ëŠ” ì´ë ‡ê²Œ ì¼ì—ˆì§€.. \\(y_i = w_0 + w_1x_i + \\epsilon_i \\quad \\epsilon_i \\sim {\\cal N}(0,\\sigma^2)\\)â†©ï¸"
  },
  {
    "objectID": "posts/02wk-2.html",
    "href": "posts/02wk-2.html",
    "title": "02wk-2: (íšŒê·€) â€“ íŒŒë¼ë©”í„°ì˜ í•™ìŠµê³¼ì • ìŒë¯¸, MSE, íŒŒì´í† ì¹˜ì‹ ì½”ë”©íŒ¨í„´ (1)",
    "section": "",
    "text": "ğŸ“˜ Note Format Guide\nThis format serves as a structured guide for organizing lecture content, personal interpretation, experiments, and study-related questions.\nğŸ“ ğŸ—£ï¸ âœï¸ ğŸ”¬ â“"
  },
  {
    "objectID": "posts/02wk-2.html#a.-print",
    "href": "posts/02wk-2.html#a.-print",
    "title": "02wk-2: (íšŒê·€) â€“ íŒŒë¼ë©”í„°ì˜ í•™ìŠµê³¼ì • ìŒë¯¸, MSE, íŒŒì´í† ì¹˜ì‹ ì½”ë”©íŒ¨í„´ (1)",
    "section": "A. print",
    "text": "A. print\n\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad=True)\nalpha = 0.001\nprint(f\"ì‹œì‘ê°’ = {What.data.reshape(-1)}\")\nfor epoc in range(30):\n    yhat = X @ What\n    loss = torch.sum((y-yhat)**2)\n    loss.backward()\n    What.data = What.data - alpha * What.grad\n    print(f'loss = {loss:.2f} \\t ì—…ë°ì´íŠ¸í­ = {-alpha * What.grad.reshape(-1)} \\t ì—…ë°ì´íŠ¸ê²°ê³¼: {What.data.reshape(-1)}')\n    What.grad = None\n\nì‹œì‘ê°’ = tensor([-5., 10.])\nloss = 8587.69   ì—…ë°ì´íŠ¸í­ = tensor([ 1.3423, -1.1889])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([-3.6577,  8.8111])\nloss = 5675.21   ì—…ë°ì´íŠ¸í­ = tensor([ 1.1029, -0.9499])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([-2.5548,  7.8612])\nloss = 3755.64   ì—…ë°ì´íŠ¸í­ = tensor([ 0.9056, -0.7596])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([-1.6492,  7.1016])\nloss = 2489.58   ì—…ë°ì´íŠ¸í­ = tensor([ 0.7431, -0.6081])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([-0.9061,  6.4935])\nloss = 1654.04   ì—…ë°ì´íŠ¸í­ = tensor([ 0.6094, -0.4872])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([-0.2967,  6.0063])\nloss = 1102.32   ì—…ë°ì´íŠ¸í­ = tensor([ 0.4995, -0.3907])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([0.2028, 5.6156])\nloss = 737.84    ì—…ë°ì´íŠ¸í­ = tensor([ 0.4091, -0.3136])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([0.6119, 5.3020])\nloss = 496.97    ì—…ë°ì´íŠ¸í­ = tensor([ 0.3350, -0.2519])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([0.9469, 5.0501])\nloss = 337.71    ì—…ë°ì´íŠ¸í­ = tensor([ 0.2742, -0.2025])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([1.2211, 4.8477])\nloss = 232.40    ì—…ë°ì´íŠ¸í­ = tensor([ 0.2243, -0.1629])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([1.4454, 4.6848])\nloss = 162.73    ì—…ë°ì´íŠ¸í­ = tensor([ 0.1834, -0.1311])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([1.6288, 4.5537])\nloss = 116.63    ì—…ë°ì´íŠ¸í­ = tensor([ 0.1500, -0.1056])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([1.7787, 4.4480])\nloss = 86.13     ì—…ë°ì´íŠ¸í­ = tensor([ 0.1226, -0.0851])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([1.9013, 4.3629])\nloss = 65.93     ì—…ë°ì´íŠ¸í­ = tensor([ 0.1001, -0.0687])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([2.0014, 4.2942])\nloss = 52.57     ì—…ë°ì´íŠ¸í­ = tensor([ 0.0818, -0.0554])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([2.0832, 4.2388])\nloss = 43.72     ì—…ë°ì´íŠ¸í­ = tensor([ 0.0668, -0.0447])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([2.1500, 4.1941])\nloss = 37.86     ì—…ë°ì´íŠ¸í­ = tensor([ 0.0545, -0.0361])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([2.2045, 4.1579])\nloss = 33.97     ì—…ë°ì´íŠ¸í­ = tensor([ 0.0445, -0.0292])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([2.2490, 4.1287])\nloss = 31.40     ì—…ë°ì´íŠ¸í­ = tensor([ 0.0363, -0.0236])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([2.2853, 4.1051])\nloss = 29.70     ì—…ë°ì´íŠ¸í­ = tensor([ 0.0296, -0.0191])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([2.3150, 4.0860])\nloss = 28.57     ì—…ë°ì´íŠ¸í­ = tensor([ 0.0242, -0.0155])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([2.3392, 4.0705])\nloss = 27.83     ì—…ë°ì´íŠ¸í­ = tensor([ 0.0197, -0.0125])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([2.3589, 4.0580])\nloss = 27.33     ì—…ë°ì´íŠ¸í­ = tensor([ 0.0161, -0.0101])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([2.3750, 4.0479])\nloss = 27.00     ì—…ë°ì´íŠ¸í­ = tensor([ 0.0131, -0.0082])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([2.3881, 4.0396])\nloss = 26.79     ì—…ë°ì´íŠ¸í­ = tensor([ 0.0107, -0.0067])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([2.3988, 4.0330])\nloss = 26.64     ì—…ë°ì´íŠ¸í­ = tensor([ 0.0087, -0.0054])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([2.4075, 4.0276])\nloss = 26.55     ì—…ë°ì´íŠ¸í­ = tensor([ 0.0071, -0.0044])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([2.4146, 4.0232])\nloss = 26.48     ì—…ë°ì´íŠ¸í­ = tensor([ 0.0058, -0.0035])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([2.4204, 4.0197])\nloss = 26.44     ì—…ë°ì´íŠ¸í­ = tensor([ 0.0047, -0.0029])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([2.4251, 4.0168])\nloss = 26.41     ì—…ë°ì´íŠ¸í­ = tensor([ 0.0038, -0.0023])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([2.4290, 4.0144])\n\n\n\nğŸ—£ï¸\n\nlossë§Œ ë³´ë©´ ì ì  ê°ì†Œí•¨, ê°ˆìˆ˜ë¡ ê°ì†Œí•˜ëŠ” í­ë„ ì‘ì•„ì§€ë©° 26 ê·¼ì²˜ë¡œ ìˆ˜ë ´\nì—…ë°ì´íŠ¸ í­ë„ ì²˜ìŒì—ëŠ” ì»¸ë‹¤ê°€ ê°ì†Œ\nì´ì— ë”°ë¼ ì—…ë°ì´íŠ¸ ê²°ê³¼ë„ ê°ˆìˆ˜ë¡ ì˜ ì•ˆ ë°”ë€œ"
  },
  {
    "objectID": "posts/02wk-2.html#b.-ì‹œê°í™”-yhatì˜-ê´€ì ì—ì„œ",
    "href": "posts/02wk-2.html#b.-ì‹œê°í™”-yhatì˜-ê´€ì ì—ì„œ",
    "title": "02wk-2: (íšŒê·€) â€“ íŒŒë¼ë©”í„°ì˜ í•™ìŠµê³¼ì • ìŒë¯¸, MSE, íŒŒì´í† ì¹˜ì‹ ì½”ë”©íŒ¨í„´ (1)",
    "section": "B. ì‹œê°í™” â€“ yhatì˜ ê´€ì ì—ì„œ!",
    "text": "B. ì‹œê°í™” â€“ yhatì˜ ê´€ì ì—ì„œ!\nğŸ—£ï¸(\n\nWhat = torch.tensor([[-5.0], [10.0]], requires_grad=True)\n\n\nyhat = X@What\nloss = torch.sum((yhat-y)**2)\nloss.backward()\nWhat.data = What.data - 0.001*What.grad\nWhat.grad = None\n\n\nplt.plot(x,y,'o')\nplt.plot(x, (X@What).data, '--', color=\"C1\") # ì„  ìƒ‰ê¹” ì£¼í™©ìƒ‰ ê³ ì •\n\n\n\n\n\n\n\n\n\nì•„ë˜ ì½”ë“œë¥¼ ë°˜ë³µí•˜ë©° ì§€ì¼œë³´ë©´ ì„ ì´ ë³€í™”í•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŒ (ë°‘ì˜ ê·¸ë˜í”„ëŠ” ì—¬ëŸ¬ë²ˆ ë°˜ë³µí•œ ìµœì¢… ê²°ê³¼)\n\n\nyhat = X@What\nloss = torch.sum((yhat-y)**2)\nloss.backward()\nWhat.data = What.data - 0.001*What.grad\nWhat.grad = None\nplt.plot(x,y,'o')\nplt.plot(x, (X@What).data, '--', color=\"C1\")\n\n\n\n\n\n\n\n\n\ní•œ ê°€ì§€ ì•„ì‰¬ìš´ ì : ì¤‘ê°„ ê³¼ì •ì˜ ê·¸ë˜í”„ê°€ ì‚¬ë¼ì§\n\n\nWhat = torch.tensor([[-5.0], [10.0]], requires_grad=True)\n\n\nplt.plot(x,y,'o')\nfig = plt.gcf() # ì¤‘ê°„ ê·¸ë¦¼ì„ ì €ì¥ (í˜¸ì¶œ ê°€ëŠ¥) get current figure\n\n\n\n\n\n\n\n\n\nax = fig.gca() # get current axes (axes: axisì˜ ë³µìˆ˜í˜•, ì—¬ê¸°ì„œëŠ” xì¶•,yì¶• ëª¨ë‘ë¥¼ ì§€ì¹­)\nyhat = X@What\nloss = torch.sum((yhat-y)**2)\nloss.backward()\nWhat.data = What.data - 0.001*What.grad\nWhat.grad = None\nax.plot(x, (X@What).data, '--', color=\"C1\") # pltë¥¼ axë¡œ ìˆ˜ì •\n\n\nfig # 1ë²ˆ ì‹¤í–‰\n\n\n\n\n\n\n\n\n\n1ë²ˆ ë” ì‹¤í–‰í•˜ë©´ ê²¹ì³ì§\n\n\nax = fig.gca() # get current axes (axes: axisì˜ ë³µìˆ˜í˜•, ì—¬ê¸°ì„œëŠ” xì¶•,yì¶• ëª¨ë‘ë¥¼ ì§€ì¹­)\nyhat = X@What\nloss = torch.sum((yhat-y)**2)\nloss.backward()\nWhat.data = What.data - 0.001*What.grad\nWhat.grad = None\nax.plot(x, (X@What).data, '--', color=\"C1\") # pltë¥¼ axë¡œ ìˆ˜ì •\n\n\nfig # 1ë²ˆ ë” ì‹¤í–‰ (ê²¹ì³ì§)\n\n\n\n\n\n\n\n\n\nì´ˆê¸°í™” í›„ ë°˜ë³µí•˜ë©´ ì—…ë°ì´íŠ¸ëœ í­ì„ ë³¼ ìˆ˜ ìˆìŒ (ì ì  ì¤„ì–´ë“œëŠ” ê²ƒ ê°™ìŒ)\n\n\nWhat = torch.tensor([[-5.0], [10.0]], requires_grad=True)\n\n\nplt.plot(x,y,'o')\nfig = plt.gcf()\n\n\n\n\n\n\n\n\n\nax = fig.gca()\nyhat = X@What\nloss = torch.sum((yhat-y)**2)\nloss.backward()\nWhat.data = What.data - 0.001*What.grad\nWhat.grad = None\nax.plot(x, (X@What).data, '--', color=\"C1\")\nfig\n\n\n\n\n\n\n\n\n\nì œëª©ì„ ë„£ì„ ìˆ˜ë„ ìˆìŒ (set_title, ë‹¨ìˆœ ë¬¸ìì—´ ì•„ë‹ˆì—¬ë„ ê°€ëŠ¥)\n\n\nWhat = torch.tensor([[-5.0], [10.0]], requires_grad=True)\n\nplt.plot(x,y,'o')\nfig = plt.gcf()\n\n\n\n\n\n\n\n\n\nfor epoc in range(20):\n    ax = fig.gca()\n    yhat = X@What\n    loss = torch.sum((yhat-y)**2)\n    loss.backward()\n    What.data = What.data - 0.001*What.grad\n    What.grad = None\n    ax.plot(x, (X@What).data, '--', color=\"C1\")\n    ax.set_title(What.data.reshape(-1))\n    fig\n\n\nfig\n\n\n\n\n\n\n\n\n)ğŸ—£ï¸\n\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad=True)\nalpha = 0.001\nplt.plot(x,y,'o',label = \"observed\")\nfig = plt.gcf()\nax = fig.gca()\nax.plot(x,X@What.data,'--',color=\"C1\")\nfor epoc in range(30):\n    yhat = X @ What\n    loss = torch.sum((y-yhat)**2)\n    loss.backward()\n    What.data = What.data - alpha * What.grad\n    ax.plot(x,X@What.data,'--',color=\"C1\",alpha=0.1)\n    What.grad = None\n\n\n\n\n\n\n\n\nğŸ—£ï¸ alpha: ê²¹ì³ì§€ë©´ ì§„í•´ì§"
  },
  {
    "objectID": "posts/02wk-2.html#c.-ì‹œê°í™”-lossì˜-ê´€ì ì—ì„œ",
    "href": "posts/02wk-2.html#c.-ì‹œê°í™”-lossì˜-ê´€ì ì—ì„œ",
    "title": "02wk-2: (íšŒê·€) â€“ íŒŒë¼ë©”í„°ì˜ í•™ìŠµê³¼ì • ìŒë¯¸, MSE, íŒŒì´í† ì¹˜ì‹ ì½”ë”©íŒ¨í„´ (1)",
    "section": "C. ì‹œê°í™” â€“ lossì˜ ê´€ì ì—ì„œ!!",
    "text": "C. ì‹œê°í™” â€“ lossì˜ ê´€ì ì—ì„œ!!\nğŸ—£ï¸(\n\ndef plot_loss():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    w0 = np.arange(-6, 11, 0.5) \n    w1 = np.arange(-6, 11, 0.5)\n    W1,W0 = np.meshgrid(w1,w0)\n    LOSS=W0*0\n    for i in range(len(w0)):\n        for j in range(len(w1)):\n            LOSS[i,j]=torch.sum((y-w0[i]-w1[j]*x)**2)\n    ax.plot_surface(W0, W1, LOSS, rstride=1, cstride=1, color='b',alpha=0.1)\n    ax.azim = 30  ## 3d plotì˜ view ì¡°ì ˆ \n    ax.dist = 8   ## 3d plotì˜ view ì¡°ì ˆ \n    ax.elev = 5   ## 3d plotì˜ view ì¡°ì ˆ \n    ax.set_xlabel(r'$w_0$')  # xì¶• ë ˆì´ë¸” ì„¤ì •\n    ax.set_ylabel(r'$w_1$')  # yì¶• ë ˆì´ë¸” ì„¤ì •\n    ax.set_xticks([-5,0,5,10])  # xì¶• í‹± ê°„ê²© ì„¤ì •\n    ax.set_yticks([-5,0,5,10])  # yì¶• í‹± ê°„ê²© ì„¤ì •\n    plt.close(fig)  # ìë™ ì¶œë ¥ ë°©ì§€\n    return fig\n\n\nfig = plot_loss()\nfig # loss_fn(w0hat, w1hat)ì„ zì— ì°ìŒ\n\n\n\n\n\n\n\n\n\n# ì†ì‹¤ 8587.6875 ë¥¼ ê³„ì‚°í•˜ëŠ” ë˜ ë‹¤ë¥¸ ë°©ì‹\ndef l(w0hat,w1hat):\n    yhat = w0hat + w1hat*x\n    return torch.sum((y-yhat)**2)\n\n\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad=True)\nWhat\n\ntensor([[-5.],\n        [10.]], requires_grad=True)\n\n\n\nl(-5,10) # ì†ì‹¤ ê³„ì‚°\n\ntensor(8587.6875)\n\n\n\ntorch.sum((y-X@What)**2) # ë‹¤ë¥¸ ë°©ë²•\n\ntensor(8587.6875, grad_fn=&lt;SumBackward0&gt;)\n\n\n\nyhat = -5 + 10*x\ntorch.sum((y-yhat)**2) # ë‹¤ë¥¸ ë°©ë²• 2\n\ntensor(8587.6875)\n\n\n\nfig\nax = fig.gca()\nax.scatter(-5, 10, l(-5,10)) # ì  ì°ê¸°\n\n&lt;mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7fc9261c2bb0&gt;\n\n\n\nfig\n\n\n\n\n\n\n\n\n\nfig\nax = fig.gca()\nax.scatter(-1, 3, l(-1,3)) # ë‹¤ë¥¸ ì  ì°ê¸°\n\n&lt;mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7fc925df1eb0&gt;\n\n\n\nfig\n\n\n\n\n\n\n\n\n\nìœ„ ê³¼ì •ì„ ë°˜ë³µí•˜ë©´ ê³¡ë©´ì„ ê·¸ë¦´ ìˆ˜ ìˆìŒ\në°‘ì€ True ê°’ ì°ê¸°\n\n\nfig\nax = fig.gca()\nax.scatter(2.5, 4.0, l(2.5,4.0)) # True ê°’\n\n&lt;mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7fc925f14670&gt;\n\n\n\nfig\n\n\n\n\n\n\n\n\n\në°‘ì— ì •ë¦¬ëœ ì½”ë“œ ê³¼ì •\n\n\nfig = plot_loss()\nfig # loss_fn(w0hat,w1hat)\nax = fig.gca()\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad=True)\nw0hat, w1hat = What.data.reshape(-1) # ì–¸íŒ¨í‚¹\nax.scatter(w0hat, w1hat, l(w0hat, w1hat)) # x, y, z\nfig # ìµœì´ˆì˜ ì§ì„ ì— ëŒ€ì‘í•˜ëŠ” What ê°’\n\n\n\n\n\n\n\n\n\nyhat = X@What\nloss = torch.sum((y-yhat)**2)\nloss.backward()\nWhat.data = What.data - 0.001*What.grad\nWhat.grad = None\nw0hat, w1hat = What.data.reshape(-1)\nax.scatter(w0hat, w1hat, l(w0hat, w1hat))\nfig # ë°˜ë³µ ì‹¤í–‰í• ìˆ˜ë¡ updateë¨\n\n\n\n\n\n\n\n\n\nyhat = X@What\nloss = torch.sum((y-yhat)**2)\nloss.backward()\nWhat.data = What.data - 0.001*What.grad\nWhat.grad = None\nw0hat, w1hat = What.data.reshape(-1)\nax.scatter(w0hat, w1hat, l(w0hat, w1hat), color=\"C1\") # ì•ìœ¼ë¡œëŠ” ì£¼í™©ìƒ‰ìœ¼ë¡œ ìƒ‰ê¹” ê³ ì •\nfig # ë°˜ë³µ ì‹¤í–‰í• ìˆ˜ë¡ ì ì  ìµœì†Œê°€ ë˜ëŠ” ìª½ìœ¼ë¡œ ì§„í–‰ë¨\n\n\n\n\n\n\n\n\n)ğŸ—£ï¸\n\ndef plot_loss():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    w0 = np.arange(-6, 11, 0.5) \n    w1 = np.arange(-6, 11, 0.5)\n    W1,W0 = np.meshgrid(w1,w0)\n    LOSS=W0*0\n    for i in range(len(w0)):\n        for j in range(len(w1)):\n            LOSS[i,j]=torch.sum((y-w0[i]-w1[j]*x)**2)\n    ax.plot_surface(W0, W1, LOSS, rstride=1, cstride=1, color='b',alpha=0.1)\n    ax.azim = 30  ## 3d plotì˜ view ì¡°ì ˆ \n    ax.dist = 8   ## 3d plotì˜ view ì¡°ì ˆ \n    ax.elev = 5   ## 3d plotì˜ view ì¡°ì ˆ \n    ax.set_xlabel(r'$w_0$')  # xì¶• ë ˆì´ë¸” ì„¤ì •\n    ax.set_ylabel(r'$w_1$')  # yì¶• ë ˆì´ë¸” ì„¤ì •\n    ax.set_xticks([-5,0,5,10])  # xì¶• í‹± ê°„ê²© ì„¤ì •\n    ax.set_yticks([-5,0,5,10])  # yì¶• í‹± ê°„ê²© ì„¤ì •\n    plt.close(fig)  # ìë™ ì¶œë ¥ ë°©ì§€\n    return fig\n\n\n# ì†ì‹¤ 8587.6875 ë¥¼ ê³„ì‚°í•˜ëŠ” ë˜ ë‹¤ë¥¸ ë°©ì‹\ndef l(w0hat,w1hat):\n    yhat = w0hat + w1hat*x\n    return torch.sum((y-yhat)**2)\n\n\nfig = plot_loss()\nax = fig.gca()\nax.scatter(2.5, 4, l(2.5,4), s=200, marker='*', color='red', label=r\"${\\bf W}=[2.5, 4]'$\")\nax.scatter(-5, 10, l(-5,10), s=200, marker='*', color='blue', label=r\"initial $\\hat{\\bf W}=[-5, 10]'$\")\nax.legend()\nfig\n\n\n\n\n\n\n\n\n\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad=True)\nalpha = 0.001\nfor epoc in range(30):\n    yhat = X @ What\n    loss = torch.sum((y-yhat)**2)\n    loss.backward()\n    What.data = What.data - 0.001 * What.grad\n    w0,w1 = What.data.reshape(-1) \n    ax.scatter(w0,w1,l(w0,w1),s=5,marker='o',color='blue')\n    What.grad = None\n\n\nfig\n\n\n\n\n\n\n\n\n\nğŸ—£ï¸\n\nBì˜ ì‹œê°í™”ì—ì„œ ìµœì´ˆì˜ ì§ì„ ì— ëŒ€ì‘í•˜ëŠ” ì ì´ íŒŒë€ìƒ‰ ì \nì ë“¤ì´ ë¹¨ê°„ìƒ‰ ì ìœ¼ë¡œ ì´ë™í•˜ëŠ” ê³¼ì •ì€ ì§ì„ ì´ ì˜¬ë¼ê°€ëŠ” ê³¼ì •ì— ëŒ€ì‘"
  },
  {
    "objectID": "posts/02wk-2.html#d.-ì• ë‹ˆë©”ì´ì…˜",
    "href": "posts/02wk-2.html#d.-ì• ë‹ˆë©”ì´ì…˜",
    "title": "02wk-2: (íšŒê·€) â€“ íŒŒë¼ë©”í„°ì˜ í•™ìŠµê³¼ì • ìŒë¯¸, MSE, íŒŒì´í† ì¹˜ì‹ ì½”ë”©íŒ¨í„´ (1)",
    "section": "D. ì• ë‹ˆë©”ì´ì…˜",
    "text": "D. ì• ë‹ˆë©”ì´ì…˜\n\nfrom matplotlib import animation\n\n\nplt.rcParams['figure.figsize'] = (7.5,2.5)\nplt.rcParams[\"animation.html\"] = \"jshtml\" \n\n\ndef show_animation(alpha=0.001):\n    ## 1. íˆìŠ¤í† ë¦¬ ê¸°ë¡ì„ ìœ„í•œ list ì´ˆê¸°í™”\n    loss_history = [] \n    yhat_history = [] \n    What_history = [] \n\n    ## 2. í•™ìŠµ + í•™ìŠµê³¼ì •ê¸°ë¡\n    What= torch.tensor([[-5.0],[10.0]],requires_grad=True)\n    What_history.append(What.data.tolist())\n    for epoc in range(30): \n        yhat=X@What ; yhat_history.append(yhat.data.tolist())\n        loss=torch.sum((y-yhat)**2); loss_history.append(loss.item())\n        loss.backward() \n        What.data = What.data - alpha * What.grad; What_history.append(What.data.tolist())\n        What.grad = None    \n\n    ## 3. ì‹œê°í™” \n    fig = plt.figure()\n    ax1 = fig.add_subplot(1, 2, 1)\n    ax2 = fig.add_subplot(1, 2, 2, projection='3d')\n\n    #### ax1: yhatì˜ ê´€ì ì—ì„œ.. \n    ax1.plot(x,y,'o',label=r\"$(x_i,y_i)$\")\n    line, = ax1.plot(x,yhat_history[0],label=r\"$(x_i,\\hat{y}_i)$\") \n    ax1.legend()\n    #### ax2: lossì˜ ê´€ì ì—ì„œ.. \n    w0 = np.arange(-6, 11, 0.5) \n    w1 = np.arange(-6, 11, 0.5)\n    W1,W0 = np.meshgrid(w1,w0)\n    LOSS=W0*0\n    for i in range(len(w0)):\n        for j in range(len(w1)):\n            LOSS[i,j]=torch.sum((y-w0[i]-w1[j]*x)**2)\n    ax2.plot_surface(W0, W1, LOSS, rstride=1, cstride=1, color='b',alpha=0.1)\n    ax2.azim = 30  ## 3d plotì˜ view ì¡°ì ˆ \n    ax2.dist = 8   ## 3d plotì˜ view ì¡°ì ˆ \n    ax2.elev = 5   ## 3d plotì˜ view ì¡°ì ˆ \n    ax2.set_xlabel(r'$w_0$')  # xì¶• ë ˆì´ë¸” ì„¤ì •\n    ax2.set_ylabel(r'$w_1$')  # yì¶• ë ˆì´ë¸” ì„¤ì •\n    ax2.set_xticks([-5,0,5,10])  # xì¶• í‹± ê°„ê²© ì„¤ì •\n    ax2.set_yticks([-5,0,5,10])  # yì¶• í‹± ê°„ê²© ì„¤ì •\n    ax2.scatter(2.5, 4, l(2.5,4), s=200, marker='*', color='red', label=r\"${\\bf W}=[2.5, 4]'$\")\n    ax2.scatter(-5, 10, l(-5,10), s=200, marker='*', color='blue')\n    ax2.legend()\n    def animate(epoc):\n        line.set_ydata(yhat_history[epoc])\n        ax2.scatter(np.array(What_history)[epoc,0],np.array(What_history)[epoc,1],loss_history[epoc],color='grey')\n        fig.suptitle(f\"alpha = {alpha} / epoch = {epoc}\")\n        return line\n\n    ani = animation.FuncAnimation(fig, animate, frames=30)\n    plt.close()\n    return ani\n\n\nğŸ—£ï¸ alpha:\n\ní•™ìŠµë¥ : updateë˜ëŠ” í­ (ML ê´€ì )\nstep size: ì˜¤ë¥¸ìª½ ê·¸ë¦¼ í•¨ìˆ˜ ê´€ì  (ì‚°ì—… ê³µí•™ ê´€ì )\n\n\n\nani = show_animation(alpha=0.001)\nani\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/02wk-2.html#e.-í•™ìŠµë¥ ì—-ë”°ë¥¸-ì‹œê°í™”",
    "href": "posts/02wk-2.html#e.-í•™ìŠµë¥ ì—-ë”°ë¥¸-ì‹œê°í™”",
    "title": "02wk-2: (íšŒê·€) â€“ íŒŒë¼ë©”í„°ì˜ í•™ìŠµê³¼ì • ìŒë¯¸, MSE, íŒŒì´í† ì¹˜ì‹ ì½”ë”©íŒ¨í„´ (1)",
    "section": "E. í•™ìŠµë¥ ì— ë”°ë¥¸ ì‹œê°í™”",
    "text": "E. í•™ìŠµë¥ ì— ë”°ë¥¸ ì‹œê°í™”\n- \\(\\alpha\\)ê°€ ë„ˆë¬´ ì‘ë‹¤ë©´ ë¹„íš¨ìœ¨ì ì„\n\nshow_animation(alpha=0.0001)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\nğŸ—£ï¸ 0.001 -&gt; 0.0001\n\nì•„ê¹Œë³´ë‹¤ ê°€ëŠ” ë‘¥ ë§ˆëŠ” ë‘¥ í•¨\n\n\n- \\(\\alpha\\)ê°€ í¬ë‹¤ê³  ë¬´ì¡°ê±´ ì¢‹ì€ê±´ ë˜ ì•„ë‹˜\n\nshow_animation(alpha=0.0083)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\nğŸ—£ï¸ 0.001 -&gt; 0.0083 (ì§ì ‘ ì°¾ì€ ìˆ«ì)\n\nì²˜ìŒë¶€í„° ìµœì†Œì ì„ ì§€ë‚˜ë²„ë¦¼ (ì§ì„ ì´ ì ë“¤ ìœ„ë¡œ ë°”ë¡œ ì˜¬ë¼ê°) -&gt; ë°”ëŒì§í•˜ì§€ ì•ŠìŒ\nì´í›„ ì§ì„ ì´ ë‹¤ì‹œ ì ë“¤ ì•„ë˜ë¡œ ë‚´ë ¤ì˜´\nì™”ë‹¤ê°”ë‹¤í•˜ë©´ì„œ ë‚´ë ¤ì˜¤ëŠ” ê²ƒ ê°™ê¸°ëŠ” í•˜ë‚˜ íš¨ìœ¨ì ì¸ ëŠë‚Œì€ ì•„ë‹˜\n\n\n- ìˆ˜í‹€ë¦¬ë©´ ìˆ˜ë ´ì•ˆí• ìˆ˜ë„??\n\nshow_animation(alpha=0.0085)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\nğŸ—£ï¸ 0.001 -&gt; 0.0085\n\nì§ì „ì˜ 0.0083ê³¼ ì–¼ë§ˆ ì°¨ì´ê°€ ë‚˜ì§€ë„ ì•ŠëŠ”ë°\nì´ë²ˆì—ëŠ” ì™”ë‹¤ê°”ë‹¤í•˜ë©´ì„œ ìˆ˜ë ´í•˜ì§€ë„ ì•ŠìŒ\nì˜¤íˆë ¤ ê°ˆìˆ˜ë¡ í¬ë¬¼ì„  ëª¨ì–‘ìœ¼ë¡œ ì ì  ì˜¬ë¼ê°\n\n\n- ê·¸ëƒ¥ ë§í• ìˆ˜ë„??\n\nshow_animation(alpha=0.01)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\nğŸ—£ï¸ 0.001 -&gt; 0.01\n\nê¸°ìš¸ê¸°ê°€ ë¬´í•œëŒ€ê°€ ë¨\nêµí›ˆ: alphaë¥¼ ì˜ ì„ íƒí•´ì•¼ ìˆ˜ë ´í•¨\n\n\n\n\nplt.rcdefaults()\nplt.rcParams['figure.figsize'] = 4.5,3.0"
  },
  {
    "objectID": "posts/02wk-2.html#a.-ê¸°ë³¸íŒ¨í„´",
    "href": "posts/02wk-2.html#a.-ê¸°ë³¸íŒ¨í„´",
    "title": "02wk-2: (íšŒê·€) â€“ íŒŒë¼ë©”í„°ì˜ í•™ìŠµê³¼ì • ìŒë¯¸, MSE, íŒŒì´í† ì¹˜ì‹ ì½”ë”©íŒ¨í„´ (1)",
    "section": "A. ê¸°ë³¸íŒ¨í„´",
    "text": "A. ê¸°ë³¸íŒ¨í„´\nğŸ—£ï¸ SSE ë§ê³  MSEë¡œ\n\n## -- ì™¸ìš°ì„¸ìš”!!! -- ##\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad = True)\nfor epoc in range(30):\n    # step1: yhat \n    yhat = X@What \n    # step2: loss\n    loss = torch.sum((y-yhat)**2)/100\n    # step3: ë¯¸ë¶„\n    loss.backward()\n    # step4: update\n    What.data = What.data - 0.1 * What.grad\n    What.grad = None\n\n\nplt.plot(x,y,'o')\nplt.plot(x,X@What.data,'--')\nplt.title(f'What={What.data.reshape(-1)}');"
  },
  {
    "objectID": "posts/02wk-2.html#b.-step2ì˜-ìˆ˜ì •-loss_fn-ì´ìš©",
    "href": "posts/02wk-2.html#b.-step2ì˜-ìˆ˜ì •-loss_fn-ì´ìš©",
    "title": "02wk-2: (íšŒê·€) â€“ íŒŒë¼ë©”í„°ì˜ í•™ìŠµê³¼ì • ìŒë¯¸, MSE, íŒŒì´í† ì¹˜ì‹ ì½”ë”©íŒ¨í„´ (1)",
    "section": "B. Step2ì˜ ìˆ˜ì • â€“ loss_fn ì´ìš©",
    "text": "B. Step2ì˜ ìˆ˜ì • â€“ loss_fn ì´ìš©\nğŸ—£ï¸(\n\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad = True)\n\n\nyhat=X@What\n\n\ntorch.sum((y-yhat)**2)/100 # MSE\n\ntensor(85.8769, grad_fn=&lt;DivBackward0&gt;)\n\n\n\ntorch.mean((y-yhat)**2) # MSE\n\ntensor(85.8769, grad_fn=&lt;MeanBackward0&gt;)\n\n\n\ndef loss_fn(yhat, y):\n    return torch.mean((y-yhat)**2)\n\n\nloss_fn(yhat,y)\n\ntensor(85.8769, grad_fn=&lt;MeanBackward0&gt;)\n\n\n\nloss_fn ì›ë¦¬ë¥¼ ì˜ ëª¨ë¥¸ë‹¤ë©´ pytorch í•¨ìˆ˜ ì´ìš©\n\n\nloss_fn = torch.nn.MSELoss()\nloss_fn(yhat,y) # ê²°ê³¼ëŠ” ë™ì¼\n\ntensor(85.8769, grad_fn=&lt;MseLossBackward0&gt;)\n\n\n\ní‹€ë¦° ì„¤ëª…\n\ntorch.nn.MSELossëŠ” í•¨ìˆ˜ì¸ë°, â€œNone -&gt; MSEë¥¼ ê³„ì‚°í•´ì£¼ëŠ” í•¨ìˆ˜â€ì¸ í•¨ìˆ˜\n\në§ëŠ” ì„¤ëª…\n\ntorch.nn.MSELossëŠ” callable objectë¥¼ ìƒì„±í•˜ëŠ” í´ë˜ìŠ¤\n\n\n)ğŸ—£ï¸\n\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad = True)\nloss_fn = torch.nn.MSELoss()\nfor epoc in range(30):\n    # step1: yhat \n    yhat = X@What \n    # step2: loss\n    #loss = torch.sum((y-yhat)**2)/100\n    loss = loss_fn(yhat,y) # ì—¬ê¸°ì„œëŠ” í° ìƒê´€ì—†ì§€ë§Œ ìŠµê´€ì ìœ¼ë¡œ yhatì„ ë¨¼ì €ë„£ëŠ” ì—°ìŠµì„ í•˜ì!!\n    # step3: ë¯¸ë¶„\n    loss.backward()\n    # step4: update\n    What.data = What.data - 0.1 * What.grad\n    What.grad = None\n\nğŸ—£ï¸ loss_fnì€ ë¬´ì¡°ê±´ yhat ë¨¼ì €\n\nplt.plot(x,y,'o')\nplt.plot(x,X@What.data,'--')\nplt.title(f'What={What.data.reshape(-1)}');"
  },
  {
    "objectID": "posts/02wk-2.html#c.-step1ì˜-ìˆ˜ì •-net-ì´ìš©",
    "href": "posts/02wk-2.html#c.-step1ì˜-ìˆ˜ì •-net-ì´ìš©",
    "title": "02wk-2: (íšŒê·€) â€“ íŒŒë¼ë©”í„°ì˜ í•™ìŠµê³¼ì • ìŒë¯¸, MSE, íŒŒì´í† ì¹˜ì‹ ì½”ë”©íŒ¨í„´ (1)",
    "section": "C. Step1ì˜ ìˆ˜ì • â€“ net ì´ìš©",
    "text": "C. Step1ì˜ ìˆ˜ì • â€“ net ì´ìš©\nğŸ—£ï¸ yhat = X@Whatë„ ì•Œê³  ì‹¶ì§€ ì•Šë‹¤ë©´ (ë„¤íŠ¸ì›Œí¬ ì´ìš©)\n# net â€“ net ì˜¤ë¸Œì íŠ¸ë€?\nì›ë˜ yhatì„ ì´ëŸ°ì‹ìœ¼ë¡œ êµ¬í–ˆëŠ”ë° ~\n\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad = True)\nyhat= X@What\nyhat[:5]\n\ntensor([[-29.8211],\n        [-28.6215],\n        [-24.9730],\n        [-21.2394],\n        [-19.7919]], grad_fn=&lt;SliceBackward0&gt;)\n\n\nì•„ë˜ì™€ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ì½”ë“œë¥¼ ì§œê³  ì‹¶ìŒ..\nyhat = net(X) # \nğŸ—£ï¸ Xë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ yhatì„ ì¶œë ¥í•˜ê³  ì‹¶ìŒ\nìœ„ì™€ ê°™ì€ ì½”ë“œë¥¼ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” netì€ torchì—ì„œ ì§€ì›í•˜ê³  ì•„ë˜ì™€ ê°™ì´ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ.\nğŸ—£ï¸(\n\n# torch.nn.Linear?\n\nnet = torch.nn.Linear(\n    in_features= ??,\n    out_features= ??,\n    bias= False # defaultëŠ” True\n)\n\nin_features: ì…ë ¥(X)ì— ëŒ€í•œ ì°¨ì› (featuresë¥¼ dimensionìœ¼ë¡œ ìƒê°)\n\n\nX.shape # 100ì€ ê´€ì¸¡ê°’ ê°œìˆ˜ì— ë”°ë¼ ë°”ë€” ìˆ˜ ìˆê³ , 2ëŠ” ëª¨í˜•ì´ ì •í•´ì§€ë©´ ì•ˆ ë°”ë€œ\n\ntorch.Size([100, 2])\n\n\n\nout_features: ì¶œë ¥(y)ì— ëŒ€í•œ ì°¨ì›\n\n\ny.shape # ë§ˆì°¬ê°€ì§€ë¡œ 1\n\ntorch.Size([100, 1])\n\n\n\nnet = torch.nn.Linear(\n    in_features= 2,\n    out_features= 1,\n    bias= False\n)\n\n\nyhat = net(X)\nyhat[:5]\n\ntensor([[-0.1600],\n        [-0.1362],\n        [-0.0639],\n        [ 0.0101],\n        [ 0.0388]], grad_fn=&lt;SliceBackward0&gt;)\n\n\n\nì›ë˜ êµ¬í•œ yhatê³¼ ë¹„êµí•˜ë©´ Whatì„ ì„¤ì •í•˜ì§€ ì•Šì•˜ìœ¼ë¯€ë¡œ ë‹¹ì—°íˆ ë‹¤ë¦„\n\n\nnet.weight # Whatê³¼ ê°™ë‹¤ê³  ìƒê°í•˜ë©´ ë¨\n\nParameter containing:\ntensor([[0.3320, 0.1982]], requires_grad=True)\n\n\n\nì—„ë°€íˆ ë§í•˜ë©´ net.weightëŠ” 2x1 matrixê°€ ì•„ë‹ˆë¼ 1x2 martix\n\nì»´í“¨í„° ê³µí•™ì  ì´ìœ ë¡œ ì´ë ‡ê²Œ ë˜ì–´ ìˆìŒ (column vectorë³´ë‹¤ row vector ì—°ì‚°ì´ ì‰½ë‹¤ê³  í•¨)\n\n\n\nnet.weight.T # ì´ê²Œ ì§„ì§œ Whatê³¼ ë™ì¼\n\ntensor([[0.3320],\n        [0.1982]], grad_fn=&lt;PermuteBackward0&gt;)\n\n\n\nnet.weight.data = torch.tensor([[-5.0],[10.0]]).T\nnet.weight.data\n\ntensor([[-5., 10.]])\n\n\n\nyhat= net(X)\nyhat[:5]\n\ntensor([[-29.8211],\n        [-28.6215],\n        [-24.9730],\n        [-21.2394],\n        [-19.7919]], grad_fn=&lt;SliceBackward0&gt;)\n\n\n\nì›ë˜ êµ¬í•œ yhatê³¼ ë™ì¼\n\n)ğŸ—£ï¸\n\n# yhat = net(X) \nnet = torch.nn.Linear(\n    in_features=2, # X:(n,2) --&gt; 2 \n    out_features=1, # yhat:(n,1) --&gt; 1 \n    bias=False \n)\n\n\nnet.weight.data = torch.tensor([[-5.0], [10.0]]).T # .T ë¥¼ í•´ì•¼í•¨. ì™¸ìš°ì„¸ìš” \nnet.weight\n\nParameter containing:\ntensor([[-5., 10.]], requires_grad=True)\n\n\n\nnet(X)[:5]\n\ntensor([[-29.8211],\n        [-28.6215],\n        [-24.9730],\n        [-21.2394],\n        [-19.7919]], grad_fn=&lt;SliceBackward0&gt;)\n\n\n\n(X@What)[:5]\n\ntensor([[-29.8211],\n        [-28.6215],\n        [-24.9730],\n        [-21.2394],\n        [-19.7919]], grad_fn=&lt;SliceBackward0&gt;)\n\n\n\n(X@net.weight.T)[:5]\n\ntensor([[-29.8211],\n        [-28.6215],\n        [-24.9730],\n        [-21.2394],\n        [-19.7919]], grad_fn=&lt;SliceBackward0&gt;)\n\n\n#\n- ìˆ˜ì •ëœì½”ë“œ\n\n# step1ì„ ìœ„í•œ ì‚¬ì „ì¤€ë¹„\nnet = torch.nn.Linear(\n    in_features=2,\n    out_features=1,\n    bias=False\n)\nnet.weight.data = torch.tensor([[-5.0,  10.0]])\n# step2ë¥¼ ìœ„í•œ ì‚¬ì „ì¤€ë¹„\nloss_fn = torch.nn.MSELoss()\nfor epoc in range(30):\n    # step1: yhat\n    # yhat = X@What \n    yhat = net(X)\n    # step2: loss\n    loss = loss_fn(yhat,y)\n    # step3: ë¯¸ë¶„\n    loss.backward()\n    # step4: update\n    net.weight.data = net.weight.data - 0.1 * net.weight.grad\n    net.weight.grad = None\n\nğŸ—£ï¸ What.data -&gt; net.weight.data, What.grad -&gt; net.weight.grad\n\nplt.plot(x,y,'o')\nplt.plot(x,net(X).data,'--')\nplt.title(f'net.weight={net.weight.data.reshape(-1)}');"
  },
  {
    "objectID": "posts/02wk-2.html#d.-step4ì˜-ìˆ˜ì •-optimizerì˜-ì´ìš©",
    "href": "posts/02wk-2.html#d.-step4ì˜-ìˆ˜ì •-optimizerì˜-ì´ìš©",
    "title": "02wk-2: (íšŒê·€) â€“ íŒŒë¼ë©”í„°ì˜ í•™ìŠµê³¼ì • ìŒë¯¸, MSE, íŒŒì´í† ì¹˜ì‹ ì½”ë”©íŒ¨í„´ (1)",
    "section": "D. Step4ì˜ ìˆ˜ì • â€“ optimizerì˜ ì´ìš©",
    "text": "D. Step4ì˜ ìˆ˜ì • â€“ optimizerì˜ ì´ìš©\n- ì†Œë§: ì•„ë˜ì˜ ê³¼ì •ì„ ì¢€ ë” í¸í•˜ê²Œ í–ˆìœ¼ë©´..\nnet.weight.data = net.weight.data - 0.1 * net.weight.grad\nnet.weight.data = None \n# optimizer â€“ ì´ê±¸ ì´ìš©í•˜ë©´ update ê³¼ì •ì„ ì†ì‰½ê²Œ í•  ìˆ˜ ìˆìŒ\nê¸°ì¡´ì½”ë“œ\n\n## -- ì¤€ë¹„ê³¼ì • -- ## \n# step1ì„ ìœ„í•œ ì‚¬ì „ì¤€ë¹„\nnet = torch.nn.Linear(\n    in_features=2,\n    out_features=1,\n    bias=False\n)\nnet.weight.data = torch.tensor([[-5.0,  10.0]])\n# step2ë¥¼ ìœ„í•œ ì‚¬ì „ì¤€ë¹„\nloss_fn = torch.nn.MSELoss()\n\n\n## -- 1ì—í­ì§„í–‰ -- ## \n# step1: \nyhat = net(X)\n# step2: loss\nloss = loss_fn(yhat,y)\n# step3: ë¯¸ë¶„\nloss.backward()\n# step4: update\nprint(net.weight.data)\nnet.weight.data = net.weight.data - 0.1 * net.weight.grad\nprint(net.weight.data)\nnet.weight.grad = None\n\ntensor([[-5., 10.]])\ntensor([[-3.6577,  8.8111]])\n\n\n\n## -- 2ì—í­ì§„í–‰ -- ## \n# step1: 2ì—í­ì§„í–‰\nyhat = net(X)\n# step2: loss\nloss = loss_fn(yhat,y)\n# step3: ë¯¸ë¶„\nloss.backward()\n# step4: update\nprint(net.weight.data)\nnet.weight.data = net.weight.data - 0.1 * net.weight.grad\nprint(net.weight.data)\nnet.weight.grad = None\n\ntensor([[-3.6577,  8.8111]])\ntensor([[-2.5548,  7.8612]])\n\n\nìƒˆë¡œìš´ì½”ë“œ â€“ optimizer ì´ìš©\nğŸ—£ï¸(\n\ntorch.optim.SGD : optimizerë¥¼ ë§Œë“¤ì–´ì¤Œ\ntorch.optim.SGDëŠ” net.weightë¥¼ ê°–ê³  ìˆì–´ì•¼ í•¨(What)\n\nnet.weightëŠ” net.parameters()ë¡œ ë³¼ ìˆ˜ ìˆìŒ\nnet.parameters()ëŠ” generator: iterable object -&gt; listí™” ê°€ëŠ¥\n\ntorch.optim.SGDëŠ” í•™ìŠµë¥  lrë„ ê°–ê³  ìˆì–´ì•¼ í•¨\n\n\nnet.weight\n\nParameter containing:\ntensor([[-2.5548,  7.8612]], requires_grad=True)\n\n\n\nnet.parameters()\n\n&lt;generator object Module.parameters at 0x7fc922ade820&gt;\n\n\n\nlist(net.parameters()) # ê°’ì„ ë³´ë ¤ë©´\n\n[Parameter containing:\n tensor([[-2.5548,  7.8612]], requires_grad=True)]\n\n\n\n# optimizr = torch.optim.SGD(net.parameters(), lr=0.1) # net.parameters(): generator\n\n\nnet.weight.data = net.weight.data - 0.1 * net.weight.grad\n\n=&gt; optimizr.step()\n\nnet.weight.grad = None\n\n=&gt; optimizr.zero_grad()\n\n\n)ğŸ—£ï¸\n\n## -- ì¤€ë¹„ê³¼ì • -- ## \n# step1ì„ ìœ„í•œ ì‚¬ì „ì¤€ë¹„\nnet = torch.nn.Linear(\n    in_features=2,\n    out_features=1,\n    bias=False\n)\nnet.weight.data = torch.tensor([[-5.0,  10.0]])\n# step2ë¥¼ ìœ„í•œ ì‚¬ì „ì¤€ë¹„\nloss_fn = torch.nn.MSELoss()\n# step4ë¥¼ ìœ„í•œ ì‚¬ì „ì¤€ë¹„\noptimizr = torch.optim.SGD(net.parameters(),lr=0.1)\n\n\n## -- 1ì—í­ì§„í–‰ -- ## \nyhat = net(X)\n# step2: loss\nloss = loss_fn(yhat,y)\n# step3: ë¯¸ë¶„\nloss.backward()\n# step4: update\nprint(net.weight.data)\n#net.weight.data = net.weight.data - 0.1 * net.weight.grad\noptimizr.step()\nprint(net.weight.data)\n#net.weight.grad = None\noptimizr.zero_grad()\n\ntensor([[-5., 10.]])\ntensor([[-3.6577,  8.8111]])\n\n\n\n## -- 2ì—í­ì§„í–‰ -- ## \nyhat = net(X)\n# step2: loss\nloss = loss_fn(yhat,y)\n# step3: ë¯¸ë¶„\nloss.backward()\n# step4: update\nprint(net.weight.data)\n#net.weight.data = net.weight.data - 0.1 * net.weight.grad\noptimizr.step()\nprint(net.weight.data)\n#net.weight.grad = None\noptimizr.zero_grad()\n\ntensor([[-3.6577,  8.8111]])\ntensor([[-2.5548,  7.8612]])\n\n\n#\n- ìˆ˜ì •ëœì½”ë“œ\n\n# step1ì„ ìœ„í•œ ì‚¬ì „ì¤€ë¹„\nnet = torch.nn.Linear(\n    in_features=2,\n    out_features=1,\n    bias=False\n)\nnet.weight.data = torch.tensor([[-5.0,  10.0]])\n# step2ë¥¼ ìœ„í•œ ì‚¬ì „ì¤€ë¹„\nloss_fn = torch.nn.MSELoss()\n# step4ë¥¼ ìœ„í•œ ì‚¬ì „ì¤€ë¹„ \noptimizr = torch.optim.SGD(net.parameters(),lr=0.1)\nfor epoc in range(30):\n    # step1: yhat \n    yhat = net(X)\n    # step2: loss\n    loss = loss_fn(yhat,y)\n    # step3: ë¯¸ë¶„\n    loss.backward()\n    # step4: update\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'o')\nplt.plot(x,yhat.data,'--')\nplt.title(f'net.weight={net.weight.data.reshape(-1)}');"
  },
  {
    "objectID": "posts/01wk-1.html",
    "href": "posts/01wk-1.html",
    "title": "01wk-1: (í† ì¹˜) â€“ ê°•ì˜ì†Œê°œ, íŒŒì´í† ì¹˜ ê¸°ë³¸",
    "section": "",
    "text": "ğŸ“˜ Note Format Guide\nThis format serves as a structured guide for organizing lecture content, personal interpretation, experiments, and study-related questions.\nğŸ“ ğŸ—£ï¸ âœï¸ ğŸ”¬ â“"
  },
  {
    "objectID": "posts/01wk-1.html#a.-torch",
    "href": "posts/01wk-1.html#a.-torch",
    "title": "01wk-1: (í† ì¹˜) â€“ ê°•ì˜ì†Œê°œ, íŒŒì´í† ì¹˜ ê¸°ë³¸",
    "section": "A. torch",
    "text": "A. torch\nğŸ—£ï¸ torchëŠ” numpyì™€ ë¹„ìŠ· (ë²¡í„° ë§Œë“¤ê¸° ë“±)\n- ë²¡í„°\n\ntorch.tensor([1,2,3])\n\ntensor([1, 2, 3])\n\n\n- ë²¡í„°ì˜ ë§ì…ˆ\n\ntorch.tensor([1,2,3]) + torch.tensor([2,2,2])\n\ntensor([3, 4, 5])\n\n\n- ë¸Œë¡œë“œìºìŠ¤íŒ…\n\ntorch.tensor([1,2,3]) + 2\n\ntensor([3, 4, 5])"
  },
  {
    "objectID": "posts/01wk-1.html#b.-ë²¡í„°ì™€-ë§¤íŠ¸ë¦­ìŠ¤",
    "href": "posts/01wk-1.html#b.-ë²¡í„°ì™€-ë§¤íŠ¸ë¦­ìŠ¤",
    "title": "01wk-1: (í† ì¹˜) â€“ ê°•ì˜ì†Œê°œ, íŒŒì´í† ì¹˜ ê¸°ë³¸",
    "section": "B. ë²¡í„°ì™€ ë§¤íŠ¸ë¦­ìŠ¤",
    "text": "B. ë²¡í„°ì™€ ë§¤íŠ¸ë¦­ìŠ¤\nğŸ—£ï¸ torch.tensorëŠ” np.arrayì™€ ë¹„ìŠ·\n- \\(3 \\times 2\\) matrix\n\ntorch.tensor([[1,2],[3,4],[5,6]]) \n\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])\n\n\n- \\(3 \\times 1\\) matrix = \\(3 \\times 1\\) column vector\n\ntorch.tensor([[1],[3],[5]]) \n\ntensor([[1],\n        [3],\n        [5]])\n\n\n- \\(1 \\times 2\\) matrix = \\(1 \\times 2\\) row vector\n\ntorch.tensor([[1,2]]) \n\ntensor([[1, 2]])\n\n\nğŸ—£ï¸ torch.tensor([[1,2],[3,4],[5,6]])ì—ì„œ [3,4],[5,6] ì‚­ì œë¼ê³  ìƒê°\nğŸ—£ï¸ column vectorì™€ row vectorëŠ” êµ¬ë¶„ë˜ê³  ì„ ì–¸ ë°©ë²•ì´ ë‹¤ë¦„\n- ë”í•˜ê¸°\në¸Œë¡œë“œìºìŠ¤íŒ…(í¸í•œê±°)\n\ntorch.tensor([[1,2],[3,4],[5,6]]) - 1\n\ntensor([[0, 1],\n        [2, 3],\n        [4, 5]])\n\n\nğŸ—£ï¸ â€œmatrix - scalarâ€ëŠ” ë¶ˆê°€ëŠ¥í•˜ì§€ë§Œ ì•Œì•„ì„œ ì›ì†Œë³„ë¡œ ì „ë¶€ ëºŒ\n\ntorch.tensor([[1,2],[3,4],[5,6]]) + torch.tensor([[-1],[-3],[-5]])\n\ntensor([[0, 1],\n        [0, 1],\n        [0, 1]])\n\n\nğŸ—£ï¸ (3, 2) - (3, 1)ì„ ì•Œì•„ì„œ ëºŒ\nâœï¸ torch.tensor([[-1,-1],[-3, 3],[-5,-5]])\n\ntorch.tensor([[1,2],[3,4],[5,6]]) + torch.tensor([[-1,-2]])\n\ntensor([[0, 0],\n        [2, 2],\n        [4, 4]])\n\n\nğŸ—£ï¸ (3, 2) - (1, 2)ì„ ì•Œì•„ì„œ ëºŒ\nâœï¸ torch.tensor([[-1,-2],[-1,-2],[-1,-2]])\nì˜ëª»ëœ ë¸Œë¡œë“œìºìŠ¤íŒ…\n\ntorch.tensor([[1,2],[3,4],[5,6]]) + torch.tensor([[-1,-3,-5]])\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[11], line 1\n----&gt; 1 torch.tensor([[1,2],[3,4],[5,6]]) + torch.tensor([[-1,-3,-5]])\n\nRuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1\n\n\n\nğŸ—£ï¸ ì„¸ë¡œë¡œ ì“°ê±°ë‚˜ ê°€ë¡œë¡œ ë‘ ê°œì˜ ì›ì†Œë§Œ ì¼ìœ¼ë©´ ê°€ëŠ¥\nâœï¸ torch.tensor([[-1],[-3],[-5]]) ë˜ëŠ” torch.tensor([[-1,-3],[-1,-3],[-1,-3]]) ë“±\n\ntorch.tensor([[1,2],[3,4],[5,6]]) + torch.tensor([[-1],[-2]])\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[12], line 1\n----&gt; 1 torch.tensor([[1,2],[3,4],[5,6]]) + torch.tensor([[-1],[-2]])\n\nRuntimeError: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 0\n\n\n\nğŸ—£ï¸ (3, 2) - (2, 1) ëŠ” ì•Œì•„ì„œ ì±„ìš°ê¸° ì–´ë ¤ìš°ë¯€ë¡œ error\nì´ìƒí•œ ê²ƒ\n\ntorch.tensor([[1,2],[3,4],[5,6]]) + torch.tensor([-1,-2])\n\ntensor([[0, 0],\n        [2, 2],\n        [4, 4]])\n\n\nğŸ—£ï¸ (3, 2) matrix - ê¸¸ì´ê°€ 2ì¸ vector(2x1, 1x2 ë‘˜ ë‹¤ ì•„ë‹˜)\nğŸ—£ï¸ â€œmatrix - vectorâ€ë¥¼ row vectorë¡œ í•´ì„í•˜ê³  ëŠ˜ë ¤ì„œ ê³„ì‚°í•œ ë“¯\nâœï¸ torch.tensor([[-1,-2],[-1,-2],[-1,-2]])\nğŸ”¬(\n\nì°¨ì› ìˆ˜ë§Œ ì•Œê³  ì‹¶ì„ ë•Œ â†’ tensor.dim() ë˜ëŠ” tensor.ndim\nê° ì°¨ì›ì˜ í¬ê¸°ê¹Œì§€ ì•Œê³  ì‹¶ì„ ë•Œ â†’ tensor.shape ë˜ëŠ” tensor.size()\n\n\nprint(torch.tensor([[1,2],[3,4],[5,6]]).dim())\nprint(torch.tensor([[1,2],[3,4],[5,6]]).shape)\nprint(torch.tensor([-1,-2]).dim())\nprint(torch.tensor([-1,-2]).shape)\nprint(torch.tensor([[1,2],[3,4],[5,6]]).ndim)\nprint(torch.tensor([[1,2],[3,4],[5,6]]).size())\nprint(torch.tensor([-1,-2]).ndim)\nprint(torch.tensor([-1,-2]).size())\n\n2\ntorch.Size([3, 2])\n1\ntorch.Size([2])\n2\ntorch.Size([3, 2])\n1\ntorch.Size([2])\n\n\n\nì°¸ê³  (Chat GPT4o)\n\n\nNumPyì™€ PyTorch ì°¨ì´ ì •ë¦¬\n\n\n\n\nê¸°ëŠ¥\nPyTorch\nNumPy\n\n\n\n\nì°¨ì› ìˆ˜\n.dim() ë˜ëŠ” .ndim\n.ndim\n\n\nshape í™•ì¸\n.shape ë˜ëŠ” .size()\n.shape\n\n\ní¬ê¸° ë³€ê²½\n.view(), .reshape()\n.reshape()\n\n\níƒ€ì…\ntorch.Tensor\nnp.ndarray\n\n\n\n\nì‹¤ì „ íŒ:\n\nPyTorchì˜ .dim()ë§Œ NumPyì—ì„œ ì•ˆ ë¨¹íŒë‹¤ëŠ” ê²ƒë§Œ ê¸°ì–µí•˜ë©´ ë‘˜ ë‹¤ ê±°ì˜ ë¹„ìŠ·í•˜ê²Œ ë‹¤ë£° ìˆ˜ ìˆìŒ\në‹¤ì°¨ì› ë°°ì—´ì„ ë‹¤ë£° ë•Œ .ndim, .shapeëŠ” ì–‘ìª½ ëª¨ë‘ ì•ˆì „í•˜ê²Œ ì“¸ ìˆ˜ ìˆëŠ” í•µì‹¬ ë„êµ¬\ndim()ì€ PyTorch ê³ ìœ  ë©”ì„œë“œ\n\n\n)ğŸ”¬\n\ntorch.tensor([[1,2],[3,4],[5,6]]) + torch.tensor([-1,-3,-5])\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[15], line 1\n----&gt; 1 torch.tensor([[1,2],[3,4],[5,6]]) + torch.tensor([-1,-3,-5])\n\nRuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1\n\n\n\nğŸ—£ï¸ ê¸¸ì´ê°€ 3ì¸ vectorë¥¼ column vectorë¡œ í•´ì„í•˜ê³  (3,2)ë¡œ ì±„ì›Œì„œ ê³„ì‚°í•  ê²ƒ ê°™ì§€ë§Œ X (ì´ë²ˆì— ë°œê²¬)\n- í–‰ë ¬ê³±\nì •ìƒì ì¸ í–‰ë ¬ê³±\n\ntorch.tensor([[1,2],[3,4],[5,6]]) @ torch.tensor([[1],[2]])\n\ntensor([[ 5],\n        [11],\n        [17]])\n\n\nğŸ—£ï¸ (3,2) matirx @ (2,1) vector = (3,1) matrix\n\ntorch.tensor([[1,2,3]]) @ torch.tensor([[1,2],[3,4],[5,6]]) \n\ntensor([[22, 28]])\n\n\nğŸ—£ï¸ (1,3) @ (3,2) = (1,2)\nì˜ëª»ëœ í–‰ë ¬ê³±\n\ntorch.tensor([[1,2],[3,4],[5,6]]) @ torch.tensor([[1,2]])\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[18], line 1\n----&gt; 1 torch.tensor([[1,2],[3,4],[5,6]]) @ torch.tensor([[1,2]])\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (3x2 and 1x2)\n\n\n\nğŸ—£ï¸ (3,2) @ (1,2) ë¶ˆê°€\n\ntorch.tensor([[1],[2],[3]]) @ torch.tensor([[1,2],[3,4],[5,6]]) \n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[19], line 1\n----&gt; 1 torch.tensor([[1],[2],[3]]) @ torch.tensor([[1,2],[3,4],[5,6]]) \n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (3x1 and 3x2)\n\n\n\nğŸ—£ï¸ (3,1) @ (3,2) ë¶ˆê°€\nì´ìƒí•œ ê²ƒ\n\ntorch.tensor([[1,2],[3,4],[5,6]]) @ torch.tensor([1,2]) # ì´ê²Œ ì™œ ê°€ëŠ¥..\n\ntensor([ 5, 11, 17])\n\n\nğŸ—£ï¸ (3,2) @ (2) ê¸¸ì´ê°€ 2ì¸ vector / ì‚¬ëŒë§ˆë‹¤ í•´ì„ ì• ë§¤ (2,1)? (1,2)? / ê³±í•˜ê¸°ë¥¼ ìœ„í•´ (2,1) column vectorë¡œ í•´ì„\nğŸ—£ï¸ (3,2) @ (2,1)ë¡œ í•´ì„ í›„ ê³„ì‚°í•˜ì—¬ (3) ê¸¸ì´ê°€ 3ì¸ vectorê°€ ë‚˜ì˜´\n\ntorch.tensor([1,2,3]) @ torch.tensor([[1,2],[3,4],[5,6]]) # ì´ê±´ ì™œ ê°€ëŠ¥?\n\ntensor([22, 28])\n\n\nğŸ—£ï¸ (3) @ (3,2)ì—ì„œ (3)ì„ (1,3) row vectorë¡œ í•´ì„\nğŸ—£ï¸( ì—„ë°€í•˜ê²Œ í•˜ë ¤ë©´\n\ntorch.tensor([[1,2,3]]) @ torch.tensor([[1,2],[3,4],[5,6]])\n\ntensor([[22, 28]])\n\n\nâœï¸ ë‹¹ì—°íˆ ê²°ê³¼ì˜ ì°¨ì›ë„ ë‹¤ë¦„\n)ğŸ—£ï¸"
  },
  {
    "objectID": "posts/01wk-1.html#c.-transpose-reshape",
    "href": "posts/01wk-1.html#c.-transpose-reshape",
    "title": "01wk-1: (í† ì¹˜) â€“ ê°•ì˜ì†Œê°œ, íŒŒì´í† ì¹˜ ê¸°ë³¸",
    "section": "C. transpose, reshape",
    "text": "C. transpose, reshape\n- transpose\n\ntorch.tensor([[1,2],[3,4]]).T \n\ntensor([[1, 3],\n        [2, 4]])\n\n\n\ntorch.tensor([[1],[3]]).T \n\ntensor([[1, 3]])\n\n\nğŸ—£ï¸ column vector -&gt; row vector\n\ntorch.tensor([[1,2]]).T \n\ntensor([[1],\n        [2]])\n\n\nğŸ—£ï¸ row vector -&gt; column vector\nğŸ—£ï¸ ì°¨ì›ì„ ë°”ê¾¸ëŠ” íš¨ê³¼ (1,2) -&gt; (2,1)\n- reshape\nğŸ—£ï¸( ì°¨ì› ë³´ê¸°\n\ntorch.tensor([[1,2]]).shape\n\ntorch.Size([1, 2])\n\n\nì„ column vectorë¡œ ë°”ê¾¸ê³  ì‹¶ìœ¼ë©´\n\ntorch.tensor([[1,2]]).reshape(2,1)\n\ntensor([[1],\n        [2]])\n\n\ntransposeì™€ ë™ì¼\n)ğŸ—£ï¸\nì¼ë°˜ì ì¸ ì‚¬ìš©\n\ntorch.tensor([[1,2],[3,4],[5,6]]).reshape(2,3)\n\ntensor([[1, 2, 3],\n        [4, 5, 6]])\n\n\n\ntorch.tensor([[1,2],[3,4],[5,6]])\n\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])\n\n\n\ntorch.tensor([[1,2],[3,4],[5,6]]).reshape(1,6)\n\ntensor([[1, 2, 3, 4, 5, 6]])\n\n\nğŸ—£ï¸ (3,2) -&gt; (1,6)\n\ntorch.tensor([[1,2],[3,4],[5,6]]).reshape(6)\n\ntensor([1, 2, 3, 4, 5, 6])\n\n\nğŸ—£ï¸ (3,2)ë¥¼ ê·¸ëƒ¥ 6ìœ¼ë¡œ : ê¸¸ì´ê°€ 6ì¸ vectorë¡œ ë°”ê¿ˆ\ní¸í•œ ê²ƒ\n\ntorch.tensor([[1,2],[3,4],[5,6]]).reshape(2,-1)\n\ntensor([[1, 2, 3],\n        [4, 5, 6]])\n\n\nğŸ—£ï¸ torch.tensor([[1,2],[3,4],[5,6]]).reshape(2,??)ë¥¼ ì›í•  ë•Œ ??ë¥¼ ì•Œì•„ì„œ ë§ì¶¤ (ë¶ˆê°€ëŠ¥í•˜ë©´ error)\n\ntorch.tensor([[1,2],[3,4],[5,6]]).reshape(6,-1)\n\ntensor([[1],\n        [2],\n        [3],\n        [4],\n        [5],\n        [6]])\n\n\n\ntorch.tensor([[1,2],[3,4],[5,6]]).reshape(-1,6)\n\ntensor([[1, 2, 3, 4, 5, 6]])\n\n\n\ntorch.tensor([[1,2],[3,4],[5,6]]).reshape(-1)\n\ntensor([1, 2, 3, 4, 5, 6])\n\n\nğŸ—£ï¸ ì „ì²´ë¥¼ vectorë¡œ ë°”ê¾¸ê³  ì‹¶ì„ ë•Œ (1ì°¨ì›)"
  },
  {
    "objectID": "posts/01wk-1.html#d.-concat-stack-starstarstar",
    "href": "posts/01wk-1.html#d.-concat-stack-starstarstar",
    "title": "01wk-1: (í† ì¹˜) â€“ ê°•ì˜ì†Œê°œ, íŒŒì´í† ì¹˜ ê¸°ë³¸",
    "section": "D. concat, stack \\((\\star\\star\\star)\\)",
    "text": "D. concat, stack \\((\\star\\star\\star)\\)\n- concat\n\na = torch.tensor([[1],[3],[5]])\nb = torch.tensor([[2],[4],[6]])\ntorch.concat([a,b],axis=1)\n\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])\n\n\nğŸ—£ï¸(\n\na\n\ntensor([[1],\n        [3],\n        [5]])\n\n\n\nb\n\ntensor([[2],\n        [4],\n        [6]])\n\n\naì™€ bë¥¼ ëª¨ë‘ vectorë¡œ ê°–ê³  ìˆëŠ”ë° [a b]ì²˜ëŸ¼ ë†“ê³  ì‹¶ì„ ë•Œ ì‚¬ìš©\n\na, b\n\n(tensor([[1],\n         [3],\n         [5]]),\n tensor([[2],\n         [4],\n         [6]]))\n\n\n\ntorch.concat([a,b]) # ì´ë ‡ê²Œ í•˜ë©´ ì¢Œìš°ê°€ ì•„ë‹ˆë¼ ìœ„ ì•„ë˜ë¡œ í•©ì³ì§\n\ntensor([[1],\n        [3],\n        [5],\n        [2],\n        [4],\n        [6]])\n\n\n(3,1)ê³¼ (3,1)ì„ (3,2)ë¡œ ë§Œë“¤ê³  ì‹¶ì—ˆëŠ”ë° (6,1)ì´ ë¨ -&gt; axis=1 ì˜µì…˜ ì‚¬ìš©í•˜ë©´ (3,2) ê°€ëŠ¥ (ëª¨ë¥´ê² ìœ¼ë©´ ë°‘ì˜ ë§í¬ ì°¸ì¡°)\n)ğŸ—£ï¸\n\ntorch.concat([a,b],axis=1)\n\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])\n\n\n- stack\n\na = torch.tensor([1,3,5])\nb = torch.tensor([2,4,6])\ntorch.stack([a,b],axis=1)\n\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])\n\n\nğŸ—£ï¸(\n\na\n\ntensor([1, 3, 5])\n\n\n\nb\n\ntensor([2, 4, 6])\n\n\n\na.reshape(3,1) # ì°¸ê³ ) concat ì„¤ëª… ì˜ˆì‹œì™€ ë™ì¼\n\ntensor([[1],\n        [3],\n        [5]])\n\n\n\ntorch.concat([a.reshape(3,1), b.reshape(3,1)], axis=1) # ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“  í›„ ì´ë ‡ê²Œ í•˜ë©´ ë˜ê¸´í•˜ë‚˜ ë„ˆë¬´ í˜ë“¦\n\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])\n\n\n\ntorch.stack([a,b], axis=1) # ê°™ì€ ê²°ê³¼\n\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])\n\n\nì°¨ì´: concatì€ ë°”ê¾¸ë ¤ëŠ” ëŒ€ìƒì˜ dimensionì„ ë°”ê¾¸ì§€ëŠ” X (matrixëŠ” matrixë¡œ, vectorëŠ” vectorë¡œ) / stackì€ dimensionì„ í•˜ë‚˜ ëŠ˜ë ¤ì„œ ë°”ê¿”ì¤Œ\nconcatê³¼ stack ë‘˜ ë‹¤ ì•Œë©´ ì¢‹ìŒ\n)ğŸ—£ï¸\n\ntorch.concat([a.reshape(3,1),b.reshape(3,1)],axis=1)\n\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])\n\n\n\n\n\n\n\n\nWarning\n\n\n\nconcatê³¼ stackì„ ì§€ê¸ˆ ì²˜ìŒë³¸ë‹¤ë©´ ì•„ë˜ë¥¼ ë³µìŠµí•˜ì‹œëŠ”ê²Œ ì¢‹ìŠµë‹ˆë‹¤.\nhttps://guebin.github.io/PP2024/posts/06wk-2.html#numpyì™€-ì¶•axis"
  }
]