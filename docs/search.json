[
  {
    "objectID": "posts/01wk-1.html",
    "href": "posts/01wk-1.html",
    "title": "01wk-1: (í† ì¹˜) â€“ ê°•ì˜ì†Œê°œ, íŒŒì´í† ì¹˜ ê¸°ë³¸",
    "section": "",
    "text": "ğŸ“˜ Note Format Guide\nThis format serves as a structured guide for organizing lecture content, personal interpretation, experiments, and study-related questions.\nğŸ“ ğŸ—£ï¸ âœï¸ ğŸ”¬ â“"
  },
  {
    "objectID": "posts/01wk-1.html#a.-torch",
    "href": "posts/01wk-1.html#a.-torch",
    "title": "01wk-1: (í† ì¹˜) â€“ ê°•ì˜ì†Œê°œ, íŒŒì´í† ì¹˜ ê¸°ë³¸",
    "section": "A. torch",
    "text": "A. torch\nğŸ—£ï¸ torchëŠ” numpyì™€ ë¹„ìŠ· (ë²¡í„° ë§Œë“¤ê¸° ë“±)\n- ë²¡í„°\n\ntorch.tensor([1,2,3])\n\ntensor([1, 2, 3])\n\n\n- ë²¡í„°ì˜ ë§ì…ˆ\n\ntorch.tensor([1,2,3]) + torch.tensor([2,2,2])\n\ntensor([3, 4, 5])\n\n\n- ë¸Œë¡œë“œìºìŠ¤íŒ…\n\ntorch.tensor([1,2,3]) + 2\n\ntensor([3, 4, 5])"
  },
  {
    "objectID": "posts/01wk-1.html#b.-ë²¡í„°ì™€-ë§¤íŠ¸ë¦­ìŠ¤",
    "href": "posts/01wk-1.html#b.-ë²¡í„°ì™€-ë§¤íŠ¸ë¦­ìŠ¤",
    "title": "01wk-1: (í† ì¹˜) â€“ ê°•ì˜ì†Œê°œ, íŒŒì´í† ì¹˜ ê¸°ë³¸",
    "section": "B. ë²¡í„°ì™€ ë§¤íŠ¸ë¦­ìŠ¤",
    "text": "B. ë²¡í„°ì™€ ë§¤íŠ¸ë¦­ìŠ¤\nğŸ—£ï¸ torch.tensorëŠ” np.arrayì™€ ë¹„ìŠ·\n- \\(3 \\times 2\\) matrix\n\ntorch.tensor([[1,2],[3,4],[5,6]]) \n\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])\n\n\n- \\(3 \\times 1\\) matrix = \\(3 \\times 1\\) column vector\n\ntorch.tensor([[1],[3],[5]]) \n\ntensor([[1],\n        [3],\n        [5]])\n\n\n- \\(1 \\times 2\\) matrix = \\(1 \\times 2\\) row vector\n\ntorch.tensor([[1,2]]) \n\ntensor([[1, 2]])\n\n\nğŸ—£ï¸ torch.tensor([[1,2],[3,4],[5,6]])ì—ì„œ [3,4],[5,6] ì‚­ì œë¼ê³  ìƒê°\nğŸ—£ï¸ column vectorì™€ row vectorëŠ” êµ¬ë¶„ë˜ê³  ì„ ì–¸ ë°©ë²•ì´ ë‹¤ë¦„\n- ë”í•˜ê¸°\në¸Œë¡œë“œìºìŠ¤íŒ…(í¸í•œê±°)\n\ntorch.tensor([[1,2],[3,4],[5,6]]) - 1\n\ntensor([[0, 1],\n        [2, 3],\n        [4, 5]])\n\n\nğŸ—£ï¸ â€œmatrix - scalarâ€ëŠ” ë¶ˆê°€ëŠ¥í•˜ì§€ë§Œ ì•Œì•„ì„œ ì›ì†Œë³„ë¡œ ì „ë¶€ ëºŒ\n\ntorch.tensor([[1,2],[3,4],[5,6]]) + torch.tensor([[-1],[-3],[-5]])\n\ntensor([[0, 1],\n        [0, 1],\n        [0, 1]])\n\n\nğŸ—£ï¸ (3, 2) - (3, 1)ì„ ì•Œì•„ì„œ ëºŒ\nâœï¸ torch.tensor([[-1,-1],[-3, 3],[-5,-5]])\n\ntorch.tensor([[1,2],[3,4],[5,6]]) + torch.tensor([[-1,-2]])\n\ntensor([[0, 0],\n        [2, 2],\n        [4, 4]])\n\n\nğŸ—£ï¸ (3, 2) - (1, 2)ì„ ì•Œì•„ì„œ ëºŒ\nâœï¸ torch.tensor([[-1,-2],[-1,-2],[-1,-2]])\nì˜ëª»ëœ ë¸Œë¡œë“œìºìŠ¤íŒ…\n\ntorch.tensor([[1,2],[3,4],[5,6]]) + torch.tensor([[-1,-3,-5]])\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[11], line 1\n----&gt; 1 torch.tensor([[1,2],[3,4],[5,6]]) + torch.tensor([[-1,-3,-5]])\n\nRuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1\n\n\n\nğŸ—£ï¸ ì„¸ë¡œë¡œ ì“°ê±°ë‚˜ ê°€ë¡œë¡œ ë‘ ê°œì˜ ì›ì†Œë§Œ ì¼ìœ¼ë©´ ê°€ëŠ¥\nâœï¸ torch.tensor([[-1],[-3],[-5]]) ë˜ëŠ” torch.tensor([[-1,-3],[-1,-3],[-1,-3]]) ë“±\n\ntorch.tensor([[1,2],[3,4],[5,6]]) + torch.tensor([[-1],[-2]])\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[12], line 1\n----&gt; 1 torch.tensor([[1,2],[3,4],[5,6]]) + torch.tensor([[-1],[-2]])\n\nRuntimeError: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 0\n\n\n\nğŸ—£ï¸ (3, 2) - (2, 1) ëŠ” ì•Œì•„ì„œ ì±„ìš°ê¸° ì–´ë ¤ìš°ë¯€ë¡œ error\nì´ìƒí•œ ê²ƒ\n\ntorch.tensor([[1,2],[3,4],[5,6]]) + torch.tensor([-1,-2])\n\ntensor([[0, 0],\n        [2, 2],\n        [4, 4]])\n\n\nğŸ—£ï¸ (3, 2) matrix - ê¸¸ì´ê°€ 2ì¸ vector(2x1, 1x2 ë‘˜ ë‹¤ ì•„ë‹˜)\nğŸ—£ï¸ â€œmatrix - vectorâ€ë¥¼ row vectorë¡œ í•´ì„í•˜ê³  ëŠ˜ë ¤ì„œ ê³„ì‚°í•œ ë“¯\nâœï¸ torch.tensor([[-1,-2],[-1,-2],[-1,-2]])\nğŸ”¬(\n\nì°¨ì› ìˆ˜ë§Œ ì•Œê³  ì‹¶ì„ ë•Œ â†’ tensor.dim() ë˜ëŠ” tensor.ndim\nê° ì°¨ì›ì˜ í¬ê¸°ê¹Œì§€ ì•Œê³  ì‹¶ì„ ë•Œ â†’ tensor.shape ë˜ëŠ” tensor.size()\n\n\nprint(torch.tensor([[1,2],[3,4],[5,6]]).dim())\nprint(torch.tensor([[1,2],[3,4],[5,6]]).shape)\nprint(torch.tensor([-1,-2]).dim())\nprint(torch.tensor([-1,-2]).shape)\nprint(torch.tensor([[1,2],[3,4],[5,6]]).ndim)\nprint(torch.tensor([[1,2],[3,4],[5,6]]).size())\nprint(torch.tensor([-1,-2]).ndim)\nprint(torch.tensor([-1,-2]).size())\n\n2\ntorch.Size([3, 2])\n1\ntorch.Size([2])\n2\ntorch.Size([3, 2])\n1\ntorch.Size([2])\n\n\n\nì°¸ê³  (Chat GPT4o)\n\n\nNumPyì™€ PyTorch ì°¨ì´ ì •ë¦¬\n\n\n\n\nê¸°ëŠ¥\nPyTorch\nNumPy\n\n\n\n\nì°¨ì› ìˆ˜\n.dim() ë˜ëŠ” .ndim\n.ndim\n\n\nshape í™•ì¸\n.shape ë˜ëŠ” .size()\n.shape\n\n\ní¬ê¸° ë³€ê²½\n.view(), .reshape()\n.reshape()\n\n\níƒ€ì…\ntorch.Tensor\nnp.ndarray\n\n\n\n\nì‹¤ì „ íŒ:\n\nPyTorchì˜ .dim()ë§Œ NumPyì—ì„œ ì•ˆ ë¨¹íŒë‹¤ëŠ” ê²ƒë§Œ ê¸°ì–µí•˜ë©´ ë‘˜ ë‹¤ ê±°ì˜ ë¹„ìŠ·í•˜ê²Œ ë‹¤ë£° ìˆ˜ ìˆìŒ\në‹¤ì°¨ì› ë°°ì—´ì„ ë‹¤ë£° ë•Œ .ndim, .shapeëŠ” ì–‘ìª½ ëª¨ë‘ ì•ˆì „í•˜ê²Œ ì“¸ ìˆ˜ ìˆëŠ” í•µì‹¬ ë„êµ¬\ndim()ì€ PyTorch ê³ ìœ  ë©”ì„œë“œ\n\n\n)ğŸ”¬\n\ntorch.tensor([[1,2],[3,4],[5,6]]) + torch.tensor([-1,-3,-5])\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[15], line 1\n----&gt; 1 torch.tensor([[1,2],[3,4],[5,6]]) + torch.tensor([-1,-3,-5])\n\nRuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1\n\n\n\nğŸ—£ï¸ ê¸¸ì´ê°€ 3ì¸ vectorë¥¼ column vectorë¡œ í•´ì„í•˜ê³  (3,2)ë¡œ ì±„ì›Œì„œ ê³„ì‚°í•  ê²ƒ ê°™ì§€ë§Œ X (ì´ë²ˆì— ë°œê²¬)\n- í–‰ë ¬ê³±\nì •ìƒì ì¸ í–‰ë ¬ê³±\n\ntorch.tensor([[1,2],[3,4],[5,6]]) @ torch.tensor([[1],[2]])\n\ntensor([[ 5],\n        [11],\n        [17]])\n\n\nğŸ—£ï¸ (3,2) matirx @ (2,1) vector = (3,1) matrix\n\ntorch.tensor([[1,2,3]]) @ torch.tensor([[1,2],[3,4],[5,6]]) \n\ntensor([[22, 28]])\n\n\nğŸ—£ï¸ (1,3) @ (3,2) = (1,2)\nì˜ëª»ëœ í–‰ë ¬ê³±\n\ntorch.tensor([[1,2],[3,4],[5,6]]) @ torch.tensor([[1,2]])\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[18], line 1\n----&gt; 1 torch.tensor([[1,2],[3,4],[5,6]]) @ torch.tensor([[1,2]])\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (3x2 and 1x2)\n\n\n\nğŸ—£ï¸ (3,2) @ (1,2) ë¶ˆê°€\n\ntorch.tensor([[1],[2],[3]]) @ torch.tensor([[1,2],[3,4],[5,6]]) \n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[19], line 1\n----&gt; 1 torch.tensor([[1],[2],[3]]) @ torch.tensor([[1,2],[3,4],[5,6]]) \n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (3x1 and 3x2)\n\n\n\nğŸ—£ï¸ (3,1) @ (3,2) ë¶ˆê°€\nì´ìƒí•œ ê²ƒ\n\ntorch.tensor([[1,2],[3,4],[5,6]]) @ torch.tensor([1,2]) # ì´ê²Œ ì™œ ê°€ëŠ¥..\n\ntensor([ 5, 11, 17])\n\n\nğŸ—£ï¸ (3,2) @ (2) ê¸¸ì´ê°€ 2ì¸ vector / ì‚¬ëŒë§ˆë‹¤ í•´ì„ ì• ë§¤ (2,1)? (1,2)? / ê³±í•˜ê¸°ë¥¼ ìœ„í•´ (2,1) column vectorë¡œ í•´ì„\nğŸ—£ï¸ (3,2) @ (2,1)ë¡œ í•´ì„ í›„ ê³„ì‚°í•˜ì—¬ (3) ê¸¸ì´ê°€ 3ì¸ vectorê°€ ë‚˜ì˜´\n\ntorch.tensor([1,2,3]) @ torch.tensor([[1,2],[3,4],[5,6]]) # ì´ê±´ ì™œ ê°€ëŠ¥?\n\ntensor([22, 28])\n\n\nğŸ—£ï¸ (3) @ (3,2)ì—ì„œ (3)ì„ (1,3) row vectorë¡œ í•´ì„\nğŸ—£ï¸( ì—„ë°€í•˜ê²Œ í•˜ë ¤ë©´\n\ntorch.tensor([[1,2,3]]) @ torch.tensor([[1,2],[3,4],[5,6]])\n\ntensor([[22, 28]])\n\n\nâœï¸ ë‹¹ì—°íˆ ê²°ê³¼ì˜ ì°¨ì›ë„ ë‹¤ë¦„\n)ğŸ—£ï¸"
  },
  {
    "objectID": "posts/01wk-1.html#c.-transpose-reshape",
    "href": "posts/01wk-1.html#c.-transpose-reshape",
    "title": "01wk-1: (í† ì¹˜) â€“ ê°•ì˜ì†Œê°œ, íŒŒì´í† ì¹˜ ê¸°ë³¸",
    "section": "C. transpose, reshape",
    "text": "C. transpose, reshape\n- transpose\n\ntorch.tensor([[1,2],[3,4]]).T \n\ntensor([[1, 3],\n        [2, 4]])\n\n\n\ntorch.tensor([[1],[3]]).T \n\ntensor([[1, 3]])\n\n\nğŸ—£ï¸ column vector -&gt; row vector\n\ntorch.tensor([[1,2]]).T \n\ntensor([[1],\n        [2]])\n\n\nğŸ—£ï¸ row vector -&gt; column vector\nğŸ—£ï¸ ì°¨ì›ì„ ë°”ê¾¸ëŠ” íš¨ê³¼ (1,2) -&gt; (2,1)\n- reshape\nğŸ—£ï¸( ì°¨ì› ë³´ê¸°\n\ntorch.tensor([[1,2]]).shape\n\ntorch.Size([1, 2])\n\n\nì„ column vectorë¡œ ë°”ê¾¸ê³  ì‹¶ìœ¼ë©´\n\ntorch.tensor([[1,2]]).reshape(2,1)\n\ntensor([[1],\n        [2]])\n\n\ntransposeì™€ ë™ì¼\n)ğŸ—£ï¸\nì¼ë°˜ì ì¸ ì‚¬ìš©\n\ntorch.tensor([[1,2],[3,4],[5,6]]).reshape(2,3)\n\ntensor([[1, 2, 3],\n        [4, 5, 6]])\n\n\n\ntorch.tensor([[1,2],[3,4],[5,6]])\n\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])\n\n\n\ntorch.tensor([[1,2],[3,4],[5,6]]).reshape(1,6)\n\ntensor([[1, 2, 3, 4, 5, 6]])\n\n\nğŸ—£ï¸ (3,2) -&gt; (1,6)\n\ntorch.tensor([[1,2],[3,4],[5,6]]).reshape(6)\n\ntensor([1, 2, 3, 4, 5, 6])\n\n\nğŸ—£ï¸ (3,2)ë¥¼ ê·¸ëƒ¥ 6ìœ¼ë¡œ : ê¸¸ì´ê°€ 6ì¸ vectorë¡œ ë°”ê¿ˆ\ní¸í•œ ê²ƒ\n\ntorch.tensor([[1,2],[3,4],[5,6]]).reshape(2,-1)\n\ntensor([[1, 2, 3],\n        [4, 5, 6]])\n\n\nğŸ—£ï¸ torch.tensor([[1,2],[3,4],[5,6]]).reshape(2,??)ë¥¼ ì›í•  ë•Œ ??ë¥¼ ì•Œì•„ì„œ ë§ì¶¤ (ë¶ˆê°€ëŠ¥í•˜ë©´ error)\n\ntorch.tensor([[1,2],[3,4],[5,6]]).reshape(6,-1)\n\ntensor([[1],\n        [2],\n        [3],\n        [4],\n        [5],\n        [6]])\n\n\n\ntorch.tensor([[1,2],[3,4],[5,6]]).reshape(-1,6)\n\ntensor([[1, 2, 3, 4, 5, 6]])\n\n\n\ntorch.tensor([[1,2],[3,4],[5,6]]).reshape(-1)\n\ntensor([1, 2, 3, 4, 5, 6])\n\n\nğŸ—£ï¸ ì „ì²´ë¥¼ vectorë¡œ ë°”ê¾¸ê³  ì‹¶ì„ ë•Œ (1ì°¨ì›)"
  },
  {
    "objectID": "posts/01wk-1.html#d.-concat-stack-starstarstar",
    "href": "posts/01wk-1.html#d.-concat-stack-starstarstar",
    "title": "01wk-1: (í† ì¹˜) â€“ ê°•ì˜ì†Œê°œ, íŒŒì´í† ì¹˜ ê¸°ë³¸",
    "section": "D. concat, stack \\((\\star\\star\\star)\\)",
    "text": "D. concat, stack \\((\\star\\star\\star)\\)\n- concat\n\na = torch.tensor([[1],[3],[5]])\nb = torch.tensor([[2],[4],[6]])\ntorch.concat([a,b],axis=1)\n\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])\n\n\nğŸ—£ï¸(\n\na\n\ntensor([[1],\n        [3],\n        [5]])\n\n\n\nb\n\ntensor([[2],\n        [4],\n        [6]])\n\n\naì™€ bë¥¼ ëª¨ë‘ vectorë¡œ ê°–ê³  ìˆëŠ”ë° [a b]ì²˜ëŸ¼ ë†“ê³  ì‹¶ì„ ë•Œ ì‚¬ìš©\n\na, b\n\n(tensor([[1],\n         [3],\n         [5]]),\n tensor([[2],\n         [4],\n         [6]]))\n\n\n\ntorch.concat([a,b]) # ì´ë ‡ê²Œ í•˜ë©´ ì¢Œìš°ê°€ ì•„ë‹ˆë¼ ìœ„ ì•„ë˜ë¡œ í•©ì³ì§\n\ntensor([[1],\n        [3],\n        [5],\n        [2],\n        [4],\n        [6]])\n\n\n(3,1)ê³¼ (3,1)ì„ (3,2)ë¡œ ë§Œë“¤ê³  ì‹¶ì—ˆëŠ”ë° (6,1)ì´ ë¨ -&gt; axis=1 ì˜µì…˜ ì‚¬ìš©í•˜ë©´ (3,2) ê°€ëŠ¥ (ëª¨ë¥´ê² ìœ¼ë©´ ë°‘ì˜ ë§í¬ ì°¸ì¡°)\n)ğŸ—£ï¸\n\ntorch.concat([a,b],axis=1)\n\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])\n\n\n- stack\n\na = torch.tensor([1,3,5])\nb = torch.tensor([2,4,6])\ntorch.stack([a,b],axis=1)\n\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])\n\n\nğŸ—£ï¸(\n\na\n\ntensor([1, 3, 5])\n\n\n\nb\n\ntensor([2, 4, 6])\n\n\n\na.reshape(3,1) # ì°¸ê³ ) concat ì„¤ëª… ì˜ˆì‹œì™€ ë™ì¼\n\ntensor([[1],\n        [3],\n        [5]])\n\n\n\ntorch.concat([a.reshape(3,1), b.reshape(3,1)], axis=1) # ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“  í›„ ì´ë ‡ê²Œ í•˜ë©´ ë˜ê¸´í•˜ë‚˜ ë„ˆë¬´ í˜ë“¦\n\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])\n\n\n\ntorch.stack([a,b], axis=1) # ê°™ì€ ê²°ê³¼\n\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])\n\n\nì°¨ì´: concatì€ ë°”ê¾¸ë ¤ëŠ” ëŒ€ìƒì˜ dimensionì„ ë°”ê¾¸ì§€ëŠ” X (matrixëŠ” matrixë¡œ, vectorëŠ” vectorë¡œ) / stackì€ dimensionì„ í•˜ë‚˜ ëŠ˜ë ¤ì„œ ë°”ê¿”ì¤Œ\nconcatê³¼ stack ë‘˜ ë‹¤ ì•Œë©´ ì¢‹ìŒ\n)ğŸ—£ï¸\n\ntorch.concat([a.reshape(3,1),b.reshape(3,1)],axis=1)\n\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])\n\n\n\n\n\n\n\n\nWarning\n\n\n\nconcatê³¼ stackì„ ì§€ê¸ˆ ì²˜ìŒë³¸ë‹¤ë©´ ì•„ë˜ë¥¼ ë³µìŠµí•˜ì‹œëŠ”ê²Œ ì¢‹ìŠµë‹ˆë‹¤.\nhttps://guebin.github.io/PP2024/posts/06wk-2.html#numpyì™€-ì¶•axis"
  },
  {
    "objectID": "posts/02wk-2.html",
    "href": "posts/02wk-2.html",
    "title": "02wk-2: (íšŒê·€) â€“ íŒŒë¼ë©”í„°ì˜ í•™ìŠµê³¼ì • ìŒë¯¸, MSE, íŒŒì´í† ì¹˜ì‹ ì½”ë”©íŒ¨í„´ (1)",
    "section": "",
    "text": "ğŸ“˜ Note Format Guide\nThis format serves as a structured guide for organizing lecture content, personal interpretation, experiments, and study-related questions.\nğŸ“ ğŸ—£ï¸ âœï¸ ğŸ”¬ â“"
  },
  {
    "objectID": "posts/02wk-2.html#a.-print",
    "href": "posts/02wk-2.html#a.-print",
    "title": "02wk-2: (íšŒê·€) â€“ íŒŒë¼ë©”í„°ì˜ í•™ìŠµê³¼ì • ìŒë¯¸, MSE, íŒŒì´í† ì¹˜ì‹ ì½”ë”©íŒ¨í„´ (1)",
    "section": "A. print",
    "text": "A. print\n\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad=True)\nalpha = 0.001\nprint(f\"ì‹œì‘ê°’ = {What.data.reshape(-1)}\")\nfor epoc in range(30):\n    yhat = X @ What\n    loss = torch.sum((y-yhat)**2)\n    loss.backward()\n    What.data = What.data - alpha * What.grad\n    print(f'loss = {loss:.2f} \\t ì—…ë°ì´íŠ¸í­ = {-alpha * What.grad.reshape(-1)} \\t ì—…ë°ì´íŠ¸ê²°ê³¼: {What.data.reshape(-1)}')\n    What.grad = None\n\nì‹œì‘ê°’ = tensor([-5., 10.])\nloss = 8587.69   ì—…ë°ì´íŠ¸í­ = tensor([ 1.3423, -1.1889])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([-3.6577,  8.8111])\nloss = 5675.21   ì—…ë°ì´íŠ¸í­ = tensor([ 1.1029, -0.9499])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([-2.5548,  7.8612])\nloss = 3755.64   ì—…ë°ì´íŠ¸í­ = tensor([ 0.9056, -0.7596])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([-1.6492,  7.1016])\nloss = 2489.58   ì—…ë°ì´íŠ¸í­ = tensor([ 0.7431, -0.6081])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([-0.9061,  6.4935])\nloss = 1654.04   ì—…ë°ì´íŠ¸í­ = tensor([ 0.6094, -0.4872])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([-0.2967,  6.0063])\nloss = 1102.32   ì—…ë°ì´íŠ¸í­ = tensor([ 0.4995, -0.3907])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([0.2028, 5.6156])\nloss = 737.84    ì—…ë°ì´íŠ¸í­ = tensor([ 0.4091, -0.3136])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([0.6119, 5.3020])\nloss = 496.97    ì—…ë°ì´íŠ¸í­ = tensor([ 0.3350, -0.2519])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([0.9469, 5.0501])\nloss = 337.71    ì—…ë°ì´íŠ¸í­ = tensor([ 0.2742, -0.2025])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([1.2211, 4.8477])\nloss = 232.40    ì—…ë°ì´íŠ¸í­ = tensor([ 0.2243, -0.1629])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([1.4454, 4.6848])\nloss = 162.73    ì—…ë°ì´íŠ¸í­ = tensor([ 0.1834, -0.1311])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([1.6288, 4.5537])\nloss = 116.63    ì—…ë°ì´íŠ¸í­ = tensor([ 0.1500, -0.1056])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([1.7787, 4.4480])\nloss = 86.13     ì—…ë°ì´íŠ¸í­ = tensor([ 0.1226, -0.0851])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([1.9013, 4.3629])\nloss = 65.93     ì—…ë°ì´íŠ¸í­ = tensor([ 0.1001, -0.0687])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([2.0014, 4.2942])\nloss = 52.57     ì—…ë°ì´íŠ¸í­ = tensor([ 0.0818, -0.0554])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([2.0832, 4.2388])\nloss = 43.72     ì—…ë°ì´íŠ¸í­ = tensor([ 0.0668, -0.0447])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([2.1500, 4.1941])\nloss = 37.86     ì—…ë°ì´íŠ¸í­ = tensor([ 0.0545, -0.0361])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([2.2045, 4.1579])\nloss = 33.97     ì—…ë°ì´íŠ¸í­ = tensor([ 0.0445, -0.0292])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([2.2490, 4.1287])\nloss = 31.40     ì—…ë°ì´íŠ¸í­ = tensor([ 0.0363, -0.0236])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([2.2853, 4.1051])\nloss = 29.70     ì—…ë°ì´íŠ¸í­ = tensor([ 0.0296, -0.0191])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([2.3150, 4.0860])\nloss = 28.57     ì—…ë°ì´íŠ¸í­ = tensor([ 0.0242, -0.0155])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([2.3392, 4.0705])\nloss = 27.83     ì—…ë°ì´íŠ¸í­ = tensor([ 0.0197, -0.0125])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([2.3589, 4.0580])\nloss = 27.33     ì—…ë°ì´íŠ¸í­ = tensor([ 0.0161, -0.0101])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([2.3750, 4.0479])\nloss = 27.00     ì—…ë°ì´íŠ¸í­ = tensor([ 0.0131, -0.0082])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([2.3881, 4.0396])\nloss = 26.79     ì—…ë°ì´íŠ¸í­ = tensor([ 0.0107, -0.0067])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([2.3988, 4.0330])\nloss = 26.64     ì—…ë°ì´íŠ¸í­ = tensor([ 0.0087, -0.0054])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([2.4075, 4.0276])\nloss = 26.55     ì—…ë°ì´íŠ¸í­ = tensor([ 0.0071, -0.0044])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([2.4146, 4.0232])\nloss = 26.48     ì—…ë°ì´íŠ¸í­ = tensor([ 0.0058, -0.0035])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([2.4204, 4.0197])\nloss = 26.44     ì—…ë°ì´íŠ¸í­ = tensor([ 0.0047, -0.0029])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([2.4251, 4.0168])\nloss = 26.41     ì—…ë°ì´íŠ¸í­ = tensor([ 0.0038, -0.0023])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([2.4290, 4.0144])\n\n\n\nğŸ—£ï¸\n\nlossë§Œ ë³´ë©´ ì ì  ê°ì†Œí•¨, ê°ˆìˆ˜ë¡ ê°ì†Œí•˜ëŠ” í­ë„ ì‘ì•„ì§€ë©° 26 ê·¼ì²˜ë¡œ ìˆ˜ë ´\nì—…ë°ì´íŠ¸ í­ë„ ì²˜ìŒì—ëŠ” ì»¸ë‹¤ê°€ ê°ì†Œ\nì´ì— ë”°ë¼ ì—…ë°ì´íŠ¸ ê²°ê³¼ë„ ê°ˆìˆ˜ë¡ ì˜ ì•ˆ ë°”ë€œ"
  },
  {
    "objectID": "posts/02wk-2.html#b.-ì‹œê°í™”-yhatì˜-ê´€ì ì—ì„œ",
    "href": "posts/02wk-2.html#b.-ì‹œê°í™”-yhatì˜-ê´€ì ì—ì„œ",
    "title": "02wk-2: (íšŒê·€) â€“ íŒŒë¼ë©”í„°ì˜ í•™ìŠµê³¼ì • ìŒë¯¸, MSE, íŒŒì´í† ì¹˜ì‹ ì½”ë”©íŒ¨í„´ (1)",
    "section": "B. ì‹œê°í™” â€“ yhatì˜ ê´€ì ì—ì„œ!",
    "text": "B. ì‹œê°í™” â€“ yhatì˜ ê´€ì ì—ì„œ!\nğŸ—£ï¸(\n\nWhat = torch.tensor([[-5.0], [10.0]], requires_grad=True)\n\n\nyhat = X@What\nloss = torch.sum((yhat-y)**2)\nloss.backward()\nWhat.data = What.data - 0.001*What.grad\nWhat.grad = None\n\n\nplt.plot(x,y,'o')\nplt.plot(x, (X@What).data, '--', color=\"C1\") # ì„  ìƒ‰ê¹” ì£¼í™©ìƒ‰ ê³ ì •\n\n\n\n\n\n\n\n\n\nì•„ë˜ ì½”ë“œë¥¼ ë°˜ë³µí•˜ë©° ì§€ì¼œë³´ë©´ ì„ ì´ ë³€í™”í•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŒ (ë°‘ì˜ ê·¸ë˜í”„ëŠ” ì—¬ëŸ¬ë²ˆ ë°˜ë³µí•œ ìµœì¢… ê²°ê³¼)\n\n\nyhat = X@What\nloss = torch.sum((yhat-y)**2)\nloss.backward()\nWhat.data = What.data - 0.001*What.grad\nWhat.grad = None\nplt.plot(x,y,'o')\nplt.plot(x, (X@What).data, '--', color=\"C1\")\n\n\n\n\n\n\n\n\n\ní•œ ê°€ì§€ ì•„ì‰¬ìš´ ì : ì¤‘ê°„ ê³¼ì •ì˜ ê·¸ë˜í”„ê°€ ì‚¬ë¼ì§\n\n\nWhat = torch.tensor([[-5.0], [10.0]], requires_grad=True)\n\n\nplt.plot(x,y,'o')\nfig = plt.gcf() # ì¤‘ê°„ ê·¸ë¦¼ì„ ì €ì¥ (í˜¸ì¶œ ê°€ëŠ¥) get current figure\n\n\n\n\n\n\n\n\n\nax = fig.gca() # get current axes (axes: axisì˜ ë³µìˆ˜í˜•, ì—¬ê¸°ì„œëŠ” xì¶•,yì¶• ëª¨ë‘ë¥¼ ì§€ì¹­)\nyhat = X@What\nloss = torch.sum((yhat-y)**2)\nloss.backward()\nWhat.data = What.data - 0.001*What.grad\nWhat.grad = None\nax.plot(x, (X@What).data, '--', color=\"C1\") # pltë¥¼ axë¡œ ìˆ˜ì •\n\n\nfig # 1ë²ˆ ì‹¤í–‰\n\n\n\n\n\n\n\n\n\n1ë²ˆ ë” ì‹¤í–‰í•˜ë©´ ê²¹ì³ì§\n\n\nax = fig.gca() # get current axes (axes: axisì˜ ë³µìˆ˜í˜•, ì—¬ê¸°ì„œëŠ” xì¶•,yì¶• ëª¨ë‘ë¥¼ ì§€ì¹­)\nyhat = X@What\nloss = torch.sum((yhat-y)**2)\nloss.backward()\nWhat.data = What.data - 0.001*What.grad\nWhat.grad = None\nax.plot(x, (X@What).data, '--', color=\"C1\") # pltë¥¼ axë¡œ ìˆ˜ì •\n\n\nfig # 1ë²ˆ ë” ì‹¤í–‰ (ê²¹ì³ì§)\n\n\n\n\n\n\n\n\n\nì´ˆê¸°í™” í›„ ë°˜ë³µí•˜ë©´ ì—…ë°ì´íŠ¸ëœ í­ì„ ë³¼ ìˆ˜ ìˆìŒ (ì ì  ì¤„ì–´ë“œëŠ” ê²ƒ ê°™ìŒ)\n\n\nWhat = torch.tensor([[-5.0], [10.0]], requires_grad=True)\n\n\nplt.plot(x,y,'o')\nfig = plt.gcf()\n\n\n\n\n\n\n\n\n\nax = fig.gca()\nyhat = X@What\nloss = torch.sum((yhat-y)**2)\nloss.backward()\nWhat.data = What.data - 0.001*What.grad\nWhat.grad = None\nax.plot(x, (X@What).data, '--', color=\"C1\")\nfig\n\n\n\n\n\n\n\n\n\nì œëª©ì„ ë„£ì„ ìˆ˜ë„ ìˆìŒ (set_title, ë‹¨ìˆœ ë¬¸ìì—´ ì•„ë‹ˆì—¬ë„ ê°€ëŠ¥)\n\n\nWhat = torch.tensor([[-5.0], [10.0]], requires_grad=True)\n\nplt.plot(x,y,'o')\nfig = plt.gcf()\n\n\n\n\n\n\n\n\n\nfor epoc in range(20):\n    ax = fig.gca()\n    yhat = X@What\n    loss = torch.sum((yhat-y)**2)\n    loss.backward()\n    What.data = What.data - 0.001*What.grad\n    What.grad = None\n    ax.plot(x, (X@What).data, '--', color=\"C1\")\n    ax.set_title(What.data.reshape(-1))\n    fig\n\n\nfig\n\n\n\n\n\n\n\n\n)ğŸ—£ï¸\n\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad=True)\nalpha = 0.001\nplt.plot(x,y,'o',label = \"observed\")\nfig = plt.gcf()\nax = fig.gca()\nax.plot(x,X@What.data,'--',color=\"C1\")\nfor epoc in range(30):\n    yhat = X @ What\n    loss = torch.sum((y-yhat)**2)\n    loss.backward()\n    What.data = What.data - alpha * What.grad\n    ax.plot(x,X@What.data,'--',color=\"C1\",alpha=0.1)\n    What.grad = None\n\n\n\n\n\n\n\n\nğŸ—£ï¸ alpha: ê²¹ì³ì§€ë©´ ì§„í•´ì§"
  },
  {
    "objectID": "posts/02wk-2.html#c.-ì‹œê°í™”-lossì˜-ê´€ì ì—ì„œ",
    "href": "posts/02wk-2.html#c.-ì‹œê°í™”-lossì˜-ê´€ì ì—ì„œ",
    "title": "02wk-2: (íšŒê·€) â€“ íŒŒë¼ë©”í„°ì˜ í•™ìŠµê³¼ì • ìŒë¯¸, MSE, íŒŒì´í† ì¹˜ì‹ ì½”ë”©íŒ¨í„´ (1)",
    "section": "C. ì‹œê°í™” â€“ lossì˜ ê´€ì ì—ì„œ!!",
    "text": "C. ì‹œê°í™” â€“ lossì˜ ê´€ì ì—ì„œ!!\nğŸ—£ï¸(\n\ndef plot_loss():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    w0 = np.arange(-6, 11, 0.5) \n    w1 = np.arange(-6, 11, 0.5)\n    W1,W0 = np.meshgrid(w1,w0)\n    LOSS=W0*0\n    for i in range(len(w0)):\n        for j in range(len(w1)):\n            LOSS[i,j]=torch.sum((y-w0[i]-w1[j]*x)**2)\n    ax.plot_surface(W0, W1, LOSS, rstride=1, cstride=1, color='b',alpha=0.1)\n    ax.azim = 30  ## 3d plotì˜ view ì¡°ì ˆ \n    ax.dist = 8   ## 3d plotì˜ view ì¡°ì ˆ \n    ax.elev = 5   ## 3d plotì˜ view ì¡°ì ˆ \n    ax.set_xlabel(r'$w_0$')  # xì¶• ë ˆì´ë¸” ì„¤ì •\n    ax.set_ylabel(r'$w_1$')  # yì¶• ë ˆì´ë¸” ì„¤ì •\n    ax.set_xticks([-5,0,5,10])  # xì¶• í‹± ê°„ê²© ì„¤ì •\n    ax.set_yticks([-5,0,5,10])  # yì¶• í‹± ê°„ê²© ì„¤ì •\n    plt.close(fig)  # ìë™ ì¶œë ¥ ë°©ì§€\n    return fig\n\n\nfig = plot_loss()\nfig # loss_fn(w0hat, w1hat)ì„ zì— ì°ìŒ\n\n\n\n\n\n\n\n\n\n# ì†ì‹¤ 8587.6875 ë¥¼ ê³„ì‚°í•˜ëŠ” ë˜ ë‹¤ë¥¸ ë°©ì‹\ndef l(w0hat,w1hat):\n    yhat = w0hat + w1hat*x\n    return torch.sum((y-yhat)**2)\n\n\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad=True)\nWhat\n\ntensor([[-5.],\n        [10.]], requires_grad=True)\n\n\n\nl(-5,10) # ì†ì‹¤ ê³„ì‚°\n\ntensor(8587.6875)\n\n\n\ntorch.sum((y-X@What)**2) # ë‹¤ë¥¸ ë°©ë²•\n\ntensor(8587.6875, grad_fn=&lt;SumBackward0&gt;)\n\n\n\nyhat = -5 + 10*x\ntorch.sum((y-yhat)**2) # ë‹¤ë¥¸ ë°©ë²• 2\n\ntensor(8587.6875)\n\n\n\nfig\nax = fig.gca()\nax.scatter(-5, 10, l(-5,10)) # ì  ì°ê¸°\n\n&lt;mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7fc9261c2bb0&gt;\n\n\n\nfig\n\n\n\n\n\n\n\n\n\nfig\nax = fig.gca()\nax.scatter(-1, 3, l(-1,3)) # ë‹¤ë¥¸ ì  ì°ê¸°\n\n&lt;mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7fc925df1eb0&gt;\n\n\n\nfig\n\n\n\n\n\n\n\n\n\nìœ„ ê³¼ì •ì„ ë°˜ë³µí•˜ë©´ ê³¡ë©´ì„ ê·¸ë¦´ ìˆ˜ ìˆìŒ\në°‘ì€ True ê°’ ì°ê¸°\n\n\nfig\nax = fig.gca()\nax.scatter(2.5, 4.0, l(2.5,4.0)) # True ê°’\n\n&lt;mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7fc925f14670&gt;\n\n\n\nfig\n\n\n\n\n\n\n\n\n\në°‘ì— ì •ë¦¬ëœ ì½”ë“œ ê³¼ì •\n\n\nfig = plot_loss()\nfig # loss_fn(w0hat,w1hat)\nax = fig.gca()\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad=True)\nw0hat, w1hat = What.data.reshape(-1) # ì–¸íŒ¨í‚¹\nax.scatter(w0hat, w1hat, l(w0hat, w1hat)) # x, y, z\nfig # ìµœì´ˆì˜ ì§ì„ ì— ëŒ€ì‘í•˜ëŠ” What ê°’\n\n\n\n\n\n\n\n\n\nyhat = X@What\nloss = torch.sum((y-yhat)**2)\nloss.backward()\nWhat.data = What.data - 0.001*What.grad\nWhat.grad = None\nw0hat, w1hat = What.data.reshape(-1)\nax.scatter(w0hat, w1hat, l(w0hat, w1hat))\nfig # ë°˜ë³µ ì‹¤í–‰í• ìˆ˜ë¡ updateë¨\n\n\n\n\n\n\n\n\n\nyhat = X@What\nloss = torch.sum((y-yhat)**2)\nloss.backward()\nWhat.data = What.data - 0.001*What.grad\nWhat.grad = None\nw0hat, w1hat = What.data.reshape(-1)\nax.scatter(w0hat, w1hat, l(w0hat, w1hat), color=\"C1\") # ì•ìœ¼ë¡œëŠ” ì£¼í™©ìƒ‰ìœ¼ë¡œ ìƒ‰ê¹” ê³ ì •\nfig # ë°˜ë³µ ì‹¤í–‰í• ìˆ˜ë¡ ì ì  ìµœì†Œê°€ ë˜ëŠ” ìª½ìœ¼ë¡œ ì§„í–‰ë¨\n\n\n\n\n\n\n\n\n)ğŸ—£ï¸\n\ndef plot_loss():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    w0 = np.arange(-6, 11, 0.5) \n    w1 = np.arange(-6, 11, 0.5)\n    W1,W0 = np.meshgrid(w1,w0)\n    LOSS=W0*0\n    for i in range(len(w0)):\n        for j in range(len(w1)):\n            LOSS[i,j]=torch.sum((y-w0[i]-w1[j]*x)**2)\n    ax.plot_surface(W0, W1, LOSS, rstride=1, cstride=1, color='b',alpha=0.1)\n    ax.azim = 30  ## 3d plotì˜ view ì¡°ì ˆ \n    ax.dist = 8   ## 3d plotì˜ view ì¡°ì ˆ \n    ax.elev = 5   ## 3d plotì˜ view ì¡°ì ˆ \n    ax.set_xlabel(r'$w_0$')  # xì¶• ë ˆì´ë¸” ì„¤ì •\n    ax.set_ylabel(r'$w_1$')  # yì¶• ë ˆì´ë¸” ì„¤ì •\n    ax.set_xticks([-5,0,5,10])  # xì¶• í‹± ê°„ê²© ì„¤ì •\n    ax.set_yticks([-5,0,5,10])  # yì¶• í‹± ê°„ê²© ì„¤ì •\n    plt.close(fig)  # ìë™ ì¶œë ¥ ë°©ì§€\n    return fig\n\n\n# ì†ì‹¤ 8587.6875 ë¥¼ ê³„ì‚°í•˜ëŠ” ë˜ ë‹¤ë¥¸ ë°©ì‹\ndef l(w0hat,w1hat):\n    yhat = w0hat + w1hat*x\n    return torch.sum((y-yhat)**2)\n\n\nfig = plot_loss()\nax = fig.gca()\nax.scatter(2.5, 4, l(2.5,4), s=200, marker='*', color='red', label=r\"${\\bf W}=[2.5, 4]'$\")\nax.scatter(-5, 10, l(-5,10), s=200, marker='*', color='blue', label=r\"initial $\\hat{\\bf W}=[-5, 10]'$\")\nax.legend()\nfig\n\n\n\n\n\n\n\n\n\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad=True)\nalpha = 0.001\nfor epoc in range(30):\n    yhat = X @ What\n    loss = torch.sum((y-yhat)**2)\n    loss.backward()\n    What.data = What.data - 0.001 * What.grad\n    w0,w1 = What.data.reshape(-1) \n    ax.scatter(w0,w1,l(w0,w1),s=5,marker='o',color='blue')\n    What.grad = None\n\n\nfig\n\n\n\n\n\n\n\n\n\nğŸ—£ï¸\n\nBì˜ ì‹œê°í™”ì—ì„œ ìµœì´ˆì˜ ì§ì„ ì— ëŒ€ì‘í•˜ëŠ” ì ì´ íŒŒë€ìƒ‰ ì \nì ë“¤ì´ ë¹¨ê°„ìƒ‰ ì ìœ¼ë¡œ ì´ë™í•˜ëŠ” ê³¼ì •ì€ ì§ì„ ì´ ì˜¬ë¼ê°€ëŠ” ê³¼ì •ì— ëŒ€ì‘"
  },
  {
    "objectID": "posts/02wk-2.html#d.-ì• ë‹ˆë©”ì´ì…˜",
    "href": "posts/02wk-2.html#d.-ì• ë‹ˆë©”ì´ì…˜",
    "title": "02wk-2: (íšŒê·€) â€“ íŒŒë¼ë©”í„°ì˜ í•™ìŠµê³¼ì • ìŒë¯¸, MSE, íŒŒì´í† ì¹˜ì‹ ì½”ë”©íŒ¨í„´ (1)",
    "section": "D. ì• ë‹ˆë©”ì´ì…˜",
    "text": "D. ì• ë‹ˆë©”ì´ì…˜\n\nfrom matplotlib import animation\n\n\nplt.rcParams['figure.figsize'] = (7.5,2.5)\nplt.rcParams[\"animation.html\"] = \"jshtml\" \n\n\ndef show_animation(alpha=0.001):\n    ## 1. íˆìŠ¤í† ë¦¬ ê¸°ë¡ì„ ìœ„í•œ list ì´ˆê¸°í™”\n    loss_history = [] \n    yhat_history = [] \n    What_history = [] \n\n    ## 2. í•™ìŠµ + í•™ìŠµê³¼ì •ê¸°ë¡\n    What= torch.tensor([[-5.0],[10.0]],requires_grad=True)\n    What_history.append(What.data.tolist())\n    for epoc in range(30): \n        yhat=X@What ; yhat_history.append(yhat.data.tolist())\n        loss=torch.sum((y-yhat)**2); loss_history.append(loss.item())\n        loss.backward() \n        What.data = What.data - alpha * What.grad; What_history.append(What.data.tolist())\n        What.grad = None    \n\n    ## 3. ì‹œê°í™” \n    fig = plt.figure()\n    ax1 = fig.add_subplot(1, 2, 1)\n    ax2 = fig.add_subplot(1, 2, 2, projection='3d')\n\n    #### ax1: yhatì˜ ê´€ì ì—ì„œ.. \n    ax1.plot(x,y,'o',label=r\"$(x_i,y_i)$\")\n    line, = ax1.plot(x,yhat_history[0],label=r\"$(x_i,\\hat{y}_i)$\") \n    ax1.legend()\n    #### ax2: lossì˜ ê´€ì ì—ì„œ.. \n    w0 = np.arange(-6, 11, 0.5) \n    w1 = np.arange(-6, 11, 0.5)\n    W1,W0 = np.meshgrid(w1,w0)\n    LOSS=W0*0\n    for i in range(len(w0)):\n        for j in range(len(w1)):\n            LOSS[i,j]=torch.sum((y-w0[i]-w1[j]*x)**2)\n    ax2.plot_surface(W0, W1, LOSS, rstride=1, cstride=1, color='b',alpha=0.1)\n    ax2.azim = 30  ## 3d plotì˜ view ì¡°ì ˆ \n    ax2.dist = 8   ## 3d plotì˜ view ì¡°ì ˆ \n    ax2.elev = 5   ## 3d plotì˜ view ì¡°ì ˆ \n    ax2.set_xlabel(r'$w_0$')  # xì¶• ë ˆì´ë¸” ì„¤ì •\n    ax2.set_ylabel(r'$w_1$')  # yì¶• ë ˆì´ë¸” ì„¤ì •\n    ax2.set_xticks([-5,0,5,10])  # xì¶• í‹± ê°„ê²© ì„¤ì •\n    ax2.set_yticks([-5,0,5,10])  # yì¶• í‹± ê°„ê²© ì„¤ì •\n    ax2.scatter(2.5, 4, l(2.5,4), s=200, marker='*', color='red', label=r\"${\\bf W}=[2.5, 4]'$\")\n    ax2.scatter(-5, 10, l(-5,10), s=200, marker='*', color='blue')\n    ax2.legend()\n    def animate(epoc):\n        line.set_ydata(yhat_history[epoc])\n        ax2.scatter(np.array(What_history)[epoc,0],np.array(What_history)[epoc,1],loss_history[epoc],color='grey')\n        fig.suptitle(f\"alpha = {alpha} / epoch = {epoc}\")\n        return line\n\n    ani = animation.FuncAnimation(fig, animate, frames=30)\n    plt.close()\n    return ani\n\n\nğŸ—£ï¸ alpha:\n\ní•™ìŠµë¥ : updateë˜ëŠ” í­ (ML ê´€ì )\nstep size: ì˜¤ë¥¸ìª½ ê·¸ë¦¼ í•¨ìˆ˜ ê´€ì  (ì‚°ì—… ê³µí•™ ê´€ì )\n\n\n\nani = show_animation(alpha=0.001)\nani\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/02wk-2.html#e.-í•™ìŠµë¥ ì—-ë”°ë¥¸-ì‹œê°í™”",
    "href": "posts/02wk-2.html#e.-í•™ìŠµë¥ ì—-ë”°ë¥¸-ì‹œê°í™”",
    "title": "02wk-2: (íšŒê·€) â€“ íŒŒë¼ë©”í„°ì˜ í•™ìŠµê³¼ì • ìŒë¯¸, MSE, íŒŒì´í† ì¹˜ì‹ ì½”ë”©íŒ¨í„´ (1)",
    "section": "E. í•™ìŠµë¥ ì— ë”°ë¥¸ ì‹œê°í™”",
    "text": "E. í•™ìŠµë¥ ì— ë”°ë¥¸ ì‹œê°í™”\n- \\(\\alpha\\)ê°€ ë„ˆë¬´ ì‘ë‹¤ë©´ ë¹„íš¨ìœ¨ì ì„\n\nshow_animation(alpha=0.0001)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\nğŸ—£ï¸ 0.001 -&gt; 0.0001\n\nì•„ê¹Œë³´ë‹¤ ê°€ëŠ” ë‘¥ ë§ˆëŠ” ë‘¥ í•¨\n\n\n- \\(\\alpha\\)ê°€ í¬ë‹¤ê³  ë¬´ì¡°ê±´ ì¢‹ì€ê±´ ë˜ ì•„ë‹˜\n\nshow_animation(alpha=0.0083)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\nğŸ—£ï¸ 0.001 -&gt; 0.0083 (ì§ì ‘ ì°¾ì€ ìˆ«ì)\n\nì²˜ìŒë¶€í„° ìµœì†Œì ì„ ì§€ë‚˜ë²„ë¦¼ (ì§ì„ ì´ ì ë“¤ ìœ„ë¡œ ë°”ë¡œ ì˜¬ë¼ê°) -&gt; ë°”ëŒì§í•˜ì§€ ì•ŠìŒ\nì´í›„ ì§ì„ ì´ ë‹¤ì‹œ ì ë“¤ ì•„ë˜ë¡œ ë‚´ë ¤ì˜´\nì™”ë‹¤ê°”ë‹¤í•˜ë©´ì„œ ë‚´ë ¤ì˜¤ëŠ” ê²ƒ ê°™ê¸°ëŠ” í•˜ë‚˜ íš¨ìœ¨ì ì¸ ëŠë‚Œì€ ì•„ë‹˜\n\n\n- ìˆ˜í‹€ë¦¬ë©´ ìˆ˜ë ´ì•ˆí• ìˆ˜ë„??\n\nshow_animation(alpha=0.0085)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\nğŸ—£ï¸ 0.001 -&gt; 0.0085\n\nì§ì „ì˜ 0.0083ê³¼ ì–¼ë§ˆ ì°¨ì´ê°€ ë‚˜ì§€ë„ ì•ŠëŠ”ë°\nì´ë²ˆì—ëŠ” ì™”ë‹¤ê°”ë‹¤í•˜ë©´ì„œ ìˆ˜ë ´í•˜ì§€ë„ ì•ŠìŒ\nì˜¤íˆë ¤ ê°ˆìˆ˜ë¡ í¬ë¬¼ì„  ëª¨ì–‘ìœ¼ë¡œ ì ì  ì˜¬ë¼ê°\n\n\n- ê·¸ëƒ¥ ë§í• ìˆ˜ë„??\n\nshow_animation(alpha=0.01)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\nğŸ—£ï¸ 0.001 -&gt; 0.01\n\nê¸°ìš¸ê¸°ê°€ ë¬´í•œëŒ€ê°€ ë¨\nêµí›ˆ: alphaë¥¼ ì˜ ì„ íƒí•´ì•¼ ìˆ˜ë ´í•¨\n\n\n\n\nplt.rcdefaults()\nplt.rcParams['figure.figsize'] = 4.5,3.0"
  },
  {
    "objectID": "posts/02wk-2.html#a.-ê¸°ë³¸íŒ¨í„´",
    "href": "posts/02wk-2.html#a.-ê¸°ë³¸íŒ¨í„´",
    "title": "02wk-2: (íšŒê·€) â€“ íŒŒë¼ë©”í„°ì˜ í•™ìŠµê³¼ì • ìŒë¯¸, MSE, íŒŒì´í† ì¹˜ì‹ ì½”ë”©íŒ¨í„´ (1)",
    "section": "A. ê¸°ë³¸íŒ¨í„´",
    "text": "A. ê¸°ë³¸íŒ¨í„´\nğŸ—£ï¸ SSE ë§ê³  MSEë¡œ\n\n## -- ì™¸ìš°ì„¸ìš”!!! -- ##\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad = True)\nfor epoc in range(30):\n    # step1: yhat \n    yhat = X@What \n    # step2: loss\n    loss = torch.sum((y-yhat)**2)/100\n    # step3: ë¯¸ë¶„\n    loss.backward()\n    # step4: update\n    What.data = What.data - 0.1 * What.grad\n    What.grad = None\n\n\nplt.plot(x,y,'o')\nplt.plot(x,X@What.data,'--')\nplt.title(f'What={What.data.reshape(-1)}');"
  },
  {
    "objectID": "posts/02wk-2.html#b.-step2ì˜-ìˆ˜ì •-loss_fn-ì´ìš©",
    "href": "posts/02wk-2.html#b.-step2ì˜-ìˆ˜ì •-loss_fn-ì´ìš©",
    "title": "02wk-2: (íšŒê·€) â€“ íŒŒë¼ë©”í„°ì˜ í•™ìŠµê³¼ì • ìŒë¯¸, MSE, íŒŒì´í† ì¹˜ì‹ ì½”ë”©íŒ¨í„´ (1)",
    "section": "B. Step2ì˜ ìˆ˜ì • â€“ loss_fn ì´ìš©",
    "text": "B. Step2ì˜ ìˆ˜ì • â€“ loss_fn ì´ìš©\nğŸ—£ï¸(\n\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad = True)\n\n\nyhat=X@What\n\n\ntorch.sum((y-yhat)**2)/100 # MSE\n\ntensor(85.8769, grad_fn=&lt;DivBackward0&gt;)\n\n\n\ntorch.mean((y-yhat)**2) # MSE\n\ntensor(85.8769, grad_fn=&lt;MeanBackward0&gt;)\n\n\n\ndef loss_fn(yhat, y):\n    return torch.mean((y-yhat)**2)\n\n\nloss_fn(yhat,y)\n\ntensor(85.8769, grad_fn=&lt;MeanBackward0&gt;)\n\n\n\nloss_fn ì›ë¦¬ë¥¼ ì˜ ëª¨ë¥¸ë‹¤ë©´ pytorch í•¨ìˆ˜ ì´ìš©\n\n\nloss_fn = torch.nn.MSELoss()\nloss_fn(yhat,y) # ê²°ê³¼ëŠ” ë™ì¼\n\ntensor(85.8769, grad_fn=&lt;MseLossBackward0&gt;)\n\n\n\ní‹€ë¦° ì„¤ëª…\n\ntorch.nn.MSELossëŠ” í•¨ìˆ˜ì¸ë°, â€œNone -&gt; MSEë¥¼ ê³„ì‚°í•´ì£¼ëŠ” í•¨ìˆ˜â€ì¸ í•¨ìˆ˜\n\në§ëŠ” ì„¤ëª…\n\ntorch.nn.MSELossëŠ” callable objectë¥¼ ìƒì„±í•˜ëŠ” í´ë˜ìŠ¤\n\n\n)ğŸ—£ï¸\n\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad = True)\nloss_fn = torch.nn.MSELoss()\nfor epoc in range(30):\n    # step1: yhat \n    yhat = X@What \n    # step2: loss\n    #loss = torch.sum((y-yhat)**2)/100\n    loss = loss_fn(yhat,y) # ì—¬ê¸°ì„œëŠ” í° ìƒê´€ì—†ì§€ë§Œ ìŠµê´€ì ìœ¼ë¡œ yhatì„ ë¨¼ì €ë„£ëŠ” ì—°ìŠµì„ í•˜ì!!\n    # step3: ë¯¸ë¶„\n    loss.backward()\n    # step4: update\n    What.data = What.data - 0.1 * What.grad\n    What.grad = None\n\nğŸ—£ï¸ loss_fnì€ ë¬´ì¡°ê±´ yhat ë¨¼ì €\n\nplt.plot(x,y,'o')\nplt.plot(x,X@What.data,'--')\nplt.title(f'What={What.data.reshape(-1)}');"
  },
  {
    "objectID": "posts/02wk-2.html#c.-step1ì˜-ìˆ˜ì •-net-ì´ìš©",
    "href": "posts/02wk-2.html#c.-step1ì˜-ìˆ˜ì •-net-ì´ìš©",
    "title": "02wk-2: (íšŒê·€) â€“ íŒŒë¼ë©”í„°ì˜ í•™ìŠµê³¼ì • ìŒë¯¸, MSE, íŒŒì´í† ì¹˜ì‹ ì½”ë”©íŒ¨í„´ (1)",
    "section": "C. Step1ì˜ ìˆ˜ì • â€“ net ì´ìš©",
    "text": "C. Step1ì˜ ìˆ˜ì • â€“ net ì´ìš©\nğŸ—£ï¸ yhat = X@Whatë„ ì•Œê³  ì‹¶ì§€ ì•Šë‹¤ë©´ (ë„¤íŠ¸ì›Œí¬ ì´ìš©)\n# net â€“ net ì˜¤ë¸Œì íŠ¸ë€?\nì›ë˜ yhatì„ ì´ëŸ°ì‹ìœ¼ë¡œ êµ¬í–ˆëŠ”ë° ~\n\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad = True)\nyhat= X@What\nyhat[:5]\n\ntensor([[-29.8211],\n        [-28.6215],\n        [-24.9730],\n        [-21.2394],\n        [-19.7919]], grad_fn=&lt;SliceBackward0&gt;)\n\n\nì•„ë˜ì™€ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ì½”ë“œë¥¼ ì§œê³  ì‹¶ìŒ..\nyhat = net(X) # \nğŸ—£ï¸ Xë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ yhatì„ ì¶œë ¥í•˜ê³  ì‹¶ìŒ\nìœ„ì™€ ê°™ì€ ì½”ë“œë¥¼ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” netì€ torchì—ì„œ ì§€ì›í•˜ê³  ì•„ë˜ì™€ ê°™ì´ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ.\nğŸ—£ï¸(\n\n# torch.nn.Linear?\n\nnet = torch.nn.Linear(\n    in_features= ??,\n    out_features= ??,\n    bias= False # defaultëŠ” True\n)\n\nin_features: ì…ë ¥(X)ì— ëŒ€í•œ ì°¨ì› (featuresë¥¼ dimensionìœ¼ë¡œ ìƒê°)\n\n\nX.shape # 100ì€ ê´€ì¸¡ê°’ ê°œìˆ˜ì— ë”°ë¼ ë°”ë€” ìˆ˜ ìˆê³ , 2ëŠ” ëª¨í˜•ì´ ì •í•´ì§€ë©´ ì•ˆ ë°”ë€œ\n\ntorch.Size([100, 2])\n\n\n\nout_features: ì¶œë ¥(y)ì— ëŒ€í•œ ì°¨ì›\n\n\ny.shape # ë§ˆì°¬ê°€ì§€ë¡œ 1\n\ntorch.Size([100, 1])\n\n\n\nnet = torch.nn.Linear(\n    in_features= 2,\n    out_features= 1,\n    bias= False\n)\n\n\nyhat = net(X)\nyhat[:5]\n\ntensor([[-0.1600],\n        [-0.1362],\n        [-0.0639],\n        [ 0.0101],\n        [ 0.0388]], grad_fn=&lt;SliceBackward0&gt;)\n\n\n\nì›ë˜ êµ¬í•œ yhatê³¼ ë¹„êµí•˜ë©´ Whatì„ ì„¤ì •í•˜ì§€ ì•Šì•˜ìœ¼ë¯€ë¡œ ë‹¹ì—°íˆ ë‹¤ë¦„\n\n\nnet.weight # Whatê³¼ ê°™ë‹¤ê³  ìƒê°í•˜ë©´ ë¨\n\nParameter containing:\ntensor([[0.3320, 0.1982]], requires_grad=True)\n\n\n\nì—„ë°€íˆ ë§í•˜ë©´ net.weightëŠ” 2x1 matrixê°€ ì•„ë‹ˆë¼ 1x2 martix\n\nì»´í“¨í„° ê³µí•™ì  ì´ìœ ë¡œ ì´ë ‡ê²Œ ë˜ì–´ ìˆìŒ (column vectorë³´ë‹¤ row vector ì—°ì‚°ì´ ì‰½ë‹¤ê³  í•¨)\n\n\n\nnet.weight.T # ì´ê²Œ ì§„ì§œ Whatê³¼ ë™ì¼\n\ntensor([[0.3320],\n        [0.1982]], grad_fn=&lt;PermuteBackward0&gt;)\n\n\n\nnet.weight.data = torch.tensor([[-5.0],[10.0]]).T\nnet.weight.data\n\ntensor([[-5., 10.]])\n\n\n\nyhat= net(X)\nyhat[:5]\n\ntensor([[-29.8211],\n        [-28.6215],\n        [-24.9730],\n        [-21.2394],\n        [-19.7919]], grad_fn=&lt;SliceBackward0&gt;)\n\n\n\nì›ë˜ êµ¬í•œ yhatê³¼ ë™ì¼\n\n)ğŸ—£ï¸\n\n# yhat = net(X) \nnet = torch.nn.Linear(\n    in_features=2, # X:(n,2) --&gt; 2 \n    out_features=1, # yhat:(n,1) --&gt; 1 \n    bias=False \n)\n\n\nnet.weight.data = torch.tensor([[-5.0], [10.0]]).T # .T ë¥¼ í•´ì•¼í•¨. ì™¸ìš°ì„¸ìš” \nnet.weight\n\nParameter containing:\ntensor([[-5., 10.]], requires_grad=True)\n\n\n\nnet(X)[:5]\n\ntensor([[-29.8211],\n        [-28.6215],\n        [-24.9730],\n        [-21.2394],\n        [-19.7919]], grad_fn=&lt;SliceBackward0&gt;)\n\n\n\n(X@What)[:5]\n\ntensor([[-29.8211],\n        [-28.6215],\n        [-24.9730],\n        [-21.2394],\n        [-19.7919]], grad_fn=&lt;SliceBackward0&gt;)\n\n\n\n(X@net.weight.T)[:5]\n\ntensor([[-29.8211],\n        [-28.6215],\n        [-24.9730],\n        [-21.2394],\n        [-19.7919]], grad_fn=&lt;SliceBackward0&gt;)\n\n\n#\n- ìˆ˜ì •ëœì½”ë“œ\n\n# step1ì„ ìœ„í•œ ì‚¬ì „ì¤€ë¹„\nnet = torch.nn.Linear(\n    in_features=2,\n    out_features=1,\n    bias=False\n)\nnet.weight.data = torch.tensor([[-5.0,  10.0]])\n# step2ë¥¼ ìœ„í•œ ì‚¬ì „ì¤€ë¹„\nloss_fn = torch.nn.MSELoss()\nfor epoc in range(30):\n    # step1: yhat\n    # yhat = X@What \n    yhat = net(X)\n    # step2: loss\n    loss = loss_fn(yhat,y)\n    # step3: ë¯¸ë¶„\n    loss.backward()\n    # step4: update\n    net.weight.data = net.weight.data - 0.1 * net.weight.grad\n    net.weight.grad = None\n\nğŸ—£ï¸ What.data -&gt; net.weight.data, What.grad -&gt; net.weight.grad\n\nplt.plot(x,y,'o')\nplt.plot(x,net(X).data,'--')\nplt.title(f'net.weight={net.weight.data.reshape(-1)}');"
  },
  {
    "objectID": "posts/02wk-2.html#d.-step4ì˜-ìˆ˜ì •-optimizerì˜-ì´ìš©",
    "href": "posts/02wk-2.html#d.-step4ì˜-ìˆ˜ì •-optimizerì˜-ì´ìš©",
    "title": "02wk-2: (íšŒê·€) â€“ íŒŒë¼ë©”í„°ì˜ í•™ìŠµê³¼ì • ìŒë¯¸, MSE, íŒŒì´í† ì¹˜ì‹ ì½”ë”©íŒ¨í„´ (1)",
    "section": "D. Step4ì˜ ìˆ˜ì • â€“ optimizerì˜ ì´ìš©",
    "text": "D. Step4ì˜ ìˆ˜ì • â€“ optimizerì˜ ì´ìš©\n- ì†Œë§: ì•„ë˜ì˜ ê³¼ì •ì„ ì¢€ ë” í¸í•˜ê²Œ í–ˆìœ¼ë©´..\nnet.weight.data = net.weight.data - 0.1 * net.weight.grad\nnet.weight.data = None \n# optimizer â€“ ì´ê±¸ ì´ìš©í•˜ë©´ update ê³¼ì •ì„ ì†ì‰½ê²Œ í•  ìˆ˜ ìˆìŒ\nê¸°ì¡´ì½”ë“œ\n\n## -- ì¤€ë¹„ê³¼ì • -- ## \n# step1ì„ ìœ„í•œ ì‚¬ì „ì¤€ë¹„\nnet = torch.nn.Linear(\n    in_features=2,\n    out_features=1,\n    bias=False\n)\nnet.weight.data = torch.tensor([[-5.0,  10.0]])\n# step2ë¥¼ ìœ„í•œ ì‚¬ì „ì¤€ë¹„\nloss_fn = torch.nn.MSELoss()\n\n\n## -- 1ì—í­ì§„í–‰ -- ## \n# step1: \nyhat = net(X)\n# step2: loss\nloss = loss_fn(yhat,y)\n# step3: ë¯¸ë¶„\nloss.backward()\n# step4: update\nprint(net.weight.data)\nnet.weight.data = net.weight.data - 0.1 * net.weight.grad\nprint(net.weight.data)\nnet.weight.grad = None\n\ntensor([[-5., 10.]])\ntensor([[-3.6577,  8.8111]])\n\n\n\n## -- 2ì—í­ì§„í–‰ -- ## \n# step1: 2ì—í­ì§„í–‰\nyhat = net(X)\n# step2: loss\nloss = loss_fn(yhat,y)\n# step3: ë¯¸ë¶„\nloss.backward()\n# step4: update\nprint(net.weight.data)\nnet.weight.data = net.weight.data - 0.1 * net.weight.grad\nprint(net.weight.data)\nnet.weight.grad = None\n\ntensor([[-3.6577,  8.8111]])\ntensor([[-2.5548,  7.8612]])\n\n\nìƒˆë¡œìš´ì½”ë“œ â€“ optimizer ì´ìš©\nğŸ—£ï¸(\n\ntorch.optim.SGD : optimizerë¥¼ ë§Œë“¤ì–´ì¤Œ\ntorch.optim.SGDëŠ” net.weightë¥¼ ê°–ê³  ìˆì–´ì•¼ í•¨(What)\n\nnet.weightëŠ” net.parameters()ë¡œ ë³¼ ìˆ˜ ìˆìŒ\nnet.parameters()ëŠ” generator: iterable object -&gt; listí™” ê°€ëŠ¥\n\ntorch.optim.SGDëŠ” í•™ìŠµë¥  lrë„ ê°–ê³  ìˆì–´ì•¼ í•¨\n\n\nnet.weight\n\nParameter containing:\ntensor([[-2.5548,  7.8612]], requires_grad=True)\n\n\n\nnet.parameters()\n\n&lt;generator object Module.parameters at 0x7fc922ade820&gt;\n\n\n\nlist(net.parameters()) # ê°’ì„ ë³´ë ¤ë©´\n\n[Parameter containing:\n tensor([[-2.5548,  7.8612]], requires_grad=True)]\n\n\n\n# optimizr = torch.optim.SGD(net.parameters(), lr=0.1) # net.parameters(): generator\n\n\nnet.weight.data = net.weight.data - 0.1 * net.weight.grad\n\n=&gt; optimizr.step()\n\nnet.weight.grad = None\n\n=&gt; optimizr.zero_grad()\n\n\n)ğŸ—£ï¸\n\n## -- ì¤€ë¹„ê³¼ì • -- ## \n# step1ì„ ìœ„í•œ ì‚¬ì „ì¤€ë¹„\nnet = torch.nn.Linear(\n    in_features=2,\n    out_features=1,\n    bias=False\n)\nnet.weight.data = torch.tensor([[-5.0,  10.0]])\n# step2ë¥¼ ìœ„í•œ ì‚¬ì „ì¤€ë¹„\nloss_fn = torch.nn.MSELoss()\n# step4ë¥¼ ìœ„í•œ ì‚¬ì „ì¤€ë¹„\noptimizr = torch.optim.SGD(net.parameters(),lr=0.1)\n\n\n## -- 1ì—í­ì§„í–‰ -- ## \nyhat = net(X)\n# step2: loss\nloss = loss_fn(yhat,y)\n# step3: ë¯¸ë¶„\nloss.backward()\n# step4: update\nprint(net.weight.data)\n#net.weight.data = net.weight.data - 0.1 * net.weight.grad\noptimizr.step()\nprint(net.weight.data)\n#net.weight.grad = None\noptimizr.zero_grad()\n\ntensor([[-5., 10.]])\ntensor([[-3.6577,  8.8111]])\n\n\n\n## -- 2ì—í­ì§„í–‰ -- ## \nyhat = net(X)\n# step2: loss\nloss = loss_fn(yhat,y)\n# step3: ë¯¸ë¶„\nloss.backward()\n# step4: update\nprint(net.weight.data)\n#net.weight.data = net.weight.data - 0.1 * net.weight.grad\noptimizr.step()\nprint(net.weight.data)\n#net.weight.grad = None\noptimizr.zero_grad()\n\ntensor([[-3.6577,  8.8111]])\ntensor([[-2.5548,  7.8612]])\n\n\n#\n- ìˆ˜ì •ëœì½”ë“œ\n\n# step1ì„ ìœ„í•œ ì‚¬ì „ì¤€ë¹„\nnet = torch.nn.Linear(\n    in_features=2,\n    out_features=1,\n    bias=False\n)\nnet.weight.data = torch.tensor([[-5.0,  10.0]])\n# step2ë¥¼ ìœ„í•œ ì‚¬ì „ì¤€ë¹„\nloss_fn = torch.nn.MSELoss()\n# step4ë¥¼ ìœ„í•œ ì‚¬ì „ì¤€ë¹„ \noptimizr = torch.optim.SGD(net.parameters(),lr=0.1)\nfor epoc in range(30):\n    # step1: yhat \n    yhat = net(X)\n    # step2: loss\n    loss = loss_fn(yhat,y)\n    # step3: ë¯¸ë¶„\n    loss.backward()\n    # step4: update\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'o')\nplt.plot(x,yhat.data,'--')\nplt.title(f'net.weight={net.weight.data.reshape(-1)}');"
  },
  {
    "objectID": "posts/14wk-2.html",
    "href": "posts/14wk-2.html",
    "title": "14wk-2: (ê°•í™”í•™ìŠµ) â€“ 4x4 Grid World q_table, Appedix B",
    "section": "",
    "text": "ğŸ“˜ Note Format Guide\nThis format serves as a structured guide for organizing lecture content, personal interpretation, experiments, and study-related questions.\nğŸ“ ğŸ—£ï¸ âœï¸ ğŸ”¬ â“"
  },
  {
    "objectID": "posts/14wk-2.html#a.-ë¯¸ë˜ë³´ìƒ",
    "href": "posts/14wk-2.html#a.-ë¯¸ë˜ë³´ìƒ",
    "title": "14wk-2: (ê°•í™”í•™ìŠµ) â€“ 4x4 Grid World q_table, Appedix B",
    "section": "A. ë¯¸ë˜ë³´ìƒ",
    "text": "A. ë¯¸ë˜ë³´ìƒ\n\nğŸ—£ï¸\n\në°´ë”§ ê²Œì„ì€ ë¯¸ë˜ë³´ìƒì´ ì—†ìŒ (ë‹¤ìŒ Stateê°€ ì—†ìŒ)\n\n\n- ì–¸ëœ»ìƒê°í•˜ë©´ 4x4 ë¬¸ì œì—ì„œ q_tableì€ ì•„ë˜ì™€ ê°™ì´ ìƒê°í•˜ëŠ”ê²Œ í•©ë¦¬ì ì¸ë“¯ ë³´ì¸ë‹¤.\n\nq[s1,s2,a] = ìƒíƒœ (s1,s2)ì—ì„œ í–‰ë™ aë¥¼ í–ˆì„ ê²½ìš° ì–»ê²Œë˜ëŠ” ë³´ìƒì˜ í‰ê· \n\\(q(s,a) = r(s,a) = \\mathbb{E}[\\text{Reward} | \\text{State}=s, \\text{Action}=a]\\)\n\nê·¸ë ‡ì§€ë§Œ ì•„ë˜ì™€ ê°™ì´ ìƒê°í•˜ëŠ”ê²Œ ë” í•©ë¦¬ì ì´ë‹¤.\n\nq[s1,s2,a] = ìƒíƒœ (s1,s2)ì—ì„œ í–‰ë™ aë¥¼ í–ˆì„ ê²½ìš° ì–»ê²Œë˜ëŠ” ë³´ìƒì˜ í‰ê·  + ë¯¸ë˜ì— ì–»ê²Œë˜ë¦¬ë¼ ê¸°ëŒ€í•˜ëŠ” ë³´ìƒ\n\\(q(s,a) = r(s,a) + r_{\\text{future}}\\)\n\n\në‹¨, ì—¬ê¸°ì—ì„œ ë¯¸ë˜ì— ì–»ê²Œë˜ë¦¬ë¼ ê¸°ëŒ€í•˜ëŠ” ë³´ìƒì€ ìµœì„ ì˜ ì„ íƒì„ í•œë‹¤ëŠ” ì „ì œí•˜ì— ê³„ì‚°\n\n- ë¯¸ë˜ì— ì–»ê²Œë˜ë¦¬ë¼ ê¸°ëŒ€í•˜ëŠ” ë³´ìƒì€ ì–´ë–»ê²Œ ì •ì˜í•  ìˆ˜ ìˆì„ê¹Œ?\n\n# ì˜ˆì œ1 â€“ ìƒíƒœ (2,2) ì—ì„œ â€œaction=downâ€ ì„ í–ˆì„ë•Œ\n\nì¦‰ì‹œ ì–»ê²Œë˜ëŠ” ë³´ìƒê³¼\në¯¸ë˜ì— ì–»ê²Œë˜ë¦¬ë¼ ê¸°ëŒ€í•˜ëŠ” ë³´ìƒ\n\nì€ ë¬´ì—‡ì¸ê°€? ì´ê²ƒì„ ë°”íƒ•ìœ¼ë¡œ \\(s=(2,1)\\), \\(a=\\text{down}\\) ì˜ í’ˆì§ˆ(Quality)ëŠ” ì–´ë–»ê²Œ í‰ê°€í•  ìˆ˜ ìˆëŠ”ê°€?\n(í’€ì´?)\nì¦‰ì‹œ ì–»ì„ ìˆ˜ ìˆë‹¤ê³  ìƒê°ë˜ëŠ” ë³´ìƒì€-1 ì´ê³  ë¯¸ë˜ì— ì–»ìœ¼ë¦¬ë¼ ê¸°ëŒ€ë˜ëŠ” ë³´ìƒì€ 100 ì ì´ë‹¤. ë”°ë¼ì„œ 99ì ìœ¼ë¡œ í‰ê°€í•˜ëŠ”ê²Œ í•©ë¦¬ì ì¸ë“¯í•˜ë‹¤. ìˆ˜ì‹ìœ¼ë¡œ ì“°ë©´\n\\[q(s,a)=q(s_1,s_2,a)=q(2,2, \\text{down}) = -1 + 100 = r(2,2,\\text{down}) + \\max_{a'}q(3,2,a')\\]\nì™€ ê°™ì´ ì“¸ ìˆ˜ ìˆê² ë‹¤.\n\nğŸ—£ï¸\n\n\\(\\max_{a'}q(3,2,a')\\)= (3,2)ì—ì„œ í•  ìˆ˜ ìˆëŠ” ëª¨ë“  ì•¡ì…˜ë“¤ì„ ê³ ë ¤í•˜ì˜€ì„ ë•Œ ë°›ê²Œ ë˜ëŠ” ìµœëŒ€ í’ˆì§ˆ\n\\(a'\\) = 0,1,2,3 = ì™¼ìª½,ì˜¤ë¥¸ìª½,ì•„ë˜,ìœ„\n\nq(3,2,ì™¼)=?? &lt; 100\nq(3,2,ì˜¤)=100\nq(3,2,ì•„)=-10\nq(3,2,ìœ„)=?? &lt; 100\n\n\n\n#\n# ì˜ˆì œ2 â€“ ìƒíƒœ (1,2) ì—ì„œ â€œaction=downâ€ ì„ í–ˆì„ë•Œ\n\nì¦‰ì‹œ ì–»ê²Œë˜ëŠ” ë³´ìƒê³¼\në¯¸ë˜ì— ì–»ê²Œë˜ë¦¬ë¼ ê¸°ëŒ€í•˜ëŠ” ë³´ìƒ\n\nì€ ë¬´ì—‡ì¸ê°€? ì´ê²ƒì„ ë°”íƒ•ìœ¼ë¡œ \\(s=(1,2)\\), \\(a=\\text{down}\\) ì˜ í’ˆì§ˆ(Quality)ëŠ” ì–´ë–»ê²Œ í‰ê°€í•  ìˆ˜ ìˆëŠ”ê°€?\n(í’€ì´?)\nì¦‰ì‹œ ì–»ì„ ìˆ˜ ìˆë‹¤ê³  ìƒê°ë˜ëŠ” ë³´ìƒì€-1 ì´ê³  ë¯¸ë˜ì— ì–»ìœ¼ë¦¬ë¼ ê¸°ëŒ€ë˜ëŠ” ë³´ìƒì€ 99ì ì´ë‹¤. ë”°ë¼ì„œ 98ì ìœ¼ë¡œ í‰ê°€í•˜ëŠ”ê²Œ í•©ë¦¬ì ì¸ë“¯í•˜ë‹¤. ìˆ˜ì‹ìœ¼ë¡œ ì“°ë©´\n\\[q(s,a)=q(s_1,s_2,a)=q(1,2, \\text{down}) = -1 + 99 = r(1,2,\\text{down}) + \\max_{a'}q(2,2,a')\\]\nì™€ ê°™ì´ ì“¸ ìˆ˜ ìˆê² ë‹¤.\n\nğŸ—£ï¸\n\n\\(\\max_{a'}q(2,2,a')\\)= (2,2)ì—ì„œ í•  ìˆ˜ ìˆëŠ” ëª¨ë“  ì•¡ì…˜ë“¤ì„ ê³ ë ¤í•˜ì˜€ì„ ë•Œ ë°›ê²Œ ë˜ëŠ” ìµœëŒ€ í’ˆì§ˆ\n\nq(3,2,up)=?? &lt; 99\nq(3,2,down)=99\nq(3,2,right)=99\nq(3,2,left)=?? &lt; 99\n\n\n\n#\n# ì˜ˆì œ3 â€“ ìƒíƒœ (0,1) ì—ì„œ â€œaction=rightâ€ ì„ í–ˆì„ë•Œ\n\nì¦‰ì‹œ ì–»ê²Œë˜ëŠ” ë³´ìƒê³¼\në¯¸ë˜ì— ì–»ê²Œë˜ë¦¬ë¼ ê¸°ëŒ€í•˜ëŠ” ë³´ìƒ\n\nì€ ë¬´ì—‡ì¸ê°€? ì´ê²ƒì„ ë°”íƒ•ìœ¼ë¡œ \\(s=(0,1)\\), \\(a=\\text{right}\\) ì˜ í’ˆì§ˆ(Quality)ëŠ” ì–´ë–»ê²Œ í‰ê°€í•  ìˆ˜ ìˆëŠ”ê°€?\n(í’€ì´?)\nì•ì˜ ì˜ˆì œë“¤ì„ ì¼ë°˜í™”í•˜ë©´ ì•„ë˜ì™€ ê°™ì€ ìˆ˜ì‹ì„ ì“¸ ìˆ˜ ìˆë‹¤.\n\\[q(0,1, \\text{right}) = r(0,1,\\text{right}) + \\max_{a'}q(0,2,a')\\]\në”°ë¼ì„œ ë§Œì•½ì— \\(\\max_{a}q(0,2,a)\\)ì˜ ê°’ì„ ì•Œê³  ìˆë‹¤ë©´ ì´ë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤.\n#\n- (ì•„ì§ ë¶€ì¡±í•œ) ê¹¨ë‹¬ìŒ: ëª¨ë“  \\((s,a)\\)ì— ëŒ€í•˜ì—¬ \\(q(s,a)\\)ì˜ ê°’ì€ ì•„ë˜ì™€ ê°™ì´ ì •ì˜í•  ìˆ˜ ìˆê² ë‹¤.\n\\[q(s,a) = r(s,a) + \\max_{a'}q(s',a')\\]"
  },
  {
    "objectID": "posts/14wk-2.html#b.-ê°ê°€ìœ¨",
    "href": "posts/14wk-2.html#b.-ê°ê°€ìœ¨",
    "title": "14wk-2: (ê°•í™”í•™ìŠµ) â€“ 4x4 Grid World q_table, Appedix B",
    "section": "B. ê°ê°€ìœ¨",
    "text": "B. ê°ê°€ìœ¨\n# ì˜ˆì œ1 â€“ ë‹¹ì‹ ì€ ì§€ê¸ˆ ì•„ë¬´ê²ƒë„ ì“°ì—¬ ìˆì§€ ì•Šì€ ë¹ˆ ì¢…ì´ í•œ ì¥ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ì´ ì¢…ì´ì— ì“¸ ìˆ˜ ìˆëŠ” ìˆ«ìëŠ” ì˜¤ì§ ë‘ ê°€ì§€, 0 ë˜ëŠ” 1ì…ë‹ˆë‹¤. ì–´ë–¤ ìˆ«ìë¥¼ ì“°ëŠëƒì— ë”°ë¼ ë³´ìƒì´ ë‹¬ë¼ì§€ëŠ”ë°, ìˆ˜ë§ì€ ì‹¤í—˜ì„ í†µí•´ 0ì„ ì“°ë©´ ì•„ë¬´ ë³´ìƒë„ ì—†ê³ , 1ì„ ì“°ë©´ 10ë§Œ ì›ì„ ë°›ì„ ìˆ˜ ìˆë‹¤ëŠ” ì‚¬ì‹¤ì´ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ì‚¬ì‹¤ì´ í™•ì¸ëœ ì´í›„ ì´ ë¹ˆ ì¢…ì´ì˜ ê°€ì¹˜ëŠ” ì–¼ë§ˆì¼ê¹Œìš”?\n(1) 0ì›ì´ë‹¤.\n(2) 10ë§Œì›ì´ë‹¤.\n(3) 5ë§Œì›ì´ë‹¤.\n(4) ëª¨ë¥´ê² ë‹¤.\n#\n# ì˜ˆì œ2 â€“ ë‹¹ì‹  ì•ì—ëŠ” ë¹¨ê°„ìƒ‰ ì¢…ì´ í•œ ì¥ì´ ìˆìŠµë‹ˆë‹¤. ì´ ì¢…ì´ì—ëŠ” 0 ë˜ëŠ” 1 ì¤‘ í•˜ë‚˜ì˜ ìˆ«ìë¥¼ ì“¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë§Œì•½ 1ì„ ì“°ë©´ ë‹¤ìŒ ë‹¨ê³„ì¸ ì£¼í™©ìƒ‰ ì¢…ì´ í•œ ì¥ì„ ë°›ê²Œ ë©ë‹ˆë‹¤. ì£¼í™©ìƒ‰ ì¢…ì´ì—ë„ ë˜‘ê°™ì´ 0 ë˜ëŠ” 1ì„ ì“¸ ìˆ˜ ìˆê³ , ì—¬ê¸°ì— 1ì„ ì“°ë©´ ë…¸ë€ìƒ‰ ì¢…ì´, ê·¸ë‹¤ìŒì€ ì´ˆë¡ìƒ‰ ì¢…ì´, ê·¸ ë‹¤ìŒì€ íŒŒë€ìƒ‰ ì¢…ì´, ê·¸ ë‹¤ìŒì€ ë‚¨ìƒ‰ ì¢…ì´, ë§ˆì§€ë§‰ìœ¼ë¡œëŠ” ë³´ë¼ìƒ‰ ì¢…ì´ë¥¼ ìˆœì„œëŒ€ë¡œ ë°›ìŠµë‹ˆë‹¤. ì´ 7ë‹¨ê³„(ë¹¨ê°• â†’ ì£¼í™© â†’ ë…¸ë‘ â†’ ì´ˆë¡ â†’ íŒŒë‘ â†’ ë‚¨ìƒ‰ â†’ ë³´ë¼ìƒ‰)ë¥¼ ê±°ì¹œ í›„, ë³´ë¼ìƒ‰ ì¢…ì´ì— 1ì„ ì“°ë©´ ë¹„ë¡œì†Œ 10ë§Œ ì›ì˜ í˜„ê¸ˆ ë³´ìƒì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¨, ì–´ëŠ ë‹¨ê³„ì—ì„œë“  0ì„ ì“°ë©´ ì•„ë¬´ ì¼ë„ ì¼ì–´ë‚˜ì§€ ì•Šê³  ê·¸ ì¦‰ì‹œ ê²Œì„ì´ ì¢…ë£Œë©ë‹ˆë‹¤. ì¦‰, ê·¸ ì´í›„ë¡œëŠ” ì¢…ì´ë„ ë°›ì„ ìˆ˜ ì—†ê³  ë³´ìƒë„ ì—†ìŠµë‹ˆë‹¤. ì´ ì‚¬ì‹¤ì´ ì•Œë ¤ì§„ ì´í›„, ì§€ê¸ˆ ë‹¹ì‹ ì´ ë“¤ê³  ìˆëŠ” â€™ë¹¨ê°„ìƒ‰ ì¢…ì´â€™ì˜ ê°€ì¹˜ëŠ” ì–¼ë§ˆì¼ê¹Œìš”?\n(1) 0ì›ì´ë‹¤.\n(2) 10ë§Œ ì›ì´ë‹¤.\n(3) \\(\\frac{1}{2^6}\\) x 10ë§Œì›ì´ë‹¤.\n(4) ëª¨ë¥´ê² ë‹¤.\nğŸ—£ï¸(\n\në‘˜ ë‹¤ ë‹µ (4)\n\n10ë§Œì›ì„ ë°›ì•„ë„ êµ³ì´ ìˆ«ìë¥¼ ì¨ì•¼í•˜ëŠ” ì¢…ì´ë¥¼ 10ë§Œì›ì„ ë˜‘ê°™ì´ ì£¼ê³  ì‚´ ì´ìœ ëŠ” ì—†ìŒ\nì•„ë¬´ë¦¬ ë¯¸ë˜ ë³´ìƒì´ í™•ì‹¤í•´ë„ ì§€ê¸ˆ ë‹¹ì¥ ë°›ëŠ” ê²ƒë³´ë‹¤ëŠ” ê°€ê²©ì´ ë–¨ì–´ì§\n\nì˜ˆì œ 2ì—ì„œ ë¹¨ê°„ìƒ‰ ì¢…ì´ì™€ ë³´ë¼ìƒ‰ ì¢…ì´ë¥¼ ì„ íƒí•˜ë¼ê³  í•˜ë©´ ì „ë¶€ ë³´ë¼ìƒ‰ ì¢…ì´ë¥¼ ì„ íƒí•¨\n\nê±°ë˜ ë‹¹ì‚¬ì ê°„ì˜ í•©ì˜ëœ ê°ê°€ìœ¨ì— ë”°ë¼ ë‹¬ë¼ì§\n\n\n\n0.99 # ë³´ë¼ìƒ‰ ì¢…ì´ì˜ ê°€ì¹˜ë¥¼ 0.99ë¼ê³  í•œë‹¤ë©´\n\n0.99\n\n\n\n0.99**2 # ë‚¨ìƒ‰ ì¢…ì´ì˜ ê°€ì¹˜\n\n0.9801\n\n\n)ğŸ—£ï¸\n#\n- ì§ê´€: ì•„ë¬´ë¦¬ ë³´ì¥ëœ ë³´ìƒì´ë¼ê³  í•´ë„, ë¯¸ë˜ì— ì£¼ì–´ì§€ëŠ” ë³´ìƒì€ í˜„ì¬ì˜ ë³´ìƒê³¼ ë™ê¸‰ì·¨ê¸‰í•  ìˆ˜ ì—†ë‹¤.\n- ì§„ì§œ ê¹¨ë‹¬ìŒ: ëª¨ë“  \\((s,a)\\)ì— ëŒ€í•˜ì—¬ \\(q(s,a)\\)ì˜ ê°’ì€ ì•„ë˜ì™€ ê°™ì´ ì •ì˜í•˜ëŠ”ê²Œ í•©ë¦¬ì ì´ë‹¤.\n\\[q(s,a) = r(s,a) + \\gamma \\max_{a'}q(s',a')\\]\nì—¬ê¸°ì—ì„œ \\(\\gamma\\)ëŠ” 0ê³¼ 1ì‚¬ì´ì˜ ê°’ì´ë©° ê°ê°€ìœ¨(discout factor)ì´ë¼ ë¶€ë¥¸ë‹¤.\n\nğŸ—£ï¸\n\nì¦‰ì‹œ ë°›ê²Œ ë˜ëŠ” ë³´ìƒ + ê°ê°€ìœ¨ * ë¯¸ë˜ì— ë°›ê²Œ ë˜ëŠ” ë³´ìƒ"
  },
  {
    "objectID": "posts/14wk-2.html#c.-q_table-update",
    "href": "posts/14wk-2.html#c.-q_table-update",
    "title": "14wk-2: (ê°•í™”í•™ìŠµ) â€“ 4x4 Grid World q_table, Appedix B",
    "section": "C. q_table update",
    "text": "C. q_table update\n- ì§€ë‚œì‹œê°„ì½”ë“œ\n\nclass GridWorld:\n    def __init__(self):\n        self.a2d = {\n            0: np.array([0,1]), # â†’\n            1: np.array([0,-1]), # â†  \n            2: np.array([1,0]),  # â†“\n            3: np.array([-1,0])  # â†‘\n        }\n        self.state_space = gym.spaces.MultiDiscrete([4,4])\n        self.state = np.array([0,0])\n        self.reward = None\n        self.terminated = False\n    def step(self,action):\n        self.state = self.state + self.a2d[action]\n        s1,s2 = self.state\n        if (s1==3) and (s2==3):\n            self.reward = 100 \n            self.terminated = True\n        elif self.state in self.state_space:\n            self.reward = -1 \n            self.terminated = False\n        else:\n            self.reward = -10\n            self.terminated = True\n        # print(\n        #     f\"action = {action}\\t\"\n        #     f\"state = {self.state - self.a2d[action]} -&gt; {self.state}\\t\"\n        #     f\"reward = {self.reward}\\t\"\n        #     f\"termiated = {self.terminated}\"\n        # )            \n        return self.state, self.reward, self.terminated\n    def reset(self):\n        self.state = np.array([0,0])\n        self.terminated = False\n        return self.state\nclass RandomAgent:\n    def __init__(self):\n        self.state = np.array([0,0]) \n        self.action = None \n        self.reward = None \n        self.next_state = None\n        self.terminated = None\n        #---#\n        self.states = collections.deque(maxlen=500000)\n        self.actions = collections.deque(maxlen=500000)\n        self.rewards = collections.deque(maxlen=500000)\n        self.next_states = collections.deque(maxlen=500000)\n        self.terminations = collections.deque(maxlen=500000)\n        #---#\n        self.action_space = gym.spaces.Discrete(4)\n        self.n_experience = 0\n    def act(self):\n        self.action = self.action_space.sample()\n    def save_experience(self):\n        self.states.append(self.state)\n        self.actions.append(self.action)\n        self.rewards.append(self.reward)\n        self.next_states.append(self.next_state)\n        self.terminations.append(self.terminated)\n        self.n_experience = self.n_experience + 1\n    def learn(self):\n        pass\n\n\nplayer = RandomAgent()\nenv = GridWorld()\nscores = [] \nscore = 0 \n#\nfor e in range(1,100000):\n    #---ì—í”¼ì†Œë“œì‹œì‘---#\n    while True:\n        # step1 -- ì•¡ì…˜ì„ íƒ\n        player.act()\n        # step2 -- í™˜ê²½ë°˜ì‘ \n        player.next_state, player.reward, player.terminated = env.step(player.action)\n        # step3 -- ê²½í—˜ê¸°ë¡ & í•™ìŠµ \n        player.save_experience()\n        player.learn()\n        # step4 --ì¢…ë£Œ ì¡°ê±´ ì²´í¬ & í›„ì† ì²˜ë¦¬\n        if env.terminated:\n            score = score + player.reward\n            scores.append(score)\n            score = 0 \n            player.state = env.reset() \n            break\n        else: \n            score = score + player.reward         \n            player.state = player.next_state\n\n\n\n\n\n\n\nImportant\n\n\n\nê°•ì˜ë…¸íŠ¸ ìˆ˜ì • 2025-06-12\në…¸ê·œí˜¸í•™ìƒì˜ ë„ì›€ìœ¼ë¡œ ì˜ˆì „ê°•ì˜ì˜ ì˜¤ë¥˜ë¥¼ ë°œê²¬í•˜ì—¬ ìˆ˜ì •í•˜ì˜€ìŠµë‹ˆë‹¤.\n# ìˆ˜ì •ì „\n...\n        if env.terminated:\n            ...\n        else: \n            score = score + player.reward\n            scores.append(score)            \n            player.state = player.next_state\n            \n# ìˆ˜ì •í›„\n        if env.terminated:\n            ...\n        else: \n            score = score + player.reward\n#            scores.append(score)            ### &lt;--- ì—¬ê¸°ë¥¼ ì£¼ì„ì²˜ë¦¬í•´ì•¼í•¨!! \n            player.state = player.next_state\n\n\n- ìƒí™©: playerê°€ ê²½í—˜ì€ ìˆëŠ”ë°, q_tableì„ ë§Œë“¤ì¤„ ëª¨ë¦„ (ë°ì´í„°ëŠ” ìˆìŒ, í•™ìŠµì´ ì•ˆëœìƒíƒœ)\n\nplayer.n_experience\n\n326783\n\n\n- ì €ë²ˆì‹œê°„ì— ë°°ìš´ q_table\n\nq_table = np.zeros((4,4,4))\nmemory = zip(player.states, player.actions, player.rewards)\nfor (s1,s2), a, r in memory:\n    qhat = q_table[s1,s2,a] # ë‚´ê°€ ìƒê°í–ˆë˜ê°“\n    q = r # ì‹¤ì œê°’\n    diff = q-qhat # ì°¨ì´\n    q_table[s1,s2,a] = q_table[s1,s2,a]  + 0.01*diff# update\n\n\nfor a in [0,1,2,3]: \n    print(f\"action = {a}\")\n    print(f\"q[...,{a}] = {q_table[...,a].round(3)}\")\n    print(\"---\")\n\naction = 0\nq[...,0] = [[ -1.     -1.     -1.    -10.   ]\n [ -1.     -1.     -1.    -10.   ]\n [ -1.     -1.     -1.     -9.999]\n [ -1.     -1.     99.991   0.   ]]\n---\naction = 1\nq[...,1] = [[-10.  -1.  -1.  -1.]\n [-10.  -1.  -1.  -1.]\n [-10.  -1.  -1.  -1.]\n [-10.  -1.  -1.   0.]]\n---\naction = 2\nq[...,2] = [[ -1.     -1.     -1.     -1.   ]\n [ -1.     -1.     -1.     -1.   ]\n [ -1.     -1.     -1.     99.993]\n [-10.    -10.     -9.999   0.   ]]\n---\naction = 3\nq[...,3] = [[-10. -10. -10. -10.]\n [ -1.  -1.  -1.  -1.]\n [ -1.  -1.  -1.  -1.]\n [ -1.  -1.  -1.   0.]]\n---\n\n\n\nğŸ—£ï¸\n\naction 0: ì˜¤ë¥¸ìª½\naction 1: ì™¼ìª½\naction 2: ì•„ë˜\naction 3: ìœ„\n\n\n- ì´ë²ˆì‹œê°„ì— ë°°ìš´ q_table: ì•„ë˜ë¥¼ ì´ìš©í•œë‹¤.\n\\[q(s,a) = r(s,a) + \\gamma \\max_{a'}q(s',a')\\]\n\n\n\n\n\n\nNote\n\n\n\nì‚¬ì‹¤ ì¢€ ë” ì‹¤ìš©ì ìœ¼ë¡œëŠ”(=ì½”ë”©ì¹œí™”ì ìœ¼ë¡œëŠ”) ì•„ë˜ì˜ ìˆ˜ì‹ì„ ì“°ëŠ”ê²Œ ì¢‹ë‹¤.\n\\[q(s,a) = \\begin{cases} r(s,a) & \\text{if terminated} \\\\ r(s,a) + \\gamma \\max_{a'}q(s',a') & \\text{else} \\end{cases}\\]\n\n\nğŸ—£ï¸(\nq_table = np.zeros((4,4,4))\nmemory = zip(player.states, player.actions, player.rewards)\nfor (s1,s2), a, r in memory:\n    qhat = q_table[s1,s2,a] # ë‚´ê°€ ìƒê°í–ˆë˜ ê°’\n    if tmd:\n        q = r # ì‹¤ì œê°’\n    else:\n        future = q_table[ss1,ss2,:].max() # ss1,ss2 : ë‹¤ìŒ ìƒíƒœ\n        q = r + 0.95*future\n    diff = q-qhat # ì°¨ì´\n    q_table[s1,s2,a] = q_table[s1,s2,a]  + 0.01*diff# update\n\nì •ì˜ ë˜ì–´ ìˆì§€ ì•Šì€ ë‚´ìš©ì„ ì“°ë©´\n\n\n# player.next_states # ë‹¤ìŒ ìƒíƒœê°€ ì €ì¥ë˜ì–´ ìˆìŒ\n\n\n# player.terminations # True, Falseê°€ ì €ì¥ë˜ì–´ ìˆìŒ\n\n\nq_table = np.zeros((4,4,4))\nmemory = zip(player.states, player.actions, player.rewards, player.next_states, player.terminations)\nfor (s1,s2), a, r, (ss1,ss2), tmd in memory:\n    qhat = q_table[s1,s2,a] # ë‚´ê°€ ìƒê°í–ˆë˜ ê°’\n    if tmd:\n        q = r # ì‹¤ì œê°’\n    else:\n        future = q_table[ss1,ss2,:].max() # ss1,ss2 : ë‹¤ìŒ ìƒíƒœ\n        q = r + 0.95*future\n    diff = q-qhat # ì°¨ì´\n    q_table[s1,s2,a] = q_table[s1,s2,a]  + 0.01*diff# update\n\n\nq_table[...,0] # action 0 ì„ í–ˆì„ ë•Œ í…Œì´ë¸”\n\narray([[72.83952058, 77.72350932, 82.80344973, -9.99996536],\n       [77.72636791, 82.87118608, 88.27814712, -9.99999613],\n       [82.86957406, 88.28769271, 93.99071303, -9.9993985 ],\n       [88.24270099, 93.97700808, 99.99082608,  0.        ]])\n\n\n\nê°ê°€ìœ¨ì„ 5%ê°€ ì•„ë‹ˆë¼ 1%ë§Œ ê¹ìœ¼ë©´ ê°’ì´ ì¡°ê¸ˆ ë” ì»¤ì§ (ë¯¸ë˜ì— ëŒ€í•œ ë³´ìƒì„ ì¡°ê¸ˆ ë” ë†’ê²Œ ì¹˜ë¯€ë¡œ)\n\n\nq_table = np.zeros((4,4,4))\nmemory = zip(player.states, player.actions, player.rewards, player.next_states, player.terminations)\nfor (s1,s2), a, r, (ss1,ss2), tmd in memory:\n    qhat = q_table[s1,s2,a] # ë‚´ê°€ ìƒê°í–ˆë˜ ê°’\n    if tmd:\n        q = r # ì‹¤ì œê°’\n    else:\n        future = q_table[ss1,ss2,:].max() # ss1,ss2 : ë‹¤ìŒ ìƒíƒœ\n        q = r + 0.99*future\n    diff = q-qhat # ì°¨ì´\n    q_table[s1,s2,a] = q_table[s1,s2,a]  + 0.01*diff# update\n\n\nq_table[...,0] # action 0 ì„ í–ˆì„ ë•Œ í…Œì´ë¸”\n\narray([[90.18056786, 92.09886903, 93.96750757, -9.99996536],\n       [92.10224034, 94.04416666, 95.99626811, -9.99999613],\n       [94.04234232, 96.00663449, 97.990322  , -9.9993985 ],\n       [95.95777379, 97.97603998, 99.99082608,  0.        ]])\n\n\n\ní•™ìŠµí•˜ê¸°ì—ëŠ” 5%ê°€ ë‚˜ì„ ê²ƒ ê°™ìŒ (ê°’ë§ˆë‹¤ ì°¨ì´ê°€ í¬ë¯€ë¡œ)\n\n\nq_table = np.zeros((4,4,4))\nmemory = zip(player.states, player.actions, player.rewards, player.next_states, player.terminations)\nfor (s1,s2), a, r, (ss1,ss2), tmd in memory:\n    qhat = q_table[s1,s2,a] # ë‚´ê°€ ìƒê°í–ˆë˜ ê°’\n    if tmd:\n        q = r # ì‹¤ì œê°’\n    else:\n        future = q_table[ss1,ss2,:].max() # ss1,ss2 : ë‹¤ìŒ ìƒíƒœ\n        q = r + 0.95*future\n    diff = q-qhat # ì°¨ì´\n    q_table[s1,s2,a] = q_table[s1,s2,a]  + 0.01*diff# update\n\n\nq_table[...,0] # action 0 ì„ í–ˆì„ ë•Œ í…Œì´ë¸”\n\narray([[72.83952058, 77.72350932, 82.80344973, -9.99996536],\n       [77.72636791, 82.87118608, 88.27814712, -9.99999613],\n       [82.86957406, 88.28769271, 93.99071303, -9.9993985 ],\n       [88.24270099, 93.97700808, 99.99082608,  0.        ]])\n\n\n\nq_table[...,1] # ì™¼ìª½ìœ¼ë¡œ ê°”ì„ ë•Œ í…Œì´ë¸”\n\narray([[-10.        ,  68.19615723,  72.83448762,  77.68726629],\n       [-10.        ,  72.83795619,  77.72290021,  82.84620653],\n       [-10.        ,  77.72026164,  82.86332991,  88.19689967],\n       [ -9.99997682,  82.748506  ,  88.11876519,   0.        ]])\n\n\n\nq_table[...,2] # ì•„ë˜ë¡œ ê°”ì„ ë•Œ í…Œì´ë¸”\n\narray([[72.83963932, 77.72633004, 82.86856325, 88.23479886],\n       [77.72428797, 82.8712446 , 88.28755321, 93.9835884 ],\n       [82.81365039, 88.27147768, 93.98527541, 99.99427983],\n       [-9.99998066, -9.99999722, -9.99915348,  0.        ]])\n\n\n\nq_table[...,3] # ìœ„ë¡œ ê°”ì„ ë•Œ í…Œì´ë¸”\n\narray([[-10.        , -10.        , -10.        ,  -9.99997587],\n       [ 68.19612628,  72.8376872 ,  77.7195001 ,  82.71315412],\n       [ 72.83517558,  77.72302563,  82.86358289,  88.14411865],\n       [ 77.68021549,  82.84140857,  88.18561423,   0.        ]])\n\n\n\nì•¡ì…˜ì— ë”°ë¼ ì¶œë ¥\n\n\nfor a in [0,1,2,3]: \n    print(f\"action = {a}\")\n    print(f\"q[...,{a}] = {q_table[...,a].round(3)}\")\n    print(\"---\")\n\naction = 0\nq[...,0] = [[ 72.84   77.724  82.803 -10.   ]\n [ 77.726  82.871  88.278 -10.   ]\n [ 82.87   88.288  93.991  -9.999]\n [ 88.243  93.977  99.991   0.   ]]\n---\naction = 1\nq[...,1] = [[-10.     68.196  72.834  77.687]\n [-10.     72.838  77.723  82.846]\n [-10.     77.72   82.863  88.197]\n [-10.     82.749  88.119   0.   ]]\n---\naction = 2\nq[...,2] = [[ 72.84   77.726  82.869  88.235]\n [ 77.724  82.871  88.288  93.984]\n [ 82.814  88.271  93.985  99.994]\n [-10.    -10.     -9.999   0.   ]]\n---\naction = 3\nq[...,3] = [[-10.    -10.    -10.    -10.   ]\n [ 68.196  72.838  77.72   82.713]\n [ 72.835  77.723  82.864  88.144]\n [ 77.68   82.841  88.186   0.   ]]\n---\n\n\n\n# player.q_table # ì •ì˜íŒ ì ì´ ì—†ìœ¼ë¯€ë¡œ error\n\n\nplayer.q_table = q_table # ë°©ê¸ˆ í•œ ê²ƒì„ ë®ì–´ ì”Œìš°ë©´\n\n\nplayer.q_table\n\narray([[[ 72.83952058, -10.        ,  72.83963932, -10.        ],\n        [ 77.72350932,  68.19615723,  77.72633004, -10.        ],\n        [ 82.80344973,  72.83448762,  82.86856325, -10.        ],\n        [ -9.99996536,  77.68726629,  88.23479886,  -9.99997587]],\n\n       [[ 77.72636791, -10.        ,  77.72428797,  68.19612628],\n        [ 82.87118608,  72.83795619,  82.8712446 ,  72.8376872 ],\n        [ 88.27814712,  77.72290021,  88.28755321,  77.7195001 ],\n        [ -9.99999613,  82.84620653,  93.9835884 ,  82.71315412]],\n\n       [[ 82.86957406, -10.        ,  82.81365039,  72.83517558],\n        [ 88.28769271,  77.72026164,  88.27147768,  77.72302563],\n        [ 93.99071303,  82.86332991,  93.98527541,  82.86358289],\n        [ -9.9993985 ,  88.19689967,  99.99427983,  88.14411865]],\n\n       [[ 88.24270099,  -9.99997682,  -9.99998066,  77.68021549],\n        [ 93.97700808,  82.748506  ,  -9.99999722,  82.84140857],\n        [ 99.99082608,  88.11876519,  -9.99915348,  88.18561423],\n        [  0.        ,   0.        ,   0.        ,   0.        ]]])\n\n\n)ğŸ—£ï¸\n\nq_table = np.zeros((4,4,4))\nmemory = zip(player.states, player.actions, player.rewards, player.next_states, player.terminations)\nfor (s1,s2), a, r, (ss1,ss2), tmd  in memory:\n    qhat = q_table[s1,s2,a] # ë‚´ê°€ ìƒê°í–ˆë˜ê°’\n    if tmd: \n        q = r # ì‹¤ì œê°’\n    else: \n        future = q_table[ss1,ss2,:].max()\n        q = r + 0.95 * future\n    diff = q-qhat # ì°¨ì´\n    q_table[s1,s2,a] = q_table[s1,s2,a]  + 0.01*diff# update\n\n\nfor a in [0,1,2,3]: \n    print(f\"action = {a}\")\n    print(f\"q[...,{a}] = {q_table[...,a].round(3)}\")\n    print(\"---\")\n\naction = 0\nq[...,0] = [[ 72.838  77.721  82.807 -10.   ]\n [ 77.725  82.87   88.276 -10.   ]\n [ 82.867  88.285  93.989  -9.999]\n [ 88.229  93.978  99.991   0.   ]]\n---\naction = 1\nq[...,1] = [[-10.     68.195  72.833  77.683]\n [-10.     72.836  77.721  82.849]\n [-10.     77.717  82.862  88.186]\n [-10.     82.723  88.128   0.   ]]\n---\naction = 2\nq[...,2] = [[ 72.838  77.724  82.867  88.241]\n [ 77.721  82.869  88.286  93.982]\n [ 82.801  88.272  93.985  99.993]\n [-10.    -10.     -9.999   0.   ]]\n---\naction = 3\nq[...,3] = [[-10.    -10.    -10.    -10.   ]\n [ 68.195  72.835  77.718  82.734]\n [ 72.832  77.721  82.863  88.122]\n [ 77.678  82.845  88.216   0.   ]]\n---\n\n\naction 0: ì˜¤ë¥¸ìª½\naction 1: ì™¼ìª½\naction 2: ì•„ë˜ìª½\naction 3: ìœ„ìª½\n\nplayer.q_table = q_table\n\n- ì´ì œ í”Œë ˆì´ì–´ì˜ í–‰ë™ì€?\nğŸ—£ï¸(\n\ns1=0\ns2=0\n\n\nplayer.q_table[s1,s2,:]\n\narray([ 72.83952058, -10.        ,  72.83963932, -10.        ])\n\n\n\nìˆ«ì 4ê°œ (ê° ì•¡ì…˜ì„ í–ˆì„ ë•Œì˜ ê²°ê³¼), 0 ë˜ëŠ” 2ì˜ í–‰ë™ì„ í•˜ë©´ ë¨\n\n\nplayer.q_table[s1,s2,:].argmax() # ì´ê²ƒì´ actionì´ ë˜ì–´ì•¼ í•¨\n\nnp.int64(2)\n\n\n\ndef act(player,s1,s2):\n    action = player.q_table[s1,s2,:].argmax()\n    return action\n\n\nact(player,0,0) # player ì˜¤ë¸Œì íŠ¸ ì•ˆì—ëŠ” q_tableì´ ìˆìŒ\n\nnp.int64(2)\n\n\naction 0: ì˜¤ë¥¸ìª½\naction 1: ì™¼ìª½\naction 2: ì•„ë˜ìª½\naction 3: ìœ„ìª½\nâœï¸ ê°•ì˜ ì˜ìƒê³¼ ì„¸ë¶€ ë°©í–¥ì€ ë‹¤ë¦„ (í° í‹€ì€ ë™ì¼)\n\nact(player,0,0) # ì¶œë°œì—ì„œ ì•„ë˜ìª½ì´ë¯€ë¡œ ë§ëŠ” ê²ƒ ê°™ìŒ (ê°•ì˜ ì˜ìƒì€ ì˜¤ë¥¸ìª½)\n\nnp.int64(2)\n\n\n\nact(player,1,0) # ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì´ë™ (1,0) =&gt; (1,1)\n\nnp.int64(0)\n\n\n\nact(player,1,1) # ì•„ë˜ìª½ìœ¼ë¡œ ì´ë™ (1,1) =&gt; (2,1)\n\nnp.int64(2)\n\n\n\nact(player,2,1) # ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì´ë™ (2,1) =&gt; (2,2)\n\nnp.int64(0)\n\n\n\nact(player,2,2) # ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì´ë™ (2,2) =&gt; (2,3)\n\nnp.int64(0)\n\n\n\nact(player,2,3) # ì•„ë˜ìª½ìœ¼ë¡œ ì´ë™ (2,3) =&gt; (3,3)\n\nnp.int64(2)\n\n\n)ğŸ—£ï¸\n\nplayer.q_table\n\narray([[[ 72.83771663, -10.        ,  72.83771538, -10.        ],\n        [ 77.72139771,  68.19454652,  77.72442966, -10.        ],\n        [ 82.80747929,  72.83310558,  82.86672465, -10.        ],\n        [ -9.99998197,  77.6827334 ,  88.24107054,  -9.99998511]],\n\n       [[ 77.72454812, -10.        ,  77.72127368,  68.19457346],\n        [ 82.86950596,  72.83600446,  82.86912699,  72.83541752],\n        [ 88.27585546,  77.72068451,  88.28563001,  77.71797997],\n        [ -9.99999683,  82.84920596,  93.98163671,  82.73393175]],\n\n       [[ 82.8669733 , -10.        ,  82.8012108 ,  72.83236834],\n        [ 88.28521264,  77.71722413,  88.27171754,  77.72136508],\n        [ 93.9886205 ,  82.86197892,  93.98461924,  82.86278052],\n        [ -9.99904498,  88.18588483,  99.99321405,  88.12228859]],\n\n       [[ 88.22883855,  -9.9999832 ,  -9.99997635,  77.67756866],\n        [ 93.97813902,  82.72305248,  -9.99999792,  82.84474465],\n        [ 99.99082608,  88.12798376,  -9.99917862,  88.21551322],\n        [  0.        ,   0.        ,   0.        ,   0.        ]]])\n\n\n\ndef act(player,s1,s2):\n    action = player.q_table[s1,s2,:].argmax()\n    return action\n\n\nact(player,0,0)\n\n0\n\n\n\nplayer.q_table[0,0,:]\n\narray([ 72.83771663, -10.        ,  72.83771538, -10.        ])\n\n\nğŸ—£ï¸(\n\nQ (q_table) â€“&gt; DQN (Deep, Network)\n\n# SARSA\nQ learning ì€ SARS ê¹Œì§€ë§Œ ìƒê°í•¨ (next actionì€ ìµœì ì˜ ê°’ì„ ê°€ì •)\n#---#\nq_table ---&gt; ì–´ë– í•œ í–‰ë™ì„ í• ì§€ ê²°ì •\ní•™ìŠµ ì´í›„ì—ëŠ” ì •í•´ì§„ pathë¡œë§Œ ê°\n\nact(player,0,0)\n\nnp.int64(2)\n\n\n\nplayer.q_table[0,0,:]\n\narray([ 72.83952058, -10.        ,  72.83963932, -10.        ])\n\n\n0ê³¼ 2ëŠ” ì‚¬ì‹¤ìƒ ë¹„ë“±í•œ ê°€ì¹˜ë¥¼ ê°–ê³  ìˆìŒì—ë„ ë§¤ìš° ì‘ì€ ì°¨ì´ ë•Œë¬¸ì— í•­ìƒ 2ë§Œ ì„ íƒ\nì´ê²Œ ë§ëŠ” ê±´ê°€ ì‹¶ìœ¼ë©´ì„œ í™•ë¥ ë¡œ ë°”ê¿”ì„œ ë°˜ë°˜ìœ¼ë¡œ ì„ íƒí•˜ëŠ” ê²ƒë„ ìƒê°í•´ë³¼ë§Œ í•¨\n(ê²°ì •ì  ì„¤ê³„ vs í™•ë¥ ì  ì„¤ê³„)\n\ní•˜ì§€ë§Œ ë°´ë”§ê²Œì„ì—ì„œ ë³´ìƒì´ 1, 10ì´ ì•„ë‹ˆë¼ 1, 2ë¼ë©´\n1ì„ 1/(1+2)ì˜ í™•ë¥ ë¡œ ì„ íƒí•˜ëŠ” ê²ƒì€ ë„ˆë¬´ í° ê²ƒ ê°™ìŒ\nsoftmax í•¨ìˆ˜ë¥¼ ì·¨í•´ì„œ 2ì— ë§ì€ í™•ë¥ ì„ ì¤„ ìˆ˜ë„ ìˆìŒ (ë§Œë“¤ê¸° ë‚˜ë¦„)\n\ní•­ìƒ ê²°ì •ì ìœ¼ë¡œ ì„¤ê³„í•˜ëŠ” ê²ƒì´ bestê°€ ì•„ë‹˜ì„ ì•Œ ìˆ˜ ìˆìŒ\n\në§ì€ ë³€í˜• ê°€ëŠ¥\n100ë²ˆ ì¤‘ 95ë²ˆì€ ê²°ì •ì , 5ë²ˆì€ í™•ë¥ ì  ë“±\n)ğŸ—£ï¸"
  },
  {
    "objectID": "posts/06wk-2.html",
    "href": "posts/06wk-2.html",
    "title": "06wk-2: (ì‹ ê²½ë§) â€“ ë‹¤í•­ë¶„ë¥˜, FashionMNIST",
    "section": "",
    "text": "ğŸ“˜ Note Format Guide\nThis format serves as a structured guide for organizing lecture content, personal interpretation, experiments, and study-related questions.\nğŸ“ ğŸ—£ï¸ âœï¸ ğŸ”¬ â“"
  },
  {
    "objectID": "posts/06wk-2.html#a.-ì´í•­ë¶„ë¥˜ì™€-bcewithlogitsloss",
    "href": "posts/06wk-2.html#a.-ì´í•­ë¶„ë¥˜ì™€-bcewithlogitsloss",
    "title": "06wk-2: (ì‹ ê²½ë§) â€“ ë‹¤í•­ë¶„ë¥˜, FashionMNIST",
    "section": "A. ì´í•­ë¶„ë¥˜ì™€ BCEWithLogitsLoss",
    "text": "A. ì´í•­ë¶„ë¥˜ì™€ BCEWithLogitsLoss\n- ë°ì´í„°\n\ntrain_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True)\nto_tensor = torchvision.transforms.ToTensor()\nX0_train = torch.stack([to_tensor(Xi) for Xi, yi in train_dataset if yi==0])\nX1_train = torch.stack([to_tensor(Xi) for Xi, yi in train_dataset if yi==1])\nX = torch.concat([X0_train,X1_train],axis=0).reshape(-1,784)\ny = torch.tensor([0.0]*len(X0_train) + [1.0]*len(X1_train)).reshape(-1,1)\n\nğŸ—£ï¸(\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(784,32),\n    torch.nn.ReLU(),\n    torch.nn.Linear(32,1),\n    torch.nn.Sigmoid()\n)\n\n\nnet(X)\n\ntensor([[0.4577],\n        [0.4624],\n        [0.4653],\n        ...,\n        [0.4700],\n        [0.4620],\n        [0.4738]], grad_fn=&lt;SigmoidBackward0&gt;)\n\n\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(784,32),\n    torch.nn.ReLU(),\n    torch.nn.Linear(32,1),\n)\nsig = torch.nn.Sigmoid()\n\n\nsig(net(X))\n\ntensor([[0.4577],\n        [0.4624],\n        [0.4653],\n        ...,\n        [0.4700],\n        [0.4620],\n        [0.4738]], grad_fn=&lt;SigmoidBackward0&gt;)\n\n\n\ntorch.exp(net(X))/(1+torch.exp(net(X)))\n\ntensor([[0.4577],\n        [0.4624],\n        [0.4653],\n        ...,\n        [0.4700],\n        [0.4620],\n        [0.4738]], grad_fn=&lt;DivBackward0&gt;)\n\n\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(784,32),\n    torch.nn.ReLU(),\n    torch.nn.Linear(32,1),\n)\nsig = torch.nn.Sigmoid()\nloss_fn = torch.nn.BCELoss()\noptimizr = torch.optim.Adam(net.parameters())\n#---#\nfor epoc in range(1,31):\n    #1\n    netout = net(X)\n    yhat = sig(netout)\n    #2\n    loss = loss_fn(yhat,y)\n    #3\n    loss.backward()\n    #4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nsig(net(X))\n\ntensor([[0.0179],\n        [0.0205],\n        [0.1106],\n        ...,\n        [0.8751],\n        [0.8740],\n        [0.8033]], grad_fn=&lt;SigmoidBackward0&gt;)\n\n\n\ntorch.manual_seed(43052)\nsig = torch.nn.Sigmoid()\nnet = torch.nn.Sequential(\n    torch.nn.Linear(784,32),\n    torch.nn.ReLU(),\n    torch.nn.Linear(32,1),\n    sig\n)\n\nloss_fn = torch.nn.BCELoss()\noptimizr = torch.optim.Adam(net.parameters())\n#---#\nfor epoc in range(1,31):\n    #1\n    #netout = net(X)\n    yhat = net(X)\n    #2\n    loss = loss_fn(yhat,y)\n    #3\n    loss.backward()\n    #4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nnet(X)\n\ntensor([[0.0179],\n        [0.0205],\n        [0.1106],\n        ...,\n        [0.8751],\n        [0.8740],\n        [0.8033]], grad_fn=&lt;SigmoidBackward0&gt;)\n\n\n\nê²°ê³¼ ë™ì¼\n\ntorch.nn.Linear(32,1)ì„ ë¹¼ë©´ optimizerì— parameterê°€ ì „ë‹¬ì´ ë˜ì§€ ì•Šì•„ updateê°€ ë˜ì§€ ì•ŠìŒ\nsigì—ëŠ” í•™ìŠµí•´ì•¼í•  parameterê°€ ì—†ìœ¼ë¯€ë¡œ ë¹¼ë„(ë¹¼ê³  ë‚˜ì˜¨ ê²°ê³¼ì— sig) ë˜ê³  ë„£ì–´ë„ ë¨\n\n\n\nsig(net(X)) # í™•ë¥ \n\ntensor([[0.5045],\n        [0.5051],\n        [0.5276],\n        ...,\n        [0.7058],\n        [0.7056],\n        [0.6907]], grad_fn=&lt;SigmoidBackward0&gt;)\n\n\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(784,32),\n    torch.nn.ReLU(),\n    torch.nn.Linear(32,1),\n)\nsig = torch.nn.Sigmoid()\nloss_fn = torch.nn.BCELoss()\noptimizr = torch.optim.Adam(net.parameters())\n#---#\nfor epoc in range(1,31):\n    #1\n    netout = net(X)\n    yhat = sig(netout)\n    #2\n    loss = loss_fn(yhat,y)\n    #3\n    loss.backward()\n    #4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nlogits = netout = net(X) # ë¡œì§“\n\n\nsig(logits) # sig(net(X))ê³¼ ë™ì¼\n\ntensor([[0.0179],\n        [0.0205],\n        [0.1106],\n        ...,\n        [0.8751],\n        [0.8740],\n        [0.8033]], grad_fn=&lt;SigmoidBackward0&gt;)\n\n\n\ntorch.exp(logits)/(1+torch.exp(logits))\n\ntensor([[0.0179],\n        [0.0205],\n        [0.1106],\n        ...,\n        [0.8751],\n        [0.8740],\n        [0.8033]], grad_fn=&lt;DivBackward0&gt;)\n\n\n\në§Œì•½ logitì´ 0ì´ë©´ 1/2\n\n\ntorch.exp(torch.tensor([0]))/(1+torch.exp(torch.tensor([0])))\n\ntensor([0.5000])\n\n\n\në§Œì•½ logitì´ ì–‘ìˆ˜ë©´ ì¦ê°€í• ìˆ˜ë¡ ì „ì²´ ê°’ì€ 1ì— ê°€ê¹Œì›Œì§\në§Œì•½ logitì´ ìŒìˆ˜ë©´ ì¦ê°€í• ìˆ˜ë¡ ì „ì²´ ê°’ì€ 0ì— ê°€ê¹Œì›Œì§\nìœ„ì˜ ë‚´ìš©ì„ í™œìš©í•˜ì—¬ accuracy ì ìš© ê°€ëŠ¥\n\n\n((sig(netout) &gt; 0.5) == y).float().mean()\n\ntensor(0.9956)\n\n\n\nsig(netout)&gt;0.5\n\ntensor([[False],\n        [False],\n        [False],\n        ...,\n        [ True],\n        [ True],\n        [ True]])\n\n\n\nnetout&gt;0 # ìœ„ì™€ ë™ì¼\n\ntensor([[False],\n        [False],\n        [False],\n        ...,\n        [ True],\n        [ True],\n        [ True]])\n\n\n\n((netout&gt;0) == y).float().mean() # ìœ„ì™€ ë™ì¼\n\ntensor(0.9956)\n\n\n)ğŸ—£ï¸\n- ì˜ˆì „ì— ì í•©í–ˆë˜ ì½”ë“œì—ì„œ sigë¥¼ ë¶„ë¦¬í•œê²ƒ\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(784,32),\n    torch.nn.ReLU(),\n    torch.nn.Linear(32,1),\n)\nsig = torch.nn.Sigmoid()\nloss_fn = torch.nn.BCELoss()\noptimizr = torch.optim.Adam(net.parameters())\n#---#\nfor epoc in range(1,31):\n    #1\n    netout = net(X) # netout = logits\n    yhat = sig(netout) # yhat = probs\n    #2\n    loss = loss_fn(yhat,y)\n    #3\n    loss.backward()\n    #4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n# netout(=logits) ì˜ íŠ¹ì§•\n\n\\(netout &gt;0 \\Leftrightarrow sig(netout) &gt; 0.5\\)\n\\(netout &lt;0 \\Leftrightarrow sig(netout) &lt; 0.5\\)\n\n\n((net(X)&gt;0) ==y).float().mean()\n\ntensor(0.9956)\n\n\n- ì•„ë˜ì˜ ì½”ë“œëŠ” ìœ„ì˜ ì½”ë“œì™€ ê°™ì€ ì½”ë“œì„\n\nğŸ—£ï¸\n\nlossì—ì„œ logitsê³¼ yë¥¼ ë°”ë¡œ ë¹„êµí•˜ê³  ì‹¶ìŒ\nBCELoss ëŒ€ì‹  BCEWithLogitsLoss ì‚¬ìš© (yhatê³¼ yê°€ ì•„ë‹Œ logitsê³¼ yë¥¼ ë¹„êµí•˜ì—¬ lossê³„ì‚°)\nlossì— yhatì„ ì§‘ì–´ë„£ì„ í•„ìš”ê°€ ì—†ê¸° ë•Œë¬¸ì— yhat êµ¬í•˜ëŠ” ì½”ë“œ ì‚­ì œ\nì´ë ‡ê²Œ ë°”ê¾¸ë©´ ì»´í“¨í„° ê³µí•™ì ìœ¼ë¡œ ë‚˜ìŒ (ì—°ì‚° í¸ë¦¬)\n\n\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(784,32),\n    torch.nn.ReLU(),\n    torch.nn.Linear(32,1),\n)\nloss_fn = torch.nn.BCEWithLogitsLoss()\noptimizr = torch.optim.Adam(net.parameters())\n#---#\nfor epoc in range(1,31):\n    #1\n    netout = net(X) \n    #2\n    loss = loss_fn(netout,y)\n    #3\n    loss.backward()\n    #4 \n    optimizr.step()\n    optimizr.zero_grad()\n\nğŸ—£ï¸(\n\nê° ì½”ë“œ ë§ˆì§€ë§‰ì— ë‹¤ìŒì„ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë¹„êµí•˜ë©´ ê°™ìŒì„ ì•Œ ìˆ˜ ìˆìŒ\n\n##--ì—í­ì´í›„ì½”ë“œ--#\nacc = ((net(X)&gt;0) ==u).float().mean()\nprint(epoc, acc)\n)ğŸ—£ï¸"
  },
  {
    "objectID": "posts/06wk-2.html#b.-ë²”ì£¼í˜•ìë£Œì˜-ë³€í™˜",
    "href": "posts/06wk-2.html#b.-ë²”ì£¼í˜•ìë£Œì˜-ë³€í™˜",
    "title": "06wk-2: (ì‹ ê²½ë§) â€“ ë‹¤í•­ë¶„ë¥˜, FashionMNIST",
    "section": "B. ë²”ì£¼í˜•ìë£Œì˜ ë³€í™˜",
    "text": "B. ë²”ì£¼í˜•ìë£Œì˜ ë³€í™˜\n- ë²”ì£¼í˜•ìë£Œë¥¼ ìˆ«ìë¡œ ì–´ë–»ê²Œ ë°”ê¿€ê¹Œ?\n\nì‹¤íŒ¨ / ì„±ê³µ \\(\\to\\) 0 / 1\nìˆ«ì0ê·¸ë¦¼ / ìˆ«ì1ê·¸ë¦¼ \\(\\to\\) 0 / 1\nê°•ì•„ì§€ê·¸ë¦¼ / ê³ ì–‘ì´ê·¸ë¦¼ \\(\\to\\) 0 / 1\nê°•ì•„ì§€ê·¸ë¦¼ / ê³ ì–‘ì´ê·¸ë¦¼ / í† ë¼ê·¸ë¦¼ \\(\\to\\) 0 / 1 / 2 ?????\n\n- ì£¼ì…ì‹êµìœ¡: ê°•ì•„ì§€ê·¸ë¦¼/ê³ ì–‘ì´ê·¸ë¦¼/í† ë¼ê·¸ë¦¼ì¼ ê²½ìš° ìˆ«ìí™”ì‹œí‚¤ëŠ” ë°©ë²•\n\nì˜ëª»ëœë°©ì‹: ê°•ì•„ì§€ê·¸ë¦¼ = 0, ê³ ì–‘ì´ê·¸ë¦¼ = 1, í† ë¼ê·¸ë¦¼ = 2\nì˜¬ë°”ë¥¸ë°©ì‹: ê°•ì•„ì§€ê·¸ë¦¼ = [1,0,0], ê³ ì–‘ì´ê·¸ë¦¼ = [0,1,0], í† ë¼ê·¸ë¦¼ = [0,0,1] ### &lt;â€“ ì´ëŸ°ë°©ì‹ì„ ì›í•«ì¸ì½”ë”©ì´ë¼í•¨\n\nğŸ—£ï¸(\n[1,0,0]\n[1,0,0]\n[0,1,0]\n[0,0,1]\ncolumn ë³„ë¡œ í‰ê· \n[0.5,0.25,0.25]\nê°•ì•„ì§€ê°€ 50% ìˆë‹¤ê³  ìƒê° ê°€ëŠ¥ (í‰ê· ì´ ì˜ë¯¸ë¥¼ ê°€ì§)\n)ğŸ—£ï¸\n- ì™œ?\n\nì„¤ëª…1: ê°•ì•„ì§€ê·¸ë¦¼, ê³ ì–‘ì´ê·¸ë¦¼, í† ë¼ê·¸ë¦¼ì€ ì„œì—´ì¸¡ë„ê°€ ì•„ë‹ˆë¼ ëª…ëª©ì²™ë„ì„. ê·¸ë˜ì„œ ë²”ì£¼ë¥¼ 0,1,2 ë¡œ ìˆ«ìí™”í•˜ë©´ í‰ê· ë“±ì˜ ì˜ë¯¸ê°€ ì—†ìŒ (ì‚¬íšŒì¡°ì‚¬ë¶„ì„ì‚¬ 2ê¸‰ ìŠ¤íƒ€ì¼)\nì„¤ëª…2: ë²”ì£¼í˜•ì€ ì›í•«ì¸ì½”ë”©ìœ¼ë¡œ í•´ì•¼í•¨ (â€œ30ì¼ë§Œì— ëë‚´ëŠ” ì‹¤ì „ë¨¸ì‹ ëŸ¬ë‹â€ ì´ëŸ° ì±…ì— ë‚˜ì˜¤ëŠ” ìŠ¤íƒ€ì¼)\nì„¤ëª…3: ë™ì „ì„ í•œë²ˆ ë˜ì ¸ì„œ ë‚˜ì˜¤ëŠ” ê²°ê³¼ëŠ” \\(n=1\\)ì¸ ì´í•­ë¶„í¬ë¥¼ ë”°ë¦„. ì£¼ì‚¬ìœ„ í•œë²ˆ ë˜ì ¸ì„œ ë‚˜ì˜¤ëŠ” ëˆˆê¸ˆì˜ ìˆ«ìëŠ” \\(n=1\\)ì¸ ë‹¤í•­ë¶„í¬ë¥¼ ë”°ë¦„. \\(n=1\\)ì¸ ì´í•­ë¶„í¬ì˜ ì‹¤í˜„ê°’ì€ 0,1 ì´ê³ , \\(n=1\\)ì¸ ë‹¤í•­ë¶„í¬ì˜ ì‹¤í˜„ê°’ì€ [1,0,0], [0,1,0], [0,0,1] ì´ë¯€ë¡œ ë‹¹ì—°íˆ \\(y_i\\) ëŠ” [1,0,0], [0,1,0], [0,0,1] ì¤‘ í•˜ë‚˜ì˜ í˜•íƒœë¥¼ ê°€ì§„ë‹¤ê³  ê°€ì •í•˜ëŠ”ê²Œ ë°”ëŒì§í•¨ (ì´ ì„¤ëª…ì´ ì´ ì¤‘ì—ì„œ ê°€ì¥ ì •í™•í•œ ì„¤ëª…ì„)\n\nğŸ—£ï¸(\nëª¨ë¸ë§ = ìŠ¤íŠ¸ëŸ­ì²˜ + ì˜¤ì°¨\ny = 0,1,0,0,1\nprob = 0.2. 0.8 (ìŠ¤íŠ¸ëŸ­ì²˜)\nì˜¤ì°¨í•­ì— ëŒ€í•œ ëª¨ë¸: ì´í•­ ë¶„í¬\ny = ì•ë©´, ë’·ë©´ ---&gt; 0,1 # n=1ì¸ ì´í•­ë¶„í¬ë¥¼ ë”°ë¦„ = ë² ë¥´ëˆ„ì´ë¥¼ ë”°ë¦„\ny = ì£¼ì‚¬ìœ„ 1,2,3,4,5,6 ---&gt; ë‹¤í•­ë¶„í¬ë¥¼ ë”°ë¦„ n=1\nn=1ì¸ ë‹¤í•­ë¶„í¬ì˜ realization: [1,0,0,0,0,0] ...\n)ğŸ—£ï¸"
  },
  {
    "objectID": "posts/06wk-2.html#c.-ì‹¤ìŠµ-3ê°œì˜-í´ë˜ìŠ¤ë¥¼-êµ¬ë¶„",
    "href": "posts/06wk-2.html#c.-ì‹¤ìŠµ-3ê°œì˜-í´ë˜ìŠ¤ë¥¼-êµ¬ë¶„",
    "title": "06wk-2: (ì‹ ê²½ë§) â€“ ë‹¤í•­ë¶„ë¥˜, FashionMNIST",
    "section": "C. ì‹¤ìŠµ: 3ê°œì˜ í´ë˜ìŠ¤ë¥¼ êµ¬ë¶„",
    "text": "C. ì‹¤ìŠµ: 3ê°œì˜ í´ë˜ìŠ¤ë¥¼ êµ¬ë¶„\n- ë°ì´í„°ì¤€ë¹„\n\ntrain_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True)\nto_tensor = torchvision.transforms.ToTensor()\nX0 = torch.stack([to_tensor(Xi) for Xi, yi in train_dataset if yi==0])\nX1 = torch.stack([to_tensor(Xi) for Xi, yi in train_dataset if yi==1])\nX2 = torch.stack([to_tensor(Xi) for Xi, yi in train_dataset if yi==2])\nX = torch.concat([X0,X1,X2]).reshape(-1,1*28*28)\ny = torch.tensor([0]*len(X0) + [1]*len(X1)+ [2]*len(X2)).reshape(-1,1).float()\n\nğŸ—£ï¸(\n\nX.shape\n\ntorch.Size([18623, 784])\n\n\n\ny\n\ntensor([[0.],\n        [0.],\n        [0.],\n        ...,\n        [2.],\n        [2.],\n        [2.]])\n\n\n\ny.reshape(-1)\n\ntensor([0., 0., 0.,  ..., 2., 2., 2.])\n\n\n\nset(y.reshape(-1).tolist())\n\n{0.0, 1.0, 2.0}\n\n\n\nplt.imshow(X[0].reshape(28,28), cmap=\"gray\")\n\n\n\n\n\n\n\n\n\nplt.imshow(X[10000].reshape(28,28), cmap=\"gray\")\n\n\n\n\n\n\n\n\n\nplt.imshow(X[-1].reshape(28,28), cmap=\"gray\")\n\n\n\n\n\n\n\n\n\ny.shape\n\ntorch.Size([18623, 1])\n\n\n\ny\n\ntensor([[0.],\n        [0.],\n        [0.],\n        ...,\n        [2.],\n        [2.],\n        [2.]])\n\n\n\nyê°€ ì´ë ‡ê²Œ ì •ë¦¬ë˜ì–´ ìˆìœ¼ë©´ ì•ˆ ë¨\n\n[[0],[0],[1],[2]] -&gt; [[1,0,0],[1,0,0],[0,1,0],[0,0,1]] ì´ëŸ°ì‹ìœ¼ë¡œ ë°”ê¾¸ê³  ì‹¶ìŒ\n\n# torch.nn.functional.one_hot([0,0,1,2]) # error\n\n\ntorch.nn.functional.one_hot(torch.tensor([0,0,1,2]))\n\ntensor([[1, 0, 0],\n        [1, 0, 0],\n        [0, 1, 0],\n        [0, 0, 1]])\n\n\n\n1ì°¨ì›ì˜ tensorë¡œ ë„£ì–´ì£¼ë©´ ë°”ê¿”ì¤Œ\n\n\ny.reshape(-1)\n\ntensor([0., 0., 0.,  ..., 2., 2., 2.])\n\n\n\nì´ëŒ€ë¡œ ì§‘ì–´ ë„£ìœ¼ë©´ error (floatìœ¼ë¡œ ë“¤ì–´ê°)\n1ì°¨ì›ì˜ tensor + intë¡œ ë„£ì–´ì£¼ë©´ ë°”ê¿”ì¤Œ\n\n\ny.reshape(-1).long()\n\ntensor([0, 0, 0,  ..., 2, 2, 2])\n\n\n)ğŸ—£ï¸\n\ny = torch.nn.functional.one_hot(y.reshape(-1).long()).float()\ny\n\ntensor([[1., 0., 0.],\n        [1., 0., 0.],\n        [1., 0., 0.],\n        ...,\n        [0., 0., 1.],\n        [0., 0., 1.],\n        [0., 0., 1.]])\n\n\nğŸ—£ï¸ ë‹¤ì‹œ floatìœ¼ë¡œ ë°”ê¿”ì¤Œ\n- ì í•©\nğŸ—£ï¸(\n\nX.shape, y.shape\n\n(torch.Size([18623, 784]), torch.Size([18623, 3]))\n\n\n\nSigmoidëŠ” 1ì°¨ì›ì¼ ë•Œ ì ìš©ë˜ë¯€ë¡œ 1ì°¨ì›ì´ ì•„ë‹Œ ì§€ê¸ˆì€ ë‹¤ë¥¸ ê²ƒì´ í•„ìš”\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(784,32), # ì¼ë‹¨ 32 ì •ë„\n    torch.nn.ReLU(), # í‘œí˜„ë ¥ ì˜¬ë¦¬ê¸°\n    torch.nn.Linear(32,3)\n)\nloss_fn = torch.nn.CrossEntropyLoss() # ë‹¤ì°¨ì›ì¼ ë•Œ ì‚¬ìš© (logitsê³¼ y ë¹„êµ)\n\n\nnet(X)\n\ntensor([[-0.0676,  0.0558,  0.0013],\n        [-0.0191,  0.0977, -0.0303],\n        [-0.0771,  0.0898,  0.0959],\n        ...,\n        [-0.0168,  0.0298,  0.0016],\n        [ 0.0077,  0.0703,  0.0517],\n        [ 0.0628,  0.0072,  0.0380]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\n\nloss_fn(net(X),y) # yê°€ floatì´ ì•„ë‹ˆë©´ error\n\ntensor(1.0771, grad_fn=&lt;DivBackward1&gt;)\n\n\n\nì°¸ê³ ) BCEWithLigitsLoss: Binary Cross Entropy\n\n)ğŸ—£ï¸\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(784,32),\n    torch.nn.ReLU(),\n    torch.nn.Linear(32,3),\n)\nloss_fn = torch.nn.CrossEntropyLoss() # ì˜ë¯¸ìƒ CEWithLogitsLoss\noptimizr = torch.optim.Adam(net.parameters())\nfor epoc in range(1,31):\n    #1\n    netout = net(X) # netout: (n,3) \n    #2\n    loss = loss_fn(netout,y) \n    #3\n    loss.backward()\n    #4 \n    optimizr.step()\n    optimizr.zero_grad()\n\nğŸ—£ï¸(\n\naccuracy ê³„ì‚°\n\n\nnetout[::2] # logit ê°’ (í´ìˆ˜ë¡ 1ì´ ë‚˜ì˜¬ í™•ë¥ ì´ ë†’ìŒ)\n\ntensor([[ 2.7971, -1.7052, -1.4072],\n        [ 1.3707, -0.7746, -0.2272],\n        [ 4.1326, -2.8297, -1.6813],\n        ...,\n        [-0.5738, -2.3579,  1.8868],\n        [ 0.3950, -1.6556,  1.0137],\n        [-0.7091, -1.2342,  1.3557]], grad_fn=&lt;SliceBackward0&gt;)\n\n\n\ntorch.exp(netout) # ì „ë¶€ ì–‘ìˆ˜ê°€ ë¨\n\ntensor([[16.3971,  0.1817,  0.2448],\n        [19.7109,  0.2345,  0.2365],\n        [ 3.9381,  0.4609,  0.7968],\n        ...,\n        [ 1.4843,  0.1910,  2.7557],\n        [ 0.5556,  0.1023,  9.1077],\n        [ 0.4921,  0.2911,  3.8796]], grad_fn=&lt;ExpBackward0&gt;)\n\n\n\np1 = 16.3971 / (16.3971+0.1817+0.2448)\np2 = 0.1817 / (16.3971+0.1817+0.2448)\np3 = 0.2448 / (16.3971+0.1817+0.2448)\n\n\np1,p2,p3 # 0~1ì‚¬ì´ì— ìˆìŒ # ë¬´ì¡°ê±´ ì–‘ìˆ˜ # p1+p2+p3 = 1\n# --&gt; ê°ê° ì¹´í…Œê³ ë¦¬ì— ì†í•  í™•ë¥ ì´ë¼ê³  í•´ì„ ê°€ëŠ¥ (softmax function)\n# ìˆ«ìê°€ í¬ë©´ ê·¸ ì¹´í…Œê³ ë¦¬ì— ì†í•œë‹¤ê³  ë„¤íŠ¸ì›Œí¬ê°€ ê°•í•˜ê²Œ í™•ì‹ \n\n(0.9746487077676597, 0.010800304334387409, 0.014550987897952877)\n\n\n\nnetout\n\ntensor([[ 2.7971, -1.7052, -1.4072],\n        [ 2.9812, -1.4505, -1.4417],\n        [ 1.3707, -0.7746, -0.2272],\n        ...,\n        [ 0.3950, -1.6556,  1.0137],\n        [-0.5878, -2.2794,  2.2091],\n        [-0.7091, -1.2342,  1.3557]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\n\nnetout.argmax(axis=1) # ì˜ˆì¸¡ê°’\n\ntensor([0, 0, 0,  ..., 2, 2, 2])\n\n\n\ny\n\ntensor([[1., 0., 0.],\n        [1., 0., 0.],\n        [1., 0., 0.],\n        ...,\n        [0., 0., 1.],\n        [0., 0., 1.],\n        [0., 0., 1.]])\n\n\n\ny.argmax(axis=1)\n\ntensor([0, 0, 0,  ..., 2, 2, 2])\n\n\n)ğŸ—£ï¸\n\n(netout.argmax(axis=1) == y.argmax(axis=1)).float().mean()\n\ntensor(0.9674)"
  },
  {
    "objectID": "posts/06wk-2.html#d.-ê²°ë¡ -ì™¸ìš°ì„¸ì—¬",
    "href": "posts/06wk-2.html#d.-ê²°ë¡ -ì™¸ìš°ì„¸ì—¬",
    "title": "06wk-2: (ì‹ ê²½ë§) â€“ ë‹¤í•­ë¶„ë¥˜, FashionMNIST",
    "section": "D. ê²°ë¡  â€“ ì™¸ìš°ì„¸ì—¬",
    "text": "D. ê²°ë¡  â€“ ì™¸ìš°ì„¸ì—¬\n- íŒŒì´í† ì¹˜ë²„ì „ // ì½”ë”©ìš©\n\n\n\në¶„ë¥˜\nnetoutì˜ ì˜ë¯¸\nì†ì‹¤í•¨ìˆ˜\n\n\n\n\nì´í•­ë¶„ë¥˜\nprob\nBCELoss\n\n\nì´í•­ë¶„ë¥˜\nlogit\nBCEWithLogitsLoss\n\n\në‹¤í•­ë¶„ë¥˜\nprobs\nNA\n\n\në‹¤í•­ë¶„ë¥˜\nlogits\nCrossEntropyLoss\n\n\n\n\nğŸ—£ï¸\n\nprobì¸ë° BCEWithLogitsLoss: Sigmoid 2ë²ˆ\nlogitì¸ë° BCELoss: Sigmoid 0ë²ˆ\n2ë²ˆì§¸ì™€ 4ë²ˆì§¸ê°€ ì½”ë”©ì ìœ¼ë¡œ ë‚˜ìŒ, ê°œë…ì ìœ¼ë¡œ ì´í•´í•˜ê¸°ëŠ” 1ë²ˆì§¸ê°€ ë‚˜ìŒ\n\n\n\nCrossEntropyLoss ì´ê±° ì´ë¦„ì´ ì™„ì „ ë§ˆìŒì— ì•ˆë“¤ì–´ìš”.. CEWithLogitsLoss ë¼ê³  í•˜ëŠ”ê²Œ ë” ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.\n\n- ì¼ë°˜ì ê°œë… // ì´ë¡ ìš©\n\n\n\n\n\n\n\n\n\në¶„ë¥˜\nì˜¤ì°¨í•­ì˜ê°€ì •\në§ˆì§€ë§‰í™œì„±í™”í•¨ìˆ˜\nì†ì‹¤í•¨ìˆ˜\n\n\n\n\nì´í•­ë¶„ë¥˜\nì´í•­ë¶„í¬\nsigmoid1\nBinary Cross Entropy\n\n\në‹¤í•­ë¶„ë¥˜\në‹¤í•­ë¶„í¬\nsoftmax2\nCross Entropy\n\n\n\n- ì°¸ê³  (sigmoid, softmax ê³„ì‚°ê³¼ì •ë¹„êµ)\n\n\\(prob = \\text{sig}(logit) =\\frac{\\exp(logit)}{1+\\exp(logit)}\\)\n\\(probs= \\text{softmax}\\left(\\begin{bmatrix} logit_1 \\\\ logit_2 \\\\ logit_3\\end{bmatrix}\\right) =\\begin{bmatrix} \\frac{\\exp(logit_1)}{\\exp(logit_1)+\\exp(logit_2)+\\exp(logit_3)} \\\\\n\\frac{\\exp(logit_2)}{\\exp(logit_1)+\\exp(logit_2)+\\exp(logit_3)} \\\\\n\\frac{\\exp(logit_3)}{\\exp(logit_1)+\\exp(logit_2)+\\exp(logit_3)} \\end{bmatrix}\\)"
  },
  {
    "objectID": "posts/06wk-2.html#a.-ë°ì´í„°",
    "href": "posts/06wk-2.html#a.-ë°ì´í„°",
    "title": "06wk-2: (ì‹ ê²½ë§) â€“ ë‹¤í•­ë¶„ë¥˜, FashionMNIST",
    "section": "A. ë°ì´í„°",
    "text": "A. ë°ì´í„°\nhttps://arxiv.org/abs/1708.07747 [@xiao2017fashion]\n\ntrain_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True)\ntest_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True)\nto_tensor = torchvision.transforms.ToTensor()\nX = torch.stack([to_tensor(img) for img, lbl in train_dataset])\ny = torch.tensor([lbl for img, lbl in train_dataset])\ny = torch.nn.functional.one_hot(y).float()\nXX = torch.stack([to_tensor(img) for img, lbl in test_dataset])\nyy = torch.tensor([lbl for img, lbl in test_dataset])\nyy = torch.nn.functional.one_hot(yy).float()\n\n100.0%\n100.0%\n100.0%\n100.0%\n\n\nğŸ—£ï¸(\n\nX.shape, y.shape, XX.shape, yy.shape\n\n(torch.Size([60000, 1, 28, 28]),\n torch.Size([60000, 10]),\n torch.Size([10000, 1, 28, 28]),\n torch.Size([10000, 10]))\n\n\n\nX[0,0,:,:].shape # 4ì°¨ì›\n\ntorch.Size([28, 28])\n\n\n\nplt.imshow(X[0,0,:,:], cmap=\"gray\")\n\n\n\n\n\n\n\n\n\nplt.imshow(X[1,0,:,:], cmap=\"gray\") # ë‹¤ìŒ ê·¸ë¦¼\n\n\n\n\n\n\n\n\n\nplt.imshow(X[2,0,:,:], cmap=\"gray\") # ë‹¤ìŒ ê·¸ë¦¼\n\n\n\n\n\n\n\n\n\ny # ì›í•«ì¸ì½”ë”© ë˜ì–´ ìˆìŒ\n\ntensor([[0., 0., 0.,  ..., 0., 0., 1.],\n        [1., 0., 0.,  ..., 0., 0., 0.],\n        [1., 0., 0.,  ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [1., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]])\n\n\n\ntorchvision.datasets.FashionMNIST.classes\n\n['T-shirt/top',\n 'Trouser',\n 'Pullover',\n 'Dress',\n 'Coat',\n 'Sandal',\n 'Shirt',\n 'Sneaker',\n 'Bag',\n 'Ankle boot']\n\n\n\ntorchvision.datasets.FashionMNIST.classes[9] # label 9ê°€ ì˜ë¯¸í•˜ëŠ” ê²ƒ\n\n'Ankle boot'\n\n\n\nplt.imshow(X[2,0,:,:], cmap=\"gray\")\nprint(y[2,:])\n\ntensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n\n\n\n\n\n\n\n\n\n\nplt.imshow(X[2,0,:,:], cmap=\"gray\")\nprint(y[2,:].argmax())\n\ntensor(0)\n\n\n\n\n\n\n\n\n\n\nplt.imshow(X[2,0,:,:], cmap=\"gray\")\nprint(y[2,:].argmax().item())\n\n0\n\n\n\n\n\n\n\n\n\n\nplt.imshow(X[2,0,:,:], cmap=\"gray\")\nprint(torchvision.datasets.FashionMNIST.classes[y[2,:].argmax().item()])\n\nT-shirt/top\n\n\n\n\n\n\n\n\n\n)ğŸ—£ï¸\n\nobs_idx = 301\nplt.imshow(X[obs_idx,0,:,:],cmap=\"gray\")\nplt.title(torchvision.datasets.FashionMNIST.classes[y[obs_idx,:].argmax().item()]);"
  },
  {
    "objectID": "posts/06wk-2.html#b.-ê°„ë‹¨í•œ-ì‹ ê²½ë§",
    "href": "posts/06wk-2.html#b.-ê°„ë‹¨í•œ-ì‹ ê²½ë§",
    "title": "06wk-2: (ì‹ ê²½ë§) â€“ ë‹¤í•­ë¶„ë¥˜, FashionMNIST",
    "section": "B. ê°„ë‹¨í•œ ì‹ ê²½ë§",
    "text": "B. ê°„ë‹¨í•œ ì‹ ê²½ë§\nğŸ—£ï¸(\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(784,32), # 28*28\n    torch.nn.ReLU(),\n    torch.nn.Linear(32,10)\n)\nloss.fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(net.parameters())\n#---#\n\n\n# net(X) # error\n\n\nX.shape # 784ê°€ ì•„ë‹˜\n\ntorch.Size([60000, 1, 28, 28])\n\n\n\nnet(X.reshape(-1,784))\n\ntensor([[-0.1190,  0.1954, -0.1191,  ...,  0.0729,  0.1721, -0.0408],\n        [-0.0379,  0.2657, -0.0238,  ..., -0.0695,  0.1528, -0.0089],\n        [-0.0768,  0.1645,  0.0813,  ...,  0.0280,  0.0686, -0.0330],\n        ...,\n        [-0.1537,  0.0787, -0.0260,  ..., -0.0972,  0.0474,  0.0150],\n        [-0.0558,  0.1724,  0.0855,  ...,  0.0176,  0.0808, -0.0563],\n        [-0.0231,  0.2125,  0.0721,  ...,  0.0299,  0.2542, -0.0449]],\n       grad_fn=&lt;AddmmBackward0&gt;)\n\n\n\në§¤ë²ˆ í•˜ê¸°ëŠ” ê·€ì°®ìŒ (ë‹¤ìŒê³¼ ê°™ì´ í•˜ê³  ì‹¶ìŒ)\n\nnet = torch.nn.Sequential(\n    í˜•íƒœë¥¼ ë°”ê¿”ì£¼ëŠ” ë³€í™˜,\n    torch.nn.Linear(784,32), # 28*28\n    torch.nn.ReLU(),\n    torch.nn.Linear(32,10)\n)\nloss.fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(net.parameters())\n#---#\n\nclass Flatten(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self,inp):\n        out=inp.reshape(-1,784)\n        return out\n\n\nnet = torch.nn.Sequential(\n    Flatten(),\n    torch.nn.Linear(784,32), # 28*28\n    torch.nn.ReLU(),\n    torch.nn.Linear(32,10)\n)\nloss.fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(net.parameters())\n#---#\n\n\nnet(X)\n\ntensor([[ 0.1065, -0.0595, -0.0168,  ...,  0.0957,  0.0512,  0.1637],\n        [-0.0600, -0.4660,  0.2520,  ...,  0.4221,  0.0290,  0.1224],\n        [ 0.0321, -0.2154,  0.0601,  ...,  0.2178, -0.0819,  0.0307],\n        ...,\n        [ 0.0069, -0.3970,  0.0333,  ...,  0.2753,  0.0132,  0.1855],\n        [ 0.0425, -0.2264,  0.0967,  ...,  0.2295, -0.0723,  0.0762],\n        [ 0.0436, -0.2097,  0.0193,  ...,  0.2153, -0.0889, -0.0272]],\n       grad_fn=&lt;AddmmBackward0&gt;)\n\n\nclass Flatten(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self,inp):\n        # out=inp.reshape(-1,784) # ì—¬ê¸° ë¹¼ê³  ê³ ì •\n        return out\n\nì‚¬ìš©ìê°€ ì›í•˜ëŠ” ëŒ€ë¡œ í•  ìˆ˜ëŠ” ìˆì§€ë§Œ ê·€ì°®ìŒ (torchì— ê°™ì€ ê¸°ëŠ¥ì„ ì§€ì›í•´ì¤Œ)\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Flatten(),\n    torch.nn.Linear(784,32), # 28*28\n    torch.nn.ReLU(),\n    torch.nn.Linear(32,10)\n)\nloss.fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(net.parameters())\n#---#\n\n\nnet(X)\n\ntensor([[-0.1967,  0.0710, -0.1439,  ..., -0.0351, -0.2101, -0.0983],\n        [-0.0537,  0.0729, -0.1805,  ...,  0.0139, -0.1520, -0.0359],\n        [-0.1419,  0.1204, -0.0488,  ..., -0.0232, -0.0582, -0.0687],\n        ...,\n        [-0.1134,  0.1152, -0.0716,  ...,  0.0311, -0.1294, -0.1053],\n        [-0.1655,  0.1211, -0.0752,  ..., -0.0604, -0.0637, -0.0946],\n        [-0.2413,  0.1072,  0.0042,  ..., -0.0254, -0.1170, -0.1348]],\n       grad_fn=&lt;AddmmBackward0&gt;)\n\n\n\nì§„í–‰\n\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Flatten(),\n    torch.nn.Linear(784,32),\n    torch.nn.ReLU(),\n    torch.nn.Linear(32,10)\n)\nloss.fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(net.parameters())\n#--#\n\n\nfor epoc in range(1,301):\n    #---ì—í­ì‹œì‘---#\n    # 1\n    netout = net(X)\n    # 2\n    loss = loss_fn(netout,y)\n    # 3\n    loss.backward()\n    # 4\n    optimizr.step()\n    optimizr.zero_grad()\n    #---ì—í­ë---#\n    if epoc%50==0:\n        logits = net(X).data\n        acc = (logits.argmax(axis=1) == y.argmax(axis=1)).float().mean()       \n        print(f\"# of epochs={epoc}, train_acc={acc:.4f}\")  \n\n# of epochs=50, train_acc=0.7334\n# of epochs=100, train_acc=0.8052\n# of epochs=150, train_acc=0.8295\n# of epochs=200, train_acc=0.8398\n# of epochs=250, train_acc=0.8488\n# of epochs=300, train_acc=0.8556\n\n\n\nlogits = net(XX).data\nacc = (logits.argmax(axis=1) == yy.argmax(axis=1)).float().mean()       \nprint(f\"test_acc={acc:.4f}\")\n\ntest_acc=0.8383\n\n\n\nì˜¤ë²„í”¼íŒ…ë„ ìˆëŠ” ê²ƒ ê°™ê³  í›ˆë ¨ì„ ë” í•œë‹¤ê³ í•´ì„œ í›ˆë ¨ ì •í™•ë„ê°€ ë†’ì•„ì§ˆ ê²ƒ ê°™ì§€ë„ ì•ŠìŒ (í‘œí˜„ë ¥ ìì²´ê°€ ì•ˆì¢‹ì€ ê²ƒ ê°™ìŒ)\nì´ì œ 32ë„ ëŠ˜ë ¤ë³´ê³  ì—¬ëŸ¬ê°€ì§€ ì‹œë„ë¥¼ í•˜ê³  ì‹¶ì€ë° GPUë¥¼ ì‚¬ìš©í•´ì•¼í•  ê²ƒ ê°™ìŒ\nXê°€ cudaì— ì „ë¶€ ì•ˆ ì˜¬ë¼ê°ˆ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë‹¤ìŒê³¼ ê°™ì€ ì¤€ë¹„ê³¼ì • í•„ìš”\n\n)ğŸ—£ï¸\n- Step1: ë°ì´í„°ì •ë¦¬\n\nds_train = torch.utils.data.TensorDataset(X,y)\ndl_train = torch.utils.data.DataLoader(ds_train,batch_size=256,shuffle=True)\nds_test = torch.utils.data.TensorDataset(XX,yy)\ndl_test = torch.utils.data.DataLoader(ds_test,batch_size=256)\n\n\nğŸ—£ï¸\n\ntest dataë„ GPU ë©”ëª¨ë¦¬ê°€ ê±±ì •ë˜ë©´ ë”°ë¡œ ì •ë¦¬\nbatch_sizeëŠ” GPU ë©”ëª¨ë¦¬ê°€ ê°ë‹¹í•  ìˆ˜ ìˆëŠ” ì •ë„ (ì¼ë‹¨ trainê³¼ ê°™ì€ í¬ê¸°ë¡œ í•¨)\ntestëŠ” update í•˜ì§€ë„ ì•Šê³  accuracyë§Œ ê³„ì‚°í•˜ë©´ ë˜ë¯€ë¡œ êµ³ì´ shuffle í•„ìš” X\n\n\n- Step2: í•™ìŠµì— í•„ìš”í•œ ì¤€ë¹„ (ëª¨ë¸ë§)\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Flatten(),\n    torch.nn.Linear(784,32),\n    torch.nn.ReLU(),\n    torch.nn.Linear(32,10)\n).to(\"cuda:0\")\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(net.parameters())\n#---#\n\n- Step3: ì í•©\n\nfor epoc in range(1,31):\n    net.train() # ì§€ê¸ˆì€ ì•„ë‹ˆì§€ë§Œ dropout ì‚¬ìš© ì‹œ\n    #---ì—í­ì‹œì‘---#\n    for Xm,ym in dl_train:        \n        Xm = Xm.to(\"cuda:0\")\n        ym = ym.to(\"cuda:0\")\n        # 1\n        netout = net(Xm)\n        # 2 \n        loss = loss_fn(netout,ym)\n        # 3 \n        loss.backward()\n        # 4 \n        optimizr.step()\n        optimizr.zero_grad()\n    #---ì—í­ë---#\n    if epoc % 5 == 0:\n        net.eval()\n        s =0\n        for Xm,ym in dl_train:        \n            Xm = Xm.to(\"cuda:0\")\n            ym = ym.to(\"cuda:0\")        \n            logits = net(Xm).data \n            s = s+ (logits.argmax(axis=1) == ym.argmax(axis=1)).float().sum()\n        acc = s / len(X)\n        print(f\"# of epochs = {epoc},train_acc = {acc:.4f}\") \n\n# of epochs = 5,train_acc = 0.8588\n# of epochs = 10,train_acc = 0.8659\n# of epochs = 15,train_acc = 0.8779\n# of epochs = 20,train_acc = 0.8831\n# of epochs = 25,train_acc = 0.8856\n# of epochs = 30,train_acc = 0.8876\n\n\n- Step4: ì í•©ê²°ê³¼ ì‹œê°í™” ë° ë¶„ì„\n\nnet.eval()\ns =0\nfor Xm,ym in dl_test:        \n    Xm = Xm.to(\"cuda:0\")\n    ym = ym.to(\"cuda:0\")        \n    logits = net(Xm).data \n    s = s+ (logits.argmax(axis=1) == ym.argmax(axis=1)).float().sum()\nacc = s / len(XX)\nprint(f\"test_acc = {acc:.4f}\") \n\ntest_acc = 0.8638\n\n\nğŸ—£ï¸ ê°œì„ ì´ í•„ìš”í•´ë³´ì„"
  },
  {
    "objectID": "posts/06wk-2.html#c.-ì•½ê°„-ë”-ë³µì¡í•œ-ì‹ ê²½ë§",
    "href": "posts/06wk-2.html#c.-ì•½ê°„-ë”-ë³µì¡í•œ-ì‹ ê²½ë§",
    "title": "06wk-2: (ì‹ ê²½ë§) â€“ ë‹¤í•­ë¶„ë¥˜, FashionMNIST",
    "section": "C. ì•½ê°„ ë” ë³µì¡í•œ ì‹ ê²½ë§",
    "text": "C. ì•½ê°„ ë” ë³µì¡í•œ ì‹ ê²½ë§\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Flatten(),\n    torch.nn.Linear(784,256),\n    torch.nn.ReLU(),\n    torch.nn.Linear(256,10)\n).to(\"cuda:0\")\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(net.parameters())\n#---#\n\n\nfor epoc in range(1,31):\n    net.train()\n    #---ì—í­ì‹œì‘---#\n    for Xm,ym in dl_train:        \n        Xm = Xm.to(\"cuda:0\")\n        ym = ym.to(\"cuda:0\")\n        # 1\n        netout = net(Xm)\n        # 2 \n        loss = loss_fn(netout,ym)\n        # 3 \n        loss.backward()\n        # 4 \n        optimizr.step()\n        optimizr.zero_grad()\n    #---ì—í­ë---#\n    if epoc % 5 == 0:\n        net.eval()\n        s =0\n        for Xm,ym in dl_train:        \n            Xm = Xm.to(\"cuda:0\")\n            ym = ym.to(\"cuda:0\")        \n            logits = net(Xm).data \n            s = s+ (logits.argmax(axis=1) == ym.argmax(axis=1)).float().sum()\n        acc = s / len(X)\n        print(f\"# of epochs = {epoc},train_acc = {acc:.4f}\") \n\n# of epochs = 5,train_acc = 0.8843\n# of epochs = 10,train_acc = 0.9020\n# of epochs = 15,train_acc = 0.9176\n# of epochs = 20,train_acc = 0.9265\n# of epochs = 25,train_acc = 0.9345\n# of epochs = 30,train_acc = 0.9388\n\n\n\nnet.eval()\ns =0\nfor Xm,ym in dl_test:        \n    Xm = Xm.to(\"cuda:0\")\n    ym = ym.to(\"cuda:0\")        \n    logits = net(Xm).data \n    s = s+ (logits.argmax(axis=1) == ym.argmax(axis=1)).float().sum()\nacc = s / len(XX)\nprint(f\"test_acc = {acc:.4f}\") \n\ntest_acc = 0.8892\n\n\nğŸ—£ï¸ ì˜¤ë²„í”¼íŒ…ì¸ ê²ƒ ê°™ê³  ë§Œì¡±ìŠ¤ëŸ½ì§€ë„ ì•ŠìŒ"
  },
  {
    "objectID": "posts/06wk-2.html#d.-ë°œì•…",
    "href": "posts/06wk-2.html#d.-ë°œì•…",
    "title": "06wk-2: (ì‹ ê²½ë§) â€“ ë‹¤í•­ë¶„ë¥˜, FashionMNIST",
    "section": "D. ë°œì•…",
    "text": "D. ë°œì•…\n- ë…¸ë“œë¥¼ ë§ì´..\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Flatten(),\n    torch.nn.Linear(784,4096),\n    torch.nn.Dropout(0.5),\n    torch.nn.ReLU(),\n    torch.nn.Linear(4096,10)\n).to(\"cuda:0\")\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(net.parameters())\n#---#\n\n\nfor epoc in range(1,31):\n    net.train()\n    #---ì—í­ì‹œì‘---#\n    for Xm,ym in dl_train:        \n        Xm = Xm.to(\"cuda:0\")\n        ym = ym.to(\"cuda:0\")\n        # 1\n        netout = net(Xm)\n        # 2 \n        loss = loss_fn(netout,ym)\n        # 3 \n        loss.backward()\n        # 4 \n        optimizr.step()\n        optimizr.zero_grad()\n    #---ì—í­ë---#\n    if epoc % 5 == 0:\n        net.eval()\n        s =0\n        for Xm,ym in dl_train:        \n            Xm = Xm.to(\"cuda:0\")\n            ym = ym.to(\"cuda:0\")        \n            logits = net(Xm).data \n            s = s+ (logits.argmax(axis=1) == ym.argmax(axis=1)).float().sum()\n        acc = s / len(X)\n        print(f\"# of epochs = {epoc},train_acc = {acc:.4f}\") \n\n# of epochs = 5,train_acc = 0.8870\n# of epochs = 10,train_acc = 0.9024\n# of epochs = 15,train_acc = 0.9116\n# of epochs = 20,train_acc = 0.9214\n# of epochs = 25,train_acc = 0.9302\n# of epochs = 30,train_acc = 0.9307\n\n\n\nnet.eval()\ns =0\nfor Xm,ym in dl_test:        \n    Xm = Xm.to(\"cuda:0\")\n    ym = ym.to(\"cuda:0\")        \n    logits = net(Xm).data \n    s = s+ (logits.argmax(axis=1) == ym.argmax(axis=1)).float().sum()\nacc = s / len(XX)\nprint(f\"test_acc = {acc:.4f}\") \n\ntest_acc = 0.8913\n\n\nğŸ—£ï¸ í¬ê²Œ ì˜ë¯¸ ì—†ì–´ ë³´ì„\n- ë ˆì´ì–´ë¥¼ ë§ì´..\nğŸ—£ï¸ 256ì´ ê´œì°®ì€ ê²ƒ ê°™ì•„ì„œ ë§ì´ ë°˜ë³µí•´ë´„\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Flatten(),\n    torch.nn.Linear(784,256),\n    torch.nn.ReLU(),\n    torch.nn.Linear(256,256),\n    torch.nn.ReLU(),\n    torch.nn.Linear(256,256),\n    torch.nn.ReLU(), \n    torch.nn.Linear(256,256),\n    torch.nn.ReLU(),    \n    torch.nn.Linear(256,10)\n).to(\"cuda:0\")\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(net.parameters())\n#---#\n\n\nfor epoc in range(1,31):\n    net.train()\n    #---ì—í­ì‹œì‘---#\n    for Xm,ym in dl_train:        \n        Xm = Xm.to(\"cuda:0\")\n        ym = ym.to(\"cuda:0\")\n        # 1\n        netout = net(Xm)\n        # 2 \n        loss = loss_fn(netout,ym)\n        # 3 \n        loss.backward()\n        # 4 \n        optimizr.step()\n        optimizr.zero_grad()\n    #---ì—í­ë---#\n    if epoc % 5 == 0:\n        net.eval()\n        s =0\n        for Xm,ym in dl_train:        \n            Xm = Xm.to(\"cuda:0\")\n            ym = ym.to(\"cuda:0\")        \n            logits = net(Xm).data \n            s = s+ (logits.argmax(axis=1) == ym.argmax(axis=1)).float().sum()\n        acc = s / len(X)\n        print(f\"# of epochs = {epoc},train_acc = {acc:.4f}\") \n\n# of epochs = 5,train_acc = 0.8881\n# of epochs = 10,train_acc = 0.9154\n# of epochs = 15,train_acc = 0.9240\n# of epochs = 20,train_acc = 0.9303\n# of epochs = 25,train_acc = 0.9511\n# of epochs = 30,train_acc = 0.9547\n\n\n\nnet.eval()\ns =0\nfor Xm,ym in dl_test:        \n    Xm = Xm.to(\"cuda:0\")\n    ym = ym.to(\"cuda:0\")        \n    logits = net(Xm).data \n    s = s+ (logits.argmax(axis=1) == ym.argmax(axis=1)).float().sum()\nacc = s / len(XX)\nprint(f\"test_acc = {acc:.4f}\") \n\ntest_acc = 0.8907\n\n\n\ntest_acc 90% ë„˜ê¸°ëŠ”ê²Œ ì—„ì²­ í˜ë“¤ë‹¤"
  },
  {
    "objectID": "posts/06wk-2.html#f.-í•©ì„±ê³±ì‹ ê²½ë§",
    "href": "posts/06wk-2.html#f.-í•©ì„±ê³±ì‹ ê²½ë§",
    "title": "06wk-2: (ì‹ ê²½ë§) â€“ ë‹¤í•­ë¶„ë¥˜, FashionMNIST",
    "section": "F. í•©ì„±ê³±ì‹ ê²½ë§",
    "text": "F. í•©ì„±ê³±ì‹ ê²½ë§\n- https://brunch.co.kr/@hvnpoet/109\nğŸ—£ï¸(\n\nì„ í˜•ë³€í™˜ + ë¹„ì„ í˜•ë³€í™˜\nLinear transform ëŒ€ì‹  Convolution2d (ê·¸ëƒ¥ 2d ë²„ì „ì˜ ì„ í˜•ë³€í™˜ì´ë¼ê³  ì¼ë‹¨ ìƒê°)\ninput channel: í‘ë°± 1ê°œ, ì»¬ëŸ¬ 3ê°œ\noutput channel: inputì„ ëª‡ê°œë¡œ ë³€í™˜í• ì§€\në¹„ì„ í˜•ë³€í™˜ì€ ReLUë¡œ ì§„í–‰\nkernel_size: ë‹¤ìŒì— ì„¤ëª… (ìœˆë„ìš° í¬ê¸°)\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(in_channels=1, out_channels=64, kernel_size=5),\n    torch.nn.ReLU(),\n)\n\n\nX.shape\n\ntorch.Size([60000, 1, 28, 28])\n\n\n\nnet(X).shape\n\ntorch.Size([60000, 64, 24, 24])\n\n\n\nìˆ«ìê°€ ì´ëŸ° ì‹ìœ¼ë¡œ ë³€í•¨\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(in_channels=1, out_channels=64, kernel_size=5),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d(kernel_size=2)\n)\n\n\nX.shape\n\ntorch.Size([60000, 1, 28, 28])\n\n\n\nnet(X).shape\n\ntorch.Size([60000, 64, 12, 12])\n\n\n\nì´ ìƒíƒœì—ì„œ Flatten()\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(in_channels=1, out_channels=64, kernel_size=5),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d(kernel_size=2),\n    torch.nn.Flatten()\n)\n\n\nX.shape\n\ntorch.Size([60000, 1, 28, 28])\n\n\n\nnet(X).shape\n\ntorch.Size([60000, 9216])\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(in_channels=1, out_channels=64, kernel_size=5),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d(kernel_size=2),\n    torch.nn.Flatten(),\n    torch.nn.Linear(9216,10) # ì´ ì´í›„ëŠ” softmax function\n)\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(in_channels=1 ,out_channels=64,kernel_size=5),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d(kernel_size=2),    \n    torch.nn.Flatten(),\n    torch.nn.Linear(9216,10)\n).to(\"cuda:0\")\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(net.parameters())\n#---#\n\nì¸µì„ í•œ ë²ˆ ë” ë°˜ë³µ\n\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(in_channels=1 ,out_channels=64,kernel_size=5),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d(kernel_size=2),\n    torch.nn.Conv2d(in_channels=64 ,out_channels=64,kernel_size=5), # 64\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d(kernel_size=2),    \n    torch.nn.Flatten(),\n    torch.nn.Linear(9216,10)\n).to(\"cuda:0\")\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(net.parameters())\n#---#\n\n\n# net(X.to(\"cuda:0\")) # error\n\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (60000x1024 and 9216x10)\n\n)ğŸ—£ï¸\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(in_channels=1 ,out_channels=64,kernel_size=5),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d(kernel_size=2),\n    torch.nn.Conv2d(in_channels=64 ,out_channels=64,kernel_size=5),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d(kernel_size=2),    \n    torch.nn.Flatten(),\n    torch.nn.Linear(1024,10)\n).to(\"cuda:0\")\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(net.parameters())\n#---#\n\n\nğŸ—£ï¸\n\nì˜ˆì „ì—ëŠ” Flattenì„ í•˜ê³  ì‹œì‘\nì´ë²ˆì—ëŠ” Flatten ì „ì— ì´ë¯¸ì§€ ìì²´ë¥¼ ì²˜ë¦¬í•˜ëŠ” ê³¼ì •ì„ ê±°ì³¤ë”ë‹ˆ\n(Flatten ë‹¤ìŒì€ ì°¨ì›ë§Œ ë§ì¶¤ - ì„ í˜•ë³€í™˜)\nê²°ê³¼ê°€ ì˜ ë‚˜ì˜´\n\n\n\nfor epoc in range(1,31):\n    net.train()\n    #---ì—í­ì‹œì‘---#\n    for Xm,ym in dl_train:        \n        Xm = Xm.to(\"cuda:0\")\n        ym = ym.to(\"cuda:0\")\n        # 1\n        netout = net(Xm)\n        # 2 \n        loss = loss_fn(netout,ym)\n        # 3 \n        loss.backward()\n        # 4 \n        optimizr.step()\n        optimizr.zero_grad()\n    #---ì—í­ë---#\n    if epoc % 5 == 0:\n        net.eval()\n        s =0\n        for Xm,ym in dl_train:        \n            Xm = Xm.to(\"cuda:0\")\n            ym = ym.to(\"cuda:0\")        \n            logits = net(Xm).data \n            s = s+ (logits.argmax(axis=1) == ym.argmax(axis=1)).float().sum()\n        acc = s / len(X)\n        print(f\"# of epochs = {epoc},train_acc = {acc:.4f}\") \n\n# of epochs = 5,train_acc = 0.9065\n# of epochs = 10,train_acc = 0.9326\n# of epochs = 15,train_acc = 0.9438\n# of epochs = 20,train_acc = 0.9552\n# of epochs = 25,train_acc = 0.9689\n# of epochs = 30,train_acc = 0.9756\n\n\n\nğŸ—£ï¸ í‘œí˜„ë ¥ ìì²´ëŠ” ì¢‹ìŒ\nğŸ—£ï¸ ì˜¤ë²„í”¼íŒ… ê°ì•ˆí•˜ë”ë¼ë„ ì •í™•ë„ê°€ ì˜¬ë¼ê° (90% ë„˜ìŒ)\n\n\nnet.eval()\ns =0\nfor Xm,ym in dl_test:        \n    Xm = Xm.to(\"cuda:0\")\n    ym = ym.to(\"cuda:0\")        \n    logits = net(Xm).data \n    s = s+ (logits.argmax(axis=1) == ym.argmax(axis=1)).float().sum()\nacc = s / len(XX)\nprint(f\"test_acc = {acc:.4f}\") \n\ntest_acc = 0.9151\n\n\n\n\n\n\n\n\nNote\n\n\n\në„¤íŠ¸ì›Œí¬ë¥¼ ì•„ë˜ì™€ ê°™ì´ ì„¤ì •í–ˆë”ë‹ˆ\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(in_channels=1 ,out_channels=64,kernel_size=5),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d(kernel_size=2),\n    torch.nn.Conv2d(in_channels=64 ,out_channels=64,kernel_size=5),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d(kernel_size=2),    \n    torch.nn.Flatten(),\n    torch.nn.Linear(1024,10)\n)\nê²°ê³¼ê°€ ì¢‹ë„¤? ì •ë„ë§Œ ì•Œë©´ë©ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/06wk-2.html#footnotes",
    "href": "posts/06wk-2.html#footnotes",
    "title": "06wk-2: (ì‹ ê²½ë§) â€“ ë‹¤í•­ë¶„ë¥˜, FashionMNIST",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nprob=sig(logit)â†©ï¸\nprobs=soft(logits)â†©ï¸"
  },
  {
    "objectID": "posts/01wk-2.html",
    "href": "posts/01wk-2.html",
    "title": "01wk-2, 02wk-1: (íšŒê·€) â€“ íšŒê·€ëª¨í˜•, ì†ì‹¤í•¨ìˆ˜, íŒŒì´í† ì¹˜ë¥¼ ì´ìš©í•œ ì¶”ì •",
    "section": "",
    "text": "ğŸ“˜ Note Format Guide\nThis format serves as a structured guide for organizing lecture content, personal interpretation, experiments, and study-related questions.\nğŸ“ ğŸ—£ï¸ âœï¸ ğŸ”¬ â“"
  },
  {
    "objectID": "posts/01wk-2.html#a.-ì•„ì´ìŠ¤-ì•„ë©”ë¦¬ì¹´ë…¸-ê°€ì§œìë£Œ",
    "href": "posts/01wk-2.html#a.-ì•„ì´ìŠ¤-ì•„ë©”ë¦¬ì¹´ë…¸-ê°€ì§œìë£Œ",
    "title": "01wk-2, 02wk-1: (íšŒê·€) â€“ íšŒê·€ëª¨í˜•, ì†ì‹¤í•¨ìˆ˜, íŒŒì´í† ì¹˜ë¥¼ ì´ìš©í•œ ì¶”ì •",
    "section": "A. ì•„ì´ìŠ¤ ì•„ë©”ë¦¬ì¹´ë…¸ (ê°€ì§œìë£Œ)",
    "text": "A. ì•„ì´ìŠ¤ ì•„ë©”ë¦¬ì¹´ë…¸ (ê°€ì§œìë£Œ)\n- ì¹´í˜ì£¼ì¸ì¸ ë°•í˜œì›ì”¨ëŠ” ì˜¨ë„ì™€ ì•„ì´ìŠ¤ì•„ë©”ë¦¬ì¹´ë…¸ íŒë§¤ëŸ‰ì´ ê´€ê³„ê°€ ìˆë‹¤ëŠ” ê²ƒì„ ì•Œì•˜ë‹¤. êµ¬ì²´ì ìœ¼ë¡œëŠ”\n\nâ€œì˜¨ë„ê°€ ë†’ì•„ì§ˆ ìˆ˜ë¡ (=ë‚ ì”¨ê°€ ë”ìš¸ìˆ˜ë¡) ì•„ì´ìŠ¤ì•„ë©”ë¦¬ì¹´ë…¸ì˜ íŒë§¤ëŸ‰ì´ ì¦ê°€â€\n\ní•œë‹¤ëŠ” ì‚¬ì‹¤ì„ ì•Œê²Œ ë˜ì—ˆë‹¤. ì´ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•´ì„œ ì•„ë˜ì™€ ê°™ì´ 100ê°œì˜ ë°ì´í„°ë¥¼ ëª¨ì•˜ë‹¤.\n\ntemp = [-2.4821, -2.3621, -1.9973, -1.6239, -1.4792, -1.4635, -1.4509, -1.4435,\n        -1.3722, -1.3079, -1.1904, -1.1092, -1.1054, -1.0875, -0.9469, -0.9319,\n        -0.8643, -0.7858, -0.7549, -0.7421, -0.6948, -0.6103, -0.5830, -0.5621,\n        -0.5506, -0.5058, -0.4806, -0.4738, -0.4710, -0.4676, -0.3874, -0.3719,\n        -0.3688, -0.3159, -0.2775, -0.2772, -0.2734, -0.2721, -0.2668, -0.2155,\n        -0.2000, -0.1816, -0.1708, -0.1565, -0.1448, -0.1361, -0.1057, -0.0603,\n        -0.0559, -0.0214,  0.0655,  0.0684,  0.1195,  0.1420,  0.1521,  0.1568,\n         0.2646,  0.2656,  0.3157,  0.3220,  0.3461,  0.3984,  0.4190,  0.5443,\n         0.5579,  0.5913,  0.6148,  0.6469,  0.6469,  0.6523,  0.6674,  0.7059,\n         0.7141,  0.7822,  0.8154,  0.8668,  0.9291,  0.9804,  0.9853,  0.9941,\n         1.0376,  1.0393,  1.0697,  1.1024,  1.1126,  1.1532,  1.2289,  1.3403,\n         1.3494,  1.4279,  1.4994,  1.5031,  1.5437,  1.6789,  2.0832,  2.2444,\n         2.3935,  2.6056,  2.6057,  2.6632]\n\n\nsales= [-8.5420, -6.5767, -5.9496, -4.4794, -4.2516, -3.1326, -4.0239, -4.1862,\n        -3.3403, -2.2027, -2.0262, -2.5619, -1.3353, -2.0466, -0.4664, -1.3513,\n        -1.6472, -0.1089, -0.3071, -0.6299, -0.0438,  0.4163,  0.4166, -0.0943,\n         0.2662,  0.4591,  0.8905,  0.8998,  0.6314,  1.3845,  0.8085,  1.2594,\n         1.1211,  1.9232,  1.0619,  1.3552,  2.1161,  1.1437,  1.6245,  1.7639,\n         1.6022,  1.7465,  0.9830,  1.7824,  2.1116,  2.8621,  2.1165,  1.5226,\n         2.5572,  2.8361,  3.3956,  2.0679,  2.8140,  3.4852,  3.6059,  2.5966,\n         2.8854,  3.9173,  3.6527,  4.1029,  4.3125,  3.4026,  3.2180,  4.5686,\n         4.3772,  4.3075,  4.4895,  4.4827,  5.3170,  5.4987,  5.4632,  6.0328,\n         5.2842,  5.0539,  5.4538,  6.0337,  5.7250,  5.7587,  6.2020,  6.5992,\n         6.4621,  6.5140,  6.6846,  7.3497,  8.0909,  7.0794,  6.8667,  7.4229,\n         7.2544,  7.1967,  9.5006,  9.0339,  7.4887,  9.0759, 11.0946, 10.3260,\n        12.2665, 13.0983, 12.5468, 13.8340]\n\nğŸ—£ï¸ ìŒìˆ˜ íŒë§¤ëŸ‰ì€ ì¼ë‹¨ ë¬´ì‹œ\nì—¬ê¸°ì—ì„œ tempëŠ” í‰ê· ê¸°ì˜¨ì´ê³ , salesëŠ” ì•„ì´ìŠ¤ì•„ë©”ë¦¬ì¹´ë…¸ íŒë§¤ëŸ‰ì´ë‹¤. í‰ê· ê¸°ì˜¨ê³¼ íŒë§¤ëŸ‰ì˜ ê·¸ë˜í”„ë¥¼ ê·¸ë ¤ë³´ë©´ ì•„ë˜ì™€ ê°™ë‹¤.\n\nplt.plot(temp,sales,'o')\n\n\n\n\n\n\n\n\nğŸ—£ï¸ ì•½ê°„ì˜ ì˜¤ì°¨ëŠ” ìˆì§€ë§Œ ì„ ìœ¼ë¡œ ë³´ì„\nì˜¤ëŠ˜ ë°”ê¹¥ì˜ ì˜¨ë„ëŠ” 0.5ë„ ì´ë‹¤. ì•„ì´ìŠ¤ ì•„ë©”ë¼ì¹´ë…¸ë¥¼ ëª‡ì”ì •ë„ ë§Œë“¤ì–´ ë‘ë©´ ì¢‹ì„ê¹Œ?\nğŸ—£ï¸ ì´ ê·¸ë˜í”„ë¥¼ ë³´ê³  4.5ì” ì •ë„ë¡œ ì§ì‘ ê°€ëŠ¥"
  },
  {
    "objectID": "posts/01wk-2.html#b.-ê°€ì§œìë£Œë¥¼-ë§Œë“ -ë°©ë²•",
    "href": "posts/01wk-2.html#b.-ê°€ì§œìë£Œë¥¼-ë§Œë“ -ë°©ë²•",
    "title": "01wk-2, 02wk-1: (íšŒê·€) â€“ íšŒê·€ëª¨í˜•, ì†ì‹¤í•¨ìˆ˜, íŒŒì´í† ì¹˜ë¥¼ ì´ìš©í•œ ì¶”ì •",
    "section": "B. ê°€ì§œìë£Œë¥¼ ë§Œë“  ë°©ë²•",
    "text": "B. ê°€ì§œìë£Œë¥¼ ë§Œë“  ë°©ë²•\n- ë°©ë²•1: \\(y_i= w_0+w_1 x_i +\\epsilon_i = 2.5 + 4x_i +\\epsilon_i, \\quad i=1,2,\\dots,n\\)\nğŸ—£ï¸(\nxi = ì˜¨ë„ = temp\nyi = íŒë§¤ëŸ‰ = sales\níŒë§¤ëŸ‰ = 2.5 + 4*ì˜¨ë„ + ì˜¤ì°¨\n\ntorch.randn(10) # í‘œì¤€ì •ê·œë¶„í¬ì—ì„œ 10ê°œ ê°’ ì¶”ì¶œ, ê¸¸ì´ê°€ 10ì¸ vector (column vectorì¸ì§€ row vectorì¸ì§€ëŠ” ëª¨ë¦„)\n\ntensor([-0.4351, -0.4066,  1.2577, -1.1443,  0.3941, -0.2229, -0.4337,  0.8736,\n         0.6216,  1.0963])\n\n\n\ntorch.randn(100).sort() # 100ê°œ ê°’ì„ ì •ë ¬ / ì•ì€ ì •ë ¬ëœ ê°’, ë’¤ëŠ” ì¸ë±ìŠ¤\n\ntorch.return_types.sort(\nvalues=tensor([-3.3450e+00, -2.3363e+00, -1.7533e+00, -1.6534e+00, -1.4996e+00,\n        -1.4218e+00, -1.3757e+00, -1.3314e+00, -1.1898e+00, -1.1594e+00,\n        -1.1386e+00, -1.0975e+00, -1.0961e+00, -1.0899e+00, -1.0250e+00,\n        -9.7851e-01, -9.1254e-01, -8.8307e-01, -8.7845e-01, -8.4915e-01,\n        -7.4344e-01, -7.0972e-01, -7.0845e-01, -6.8746e-01, -6.7488e-01,\n        -6.6512e-01, -6.0503e-01, -5.8921e-01, -5.4838e-01, -5.1363e-01,\n        -5.0996e-01, -4.7537e-01, -4.3955e-01, -3.5707e-01, -3.4237e-01,\n        -3.4013e-01, -3.2890e-01, -3.2078e-01, -3.0216e-01, -2.9112e-01,\n        -2.8083e-01, -2.4387e-01, -2.4171e-01, -2.0109e-01, -1.9779e-01,\n        -1.9549e-01, -5.8397e-02, -2.5842e-02, -2.2056e-02,  2.0055e-03,\n         1.0348e-02,  2.2201e-02,  2.5445e-02,  2.6868e-02,  6.2116e-02,\n         1.3408e-01,  1.5172e-01,  2.0091e-01,  2.3218e-01,  2.5000e-01,\n         2.7442e-01,  2.8144e-01,  3.4857e-01,  3.7494e-01,  4.4520e-01,\n         4.8013e-01,  4.9466e-01,  5.0311e-01,  5.7595e-01,  6.2995e-01,\n         6.3221e-01,  6.5666e-01,  6.5788e-01,  6.6027e-01,  6.7909e-01,\n         7.1635e-01,  7.1752e-01,  7.2141e-01,  8.0059e-01,  8.0419e-01,\n         8.0801e-01,  8.1830e-01,  8.9444e-01,  9.6222e-01,  9.9973e-01,\n         1.1303e+00,  1.1527e+00,  1.2046e+00,  1.2086e+00,  1.2469e+00,\n         1.2752e+00,  1.2872e+00,  1.3125e+00,  1.4296e+00,  1.4390e+00,\n         1.5448e+00,  1.6129e+00,  1.6454e+00,  1.6769e+00,  1.7580e+00]),\nindices=tensor([81, 19, 56, 18, 89, 54, 27, 31, 65, 85, 94, 47,  0,  7,  8, 57, 14, 92,\n         3, 12, 86, 48,  9, 82, 62, 78,  1, 28, 32, 67, 21, 53, 10, 30, 23,  5,\n        88, 24, 63, 40, 20, 77, 34, 87, 99, 80, 41,  4, 69, 90, 35, 72, 58, 11,\n        22, 42, 76, 95, 74, 38, 46, 59, 91, 68, 43, 44, 50, 96, 51,  6, 29, 13,\n        66, 49, 73,  2, 70, 93, 97, 16, 15, 98, 55, 33, 39, 84, 25, 61, 17, 64,\n        45, 26, 75, 71, 79, 37, 60, 83, 36, 52]))\n\n\n\na = torch.randn(100).sort()\ntype(a)\n\ntorch.return_types.sort\n\n\n\na[0]\n\ntensor([-2.8188e+00, -2.7746e+00, -2.5355e+00, -2.4374e+00, -2.2716e+00,\n        -2.1492e+00, -1.8555e+00, -1.8281e+00, -1.6228e+00, -1.6164e+00,\n        -1.5151e+00, -1.5046e+00, -1.4989e+00, -1.4708e+00, -1.4605e+00,\n        -1.3748e+00, -1.3521e+00, -1.3183e+00, -1.2710e+00, -1.2416e+00,\n        -1.1459e+00, -1.0949e+00, -1.0907e+00, -1.0903e+00, -1.0481e+00,\n        -1.0313e+00, -1.0079e+00, -1.0003e+00, -9.9874e-01, -9.9081e-01,\n        -9.8943e-01, -9.7448e-01, -9.4772e-01, -9.4282e-01, -9.1282e-01,\n        -8.8605e-01, -8.6893e-01, -8.5283e-01, -7.8566e-01, -7.7867e-01,\n        -7.6961e-01, -7.4827e-01, -6.6928e-01, -6.3990e-01, -5.9842e-01,\n        -5.8057e-01, -5.5388e-01, -5.1941e-01, -5.1005e-01, -4.9040e-01,\n        -4.7796e-01, -3.9862e-01, -3.9854e-01, -3.8835e-01, -3.7719e-01,\n        -3.6587e-01, -3.0923e-01, -3.0278e-01, -2.5337e-01, -2.1358e-01,\n        -1.7441e-01, -1.4875e-01, -5.6163e-02, -3.3250e-02, -2.6646e-02,\n         2.1082e-03,  1.3442e-02,  9.5665e-02,  1.0434e-01,  1.2852e-01,\n         1.8255e-01,  2.2326e-01,  2.3160e-01,  2.5853e-01,  2.6803e-01,\n         3.3640e-01,  3.6288e-01,  3.7120e-01,  3.8451e-01,  4.0117e-01,\n         4.3763e-01,  4.5193e-01,  5.2404e-01,  6.1333e-01,  6.7461e-01,\n         6.8081e-01,  8.0477e-01,  9.1538e-01,  9.5395e-01,  1.0907e+00,\n         1.1139e+00,  1.1281e+00,  1.2559e+00,  1.2686e+00,  1.3258e+00,\n         1.3563e+00,  1.3864e+00,  1.5558e+00,  1.6258e+00,  2.1654e+00])\n\n\n\nx,_ = torch.randn(100).sort() # ì–¸íŒ¨í‚¹\nx\n\ntensor([-2.8984, -2.6607, -2.2449, -2.2072, -2.1918, -2.1538, -1.9428, -1.9416,\n        -1.8612, -1.6956, -1.6357, -1.4785, -1.4322, -1.2127, -1.1737, -0.9456,\n        -0.9244, -0.8456, -0.8190, -0.7925, -0.7609, -0.7305, -0.7011, -0.6806,\n        -0.6442, -0.6117, -0.6059, -0.5994, -0.4920, -0.4066, -0.3879, -0.3867,\n        -0.3612, -0.3604, -0.3142, -0.3112, -0.2940, -0.2812, -0.2753, -0.2665,\n        -0.2145, -0.2106, -0.1864, -0.1633, -0.1470, -0.1331, -0.1316, -0.0994,\n        -0.0954, -0.0717, -0.0586, -0.0329,  0.0095,  0.0182,  0.0214,  0.0915,\n         0.0952,  0.1077,  0.1124,  0.1612,  0.1614,  0.1969,  0.2003,  0.3242,\n         0.3424,  0.3925,  0.4078,  0.4468,  0.4536,  0.5199,  0.5238,  0.5563,\n         0.5595,  0.6236,  0.6372,  0.6451,  0.6630,  0.7122,  0.7335,  0.7569,\n         0.7589,  0.8969,  0.9318,  0.9552,  1.0023,  1.0198,  1.1083,  1.1978,\n         1.2752,  1.2928,  1.3265,  1.3825,  1.4325,  1.5292,  1.6095,  1.6239,\n         1.7316,  2.0886,  2.3070,  3.2682])\n\n\n\ntorch.manual_seed(43052) # ê°’ ê³ ì •\nx,_ = torch.randn(100).sort()\nx\n\ntensor([-2.4821, -2.3621, -1.9973, -1.6239, -1.4792, -1.4635, -1.4509, -1.4435,\n        -1.3722, -1.3079, -1.1904, -1.1092, -1.1054, -1.0875, -0.9469, -0.9319,\n        -0.8643, -0.7858, -0.7549, -0.7421, -0.6948, -0.6103, -0.5830, -0.5621,\n        -0.5506, -0.5058, -0.4806, -0.4738, -0.4710, -0.4676, -0.3874, -0.3719,\n        -0.3688, -0.3159, -0.2775, -0.2772, -0.2734, -0.2721, -0.2668, -0.2155,\n        -0.2000, -0.1816, -0.1708, -0.1565, -0.1448, -0.1361, -0.1057, -0.0603,\n        -0.0559, -0.0214,  0.0655,  0.0684,  0.1195,  0.1420,  0.1521,  0.1568,\n         0.2646,  0.2656,  0.3157,  0.3220,  0.3461,  0.3984,  0.4190,  0.5443,\n         0.5579,  0.5913,  0.6148,  0.6469,  0.6469,  0.6523,  0.6674,  0.7059,\n         0.7141,  0.7822,  0.8154,  0.8668,  0.9291,  0.9804,  0.9853,  0.9941,\n         1.0376,  1.0393,  1.0697,  1.1024,  1.1126,  1.1532,  1.2289,  1.3403,\n         1.3494,  1.4279,  1.4994,  1.5031,  1.5437,  1.6789,  2.0832,  2.2444,\n         2.3935,  2.6056,  2.6057,  2.6632])\n\n\n\n# temp # ìœ„ì˜ tempì™€ xëŠ” ë™ì¼\n\n\nsales[0] # -2.4821 * 4 + 2.5 + ì˜¤ì°¨\n\n-8.542\n\n\n\ntorch.manual_seed(43052)\nx,_ = torch.randn(100).sort()\neps = torch.randn(100)*0.5 # ì˜¤ì°¨ ë§Œë“¤ê¸° (ë¶„ì‚° ì‘ê²Œí•˜ë ¤ê³  0.5ë¥¼ ê³±í•¨)\n\n\n-2.4821 * 4 + 2.5 + eps[0] # sales[0]ê³¼ ë™ì¼\n\ntensor(-8.5420)\n\n\n\nx[1] * 4 + 2.5 + eps[1] # ë‘ ë²ˆì§¸ ê°’\n\ntensor(-6.5767)\n\n\n\nsales[1]\n\n-6.5767\n\n\n\ntorch.manual_seed(43052)\nx,_ = torch.randn(100).sort()\neps = torch.randn(100)*0.5\ny = x * 4 + 2.5 + eps # ë¸Œë¡œë“œìºìŠ¤íŒ… ì´ìš©\n\n\ntemp[:5],sales[:5]\n\n([-2.4821, -2.3621, -1.9973, -1.6239, -1.4792],\n [-8.542, -6.5767, -5.9496, -4.4794, -4.2516])\n\n\n\nx[:5], y[:5] # ìœ„ì™€ ë™ì¼\n\n(tensor([-2.4821, -2.3621, -1.9973, -1.6239, -1.4792]),\n tensor([-8.5420, -6.5767, -5.9496, -4.4794, -4.2516]))\n\n\n)ğŸ—£ï¸\n\ntorch.manual_seed(43052)\nx,_ = torch.randn(100).sort()\neps = torch.randn(100)*0.5\ny = x * 4 + 2.5 + eps\n\n\nx[:5], y[:5]\n\n(tensor([-2.4821, -2.3621, -1.9973, -1.6239, -1.4792]),\n tensor([-8.5420, -6.5767, -5.9496, -4.4794, -4.2516]))\n\n\n- ë°©ë²•2: \\({\\bf y}={\\bf X}{\\bf W} +\\boldsymbol{\\epsilon}\\)\n\n\\({\\bf y}=\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\dots \\\\ y_n\\end{bmatrix}, \\quad {\\bf X}=\\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\dots \\\\ 1 & x_n\\end{bmatrix}, \\quad {\\bf W}=\\begin{bmatrix} 2.5 \\\\ 4 \\end{bmatrix}, \\quad \\boldsymbol{\\epsilon}= \\begin{bmatrix} \\epsilon_1 \\\\ \\dots \\\\ \\epsilon_n\\end{bmatrix}\\)\n\nğŸ—£ï¸(\n\\(y_1 = 2.5 + 4x_1 + \\epsilon_1\\)\n\\(y_2 = 2.5 + 4x_2 + \\epsilon_2\\)\n\\(y_3 = 2.5 + 4x_3 + \\epsilon_3\\) â€¦ ì„ ìœ„ì™€ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆìŒ\në°©ë²•1ì€ scalarë¡œ í‘œí˜„, ë°©ë²•2ëŠ” matrixë¡œ í‘œí˜„\n\ny # ê¸¸ì´ê°€ 100ì¸ vector (ë°©ë²•1) / ë°©ë²•2ëŠ” (100,1) matrixë¡œ í‘œí˜„ë˜ì–´ì•¼ í•¨\n\ntensor([-8.5420, -6.5767, -5.9496, -4.4794, -4.2516, -3.1326, -4.0239, -4.1862,\n        -3.3403, -2.2027, -2.0262, -2.5619, -1.3353, -2.0466, -0.4664, -1.3513,\n        -1.6472, -0.1089, -0.3071, -0.6299, -0.0438,  0.4163,  0.4166, -0.0943,\n         0.2662,  0.4591,  0.8905,  0.8998,  0.6314,  1.3845,  0.8085,  1.2594,\n         1.1211,  1.9232,  1.0619,  1.3552,  2.1161,  1.1437,  1.6245,  1.7639,\n         1.6022,  1.7465,  0.9830,  1.7824,  2.1116,  2.8621,  2.1165,  1.5226,\n         2.5572,  2.8361,  3.3956,  2.0679,  2.8140,  3.4852,  3.6059,  2.5966,\n         2.8854,  3.9173,  3.6527,  4.1029,  4.3125,  3.4026,  3.2180,  4.5686,\n         4.3772,  4.3075,  4.4895,  4.4827,  5.3170,  5.4987,  5.4632,  6.0328,\n         5.2842,  5.0539,  5.4538,  6.0337,  5.7250,  5.7587,  6.2020,  6.5992,\n         6.4621,  6.5140,  6.6846,  7.3497,  8.0909,  7.0794,  6.8667,  7.4229,\n         7.2544,  7.1967,  9.5006,  9.0339,  7.4887,  9.0759, 11.0946, 10.3260,\n        12.2665, 13.0983, 12.5468, 13.8340])\n\n\n\nx # ê¸¸ì´ê°€ 100ì¸ vector (ë°©ë²•1) / ë°©ë²•2ëŠ” [1 x] ì´ëŸ°ì‹ìœ¼ë¡œ í‘œí˜„ë˜ì–´ì•¼ í•¨\n\ntensor([-2.4821, -2.3621, -1.9973, -1.6239, -1.4792, -1.4635, -1.4509, -1.4435,\n        -1.3722, -1.3079, -1.1904, -1.1092, -1.1054, -1.0875, -0.9469, -0.9319,\n        -0.8643, -0.7858, -0.7549, -0.7421, -0.6948, -0.6103, -0.5830, -0.5621,\n        -0.5506, -0.5058, -0.4806, -0.4738, -0.4710, -0.4676, -0.3874, -0.3719,\n        -0.3688, -0.3159, -0.2775, -0.2772, -0.2734, -0.2721, -0.2668, -0.2155,\n        -0.2000, -0.1816, -0.1708, -0.1565, -0.1448, -0.1361, -0.1057, -0.0603,\n        -0.0559, -0.0214,  0.0655,  0.0684,  0.1195,  0.1420,  0.1521,  0.1568,\n         0.2646,  0.2656,  0.3157,  0.3220,  0.3461,  0.3984,  0.4190,  0.5443,\n         0.5579,  0.5913,  0.6148,  0.6469,  0.6469,  0.6523,  0.6674,  0.7059,\n         0.7141,  0.7822,  0.8154,  0.8668,  0.9291,  0.9804,  0.9853,  0.9941,\n         1.0376,  1.0393,  1.0697,  1.1024,  1.1126,  1.1532,  1.2289,  1.3403,\n         1.3494,  1.4279,  1.4994,  1.5031,  1.5437,  1.6789,  2.0832,  2.2444,\n         2.3935,  2.6056,  2.6057,  2.6632])\n\n\n[1 x] ë§Œë“¤ê¸°\n\ntorch.ones(100) , x # ê¸¸ì´ê°€ 100ì¸ vector\n\n(tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n tensor([-2.4821, -2.3621, -1.9973, -1.6239, -1.4792, -1.4635, -1.4509, -1.4435,\n         -1.3722, -1.3079, -1.1904, -1.1092, -1.1054, -1.0875, -0.9469, -0.9319,\n         -0.8643, -0.7858, -0.7549, -0.7421, -0.6948, -0.6103, -0.5830, -0.5621,\n         -0.5506, -0.5058, -0.4806, -0.4738, -0.4710, -0.4676, -0.3874, -0.3719,\n         -0.3688, -0.3159, -0.2775, -0.2772, -0.2734, -0.2721, -0.2668, -0.2155,\n         -0.2000, -0.1816, -0.1708, -0.1565, -0.1448, -0.1361, -0.1057, -0.0603,\n         -0.0559, -0.0214,  0.0655,  0.0684,  0.1195,  0.1420,  0.1521,  0.1568,\n          0.2646,  0.2656,  0.3157,  0.3220,  0.3461,  0.3984,  0.4190,  0.5443,\n          0.5579,  0.5913,  0.6148,  0.6469,  0.6469,  0.6523,  0.6674,  0.7059,\n          0.7141,  0.7822,  0.8154,  0.8668,  0.9291,  0.9804,  0.9853,  0.9941,\n          1.0376,  1.0393,  1.0697,  1.1024,  1.1126,  1.1532,  1.2289,  1.3403,\n          1.3494,  1.4279,  1.4994,  1.5031,  1.5437,  1.6789,  2.0832,  2.2444,\n          2.3935,  2.6056,  2.6057,  2.6632]))\n\n\n\n# torch.stack([torch.ones(100) , x]) # ì¢Œìš°ë¡œ í•©ì¹˜ê¸° ìœ„í•´ stack ì‚¬ìš©\nprint(torch.stack([torch.ones(100) , x]).shape)\n# torch.stack([torch.ones(100) , x], axis=1) # ì›í–ˆë˜ ê²°ê³¼\nprint(torch.stack([torch.ones(100) , x], axis=1).shape)\n\n# torch.stack([torch.ones(100) , x]).T # ë‹¤ë¥¸ ë°©ë²•\nprint(torch.stack([torch.ones(100) , x]).T.shape)\n\ntorch.Size([2, 100])\ntorch.Size([100, 2])\ntorch.Size([100, 2])\n\n\n\nX = torch.stack([torch.ones(100) , x], axis=1)\nW = torch.tensor([[2.5],[4.0]])\ny = X@W + eps.reshape(100,1)\ny.shape\n\ntorch.Size([100, 1])\n\n\n\nsales[:5]\n\n[-8.542, -6.5767, -5.9496, -4.4794, -4.2516]\n\n\n\ny[:5,0]\n\ntensor([-8.5420, -6.5767, -5.9496, -4.4794, -4.2516])\n\n\nsalesì™€ y ë™ì¼\nğŸ”¬ğŸ—£ï¸(\n\n(ì°¸ê³ ) ì¸ë±ì‹± ê´€ë ¨ ì„¤ëª…\n\n\ny[:5]\n\ntensor([[-8.5420],\n        [-6.5767],\n        [-5.9496],\n        [-4.4794],\n        [-4.2516]])\n\n\nyëŠ” matrix ì´ë¯€ë¡œ\n\ny[:5,[0]] # column vectorì²˜ëŸ¼ ë¨\n\ntensor([[-8.5420],\n        [-6.5767],\n        [-5.9496],\n        [-4.4794],\n        [-4.2516]])\n\n\n\n# y[:,:] # yê°€ ê·¸ëŒ€ë¡œ ë‚˜ì˜´\n\n\ny[:5,:] # ê·¸ ì¤‘ 5ê°œë§Œ\n\ntensor([[-8.5420],\n        [-6.5767],\n        [-5.9496],\n        [-4.4794],\n        [-4.2516]])\n\n\në‚˜ì—´ ë°©ì‹ë§Œ ë‹¤ë¥´ê³  ê°’ì€ salesì™€ ë˜‘ê°™ìŒ\n)ğŸ”¬ğŸ—£ï¸\n\nX = torch.stack([torch.ones(100) , x], axis=1) # (100, 2)\nW = torch.tensor([[2.5],[4.0]]) # (2, 1)\ny = X@W + eps.reshape(100,1) # (100, 1)\nx # ì•„ë§ˆë„ (100,) \n\ntensor([-2.4821, -2.3621, -1.9973, -1.6239, -1.4792, -1.4635, -1.4509, -1.4435,\n        -1.3722, -1.3079, -1.1904, -1.1092, -1.1054, -1.0875, -0.9469, -0.9319,\n        -0.8643, -0.7858, -0.7549, -0.7421, -0.6948, -0.6103, -0.5830, -0.5621,\n        -0.5506, -0.5058, -0.4806, -0.4738, -0.4710, -0.4676, -0.3874, -0.3719,\n        -0.3688, -0.3159, -0.2775, -0.2772, -0.2734, -0.2721, -0.2668, -0.2155,\n        -0.2000, -0.1816, -0.1708, -0.1565, -0.1448, -0.1361, -0.1057, -0.0603,\n        -0.0559, -0.0214,  0.0655,  0.0684,  0.1195,  0.1420,  0.1521,  0.1568,\n         0.2646,  0.2656,  0.3157,  0.3220,  0.3461,  0.3984,  0.4190,  0.5443,\n         0.5579,  0.5913,  0.6148,  0.6469,  0.6469,  0.6523,  0.6674,  0.7059,\n         0.7141,  0.7822,  0.8154,  0.8668,  0.9291,  0.9804,  0.9853,  0.9941,\n         1.0376,  1.0393,  1.0697,  1.1024,  1.1126,  1.1532,  1.2289,  1.3403,\n         1.3494,  1.4279,  1.4994,  1.5031,  1.5437,  1.6789,  2.0832,  2.2444,\n         2.3935,  2.6056,  2.6057,  2.6632])\n\n\n(100,)ì„ (100,1)ë¡œ ë°”ê¾¸ê³  ì‹¶ìŒ\n\ntorch.manual_seed(43052)\nx,_ = torch.randn(100).sort()\neps = torch.randn(100)*0.5\ny = x * 4 + 2.5 + eps\n\nX = torch.stack([torch.ones(100) , x], axis=1) # (100, 2)\nW = torch.tensor([[2.5],[4.0]]) # (2, 1)\ny = X@W + eps.reshape(100,1) # (100, 1)\nx = X[:,[1]]\n\n\nx[:5], y[:5]\n\n(tensor([[-2.4821],\n         [-2.3621],\n         [-1.9973],\n         [-1.6239],\n         [-1.4792]]),\n tensor([[-8.5420],\n         [-6.5767],\n         [-5.9496],\n         [-4.4794],\n         [-4.2516]]))\n\n\n\ntemp[:5], sales[:5]\n\n([-2.4821, -2.3621, -1.9973, -1.6239, -1.4792],\n [-8.542, -6.5767, -5.9496, -4.4794, -4.2516])\n\n\në°©ë²• 2ì²˜ëŸ¼ matrixë¡œë„ ê°€ëŠ¥í•˜ë‹¤ëŠ” ê²ƒì„ í™•ì¸\n)ğŸ—£ï¸\nğŸ“(\nX = torch.stack([torch.ones(100),x],axis=1)\nW = torch.tensor([[2.5],[4.0]])\ny = X@W + eps.reshape(100,1)\nx = X[:,[1]]\nâœï¸ í¸ì˜ìƒ ìœ„ì˜ ì½”ë“œëŠ” ì‹¤í–‰ì‹œí‚¤ì§€ ì•ŠìŒ\n\nX[:5,:], y[:5,:]\n\n(tensor([[ 1.0000, -2.4821],\n         [ 1.0000, -2.3621],\n         [ 1.0000, -1.9973],\n         [ 1.0000, -1.6239],\n         [ 1.0000, -1.4792]]),\n tensor([[-8.5420],\n         [-6.5767],\n         [-5.9496],\n         [-4.4794],\n         [-4.2516]]))\n\n\n)ğŸ“\n- tureì™€ observed dataë¥¼ ë™ì‹œì— ì‹œê°í™”\nğŸ—£ï¸(\n\nplt.plot(temp, sales) # ì´ëŸ¬í•œ ë°ì´í„°ë¥¼ ê´€ì¸¡í–ˆë‹¤ê³  ìƒê°\n\n\n\n\n\n\n\n\n\nplt.plot(temp, sales, 'o') # scatter plot\n\n\n\n\n\n\n\n\n\nplt.plot(x, y, 'o') # ìœ„ì™€ ë™ì¼\n\n\n\n\n\n\n\n\nxì—ì„œ yë¡œ ê°€ëŠ” íŒ¨í„´ì„ ì°¾ê³  ì‹¶ìŒ\n\nplt.plot(x, y, 'o', label=\"observed data\") # ê´€ì¸¡í•œ ê°’\nplt.legend()\n\n\n\n\n\n\n\n\n\nplt.plot(x, y, 'o', label=\"observed data\") # ì ì„  + epsilon(í†µê³„ì ìœ¼ë¡œ ì„¤ëª…í•  ìˆ˜ ì—†ëŠ” í˜„ìƒ, random)\nplt.plot(x, 2.5 + 4*x, '--', label=\"true\") # ì›ë˜ ê´€ì¸¡ë˜ì–´ì•¼ í–ˆë˜ ê°’\nplt.legend()\n\n\n\n\n\n\n\n\n\ní•˜ê³  ì‹¶ì€ ê²ƒ\n\nì¹´í˜ ì£¼ì¸: ì˜¨ë„ê°€ 0.5ì¼ ë•Œ ì–¼ë§ˆë‚˜ íŒ”ë¦´ì§€ ì•Œê³  ì‹¶ìŒ\nê°€ì¥ ê°„ë‹¨: 0.5ë¥¼ ì ì„  ìœ„ì— ì˜¬ë¦° í›„ y ê°’ì„ ì˜ˆì¸¡ (0.5 * 4 + 2.5 = 4.5)\ní•˜ì§€ë§Œ ì‹¤ì œë¡œëŠ” íŒŒë€ìƒ‰ë§Œ ì•Œê³  ìˆìœ¼ë¯€ë¡œ ìœ„ì˜ ë°©ë²•ì€ cheating\n\n\n\nplt.plot(x,y,'o',label=r\"observed data: $(x_i,y_i)$\")\n#plt.plot(x,2.5+4*x,'--',label=r\"true: $(x_i, 4x_i+2.5)$ // $y=4x+2.5$ \")\nplt.legend()\n\n\n\n\n\n\n\n\n\ní•˜ê³  ì‹¶ì€ ê²ƒ\n\nìœ„ì˜ ìƒíƒœì—ì„œ ì ë‹¹í•œ ì¶”ì„¸ì„ ì„ ê·¸ë ¤ì„œ ì¶”ì •\n\n\n)ğŸ—£ï¸\n\nplt.plot(x,y,'o',label=r\"observed data: $(x_i,y_i)$\")\n#plt.plot(x,2.5+4*x,'--',label=r\"true: $(x_i, 4x_i+2.5)$ // $y=4x+2.5$ \")\nplt.legend()"
  },
  {
    "objectID": "posts/01wk-2.html#c.-íšŒê·€ë¶„ì„ì´ë€",
    "href": "posts/01wk-2.html#c.-íšŒê·€ë¶„ì„ì´ë€",
    "title": "01wk-2, 02wk-1: (íšŒê·€) â€“ íšŒê·€ëª¨í˜•, ì†ì‹¤í•¨ìˆ˜, íŒŒì´í† ì¹˜ë¥¼ ì´ìš©í•œ ì¶”ì •",
    "section": "C. íšŒê·€ë¶„ì„ì´ë€?",
    "text": "C. íšŒê·€ë¶„ì„ì´ë€?\n- í´ë¦¬ì…°: ê´€ì¸¡í•œ ìë£Œ \\((x_i,y_i)\\) ê°€ ìˆìŒ \\(\\to\\) ìš°ë¦¬ëŠ” \\((x_i,y_i)\\)ì˜ ê´€ê³„ë¥¼ íŒŒì•…í•˜ì—¬ ìƒˆë¡œìš´ \\(x\\)ê°€ ì™”ì„ë•Œ ê·¸ê²ƒì— ëŒ€í•œ ì˜ˆì¸¡ê°’(predicted value) \\(\\hat{y}\\)ì„ ì•Œì•„ë‚´ëŠ” ë²•ì¹™ì„ ì•Œê³  ì‹¶ìŒ \\(\\to\\) ê´€ê³„ë¥¼ íŒŒì•…í•˜ê¸° ìœ„í•´ì„œ \\((x_i, y_i)\\)ì˜ ì‚°ì ë„ë¥¼ ê·¸ë ¤ë³´ë‹ˆ \\(x_i\\)ì™€ \\(y_i\\)ëŠ” ì„ í˜•ì„±ì„ ê°€ì§€ê³  ìˆë‹¤ëŠ” ê²ƒì´ íŒŒì•…ë¨ \\(\\to\\) ì˜¤ì°¨í•­ì´ ë“±ë¶„ì‚°ì„±ì„ ê°€ì§€ê³  ì–´ì©Œê³  ì €ì©Œê³ â€¦ \\(\\to\\) í•˜ì—¬íŠ¼ \\((x_i,y_i)\\) ë¥¼ â€œì ë‹¹íˆ ì˜ ê´€í†µí•˜ëŠ”â€ ì–´ë– í•œ í•˜ë‚˜ì˜ ì¶”ì„¸ì„ ì„ ì˜ ì¶”ì •í•˜ë©´ ëœë‹¤.\n- íšŒê·€ë¶„ì„ì´ë€ ì‚°ì ë„ë¥¼ ë³´ê³  ì ë‹¹í•œ ì¶”ì„¸ì„ ì„ ì°¾ëŠ” ê²ƒì´ë‹¤. ì¢€ ë” ì •í™•í•˜ê²Œ ë§í•˜ë©´ \\((x_1,y_1) \\dots (x_n,y_n)\\) ìœ¼ë¡œ \\(\\begin{bmatrix} \\hat{w}_0 \\\\ \\hat{w}_1 \\end{bmatrix}\\) ë¥¼ ìµœëŒ€í•œ \\(\\begin{bmatrix} 2.5 \\\\ 4 \\end{bmatrix}\\)ì™€ ë¹„ìŠ·í•˜ê²Œ ì°¾ëŠ” ê²ƒ.\n\ngiven data : \\(\\big\\{(x_i,y_i) \\big\\}_{i=1}^{n}\\)\nparameter: \\({\\bf W}=\\begin{bmatrix} w_0 \\\\ w_1 \\end{bmatrix}\\)\nestimated parameter: \\({\\bf \\hat{W}}=\\begin{bmatrix} \\hat{w}_0 \\\\ \\hat{w}_1 \\end{bmatrix}\\)\n\nğŸ—£ï¸ y = ax + b ê¼´ì—ì„œ a, bë¥¼ ì •í•¨\n- ë” ì‰½ê²Œ ë§í•˜ë©´ ì•„ë˜ì˜ ê·¸ë¦¼ì„ ë³´ê³  â€œì ë‹¹í•œâ€ ì¶”ì„¸ì„ ì„ ì°¾ëŠ” ê²ƒì´ë‹¤.\n\nplt.plot(x,y,'o',label=r\"observed data: $(x_i,y_i)$\")\nplt.legend()\n\n\n\n\n\n\n\n\n- ì¶”ì„¸ì„ ì„ ê·¸ë¦¬ëŠ” í–‰ìœ„ = \\((w_0,w_1)\\)ì„ ì„ íƒí•˜ëŠ”ì¼"
  },
  {
    "objectID": "posts/01wk-2.html#a.-1ë‹¨ê³„-ìµœì´ˆì˜-ì ì„ ",
    "href": "posts/01wk-2.html#a.-1ë‹¨ê³„-ìµœì´ˆì˜-ì ì„ ",
    "title": "01wk-2, 02wk-1: (íšŒê·€) â€“ íšŒê·€ëª¨í˜•, ì†ì‹¤í•¨ìˆ˜, íŒŒì´í† ì¹˜ë¥¼ ì´ìš©í•œ ì¶”ì •",
    "section": "A. 1ë‹¨ê³„ â€“ ìµœì´ˆì˜ ì ì„ ",
    "text": "A. 1ë‹¨ê³„ â€“ ìµœì´ˆì˜ ì ì„ \nğŸ—£ï¸(\n\nWhat = torch.tensor([[-5.0],[10.0]], requires_grad=True)\nWhat\n\ntensor([[-5.],\n        [10.]], requires_grad=True)\n\n\n\nyhat = X@What\n\n\n# plt.plot(x, y, 'o')\n# plt.plot(x, yhat, '--')\n\n\nì‹¤í–‰ì‹œí‚¤ë©´ error\nrequires_grad=Trueë¥¼ ì—†ì• ë©´ error ë°œìƒ X\nrequires_grad=True\n\në¯¸ë¶„ì´ í•„ìš”í•¨ì„ ë‚˜íƒ€ë‚´ëŠ” ì˜µì…˜\nì§€ê¸ˆì€ ì˜ë¯¸ë¥¼ ì •í™•í•˜ê²Œ ì•Œ ìˆ˜ ì—†ì§€ë§Œ í¸ì˜ìƒ ì´ë¦„ì„ ë¯¸ë¶„ê¼¬ë¦¬í‘œë¼ê³  ë¶€ë¥´ê² ìŒ\n\n\n\nWhat+1\n\ntensor([[-4.],\n        [11.]], grad_fn=&lt;AddBackward0&gt;)\n\n\n\nê¼¬ë¦¬í‘œê°€ ë°”ë€Œê¸´ í•˜ë‚˜ í° ì§€ì¥ì€ ì—†ìŒ\n\n\n# yhat\n\n\nyhatì„ ì‹¤í–‰ì‹œì¼œë„ ê³„ì‚°ì„ ì˜ ë˜ë‚˜ ê¼¬ë¦¬í‘œê°€ ìˆìŒ\nê¼¬ë¦¬í‘œ ë•Œë¬¸ì— ê·¸ë˜í”„ë¥¼ ê·¸ë¦¬ë©´ errorê°€ ë°œìƒ\ní•´ê²°ì±… (ê¼¬ë¦¬í‘œë¥¼ ì œê±°í•œë‹¤ê³  ìƒê°, ê¼¬ë¦¬í‘œê°€ ìˆìœ¼ë©´ ê³„ì‚°ì€ ê°€ëŠ¥í•˜ë‚˜ ê·¸ë˜í”„ ê·¸ë¦¬ê¸° ë¶ˆê°€ëŠ¥)\n\nRuntimeError: Canâ€™t call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.\n.data\n\n\n\n# yhat.detach()\n\n\n# yhat.data\n\n\nplt.plot(x, y, 'o')\nplt.plot(x, yhat.detach(), '--') # ê·¸ë¦¼ì„ ê·¸ë¦¬ê¸° ìœ„í•´ì„œ yhatì˜ ë¯¸ë¶„ê¼¬ë¦¬í‘œë¥¼ ì œê±°\n\n\n\n\n\n\n\n\n)ğŸ—£ï¸\nğŸ—£ï¸ ê·¸ëƒ¥ ì•„ë¬´ ì§ì„ ì„ ê·¸ìŒ (2ë‹¨ê³„ë§Œ ì˜ ë˜ë©´ ìƒê´€ X)\n\nWhat = torch.tensor([[-5.0],[10.0]])\nWhat\n\ntensor([[-5.],\n        [10.]])\n\n\n\nyhat = X@What \n\n\nplt.plot(x,y,'o')\nplt.plot(x,yhat.data,'--')"
  },
  {
    "objectID": "posts/01wk-2.html#b.-2ë‹¨ê³„-update",
    "href": "posts/01wk-2.html#b.-2ë‹¨ê³„-update",
    "title": "01wk-2, 02wk-1: (íšŒê·€) â€“ íšŒê·€ëª¨í˜•, ì†ì‹¤í•¨ìˆ˜, íŒŒì´í† ì¹˜ë¥¼ ì´ìš©í•œ ì¶”ì •",
    "section": "B. 2ë‹¨ê³„ â€“ update",
    "text": "B. 2ë‹¨ê³„ â€“ update\n- â€™ì ë‹¹í•œ ì •ë„â€™ë¥¼ íŒë‹¨í•˜ê¸° ìœ„í•œ ì¥ì¹˜: loss function ë„ì…!\n\\[loss=\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2=\\sum_{i=1}^{n}(y_i-(\\hat{w}_0+\\hat{w}_1x_i))^2=({\\bf y}-{\\bf\\hat{y}})^\\top({\\bf y}-{\\bf\\hat{y}})=({\\bf y}-{\\bf X}{\\bf \\hat{W}})^\\top({\\bf y}-{\\bf X}{\\bf \\hat{W}})\\]\nğŸ—£ï¸ lossëŠ” \\((\\hat{w}_0, \\hat{w}_1)\\)ì„ ì…ë ¥ìœ¼ë¡œ ë°›ìŒ. loss ê°’ì„ ìµœì†Œë¡œ ë§Œë“œëŠ” \\((\\hat{w}_0, \\hat{w}_1)\\)ì„ ì°¾ìœ¼ë©´ ë¨.\n- loss í•¨ìˆ˜ì˜ íŠ¹ì§•: ìœ„ ê·¸ë¦¼ì˜ ì£¼í™©ìƒ‰ ì ì„ ì´ â€˜ì ë‹¹í•  ìˆ˜ë¡â€™ lossê°’ì´ ì‘ë‹¤.\n\nplt.plot(x,y,'o')\nplt.plot(x,yhat)\n\n\n\n\n\n\n\n\n\nloss = torch.sum((y-yhat)**2)\nloss\n\ntensor(8587.6875)\n\n\n- ìš°ë¦¬ì˜ ëª©í‘œ: ì´ loss(=8587.6275)ì„ ë” ì¤„ì´ì.\n\nê¶ê·¹ì ìœ¼ë¡œëŠ” ì•„ì˜ˆ ëª¨ë“  ì¡°í•© \\((\\hat{w}_0,\\hat{w}_1)\\)ì— ëŒ€í•˜ì—¬ ê°€ì¥ ì‘ì€ lossë¥¼ ì°¾ìœ¼ë©´ ì¢‹ê² ë‹¤.\n\n- ë¬¸ì œì˜ ì¹˜í™˜: ìƒê°í•´ë³´ë‹ˆê¹Œ ìš°ë¦¬ì˜ ë¬¸ì œëŠ” ì•„ë˜ì™€ ê°™ì´ ìˆ˜í•™ì ìœ¼ë¡œ ë‹¨ìˆœí™” ë˜ì—ˆë‹¤.\n\nê°€ì¥ ì ë‹¹í•œ ì£¼í™©ìƒ‰ ì„ ì„ ì°¾ì \\(\\to\\) \\(loss(\\hat{w}_0,\\hat{w}_1)\\)ë¥¼ ìµœì†Œë¡œí•˜ëŠ” \\((\\hat{w}_0,\\hat{w}_1)\\)ì˜ ê°’ì„ ì°¾ì.\n\n- ìˆ˜ì •ëœ ëª©í‘œ: \\(loss(\\hat{w}_0,\\hat{w}_1)\\)ë¥¼ ìµœì†Œë¡œ í•˜ëŠ” \\((\\hat{w}_0,\\hat{w}_1)\\)ì„ êµ¬í•˜ë¼.\n\në‹¨ìˆœí•œ ìˆ˜í•™ë¬¸ì œê°€ ë˜ì—ˆë‹¤. ì´ê²ƒì€ ë§ˆì¹˜ \\(f(x,y)\\)ë¥¼ ìµœì†Œí™”í•˜ëŠ” \\((x,y)\\)ë¥¼ ì°¾ìœ¼ë¼ëŠ” ê²ƒì„.\ní•¨ìˆ˜ì˜ ìµœëŒ€ê°’ í˜¹ì€ ìµœì†Œê°’ì„ ì»´í“¨í„°ë¥¼ ì´ìš©í•˜ì—¬ ì°¾ëŠ”ê²ƒì„ â€œìµœì í™”â€ë¼ê³  í•˜ë©° ì´ëŠ” ì‚°ê³µêµìˆ˜ë‹˜ë“¤ì´ ê°€ì¥ ì˜í•˜ëŠ” ë¶„ì•¼ì„. (ì‚°ê³µêµìˆ˜ë‹˜ë“¤ì—ê²Œ ë¶€íƒí•˜ë©´ ì˜í•´ì¤Œ, ì‚°ê³µêµìˆ˜ë‹˜ë“¤ì€ ë³´í†µ ìµœì í™”í•´ì„œ ì–´ë””ì— ì“¸ì§€ë³´ë‹¤ ìµœì í™” ìì²´ì— ë” ê´€ì‹¬ì„ ê°€ì§€ê³  ì—°êµ¬í•˜ì‹¬)\nìµœì í™”ë¥¼ í•˜ëŠ” ë°©ë²•? ê²½ì‚¬í•˜ê°•ë²•\n\n# ê²½ì‚¬í•˜ê°•ë²• ì•„ì´ë””ì–´ (1ì°¨ì›)\n\nì„ì˜ì˜ ì ì„ ì°ëŠ”ë‹¤.\nê·¸ ì ì—ì„œ ìˆœê°„ê¸°ìš¸ê¸°ë¥¼ êµ¬í•œë‹¤. (ì ‘ì„ ) &lt;â€“ ë¯¸ë¶„\nìˆœê°„ê¸°ìš¸ê¸°(=ë¯¸ë¶„ê³„ìˆ˜)ì˜ ë¶€í˜¸ë¥¼ ì‚´í´ë³´ê³  ë¶€í˜¸ì™€ ë°˜ëŒ€ë°©í–¥ìœ¼ë¡œ ì›€ì§ì¸ë‹¤.\n\n\níŒ: ê¸°ìš¸ê¸°ì˜ ì ˆëŒ€ê°’ í¬ê¸°ì™€ ë¹„ë¡€í•˜ì—¬ ë³´í­(=ì›€ì§ì´ëŠ” ì •ë„)ì„ ì¡°ì ˆí•œë‹¤. \\(\\to\\) \\(\\alpha\\)ë¥¼ ë„ì…\n\n\nìµœì¢…ìˆ˜ì‹: \\(\\hat{w} \\leftarrow \\hat{w} - \\alpha \\times \\frac{\\partial}{\\partial w}loss(w)\\)\n\n#\nğŸ—£ï¸(\n\në³´í­: step size\ní•¨ìˆ˜ë¥¼ ìµœê³ ì°¨í•­ì´ ì–‘ìˆ˜ì¸ 2ì°¨ í•¨ìˆ˜ë¡œ ìƒê°í•˜ë©´ ì´í•´í•˜ê¸° ì‰¬ì›€\n\nxì—ì„œ aë§Œí¼ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì´ë™: x + a\nxì—ì„œ aë§Œí¼ ì™¼ìª½ìœ¼ë¡œ ì´ë™: x - a\në¯¸ë¶„ê³„ìˆ˜ê°€ 0ì¸ìª½ìœ¼ë¡œ ì›€ì§ì¼ ë•Œ\n\nxê°€ ì˜¤ë¥¸ìª½ì— ìˆìœ¼ë©´ ë¯¸ë¶„ê³„ìˆ˜ &gt; 0\nxê°€ ì™¼ìª½ì— ìˆìœ¼ë©´ ë¯¸ë¶„ê³„ìˆ˜ &lt; 0\n\në¯¸ë¶„ê³„ìˆ˜ê°€ 0ì¸ìª½ê³¼ ê°€ê¹Œìš¸ìˆ˜ë¡ ì ‘ì„  ê¸°ìš¸ê¸°ì˜ ì ˆëŒ€ê°’ì´ ì‘ì•„ì§ -&gt; \\(\\alpha\\)ë¡œ ì¡°ì ˆ\n\n\\(\\alpha\\)ê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ ìˆ˜ë ´ ì†ë„ê°€ ëŠë¦´ ìˆ˜ ìˆê³ , ë„ˆë¬´ í¬ë©´ ìˆ˜ë ´ì„ ì•ˆí•  ìˆ˜ ìˆìŒ\n\nì˜ˆì‹œ) \\(f(x) = x^2\\) ì—ì„œ \\(x=2\\)ì¼ ë•Œ \\(\\alpha = 1\\)ì´ë©´ \\(x\\)ëŠ” \\(-2\\)ì™€ \\(2\\)ë§Œ ì™”ë‹¤ê°”ë‹¤ í•¨\n\n\n)ğŸ—£ï¸\n# ê²½ì‚¬í•˜ê°•ë²• ì•„ì´ë””ì–´ (2ì°¨ì›)\n\n\nì„ì˜ì˜ ì ì„ ì°ëŠ”ë‹¤.\nê·¸ ì ì—ì„œ ìˆœê°„ê¸°ìš¸ê¸°ë¥¼ êµ¬í•œë‹¤. (ì ‘í‰ë©´) &lt;â€“ í¸ë¯¸ë¶„\nìˆœê°„ê¸°ìš¸ê¸°(=ë¯¸ë¶„ê³„ìˆ˜)ì˜ ë¶€í˜¸ë¥¼ ì‚´í´ë³´ê³  ë¶€í˜¸ì™€ ë°˜ëŒ€ë°©í–¥ìœ¼ë¡œ ê°ê° ì›€ì§ì¸ë‹¤.\n\n\níŒ: ì—¬ê¸°ì„œë„ ê¸°ìš¸ê¸°ì˜ ì ˆëŒ€ê°’ í¬ê¸°ì™€ ë¹„ë¡€í•˜ì—¬ ë³´í­(=ì›€ì§ì´ëŠ” ì •ë„)ì„ ê°ê° ì¡°ì ˆí•œë‹¤. \\(\\to\\) \\(\\alpha\\)ë¥¼ ë„ì….\n\n#\nğŸ—£ï¸(\n\nì—¬ê¸°ì„œ ì„ì˜ì˜ ì ì€ 2ì°¨ì›\ní¸ë¯¸ë¶„: í•˜ë‚˜ë§Œ ë³€ìˆ˜ë¡œ ë³´ê³  ë‚˜ë¨¸ì§€ ê³ ì •\n\nì´í›„ 1ì°¨ì› ë°©ì‹ê³¼ ë™ì¼\nì–´ë–¤ ë°©í–¥(ì™¼ìª½, ì˜¤ë¥¸ìª½)ìœ¼ë¡œ ì–¼ë§ˆë‚˜ ê°ˆ ì§€(\\(\\alpha\\))\n\n\n)ğŸ—£ï¸\n- ê²½ì‚¬í•˜ê°•ë²• = lossë¥¼ ì¤„ì´ë„ë¡ \\({\\bf \\hat{W}}\\)ë¥¼ ê°œì„ í•˜ëŠ” ë°©ë²•\n\nì—…ë°ì´íŠ¸ ê³µì‹: ìˆ˜ì •ê°’ = ì›ë˜ê°’ - \\(\\alpha\\) \\(\\times\\) ê¸°ìš¸ì–´ì§„í¬ê¸°(=ë¯¸ë¶„ê³„ìˆ˜)\nì—¬ê¸°ì—ì„œ \\(\\alpha\\)ëŠ” ì „ì²´ì ì¸ ë³´í­ì˜ í¬ê¸°ë¥¼ ê²°ì •í•œë‹¤. ì¦‰ \\(\\alpha\\)ê°’ì´ í´ìˆ˜ë¡ í•œë²ˆì˜ updateì— ì›€ì§ì´ëŠ” ì–‘ì´ í¬ë‹¤.\n\nğŸ—£ï¸ \\(\\alpha\\)ë¥¼ MLì—ì„œëŠ” í•™ìŠµë¥ ì´ë¼ê³  í•¨\n- lossëŠ” \\(\\hat{\\bf W} =\\begin{bmatrix} \\hat{w}_0 \\\\ \\hat{w}_1 \\end{bmatrix}\\) ì— ë”°ë¼ì„œ ê°’ì´ ë°”ë€ŒëŠ” í•¨ìˆ˜ë¡œ í•´ì„ê°€ëŠ¥í•˜ê³  êµ¬ì²´ì ì¸ í˜•íƒœëŠ” ì•„ë˜ì™€ ê°™ìŒ.\n\\[ loss(\\hat{w}_0,\\hat{w}_1) := loss(\\hat{\\bf W})=\\sum_{i=1}^{n}(y_i-(\\hat{w}_0+\\hat{w}_1x_i))^2=({\\bf y}-{\\bf X}{\\bf \\hat{W}})^\\top({\\bf y}-{\\bf X}{\\bf \\hat{W}})\\]\në”°ë¼ì„œ êµ¬í•˜ê³  ì‹¶ì€ê²ƒì€ ì•„ë˜ì™€ ê°™ìŒ\n\\[\\hat{\\bf W}^{LSE} = \\underset{\\bf \\hat{W}}{\\operatorname{argmin}} ~ loss(\\hat{\\bf W})\\]\n\n\n\n\n\n\nWarning\n\n\n\nì•„ë˜ì˜ ìˆ˜ì‹\n\\[\\hat{\\bf W}^{LSE} = \\underset{\\bf \\hat{W}}{\\operatorname{argmin}} ~ loss(\\hat{\\bf W})\\]\nì€ ì•„ë˜ì™€ ê°™ì´ í‘œí˜„í•´ë„ ë¬´ë°©í•©ë‹ˆë‹¤.\n\\[\\hat{\\bf W} = \\underset{\\bf W}{\\operatorname{argmin}} ~ loss({\\bf W})\\]\në§ˆì¹˜ í•¨ìˆ˜ \\(f(\\hat{x})=({\\hat x}-1)^2\\) ì„ \\(f(x)=(x-1)^2\\) ì´ë¼ê³  í‘œí˜„í•  ìˆ˜ ìˆëŠ” ê²ƒ ì²˜ëŸ¼ìš”..\n\n\nì—¬ê¸°ê¹Œì§€ 01wk-2ì—ì„œ ìˆ˜ì—…í–ˆìŠµë‹ˆë‹¤~\n\nì—¬ê¸°ë¶€í„°ëŠ” 02wk-1ì—ì„œ..\n# ì§€ë‚œì‹œê°„ ë³µìŠµ\n\n# x,X,W,y // X = [1 x], W = [w0, w1]' # íšŒê·€ë¶„ì„ì—ì„œëŠ” W=Î²\n# íšŒê·€ëª¨í˜•: y=X@W+Ïµ = X@Î²+Ïµ\n# true: E(y)=X@W\n# observed: (x,y)\n# estimated W = What = [w0hat, w1hat]' &lt;-- ì•„ë¬´ê°’ì´ë‚˜ë„£ì—ˆìŒ.. \n# estimated y = yhat = X@What = X@Î²Ì‚ \n# loss = yhatì´ë‘ yë‘ ì–¼ë§ˆë‚˜ ë¹„ìŠ·í•œì§€ = sum((y-yhat)^2)\n# (x,y) ë³´ê³  ìµœì ì˜ ì„ ë¶„ì„ ê·¸ë¦¬ëŠ”ê²ƒ = lossë¥¼ ê°€ì¥ ì‘ê²Œ ë§Œë“œëŠ” What = [w0hat, w1hat] ë¥¼ ì°¾ëŠ”ê²ƒ\n# ì „ëµ: (1) ì•„ë¬´ Whatë‚˜ ì°ëŠ”ë‹¤ (2) ê·¸ê±°ë³´ë‹¤ ë” ë‚˜ì€ Whatì„ ì°¾ëŠ”ë‹¤. (3) 1-2ë¥¼ ë°˜ë³µí•œë‹¤. \n# ì „ëµ2ê°€ ì–´ë ¤ìš´ë°, ì´ë¥¼ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì´ ê²½ì‚¬í•˜ê°•ë²• \n# ê²½ì‚¬í•˜ê°•ë²• ì•Œê³ ë¦¬ì¦˜: ë”ë‚˜ì€What = ì›ë˜What - 0.1*ë¯¸ë¶„ê°’\n\n\nWhat = torch.tensor([[-5.0],[10.0]])\nWhat\n\ntensor([[-5.],\n        [10.]])\n\n\n\nyhat = X@What \nplt.plot(x,y,'o')\nplt.plot(x,yhat,'--')\n\n\n\n\n\n\n\n\n\nloss = torch.sum((y-yhat)**2)\nloss\n\ntensor(8587.6875)\n\n\në³µìŠµë~\n#\n- ë” ë‚˜ì€ ì„ ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•˜ê¸° ìœ„í•´ì„œëŠ” ê³µì‹ â€œë”ë‚˜ì€What = ì›ë˜What - 0.1*ë¯¸ë¶„ê°’â€ ë¥¼ ì ìš©í•´ì•¼í•˜ê³  ì´ë¥¼ ìœ„í•´ì„œëŠ” ë¯¸ë¶„ê°’ì„ ê³„ì‚°í•  ìˆ˜ ìˆì–´ì•¼ í•¨.\n\n\n\n\n\n\nImportant\n\n\n\nê²½ì‚¬í•˜ê°•ë²•ì„ ì¢€ ë” ì—„ë°€í•˜ê²Œ ì¨ë³´ì. ê²½ì‚¬í•˜ê°•ë²•ì€ \\(loss(\\hat{\\bf W})\\)ë¥¼ ìµœì†Œë¡œ ë§Œë“œëŠ” \\(\\hat{\\bf W}\\)ë¥¼ ì»´í“¨í„°ë¡œ êµ¬í•˜ëŠ” ë°©ë²•ì¸ë°, êµ¬ì²´ì ìœ¼ë¡œëŠ” ì•„ë˜ì™€ ê°™ë‹¤.\n1. ì„ì˜ì˜ ì  \\(\\hat{\\bf W}\\)ë¥¼ ì°ëŠ”ë‹¤.\n2. ê·¸ ì ì—ì„œ ìˆœê°„ê¸°ìš¸ê¸°ë¥¼ êµ¬í•œë‹¤. ì¦‰ \\(\\left.\\frac{\\partial}{\\partial {\\bf W}}loss({\\bf W})\\right|_{{\\bf W}=\\hat{\\bf W}}\\) ë¥¼ ê³„ì‚°í•œë‹¤.\n3. \\(\\hat{\\bf W}\\)ì—ì„œì˜ ìˆœê°„ê¸°ìš¸ê¸°ì˜ ë¶€í˜¸ë¥¼ ì‚´í´ë³´ê³  ë¶€í˜¸ì™€ ë°˜ëŒ€ë°©í–¥ìœ¼ë¡œ ì›€ì§ì¸ë‹¤. ì´ë•Œ ê¸°ìš¸ê¸°ì˜ ì ˆëŒ€ê°’ í¬ê¸°ì™€ ë¹„ë¡€í•˜ì—¬ ë³´í­(=ì›€ì§ì´ëŠ” ì •ë„)ì„ ê°ê° ì¡°ì ˆí•œë‹¤. ì¦‰ ì•„ë˜ì˜ ìˆ˜ì‹ì— ë”°ë¼ ì—…ë°ì´íŠ¸ í•œë‹¤.\n\\[\\hat{\\bf W} \\leftarrow \\hat{\\bf W} - \\alpha \\times \\left.\\frac{\\partial}{\\partial {\\bf W}}loss({\\bf W})\\right|_{{\\bf W}=\\hat{\\bf W}}\\]\nì—¬ê¸°ì—ì„œ ë§¨ ë§ˆì§€ë§‰ ìˆ˜ì‹ì„ ê°„ë‹¨í•˜ê²Œ ì“´ ê²ƒì´ ë”ë‚˜ì€What = ì›ë˜What - 0.1*ë¯¸ë¶„ê°’ ì´ë‹¤.\n\n\n- ë¯¸ë¶„ê°’ì„ ê³„ì‚°í•˜ëŠ” ë°©ë²•1\n\n# ì†ì‹¤ 8587.6875 ë¥¼ ê³„ì‚°í•˜ëŠ” ë˜ ë‹¤ë¥¸ ë°©ì‹\ndef l(w0,w1):\n    yhat = w0 + w1*x\n    return torch.sum((y-yhat)**2)\n\n\nl(-5,10)\n\ntensor(8587.6875)\n\n\nğŸ—£ï¸(\n\nêµ³ì´ í•¨ìˆ˜ë¥¼ ë§Œë“  ì´ìœ : ë¯¸ë¶„í•˜ë ¤ê³ \ní¸ë¯¸ë¶„ êµ¬í˜„\n\nl(-5,10)\n(l(w0+h,w1) - l(w0,w1))/h: ë„í•¨ìˆ˜\n\n\n)ğŸ—£ï¸\n\nh=0.001\nprint((l(-5+h,10) - l(-5,10))/h)\nprint((l(-5,10+h) - l(-5,10))/h)\n\ntensor(-1341.7968)\ntensor(1190.4297)\n\n\nì¼ë‹¨ ì´ê±°ë¡œ ì—…ë°ì´íŠ¸í•´ë³¼ê¹Œ?\n\n# ë”ë‚˜ì€What = ì›ë˜What - 0.1*ë¯¸ë¶„ê°’\n# [-5,10] - 0.001 * [-1341.7968,1190.4297]\n\n\nsssss = What - 0.001 * torch.tensor([[-1341.7968],[1190.4297]])\nsssss\n\ntensor([[-3.6582],\n        [ 8.8096]])\n\n\n\nplt.plot(x,y,'o')\nplt.plot(x,X@What,'-') # ì›ë˜What: ì£¼í™©ìƒ‰\nplt.plot(x,X@sssss,'-') # ë”ë‚˜ì€What: ì´ˆë¡ìƒ‰\n\n\n\n\n\n\n\n\n\nì˜ ëœ ê²ƒ ê°™ê¸´í•œë°..\në¯¸ë¶„êµ¬í•˜ëŠ”ê²Œ ë„ˆë¬´ ì–´ë ¤ì›Œ..\në‹¤ë¥¸ ë°©ë²• ì—†ì„ê¹Œ?\n\n\n\n\n\n\n\nImportant\n\n\n\nì‚¬ì‹¤ ì´ ë°©ë²•ì€\n\n\\(\\frac{\\partial}{\\partial w_0}loss(w_0,w_1) \\approx \\frac{loss(w_0+h,w_1)-loss(w_0,w_1)}{h}\\)\n\\(\\frac{\\partial}{\\partial w_1}loss(w_0,w_1) \\approx \\frac{loss(w_0,w_1+h)-loss(w_0,w_1)}{h}\\)\n\nì´ ê³„ì‚°ì„ ì´ìš©í•˜ì—¬\n\\[\\frac{\\partial}{\\partial {\\bf W}}loss({\\bf W}):= \\begin{bmatrix} \\frac{\\partial}{\\partial w_0} \\\\ \\frac{\\partial}{\\partial w_1}\\end{bmatrix}loss({\\bf W}) =  \\begin{bmatrix} \\frac{\\partial}{\\partial w_0}loss({\\bf W}) \\\\ \\frac{\\partial}{\\partial w_1}loss({\\bf W})\\end{bmatrix}  =  \\begin{bmatrix} \\frac{\\partial}{\\partial w_0}loss(w_0,w_1) \\\\ \\frac{\\partial}{\\partial w_1}loss(w_0,w_1)\\end{bmatrix}\\]\në¥¼ ê³„ì‚°í•œ ê²ƒì´ë¼ ë³¼ ìˆ˜ ìˆì£ \n\n\n- ë¯¸ë¶„ê°’ì„ ê³„ì‚°í•˜ëŠ” ë°©ë²•2\n\n## ì•½ê°„ì˜ ì§€ì‹ì´ í•„ìš”í•¨. \n# loss = (y-XWhat)'(y-XWhat)\n# = (y'-What'X')(y-XWhat)\n# = y'y-y'XWhat -What'X'y + What'X'XWhat \n# lossë¥¼ Whatìœ¼ë¡œ ë¯¸ë¶„\n# loss' = -X'y - X'y + 2X'XWhat\n\nâ“ í–‰ë ¬ ë¯¸ë¶„ ë³µìŠµ í•„ìš”\n\n-2*X.T@y + 2*X.T@X@What\n\ntensor([[-1342.2524],\n        [ 1188.9302]])\n\n\nğŸ—£ï¸ ì•½ê°„ì˜ ì˜¤ì°¨ëŠ” ìˆì§€ë§Œ ìœ„ì™€ ë¹„ìŠ· (ê·¸ëŸ¬ë‚˜ ë°©ë²•1, ë°©ë²•2 ë§ê³  ë‹¤ë¥¸ ë°©ë²•ì„ ì“°ê³  ì‹¶ìŒ)\n\n\n\n\n\n\nImportant\n\n\n\nì´ ë°©ë²•ì€ \\(loss({\\bf W})\\)ì˜ ë¯¸ë¶„ì„ êµ¬í• ìˆ˜ ìˆì–´ì•¼ ì‚¬ìš©ê°€ëŠ¥í•©ë‹ˆë‹¤. ì¦‰\n\\[\\frac{\\partial}{\\partial {\\bf W}}loss({\\bf W})= -2{\\bf X}^\\top {\\bf y} + 2{\\bf X}^\\top {\\bf X}{\\bf W}\\]\në¥¼ ê³„ì‚°í•  ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.\n\n\n- ë¯¸ë¶„ê°’ì„ ê³„ì‚°í•˜ëŠ” ë°©ë²•3 â€“ ì´ íŒ¨í„´ì„ ì™¸ìš°ì„¸ì—¬\n\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad=True)\nWhat\n\ntensor([[-5.],\n        [10.]], requires_grad=True)\n\n\n\nyhat = X@What\nloss = torch.sum((y-yhat)**2)\nloss\n\ntensor(8587.6875, grad_fn=&lt;SumBackward0&gt;)\n\n\nğŸ—£ï¸ ê¼¬ë¦¬í‘œê°€ ìˆê¸´í•˜ì§€ë§Œ ê²°ê³¼ëŠ” ìœ„ì™€ ë™ì¼\n\nloss.backward() # lossë¥¼ ë¯¸ë¶„í•˜ë¼.. ê¼¬ë¦¬í‘œê°€ ìˆê²Œ í•œ Whatìœ¼ë¡œ.. \n\nğŸ—£ï¸(\n\nlossë¥¼ Whatìœ¼ë¡œ ë¯¸ë¶„\nì¼ë°˜ì ìœ¼ë¡œ ë¯¸ë¶„ì„ í•˜ë©´ ë„í•¨ìˆ˜ê°€ ë‚˜ì˜¤ì§€ë§Œ, ì´ ê²½ìš°ëŠ” ë„í•¨ìˆ˜ì—ì„œ í˜„ì¬ Whatê°’ì„ ëŒ€ì…í•œ ê²°ê³¼ê°€ ë‚˜ì˜´\nì •í™•íˆ ë§í•˜ë©´ Whatì— í•´ë‹¹í•˜ëŠ” ì ‘ì„ ì˜ ê¸°ìš¸ê¸°\nì‹¤í–‰í•´ë„ ì‹¤í–‰ê²°ê³¼ëŠ” ë‚˜ì˜¤ì§€ ì•ŠìŒ. ê²°ê³¼ëŠ” What.gradì— ì €ì¥ë˜ì–´ ìˆìŒ\n\n)ğŸ—£ï¸\n\nWhat.grad\n\ntensor([[-1342.2524],\n        [ 1188.9305]])\n\n\n- ìœ„ì˜ ì½”ë“œë¥¼ ë‹¤ì‹œ ë³µìŠµí•´ë³´ì.\nâ€“ loss.backward()ì‹¤í–‰ì „ â€“\n\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad=True)\nyhat = X@What\nloss = torch.sum((y-yhat)**2)\n\n\nWhat.data, What.grad\n\n(tensor([[-5.],\n         [10.]]),\n None)\n\n\nğŸ—£ï¸ .backward()ë¥¼ ì‹¤í–‰í•˜ì§€ ì•Šì•„ì„œ .gradì— ì•„ë¬´ ê°’ë„ ì—†ìŒ(Noneìœ¼ë¡œ ì´ˆê¸°í™” ë¨)\nâ€“ loss.backward()ì‹¤í–‰í›„ â€“\n\nloss.backward()\n\n\nWhat.data, What.grad\n\n(tensor([[-5.],\n         [10.]]),\n tensor([[-1342.2524],\n         [ 1188.9305]]))\n\n\nğŸ—£ï¸(\n\n.backward()ë¥¼ ì‹¤í–‰í•˜ë‹ˆ .gradì— ê¸°ìš¸ê¸° ê°’ì´ ê³„ì‚°ë˜ì–´ ì—…ë°ì´íŠ¸ ë¨\nloss.backward(): What.grad &lt;- Whatì—ì„œ ë¯¸ë¶„ê°’ ì¸ì¤„ ì•Œì•˜ìœ¼ë‚˜ ì‚¬ì‹¤ì€\nloss.backward(): What.grad &lt;- What.grad + Whatì—ì„œ ë¯¸ë¶„ê°’ (ì¦‰, ëˆ„ì ì„ ì‹œì¼œì„œ ë”í•¨)\n\n\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad=True)\n\n\nyhat = X@What\nloss = torch.sum((y-yhat)**2)\nloss.backward()\n\n\nWhat.data, What.grad\n\n(tensor([[-5.],\n         [10.]]),\n tensor([[-1342.2524],\n         [ 1188.9305]]))\n\n\n\nyhat = X@What\nloss = torch.sum((y-yhat)**2)\nloss.backward()\n\n\nWhat.data, What.grad\n\n(tensor([[-5.],\n         [10.]]),\n tensor([[-2684.5049],\n         [ 2377.8611]]))\n\n\n\në‘ ë°°ê°€ ë¨\nì™œ?\n\nì‚°ê³µ: ì•Œê³ ë¦¬ì¦˜ ìƒì—ì„œëŠ” What.gradì˜ ê°’ì€ loss.backward()ë¥¼ í• ë•Œë§ˆë‹¤ ì´ˆê¸°í™”ê°€ ë§ìŒ (ì´ë¡ ì ìœ¼ë¡œëŠ” ì´ê²Œ ë§ìŒ)\nì»´ê³µ: ê·¸ëŸ¬ë©´ ë‚˜ì¤‘ì— ê³„ì‚° íš¨ìœ¨ì´ ì•ˆ ì¢‹ì•„ì§ (ì›¬ë§Œí•˜ë©´ ê³„ì‚°í•œ ë¯¸ë¶„ê°’ì„ ê°–ê³  ìˆê³  ì‹¶ìŒ, í•„ìš” ì—†ìœ¼ë©´ ë”°ë¡œ ì´ˆê¸°í™”í•˜ë©´ ë¨)\ní†µê³„: ìµœì í™”ì™€ ë¯¸ë¶„ ë¹¨ë¦¬í•˜ëŠ” ê²ƒì— ê´€ì‹¬ X\n\n\n)ğŸ—£ï¸\nâœï¸ ì´í›„ ì›í™œí•œ ì½”ë“œ ì‹¤í–‰ì„ ìœ„í•œ ì½”ë“œ (ì˜ë¯¸X)\n\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad=True)\nyhat = X@What\nloss = torch.sum((y-yhat)**2)\n\nWhat.data, What.grad\n\nloss.backward()\n\nWhat.data, What.grad\n\n(tensor([[-5.],\n         [10.]]),\n tensor([[-1342.2524],\n         [ 1188.9305]]))\n\n\n# 1íšŒ ì—…ë°ì´íŠ¸ ê³¼ì •ì„ ì°¨ê·¼ì°¨ê·¼ ì‹œê°í™”í•˜ë©° ì •ë¦¬í•´ë³´ì.\n\nalpha = 0.001 \nprint(f\"{What.data} -- ìˆ˜ì •ì „\")\nprint(f\"{-alpha*What.grad} -- ìˆ˜ì •í•˜ëŠ”í­\")\nprint(f\"{What.data-alpha*What.grad} -- ìˆ˜ì •í›„\")\nprint(f\"{torch.tensor([[2.5],[4]])} -- ì°¸ê°’(ì´ê±´ ë¹„ë°€~~)\")\n\ntensor([[-5.],\n        [10.]]) -- ìˆ˜ì •ì „\ntensor([[ 1.3423],\n        [-1.1889]]) -- ìˆ˜ì •í•˜ëŠ”í­\ntensor([[-3.6577],\n        [ 8.8111]]) -- ìˆ˜ì •í›„\ntensor([[2.5000],\n        [4.0000]]) -- ì°¸ê°’(ì´ê±´ ë¹„ë°€~~)\n\n\nğŸ—£ï¸(\n\n\\(\\alpha\\)ë¥¼ 0.001ë¡œ ì¡ì€ ì´ìœ : ë¯¸ë¶„ê°’ì´ 1000 ë‹¨ìœ„ë¡œ ë‚˜ì™€ì„œ ê·¸ëŒ€ë¡œ ë„£ìœ¼ë©´ ì›í•˜ëŠ” ê²°ê³¼ê°€ ì•ˆ ë‚˜ì˜¬ ê²ƒ ê°™ìŒ\n\nì˜ ìˆ˜ë ´ë ë•Œê¹Œì§€ ì‹œí–‰ì°©ì˜¤ë¥¼ ê²ªìœ¼ë©° í•´ë´ì•¼ í•¨\n\nìˆ˜ì •í•˜ëŠ” í­: ìœ„ ê·¸ë˜í”„ì—ì„œ ì£¼í™©ìƒ‰ ì„ \nìˆ˜ì • í›„: ìœ„ ê·¸ë˜í”„ì—ì„œ ì´ˆë¡ìƒ‰ ì„ \nìˆ˜ì • ì „ë³´ë‹¤ ìˆ˜ì • í›„ê°€ ì°¸ê°’ì— ê°€ê¹Œìš°ë¯€ë¡œ ì˜¬ë°”ë¥¸ ë°©í–¥ì„ ì§„í–‰ë˜ê³  ìˆìŒì„ ì•Œ ìˆ˜ ìˆìŒ\n\n)ğŸ—£ï¸\n\nWbefore = What.data\nWafter = What.data - alpha * What.grad \nWbefore, Wafter\n\n(tensor([[-5.],\n         [10.]]),\n tensor([[-3.6577],\n         [ 8.8111]]))\n\n\n\nplt.plot(x,y,'o',label=r'observed data')\nplt.plot(x,X@Wbefore,'--', label=r\"$\\hat{\\bf y}_{before}={\\bf X}@\\hat{\\bf W}_{before}$\")\nplt.plot(x,X@Wafter,'--', label=r\"$\\hat{\\bf y}_{after}={\\bf X}@\\hat{\\bf W}_{after}$\")\nplt.legend()\n\n\n\n\n\n\n\n\n#"
  },
  {
    "objectID": "posts/01wk-2.html#c.-3ë‹¨ê³„-iteration-learn-estimate-bfhat-w",
    "href": "posts/01wk-2.html#c.-3ë‹¨ê³„-iteration-learn-estimate-bfhat-w",
    "title": "01wk-2, 02wk-1: (íšŒê·€) â€“ íšŒê·€ëª¨í˜•, ì†ì‹¤í•¨ìˆ˜, íŒŒì´í† ì¹˜ë¥¼ ì´ìš©í•œ ì¶”ì •",
    "section": "C. 3ë‹¨ê³„ â€“ iteration (=learn = estimate \\(\\bf{\\hat W}\\))",
    "text": "C. 3ë‹¨ê³„ â€“ iteration (=learn = estimate \\(\\bf{\\hat W}\\))\n- ì´ì œ 1ë‹¨ê³„ì™€ 2ë‹¨ê³„ë¥¼ ë°˜ë³µë§Œí•˜ë©´ëœë‹¤. ê·¸ë˜ì„œ ì•„ë˜ì™€ ê°™ì€ ì½”ë“œë¥¼ ì‘ì„±í•˜ë©´ ë  ê²ƒ ê°™ì€ë°â€¦\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad=True) # ìµœì´ˆì˜ ì§ì„ ì„ ë§Œë“œëŠ” ê°’\nfor epoc in range(30):\n    yhat = X@What \n    loss = torch.sum((y-yhat)**2)\n    loss.backward()\n    What.data = What.data - 0.001 * What.grad\nëŒë ¤ë³´ë©´ ì˜ ì•ˆëœë‹¤.\nğŸ—£ï¸ ì›ë˜ ì² ìëŠ” epochì´ì§€ë§Œ í¸ì˜ìƒ epocìœ¼ë¡œ ì‘ì„±, ì˜ ë˜ê¸° ìœ„í•´ì„œëŠ” ë§ˆì§€ë§‰ì— ì´ˆê¸°í™”ë¥¼ í•´ì¤˜ì•¼ í•¨\n- ì•„ë˜ì™€ ê°™ì´ í•´ì•¼í•œë‹¤.\n\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad=True) # ìµœì´ˆì˜ ì§ì„ ì„ ë§Œë“œëŠ” ê°’\nfor epoc in range(30):\n    yhat = X@What \n    loss = torch.sum((y-yhat)**2)\n    loss.backward()\n    What.data = What.data - 0.001 * What.grad\n    What.grad = None \n\n\nplt.plot(x,y,'o',label=r\"observed: $(x_i,y_i)$\")\nplt.plot(x,X@What.data,'--o', label=r\"estimated: $(x_i,\\hat{y}_i)$ -- after 30 iterations (=epochs)\", alpha=0.4 )\nplt.legend()\n\n\n\n\n\n\n\n\n- ì™œ? loss.backward() ëŠ” ì•„ë˜ì˜ ì—­í• ì„ í•˜ëŠ”ê²ƒ ì²˜ëŸ¼ ì´í•´ë˜ì—ˆì§€ë§Œ\n\nWhat.grad \\(\\leftarrow\\) Whatì—ì„œë¯¸ë¶„ê°’\n\nì‹¤ì œë¡œëŠ” ì•„ë˜ì˜ ì—­í• ì„ ìˆ˜í–‰í•˜ê¸° ë•Œë¬¸ì´ë‹¤. (ì»´í“¨í„°ê³µí•™ì ì¸ ì´ìœ ë¡œ..)\n\nWhat.grad \\(\\leftarrow\\) What.grad + Whatì—ì„œë¯¸ë¶„ê°’\n\n\n\n\n\n\n\nNote\n\n\n\nWhat.grad \\(\\leftarrow\\) What.grad + Whatì—ì„œë¯¸ë¶„ê°’ ì„ì„ í™•ì¸í•˜ê¸° ìœ„í•´ì„œ.. ì•½ê°„ì˜ í…ŒìŠ¤íŠ¸ë¥¼ í–ˆìŠµë‹ˆë‹¤.\në¨¼ì €\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad=True) # ìµœì´ˆì˜ ì§ì„ ì„ ë§Œë“œëŠ” ê°’\nprint(What.data)\nprint(What.grad)\në¥¼ í™•ì¸í•œë’¤ ì•„ë˜ë¥¼ ë°˜ë³µì‹¤í–‰í•´ë´¤ì„ë•Œ\nyhat = X@What \nloss = torch.sum((y-yhat)**2)\nloss.backward() # \nprint(What.data)\nprint(What.grad)\nWhat.dataì™€ What.grad ê°’ì´ ê³„ì† ì¼ì •í•˜ê²Œ ë‚˜ì˜¨ë‹¤ë©´\n\nWhat.grad \\(\\leftarrow\\) Whatì—ì„œë¯¸ë¶„ê°’\n\nì´ì™€ ê°™ì€ ê³„ì‚°ì´ ì§„í–‰ë˜ëŠ” ê²ƒì´ê² ê³ , What.gradì˜ ê°’ì´ ìê¾¸ ì»¤ì§„ë‹¤ë©´\n\nWhat.grad \\(\\leftarrow\\) What.grad + Whatì—ì„œë¯¸ë¶„ê°’\n\nì´ì™€ ê°™ì€ ê³„ì‚°ì´ ì§„í–‰ë˜ëŠ” ê²ƒì´ê² ì£ ?"
  },
  {
    "objectID": "posts/04wk-2.html",
    "href": "posts/04wk-2.html",
    "title": "04wk-2: (ì‹ ê²½ë§) â€“ êº½ì¸ê·¸ë˜í”„ì˜ í•œê³„(?), ì‹œë²¤ì½”ì •ë¦¬, MNIST",
    "section": "",
    "text": "ğŸ“˜ Note Format Guide\nThis format serves as a structured guide for organizing lecture content, personal interpretation, experiments, and study-related questions.\nğŸ“ ğŸ—£ï¸ âœï¸ ğŸ”¬ â“"
  },
  {
    "objectID": "posts/04wk-2.html#a.-stepì€-í‘œí˜„-ë¶ˆê°€ëŠ¥í•˜ì§€-ì•Šë‚˜",
    "href": "posts/04wk-2.html#a.-stepì€-í‘œí˜„-ë¶ˆê°€ëŠ¥í•˜ì§€-ì•Šë‚˜",
    "title": "04wk-2: (ì‹ ê²½ë§) â€“ êº½ì¸ê·¸ë˜í”„ì˜ í•œê³„(?), ì‹œë²¤ì½”ì •ë¦¬, MNIST",
    "section": "A. Stepì€ í‘œí˜„ ë¶ˆê°€ëŠ¥í•˜ì§€ ì•Šë‚˜?",
    "text": "A. Stepì€ í‘œí˜„ ë¶ˆê°€ëŠ¥í•˜ì§€ ì•Šë‚˜?\n# ì˜ˆì œ1 â€“ ì¼ë¶€ëŸ¬ ì´ìƒí•˜ê²Œ ë§Œë“  ì·¨ì—…í•©ê²©ë¥  ê³¡ì„ \n\ntorch.manual_seed(43052)\nx = torch.linspace(-1,1,2000).reshape(-1,1)\nu = 0*x-3\nu[x&lt;-0.2] = (15*x+6)[x&lt;-0.2]\nu[(-0.2&lt;x)&(x&lt;0.4)] = (0*x-1)[(-0.2&lt;x)&(x&lt;0.4)]\nsig = torch.nn.Sigmoid()\nv = Ï€ = sig(u)\ny = torch.bernoulli(v)\n\n\nplt.plot(x,y,'.',alpha=0.03, label=\"observed\")\nplt.plot(x,v,'--', label=\"unobserved\")\nplt.legend()\n\n\n\n\n\n\n\n\nğŸ—£ï¸ ëš ë–¨ì–´ì§€ëŠ” ë¶€ë¶„ì€ ì–´ë–»ê²Œ í•´ì•¼í•˜ì§€? ê¸°ìš¸ê¸°ë¥¼ ê¸‰í•˜ê²Œ ê·¼ì‚¬í•˜ëŠ” ì‹ìœ¼ë¡œ ì ‘ê·¼\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,512),\n    torch.nn.ReLU(),\n    torch.nn.Linear(512,1),\n    torch.nn.Sigmoid()\n)\nloss_fn = torch.nn.BCELoss()\noptimizr = torch.optim.Adam(net.parameters())\n#---#\nfor epoc in range(5000):\n    ## 1\n    yhat = net(x)\n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\nğŸ—£ï¸ bias ì—¬ë¶€ê°€ ì§ì„ ì˜ ê°œìˆ˜ì— ì˜í–¥ì„ ì£¼ì§€ëŠ” X, (1,2)ê°€ ì•„ë‹ˆë¼ (1,512)ë¡œ í•˜ë©´ ì—¬ëŸ¬ ë²ˆ êº¾ì¼ ê²ƒì„\n\nplt.plot(x,y,'.',alpha=0.03, label=\"observed\")\nplt.plot(x,v, label=\"true\")\nplt.plot(x,net(x).data,'--', label=\"estimated\")\nplt.legend()\n\n\n\n\n\n\n\n\nğŸ—£ï¸ trueëŠ” ê´€ì¸¡í•  ìˆ˜ ì—†ìŒ, estimatedëŠ” trueì™€ ì°¨ì´ê°€ ìˆì–´ë„ ì“¸ë§Œ í•¨\nğŸ—£ï¸(\n\nê³¼ì • ì‚´í´ë³´ê¸°\n\n\nnet\n\nSequential(\n  (0): Linear(in_features=1, out_features=512, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=512, out_features=1, bias=True)\n  (3): Sigmoid()\n)\n\n\n\nnet[:1]\n\nSequential(\n  (0): Linear(in_features=1, out_features=512, bias=True)\n)\n\n\n\nnet[:3]\n\nSequential(\n  (0): Linear(in_features=1, out_features=512, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=512, out_features=1, bias=True)\n)\n\n\n\nplt.plot(net[:3](x).data) # êº¾ì¸ ì„ \n\n\n\n\n\n\n\n\n\nì •ë‹µì€ ì•„ë‹ˆì§€ë§Œ ì ë‹¹íˆ ê·¼ì‚¬ì ìœ¼ë¡œ ì“¸ ìˆ˜ ìˆì„ ê²ƒ ê°™ìŒ\n\n\nplt.plot(net[:4](x).data) # sigmoid ê²°ê³¼\n\n\n\n\n\n\n\n\n)ğŸ—£ï¸\n#"
  },
  {
    "objectID": "posts/04wk-2.html#b.-ê³¡ì„ ì€-í‘œí˜„-ë¶ˆê°€ëŠ¥í•˜ì§€-ì•Šë‚˜",
    "href": "posts/04wk-2.html#b.-ê³¡ì„ ì€-í‘œí˜„-ë¶ˆê°€ëŠ¥í•˜ì§€-ì•Šë‚˜",
    "title": "04wk-2: (ì‹ ê²½ë§) â€“ êº½ì¸ê·¸ë˜í”„ì˜ í•œê³„(?), ì‹œë²¤ì½”ì •ë¦¬, MNIST",
    "section": "B. ê³¡ì„ ì€ í‘œí˜„ ë¶ˆê°€ëŠ¥í•˜ì§€ ì•Šë‚˜?",
    "text": "B. ê³¡ì„ ì€ í‘œí˜„ ë¶ˆê°€ëŠ¥í•˜ì§€ ì•Šë‚˜?\n# ì˜ˆì œ2 â€“ 2024ë…„ ìˆ˜ëŠ¥ ë¯¸ì 30ë²ˆ ë¬¸ì œì— ë‚˜ì˜¨ ê³¡ì„ \n\\[y_i = e^{-x_i} \\times  |\\cos(5x_i)| \\times \\sin(5x) + \\epsilon_i, \\quad \\epsilon_i \\sim N(0,\\sigma^2)\\]\n\ntorch.manual_seed(43052)\nx = torch.linspace(0,2,2000).reshape(-1,1)\neps = torch.randn(2000).reshape(-1,1)*0.05\nfx = torch.exp(-1*x)* torch.abs(torch.cos(3*x))*(torch.sin(3*x))\ny = fx + eps\n\n\nplt.plot(x,y,label=\"observed\",alpha=0.5)\nplt.plot(x,fx,label=\"true\")\n\n\n\n\n\n\n\n\nğŸ—£ï¸ ë˜ê²Œ ì„¸ë°€í•˜ê²Œ ë§ì´ êº¾ìœ¼ë©´ ê³¡ì„ ì€ ì•„ë‹ˆì§€ë§Œ ê³¡ì„ ì²˜ëŸ¼ ë³´ì¼ ìˆ˜ ìˆì„ ê²ƒ ê°™ìŒ (ì´ ìƒí™©ì—ì„œ bias ì—¬ë¶€ëŠ” ì˜ë¯¸ X)\nğŸ—£ï¸(\n\nêµ³ì´ 0ê³¼ 1 ì‚¬ì´ë¡œ ëˆ„ë¥¼ í•„ìš”ë„ ì—†ê³  - ê°’ë„ ê°–ê³  ìˆìœ¼ë¯€ë¡œ sigmoid ì·¨í•  í•„ìš”ëŠ” ì—†ì„ë“¯\nyê°€ 0 ë˜ëŠ” 1ì´ ì•„ë‹ˆê³  ì—°ì†ì ì¸ ì–´ë–¤ ê°’ì„ ê³„ì† ê°€ì§ˆ ìˆ˜ ìˆìŒ\n\níšŒê·€ë‘ ë¹„ìŠ·í•˜ë¯€ë¡œ MSELoss ì‚¬ìš©\nBCELossë¥¼ ì—¬ê¸°ì„œ ì‚¬ìš©í•œë‹¤ë©´ ì´ ê²½ìš° logì— ìŒìˆ˜ê°€ ë“¤ì–´ê°ˆ ìˆ˜ë„ ìˆìœ¼ë¯€ë¡œ X\n\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1024), # êº½ì´ì§€ì•Šì€ 1024ê°œì˜ ì§ì„ \n    torch.nn.ReLU(), # êº½ì¸(ë ë£¨ëœ) 1024ê°œì˜ ì§ì„  \n    torch.nn.Linear(1024,1), # í•©ì³ì§„ í•˜ë‚˜ì˜ êº½ì¸ ì§ì„  \n)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n## \nfor epoc in range(1000):\n    ## 1\n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,label=\"observed\",alpha=0.5)\nplt.plot(x,fx,label=\"true\")\nplt.plot(x,net(x).data,'--',label=\"estimated\")\nplt.legend()\n\n\n\n\n\n\n\n\n\nì˜ ë³´ë©´ ì§ì„  ëŠë‚Œì´ ìˆê¸°ëŠ” í•˜ë‚˜ ì´ ì •ë„ë©´ ê·¸ëŸ­ì €ëŸ­ ê´œì°®ìŒ\n\n)ğŸ—£ï¸\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,2048), # êº½ì´ì§€ì•Šì€ 1024ê°œì˜ ì§ì„ \n    torch.nn.ReLU(), # êº½ì¸(ë ë£¨ëœ) 1024ê°œì˜ ì§ì„  \n    torch.nn.Linear(2048,1), # í•©ì³ì§„ í•˜ë‚˜ì˜ êº½ì¸ ì§ì„  \n)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n## \nfor epoc in range(1000):\n    ## 1\n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,label=\"observed\",alpha=0.5)\nplt.plot(x,fx,label=\"true\")\nplt.plot(x,net(x).data,'--',label=\"estimated\")\nplt.legend()\n\n\n\n\n\n\n\n\n#"
  },
  {
    "objectID": "posts/04wk-2.html#a.-ì‹œë²¤ì½”ì •ë¦¬-ì†Œê°œ",
    "href": "posts/04wk-2.html#a.-ì‹œë²¤ì½”ì •ë¦¬-ì†Œê°œ",
    "title": "04wk-2: (ì‹ ê²½ë§) â€“ êº½ì¸ê·¸ë˜í”„ì˜ í•œê³„(?), ì‹œë²¤ì½”ì •ë¦¬, MNIST",
    "section": "A. ì‹œë²¤ì½”ì •ë¦¬ ì†Œê°œ",
    "text": "A. ì‹œë²¤ì½”ì •ë¦¬ ì†Œê°œ\n\n\n\n\n\n\nUniversal Approximation Thm [@cybenko1989approximation]\n\n\n\ní•˜ë‚˜ì˜ ì€ë‹‰ì¸µì„ ê°€ì§€ëŠ” ì•„ë˜ì™€ ê°™ì€ ê¼´ì˜ ë„¤íŠ¸ì›Œí¬ \\(net: {\\bf X}_{n \\times p} \\to {\\bf y}_{n\\times q}\\)ëŠ”\nnet = torch.nn.Sequential(\n    torch.nn.Linear(p,???),\n    torch.nn.Sigmoid(),\n    torch.nn.Linear(???,q)\n)\nëª¨ë“  ë³´ë  ê°€ì¸¡í•¨ìˆ˜ (Borel measurable function)\n\\[f: {\\bf X}_{n \\times p} \\to {\\bf y}_{n\\times q}\\]\në¥¼ ì›í•˜ëŠ” ì •í™•ë„ë¡œ â€œê·¼ì‚¬â€ì‹œí‚¬ ìˆ˜ ìˆë‹¤. ì‰½ê²Œ ë§í•˜ë©´ \\({\\bf X} \\to {\\bf y}\\) ì¸ ì–´ë– í•œ ë³µì¡í•œ ê·œì¹™ë¼ë„ í•˜ë‚˜ì˜ ì€ë‹‰ì¸µì„ ê°€ì§„ ì‹ ê²½ë§ì´ ì›í•˜ëŠ” ì •í™•ë„ë¡œ ê·¼ì‚¬ì‹œí‚¨ë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤. ì˜ˆë¥¼ë“¤ë©´ ì•„ë˜ì™€ ê°™ì€ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆë‹¤.\n\n\\({\\bf X}_{n\\times 2}\\)ëŠ” í† ìµì ìˆ˜, GPA ì´ê³  \\({\\bf y}_{n\\times 1}\\)ëŠ” ì·¨ì—…ì—¬ë¶€ì¼ ê²½ìš° \\({\\bf X} \\to {\\bf y}\\)ì¸ ê·œì¹™ì„ ì‹ ê²½ë§ì€ í•­ìƒ ì°¾ì„ ìˆ˜ ìˆë‹¤.\n\\({\\bf X}_{n \\times p}\\)ëŠ” ì£¼íƒì´ë¯¸ì§€, ì§€ì—­ì •ë³´, ì£¼íƒë©´ì , ì£¼íƒì— ëŒ€í•œ ì„¤ëª… ì´ê³  \\({\\bf y}_{n\\times 1}\\)ëŠ” ì£¼íƒê°€ê²©ì¼ ê²½ìš° \\({\\bf X} \\to {\\bf y}\\)ì¸ ê·œì¹™ì„ ì‹ ê²½ë§ì€ í•­ìƒ ì°¾ì„ ìˆ˜ ìˆë‹¤.\n\nì¦‰ í•˜ë‚˜ì˜ ì€ë‹‰ì¸µì„ ê°€ì§„ ì‹ ê²½ë§ì˜ í‘œí˜„ë ¥ì€ ê±°ì˜ ë¬´í•œëŒ€ë¼ ë³¼ ìˆ˜ ìˆë‹¤.\n\n\n\nğŸ—£ï¸\n\nì‹œë²¤ì½”ê°€ Sigmoidë¡œ ì¦ëª…í–ˆìœ¼ë‚˜ ReLUë¥¼ ë„£ì–´ë„ ìƒê´€ X\nxëŠ” pì˜ ì°¨ì›ì„ ê°–ê³  yëŠ” qì˜ ì°¨ì›ì„ ê°€ì ¸ë„ ë¨ (ê°™ì„ í•„ìš” X)\n???: ì•„ë¬´ ìˆ«ìë¥¼ ë„£ì–´ë„ ìƒê´€ì—†ìœ¼ë‚˜ 2^n ìœ¼ë¡œ ì“°ëŠ”ê²Œ ë©”ëª¨ë¦¬ì— íš¨ìœ¨ì ì´ë¼ê³  ì•Œë ¤ì ¸ ìˆìŒ\në³´ë ê°€ì¸¡í•¨ìˆ˜: ì¼ë°˜ì¸ë“¤ì´ ìƒìƒí•  ìˆ˜ ìˆëŠ” ê±°ì˜ ëª¨ë“  í•¨ìˆ˜\nì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë„ ìˆ«ìë¡œ ë°”ê¿€ ìˆ˜ ìˆìŒ =&gt; Xë¥¼ nxpë¡œ ì •ë¦¬ ê°€ëŠ¥\n\n\n\në³´ë ê°€ì¸¡í•¨ìˆ˜ì— ëŒ€í•œ ì •ì˜ëŠ” ì¸¡ë„ë¡ ì— ëŒ€í•œ ì´í•´ê°€ ìˆì–´ì•¼ ê°€ëŠ¥í•¨. ì¸¡ë„ë¡ ì— ëŒ€í•œ ë‚´ìš©ì´ ê¶ê¸ˆí•˜ë‹¤ë©´ https://guebin.github.io/SS2024/ ì„ ê³µë¶€í•´ë³´ì„¸ìš”"
  },
  {
    "objectID": "posts/04wk-2.html#b.-ì™œ-ê°€ëŠ¥í•œê°€",
    "href": "posts/04wk-2.html#b.-ì™œ-ê°€ëŠ¥í•œê°€",
    "title": "04wk-2: (ì‹ ê²½ë§) â€“ êº½ì¸ê·¸ë˜í”„ì˜ í•œê³„(?), ì‹œë²¤ì½”ì •ë¦¬, MNIST",
    "section": "B. ì™œ ê°€ëŠ¥í•œê°€??",
    "text": "B. ì™œ ê°€ëŠ¥í•œê°€??\n- ì¤€ë¹„\n\nx = torch.linspace(-10,10,200).reshape(-1,1)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=2),\n    torch.nn.Sigmoid(),\n    torch.nn.Linear(in_features=2,out_features=1)\n)\nl1,a1,l2 = net\n\nğŸ—£ï¸ 2ê°œì˜ ì§ì„  -&gt; 2ê°œì˜ ê³¡ì„ (Sigmoid) -&gt; 1ê°œë¡œ í•©ì¹¨\n\nnet\n\nSequential(\n  (0): Linear(in_features=1, out_features=2, bias=True)\n  (1): Sigmoid()\n  (2): Linear(in_features=2, out_features=1, bias=True)\n)\n\n\nğŸ—£ï¸(\n\nplt.plot(x, net[0](x).data)\n\n\n\n\n\n\n\n\n\nplt.plot(x, net[:2](x).data) # Sigmoid ê¹Œì§€\n\n\n\n\n\n\n\n\n\nplt.plot(x, net[:3](x).data) # í•©ì³ì„œ í•˜ë‚˜ì˜ Sigmoid\n\n\n\n\n\n\n\n\n)ğŸ—£ï¸\n# ìƒê°1 â€“ 2ê°œì˜ ì‹œê·¸ëª¨ì´ë“œë¥¼ ìš°ì—°íˆ ì˜ ì¡°í•©í•˜ë©´ í•˜ë‚˜ì˜ ê³„ë‹¨í•¨ìˆ˜ë¥¼ ë§Œë“¤ ìˆ˜ ìˆë‹¤.\nğŸ—£ï¸ ìˆ«ìë¥¼ ì˜ ë•Œë ¤ë§ì¶”ë‹¤ë³´ë©´..\n\nl1.weight.data = torch.tensor([[-5.00],[5.00]])\nl1.bias.data = torch.tensor([+10.00,+10.00])\n\n\nl2.weight.data = torch.tensor([[1.00,1.00]])\nl2.bias.data = torch.tensor([-1.00])\n\n\nfig,ax = plt.subplots(1,3,figsize=(9,3))\nax[0].plot(x,l1(x)[:,[0]].data,label=r\"$-5x+10$\")\nax[0].plot(x,l1(x)[:,[1]].data,label=r\"$5x+10$\")\nax[0].set_title('$l_1(x)$')\nax[0].legend()\nax[1].plot(x,a1(l1(x))[:,[0]].data,label=r\"$v_1=sig(-5x+10)$\")\nax[1].plot(x,a1(l1(x))[:,[1]].data,label=r\"$v_2=sig(5x+10)$\")\nax[1].set_title('$(a_1 \\circ l_1)(x)$')\nax[1].legend()\nax[2].plot(x,l2(a1(l1(x))).data,color='C2',label=r\"$v_1+v_2-1$\")\nax[2].set_title('$(l_2 \\circ a_1 \\circ \\l_1)(x)$')\nax[2].legend()\n\n\n\n\n\n\n\n\n#\n# ìƒê°2 â€“ ê³„ë‹¨í•¨ìˆ˜ì˜ ëª¨ì–‘ì´ ê¼­ ìƒê°1ê³¼ ê°™ì„ í•„ìš”ëŠ” ì—†ë‹¤. ì¤‘ì‹¬ì€ ì´ë™ê°€ëŠ¥í•˜ê³ , ë†’ì´ë„ ì¡°ì ˆê°€ëŠ¥í•˜ë‹¤.\nê°€ëŠ¥í•œ ì˜ˆì‹œ1\n\nl1.weight.data = torch.tensor([[-5.00],[5.00]])\nl1.bias.data = torch.tensor([+0.00,+20.00])\nl2.weight.data = torch.tensor([[1.00,1.00]])\nl2.bias.data = torch.tensor([-1.00])\nfig,ax = plt.subplots(1,3,figsize=(9,3))\nax[0].plot(x,l1(x).data.numpy(),'--',color='C0'); ax[0].set_title('$l_1(x)$')\nax[1].plot(x,a1(l1(x)).data.numpy(),'--',color='C0'); ax[1].set_title('$(a_1 \\circ l_1)(x)$')\nax[2].plot(x,l2(a1(l1(x))).data,'--',color='C0'); ax[2].set_title('$(l_2 \\circ a_1 \\circ \\l_1)(x)$');\nax[2].set_ylim(-0.1,2.6)\n\n\n\n\n\n\n\n\nê°€ëŠ¥í•œ ì˜ˆì‹œ2\n\nl1.weight.data = torch.tensor([[-5.00],[5.00]])\nl1.bias.data = torch.tensor([+20.00,+00.00])\nl2.weight.data = torch.tensor([[2.50,2.50]])\nl2.bias.data = torch.tensor([-2.50])\nfig,ax = plt.subplots(1,3,figsize=(9,3))\nax[0].plot(x,l1(x).data.numpy(),'--',color='C1'); ax[0].set_title('$l_1(x)$')\nax[1].plot(x,a1(l1(x)).data.numpy(),'--',color='C1'); ax[1].set_title('$(a_1 \\circ l_1)(x)$')\nax[2].plot(x,l2(a1(l1(x))).data,'--',color='C1'); ax[2].set_title('$(l_2 \\circ a_1 \\circ \\l_1)(x)$');\nax[2].set_ylim(-0.1,2.6)\n\n\n\n\n\n\n\n\n#\n# ìƒê°3: ì²«ë²ˆì§¸ ì„ í˜•ë³€í™˜(=\\(l_1\\))ì—ì„œ out_features=4ë¡œ í•˜ê³  ì ë‹¹í•œ ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì •í•˜ë©´ \\((l_2\\circ a_1 \\circ l_1)(x)\\)ì˜ ê²°ê³¼ë¡œ ìƒê°2ì˜ ì˜ˆì‹œ1,2ë¥¼ ì¡°í•©í•œ í˜•íƒœë„ ê°€ëŠ¥í•  ê²ƒ ê°™ë‹¤. ì¦‰ 4ê°œì˜ ì‹œê·¸ëª¨ì´ë“œë¥¼ ì˜ ì¡°í•©í•˜ë©´ 2ë‹¨ê³„ ê³„ë‹¨í•¨ìˆ˜ë¥¼ ë§Œë“¤ ìˆ˜ ìˆë‹¤.\n\nl1 = torch.nn.Linear(in_features=1,out_features=4)\na1 = torch.nn.Sigmoid()\nl2 = torch.nn.Linear(in_features=4,out_features=1)\n\n\nl1.weight.data = torch.tensor([[-5.00],[5.00],[-5.00],[5.00]])\nl1.bias.data = torch.tensor([0.00, 20.00, 20.00, 0])\nl2.weight.data = torch.tensor([[1.00,  1.00, 2.50,  2.50]])\nl2.bias.data = torch.tensor([-1.0-2.5])\n\nğŸ—£ï¸ ìˆ«ìë¥¼ ë°”ê¾¸ë©´ ëª¨ì–‘ì´ ë‹¬ë¼ì§\n\nplt.plot(l2(a1(l1(x))).data,'--')\nplt.title(r\"$(l_2 \\circ a_1 \\circ l_1)(x)$\")\n\nText(0.5, 1.0, '$(l_2 \\\\circ a_1 \\\\circ l_1)(x)$')\n\n\n\n\n\n\n\n\n\n\nì´ëŸ¬í•œ í•¨ìˆ˜ëŠ” ê³„ë‹¨ëª¨ì–‘ì´ë©°, 0ì„ ì œì™¸í•œ ì„œë¡œë‹¤ë¥¸ ê³„ë‹¨ì˜ ë†’ì´ëŠ” 2ê°œê°€ ëœë‹¤. ì´ë¥¼ ê°„ë‹¨íˆ â€œ2ë‹¨ê³„-ê³„ë‹¨í•¨ìˆ˜â€ë¼ê³  ì¹­í•˜ì.\n\n#\n# ìƒê°4 â€“ \\(2m\\)ê°œì˜ ì‹œê·¸ëª¨ì´ë“œë¥¼ ìš°ì—°íˆ ì˜ ì¡°í•©í•˜ë©´ \\(m\\)ë‹¨ê³„ ê³„ë‹¨í•¨ìˆ˜ë¥¼ ë§Œë“¤ ìˆ˜ ìˆë‹¤.\n- ì •ë¦¬1: 2ê°œì˜ ì‹œê·¸ëª¨ì´ë“œë¥¼ ìš°ì—°íˆ ì˜ ê²°í•©í•˜ë©´ ì•„ë˜ì™€ ê°™ì€ â€œ1ë‹¨ê³„-ê³„ë‹¨í•¨ìˆ˜â€ í•¨ìˆ˜ \\(h\\)ë¥¼ ë§Œë“¤ ìˆ˜ ìˆë‹¤.\nğŸ—£ï¸(\n\nsig = torch.nn.Sigmoid()\n\n\nê³±í•˜ëŠ” ìˆ«ìê°€ ì»¤ì§ˆìˆ˜ë¡ ê¸‰í•˜ê²Œ ì˜¬ë¼ê°\n\n\nplt.plot(x, sig(0.5*(x-0.5)))\n\n\n\n\n\n\n\n\n\nplt.plot(x, sig(3*(x-0.5)))\n\n\n\n\n\n\n\n\n\nplt.plot(x, sig(200*(x-0.5)))\n\n\n\n\n\n\n\n\n\nplt.plot(x, sig(200*(x-0.5)))\nplt.plot(x, -sig(200*(x+0.5)))\n\n\n\n\n\n\n\n\n\nv1 = sig(200*(x-0.5))\nv2 = -sig(200*(x+0.5))\nplt.plot(x,v1+v2)\n\n\n\n\n\n\n\n\n\nplt.plot(x, -sig(200*(x-0.5)))\nplt.plot(x, sig(200*(x+0.5)))\n\n\n\n\n\n\n\n\n)ğŸ—£ï¸\n\ndef h(x):\n    sig = torch.nn.Sigmoid()\n    v1 = -sig(200*(x-0.5))\n    v2 = sig(200*(x+0.5))\n    return v1+v2 \n\n\nplt.plot(x,h(x))\nplt.title(\"$h(x)$\")\n\nText(0.5, 1.0, '$h(x)$')\n\n\n\n\n\n\n\n\n\n- ì •ë¦¬2: ìœ„ì™€ ê°™ì€ í•¨ìˆ˜ \\(h\\)ë¥¼ ì´ìš©í•œ ì•„ë˜ì˜ ë„¤íŠ¸ì›Œí¬ë¥¼ ê³ ë ¤í•˜ì. ì´ëŠ” â€œmë‹¨ê³„-ê³„ë‹¨í•¨ìˆ˜â€ë¥¼ ë§Œë“ ë‹¤.\n\\[\\underset{(n,1)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,m)}{\\boldsymbol u^{(1)}} \\overset{h}{\\to} \\underset{(n,m)}{\\boldsymbol v^{(1)}} \\overset{l_2}{\\to} \\underset{(n,1)}{\\hat{\\boldsymbol y}}\\]\nê·¸ë¦¬ê³  ìœ„ì˜ ë„¤íŠ¸ì›Œí¬ì™€ ë™ì¼í•œ íš¨ê³¼ë¥¼ ì£¼ëŠ” ì•„ë˜ì˜ ë„¤íŠ¸ì›Œí¬ê°€ í•­ìƒ ì¡´ì¬í•¨.\nğŸ—£ï¸ 2ê°œì˜ Sigmoidë¥¼ ê°ê° ì·¨í•¨\n\\[\\underset{(n,1)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,2m)}{\\boldsymbol u^{(1)}} \\overset{sig}{\\to} \\underset{(n,2m)}{\\boldsymbol v^{(1)}} \\overset{l_2}{\\to} \\underset{(n,1)}{\\hat{\\boldsymbol y}}\\]\n#\n# ìƒê°5 â€“ ê·¸ëŸ°ë° ì–´ì§€ê°„í•œ í•¨ìˆ˜í˜•íƒœëŠ” êµ¬ë¶ˆêµ¬ë¶ˆí•œ â€œmë‹¨ê³„-ê³„ë‹¨í•¨ìˆ˜â€ë¡œ ë‹¤ ê·¼ì‚¬í•  ìˆ˜ ìˆì§€ ì•Šë‚˜?\nê·¸ë ‡ë‹¤ë©´ ì•„ë˜ì˜ ë„¤íŠ¸ì›Œí¬ì—ì„œ (1) ?? ë¥¼ ì¶©ë¶„íˆ í‚¤ìš°ê³  (2) ì ì ˆí•˜ê²Œ í•™ìŠµë§Œ ì˜ ëœë‹¤ë©´\nnet = torch.nn.Sequential(\n    torch.nn.Linear(p,???),\n    torch.nn.Sigmoid(),\n    torch.nn.Linear(???,q)\n)\nìœ„ì˜ ë„¤íŠ¸ì›Œí¬ëŠ” ê±°ì˜ ë¬´í•œí•œ í‘œí˜„ë ¥ì„ ê°€ì§„ë‹¤. â€“&gt; ì´ëŸ°ì‹ìœ¼ë¡œ ì¦ëª…í•˜ë©´ ë©ë‹ˆë‹¹\n#"
  },
  {
    "objectID": "posts/04wk-2.html#c.-hì˜-ìœ„ë ¥",
    "href": "posts/04wk-2.html#c.-hì˜-ìœ„ë ¥",
    "title": "04wk-2: (ì‹ ê²½ë§) â€“ êº½ì¸ê·¸ë˜í”„ì˜ í•œê³„(?), ì‹œë²¤ì½”ì •ë¦¬, MNIST",
    "section": "C. \\(h\\)ì˜ ìœ„ë ¥",
    "text": "C. \\(h\\)ì˜ ìœ„ë ¥\nğŸ—£ï¸ Sigmoid ëŒ€ì‹  hë¥¼ í•˜ê³  ì‹¶ìŒ\n- ì†Œë§: ì•„ë˜ì™€ ê°™ì´ netì„ ì„¤ê³„í•´ì„œ, ê·¸ ìœ„ë ¥ì„ ì²´ê°í•´ë³´ê³  ì‹¶ì€ë°..\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,??),\n    torch.nn.H(),\n    torch.nn.Linear(??,1)\n)\n- \\(h(x)\\)ë¥¼ ìƒì„±í•˜ëŠ” í´ë˜ìŠ¤ë¥¼ ë§Œë“¤ì–´ë³´ì.\nğŸ—£ï¸ Module: ìƒì† / ì˜ ëª¨ë¥´ê² ìœ¼ë©´ ë‹¤ìŒì„ templeteìœ¼ë¡œ ìƒê°í•˜ê³  ì™¸ìš°ê¸°\nclass H(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self,x):\n        # out = h(x)\n        return out\nğŸ—£ï¸(\n\nclass H(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self,x):\n        def h(x):\n            sig = torch.nn.Sigmoid()\n            v1 = -sig(200*(x-0.5))\n            v2 = sig(200*(x+0.5))\n            return v1+v2 \n        out = h(x)\n        return out \n\n\nmy_h = H()\n\n\nplt.plot(x, my_h(x))\n\n\n\n\n\n\n\n\n)ğŸ—£ï¸\n\nclass H(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self,x):\n        def h(x):\n            sig = torch.nn.Sigmoid()\n            v1 = -sig(200*(x-0.5))\n            v2 = sig(200*(x+0.5))\n            return v1+v2 \n        out = h(x)\n        return out \n\n\nh = H()\n\n- \\(h\\)ì˜ ìœ„ë ¥ì„ ì²´ê°í•´ë³´ì.\n# ì˜ˆì œ1 â€“ ìŠ¤í™ì˜ ì—­ì„¤\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DL2025/main/posts/ironyofspec.csv\")\nx = torch.tensor(df.x).float().reshape(-1,1)\ny = torch.tensor(df.y).float().reshape(-1,1)\nprob = torch.tensor(df.prob).float().reshape(-1,1)\n\nğŸ—£ï¸(\n\në‹¤ìŒì„ ì í•©ì‹œí‚¤ë ¤ê³  í•¨\n\n\nplt.plot(x,prob)\n\n\n\n\n\n\n\n\n\nnetwork ë§Œë“  ì´ì „ ë°©ì‹\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,2,bias=False),\n    torch.nn.ReLu(),\n    torch.nn.Linear(2,1),\n    torch.Sigmoid()\n)\n\nì´ì œ ì´ë ‡ê²Œ í•˜ì§€ ì•Šê³  ë‹¤ìŒê³¼ ê°™ì´ í•˜ë ¤ê³  í•¨\n\n)ğŸ—£ï¸\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,2048),\n    H(),\n    torch.nn.Linear(2048,1),\n    torch.nn.Sigmoid()\n)\nloss_fn = torch.nn.BCELoss()\noptimizr = torch.optim.Adam(net.parameters())\n#---#\nfor epoc in range(200):\n    ## 1 \n    yhat = net(x)\n    ## 2\n    loss = loss_fn(yhat,y)\n    ## 3\n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,prob)\nplt.plot(x,net(x).data,'--')\n\n\n\n\n\n\n\n\nğŸ—£ï¸ ì í•©ëœ ê²ƒì„ ë³´ë©´ ì•ˆ ë§ê¸°ëŠ” í•˜ë‚˜ ë”°ë¼ê°€ê³ ëŠ” ìˆìŒ\n#\n# ì˜ˆì œ2 â€“ ìˆ˜ëŠ¥ê³¡ì„ \n\ntorch.manual_seed(43052)\nx = torch.linspace(0,2,2000).reshape(-1,1)\neps = torch.randn(2000).reshape(-1,1)*0.05\nfx = torch.exp(-1*x)* torch.abs(torch.cos(3*x))*(torch.sin(3*x))\ny = fx + eps\n\n\nplt.plot(x,y,alpha=0.5)\nplt.plot(x,fx)\n\n\n\n\n\n\n\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,2048),\n    H(),\n    torch.nn.Linear(2048,1)\n)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n#---#\nfor epoc in range(200):\n    ## 1 \n    yhat = net(x)\n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\nğŸ—£ï¸ Sigmoidë¥¼ í•  í•„ìš”ëŠ” X\n\nplt.plot(x,y,alpha=0.5)\nplt.plot(x,fx)\nplt.plot(x,net(x).data,'--')\n\n\n\n\n\n\n\n\n#"
  },
  {
    "objectID": "posts/04wk-2.html#d.-ì˜ë¬¸ì ",
    "href": "posts/04wk-2.html#d.-ì˜ë¬¸ì ",
    "title": "04wk-2: (ì‹ ê²½ë§) â€“ êº½ì¸ê·¸ë˜í”„ì˜ í•œê³„(?), ì‹œë²¤ì½”ì •ë¦¬, MNIST",
    "section": "D. ì˜ë¬¸ì ",
    "text": "D. ì˜ë¬¸ì \nğŸ—£ï¸ ë°˜ë°•ì€ ë‹¤ìŒ ì‹œê°„\n- ì´ ìˆ˜ì—…ì„ ì˜ ì´í•´í•œ ì‚¬ëŒ: ê·¸ëƒ¥ í™œì„±í™”í•¨ìˆ˜ë¥¼ \\(h\\)ë¡œ ì“°ë©´ ë ì•„ë‹ˆì•¼? ë­í•˜ëŸ¬ relu ë¥¼ ì“°ëŠ”ê±°ì§€?\n- ë”¥ëŸ¬ë‹ì„ ì¢€ ê³µë¶€í•´ë³¸ì‚¬ëŒ1: ì™œ ë”¥ëŸ¬ë‹ì´ 2010ë…„ì´ ì§€ë‚˜ì„œì•¼ ë–³ì§€? 1989ë…„ì— ì„¸ìƒì˜ ëª¨ë“  ë¬¸ì œê°€ í’€ë ¤ì•¼ í•˜ëŠ”ê²ƒ ì•„ë‹Œê°€?\n- ë”¥ëŸ¬ë‹ì„ ì¢€ ê³µë¶€í•´ë³¸ì‚¬ëŒ2: í•˜ë‚˜ì˜ ì€ë‹‰ì¸µì„ ê°€ì§„ ë„¤í¬ì›Œí¬ëŠ” ì˜ ì•ˆì“°ì§€ ì•Šë‚˜? ì€ë‹‰ì¸µì´ ê¹Šì„ìˆ˜ë¡ ì¢‹ë‹¤ê³  ë“¤ì—ˆëŠ”ë°?\n- ì•½ê°„ì˜ ì˜êµ¬ì‹¬ì´ ìˆì§€ë§Œ ì•„ë¬´íŠ¼ ìš°ë¦¬ëŠ” ì•„ë˜ì˜ ë¬´ê¸°ë¥¼ ê°€ì§„ ê¼´ì´ ë˜ì—ˆë‹¤.\n\n\n\n\n\n\nìš°ë¦¬ì˜ ë¬´ê¸°\n\n\n\ní•˜ë‚˜ì˜ ì€ë‹‰ì¸µì„ ê°€ì§€ëŠ” ì•„ë˜ì™€ ê°™ì€ ê¼´ì˜ ë„¤íŠ¸ì›Œí¬ë¡œ,\nnet = torch.nn.Sequential(\n    torch.nn.Linear(p,???),\n    torch.nn.Sigmoid(),\n    torch.nn.Linear(???,q)\n)\n\\(f: {\\bf X}_{n \\times p} \\to {\\bf y}_{n\\times q}\\) ì¸ ëª¨ë“  ë³´ë  ê°€ì¸¡ í•¨ìˆ˜ \\(f\\) ì„ ì›í•˜ëŠ” ì •í™•ë„ë¡œ â€œê·¼ì‚¬â€ì‹œí‚¬ ìˆ˜ ìˆë‹¤."
  },
  {
    "objectID": "posts/04wk-2.html#a.-ì˜ˆë¹„í•™ìŠµ-plt.imshow",
    "href": "posts/04wk-2.html#a.-ì˜ˆë¹„í•™ìŠµ-plt.imshow",
    "title": "04wk-2: (ì‹ ê²½ë§) â€“ êº½ì¸ê·¸ë˜í”„ì˜ í•œê³„(?), ì‹œë²¤ì½”ì •ë¦¬, MNIST",
    "section": "A. ì˜ˆë¹„í•™ìŠµ â€“ plt.imshow()",
    "text": "A. ì˜ˆë¹„í•™ìŠµ â€“ plt.imshow()\n- plt.imshow(..., cmap=\"gray\") ì—ì„œ ...ì´ shapeì´ (??,??)ì´ë©´ í‘ë°±ì´ë¯¸ì§€ë¥¼ ì¶œë ¥\nğŸ—£ï¸(\n\nimg = torch.tensor([[255,100],\n                    [255,0]])\nplt.imshow(img)\n\n\n\n\n\n\n\n\n\nimg.shape # 2x2 í”½ì…€\n\ntorch.Size([2, 2])\n\n\n)ğŸ—£ï¸\n\nimg = torch.tensor([[255,100],\n                    [255,0]])\nplt.imshow(img,cmap=\"gray\")\n\n\n\n\n\n\n\n\nğŸ—£ï¸ ìˆ«ìê°€ í´ìˆ˜ë¡ í°ìƒ‰, ì‘ì„ìˆ˜ë¡ ê²€ì •ìƒ‰\n- plt.imshow(...) ì—ì„œ ...ì˜ shapeì´ (??,??,3)ì´ë©´ ì¹¼ë¼ì´ë¯¸ì§€ë¥¼ ì¶œë ¥\nğŸ—£ï¸(\n\nr = torch.tensor([[255,0],\n                  [255,0]])\ng = r*0\nb = r*0\ng\n\ntensor([[0, 0],\n        [0, 0]])\n\n\n\nr.shape\n\ntorch.Size([2, 2])\n\n\n\ntorch.stack([r,g,b],axis=-1)\n\ntensor([[[255,   0,   0],\n         [  0,   0,   0]],\n\n        [[255,   0,   0],\n         [  0,   0,   0]]])\n\n\n\ntorch.stack([r,g,b],axis=-1).shape\n\ntorch.Size([2, 2, 3])\n\n\n)ğŸ—£ï¸\n\nr = torch.tensor([[255,0],\n                  [255,0]])\ng = torch.tensor([[0,255],\n                  [0,0]])\nb = torch.tensor([[0,0],\n                  [0,255]])\nimg = torch.stack([r,g,b],axis=-1)\nplt.imshow(img)\n\n\n\n\n\n\n\n\nğŸ”¬\n\nimg\n\ntensor([[[255,   0,   0],\n         [  0, 255,   0]],\n\n        [[255,   0,   0],\n         [  0,   0, 255]]])\n\n\n- plt.imshow(...) ì—ì„œ ...ì˜ ìë£Œí˜•ì´ intì¸ì§€ floatì¸ì§€ì— ë”°ë¼ì„œ ì¸ì‹ì´ ë‹¤ë¦„\nğŸ—£ï¸ int: maxë¥¼ 255ë¡œ ê·¸ë¦¼, float: maxë¥¼ 1ë¡œ ê·¸ë¦¼\n\nr = torch.tensor([[1,0],\n                  [1,0]])\ng = torch.tensor([[0,1],\n                  [0,0]])\nb = torch.tensor([[0,0],\n                  [0,1]])\nimg = torch.stack([r,g,b],axis=-1)\nplt.imshow(img)\n\n\n\n\n\n\n\n\n\nimg[0]\n\ntensor([[1, 0, 0],\n        [0, 1, 0]])\n\n\n\nr = torch.tensor([[255,0],\n                  [255,0]])/255\ng = torch.tensor([[0,255],\n                  [0,0]])/255\nb = torch.tensor([[0,0],\n                  [0,255]])/255\nimg = torch.stack([r,g,b],axis=-1)\nplt.imshow(img)\n\n\n\n\n\n\n\n\n\nimg[0]\n\ntensor([[1., 0., 0.],\n        [0., 1., 0.]])"
  },
  {
    "objectID": "posts/04wk-2.html#b.-ë°ì´í„°",
    "href": "posts/04wk-2.html#b.-ë°ì´í„°",
    "title": "04wk-2: (ì‹ ê²½ë§) â€“ êº½ì¸ê·¸ë˜í”„ì˜ í•œê³„(?), ì‹œë²¤ì½”ì •ë¦¬, MNIST",
    "section": "B. ë°ì´í„°",
    "text": "B. ë°ì´í„°\n- ë°ì´í„° ì •ë¦¬ì½”ë“œ\n\ntrain_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True)\nto_tensor = torchvision.transforms.ToTensor()\nX3 = torch.stack([to_tensor(Xi) for Xi, yi in train_dataset if yi==3])\nX7 = torch.stack([to_tensor(Xi) for Xi, yi in train_dataset if yi==7])\nX = torch.concat([X3,X7],axis=0)\ny = torch.tensor([0.0]*len(X3) + [1.0]*len(X7))\n\n100.0%\n100.0%\n100.0%\n100.0%\n\n\nğŸ—£ï¸(\n\nX.shape # 4ì°¨ì›\n\ntorch.Size([12396, 1, 28, 28])\n\n\n\nX[0].shape\n\ntorch.Size([1, 28, 28])\n\n\n\nX[0][0].shape\n\ntorch.Size([28, 28])\n\n\n\nplt.imshow(X[0][0],cmap=\"gray\")\n\n\n\n\n\n\n\n\n\nplt.imshow(X[0].reshape(28,28),cmap=\"gray\")\n\n\n\n\n\n\n\n\n\nplt.imshow(X[1].reshape(28,28),cmap=\"gray\")\n\n\n\n\n\n\n\n\n\nplt.imshow(X[-1].reshape(28,28),cmap=\"gray\") # ëì— ìˆëŠ” ê´€ì¸¡ì¹˜\n\n\n\n\n\n\n\n\n\nplt.imshow(X[-2].reshape(28,28),cmap=\"gray\") # ëì— ìˆëŠ” ê´€ì¸¡ì¹˜\n\n\n\n\n\n\n\n\n\ny\n\ntensor([0., 0., 0.,  ..., 1., 1., 1.])\n\n\n\n0ì€ 3ì˜ ì´ë¯¸ì§€, 1ì€ 7ì˜ ì´ë¯¸ì§€\n\n\nlen(y)\n\n12396\n\n\n)ğŸ—£ï¸\n\nplt.plot(y,'.')\n\n\n\n\n\n\n\n\n- ìš°ë¦¬ëŠ” \\({\\bf X}: (n,1,28,28)\\) ì—ì„œ \\({\\bf y}: (n,1)\\)ìœ¼ë¡œ ê°€ëŠ” ë§µí•‘ì„ ë°°ìš°ê³  ì‹¶ìŒ. \\(\\to\\) ì´ëŸ°ê±´ ë°°ìš´ì ì´ ì—†ëŠ”ë°?.. \\(\\to\\) ê·¸ë ‡ë‹¤ë©´ \\({\\bf X}:(n,784) \\to {\\bf y}:(n,1)\\) ìœ¼ë¡œ ê°€ëŠ” ë§µí•‘ì„ í•™ìŠµí•˜ì.\n)ğŸ—£ï¸\n\n28*28\n\n784\n\n\n[img for img in X] = [X[0], X[1], ..., X[-1]]\n\nX[0].shape\n\ntorch.Size([1, 28, 28])\n\n\n\ntorch.stack([img.reshape(-1) for img in X])\n\ntensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]])\n\n\n\ntorch.stack([img.reshape(-1) for img in X]).shape\n\ntorch.Size([12396, 784])\n\n\n\ny.shape # vector\n\ntorch.Size([12396])\n\n\n)ğŸ—£ï¸\n\nX = torch.stack([img.reshape(-1) for img in X])\ny = y.reshape(-1,1)\n\n\nX.shape,y.shape\n\n(torch.Size([12396, 784]), torch.Size([12396, 1]))"
  },
  {
    "objectID": "posts/04wk-2.html#c.-í•™ìŠµ",
    "href": "posts/04wk-2.html#c.-í•™ìŠµ",
    "title": "04wk-2: (ì‹ ê²½ë§) â€“ êº½ì¸ê·¸ë˜í”„ì˜ í•œê³„(?), ì‹œë²¤ì½”ì •ë¦¬, MNIST",
    "section": "C. í•™ìŠµ",
    "text": "C. í•™ìŠµ\nğŸ—£ï¸ Hê°€ ë” ì¢‹ì€ ê²ƒì„ ì•Œê³  ìˆì§€ë§Œ ì‚¬ëŒë“¤ì´ ë§ì´ ì“°ëŠ” ReLUë¡œ\nğŸ—£ï¸(\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(784,32),\n    torch.nn.ReLU(),\n    torch.nn.Linear(32,1),\n    torch.nn.Sigmoid()\n)\n\n\nnet(X)\n\ntensor([[0.5066],\n        [0.5152],\n        [0.4821],\n        ...,\n        [0.5168],\n        [0.5087],\n        [0.5066]], grad_fn=&lt;SigmoidBackward0&gt;)\n\n\n\nyì™€ ë¹„ìŠ·í•œ í˜•íƒœë¡œ ì¶œë ¥ë˜ëŠ” ê²ƒì´ ì¤‘ìš”\n\n)ğŸ—£ï¸\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(784,32),\n    torch.nn.ReLU(),\n    torch.nn.Linear(32,1),\n    torch.nn.Sigmoid()\n)\nloss_fn = torch.nn.BCELoss()\noptimizr = torch.optim.Adam(net.parameters())\n#---#\nfor epoc in range(200):\n    ## 1 \n    yhat = net(X) \n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3\n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(y,'.')\nplt.plot(net(X).data,'.',alpha=0.2)\n\n\n\n\n\n\n\n\nğŸ—£ï¸ í‹€ë¦° ê²ƒë„ ìˆì§€ë§Œ ë§ì€ ê²ƒì´ ë” ë§ìŒ (2,000ë²ˆí•˜ë©´ ë” ë§ì´ ë§ì¶¤)\nğŸ—£ï¸(Accuracy\n\nRule ì •í•˜ê¸°\n\n\nnet(X).data &gt; 0.5\n\ntensor([[False],\n        [False],\n        [False],\n        ...,\n        [ True],\n        [ True],\n        [ True]])\n\n\n\n(net(X).data &gt; 0.5)*1.0\n\ntensor([[0.],\n        [0.],\n        [0.],\n        ...,\n        [1.],\n        [1.],\n        [1.]])\n\n\n\ny\n\ntensor([[0.],\n        [0.],\n        [0.],\n        ...,\n        [1.],\n        [1.],\n        [1.]])\n\n\n\n(y == (net(X).data &gt; 0.5)*1.0)\n\ntensor([[True],\n        [True],\n        [True],\n        ...,\n        [True],\n        [True],\n        [True]])\n\n\n\n(y == (net(X).data &gt; 0.5)*1.0).sum()\n\ntensor(12264)\n\n\n\nlen(y)\n\n12396\n\n\n\n12264/12396\n\n0.989351403678606\n\n\n\n((y == (net(X).data &gt; 0.5))*1.0).mean()\n\ntensor(0.9894)\n\n\n)ğŸ—£ï¸\n\n((y == (net(X).data &gt; 0.5))*1.0).mean()\n\ntensor(0.9894)\n\n\n\n\n\n\n\n\nNote\n\n\n\nì´ë¯¸ì§€ìë£Œì˜ ì°¨ì›\n\nì¹¼ë¼ì´ë¯¸ì§€ë°ì´í„° \\({\\bf X}\\)ëŠ” (n,3,h,w) ì˜ ì°¨ì›ì„ ê°€ì§€ê±°ë‚˜ (n,h,w,3)ì˜ ì°¨ì›ì„ ê°€ì§„ë‹¤.\ní‘ë°±ì´ë¯¸ì§€ë°ì´í„° \\({\\bf X}\\)ëŠ” (n,h,w) ì˜ ì°¨ì›ì„ ê°€ì§€ê±°ë‚˜ (n,1,h,w)ì˜ ì°¨ì›ì„ ê°€ì§€ê±°ë‚˜ (n,h,w,1)ì˜ ì°¨ì›ì„ ê°€ì§„ë‹¤."
  },
  {
    "objectID": "posts/04wk-1.html",
    "href": "posts/04wk-1.html",
    "title": "04wk-1: (ì‹ ê²½ë§) â€“ ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„ ê·¹ë³µ",
    "section": "",
    "text": "ğŸ“˜ Note Format Guide\nThis format serves as a structured guide for organizing lecture content, personal interpretation, experiments, and study-related questions.\n\n\n\n\n\n\n\n\nType\nWhat It Means\nWhen I Use It\n\n\n\n\nğŸ“ Lecture\nOriginal material from the professorâ€™s notes\nWhen Iâ€™m referencing core concepts or provided code\n\n\nğŸ—£ï¸ In-Class Note\nVerbal explanations shared during the lecture\nWhen I want to record something the professor said in class but didnâ€™t include in the official notes\n\n\nâœï¸ My Note\nMy thoughts, interpretations, or additional explanations\nWhen I reflect on or explain something in my own words\n\n\nğŸ”¬ Experiment\nCode I tried out or changed to explore further\nWhen I test variations or go beyond the original example\n\n\nâ“ Question\nQuestions I had while studying\nWhen I want to revisit or research something more deeply\n\n\n\nğŸ“ ğŸ—£ï¸ âœï¸ ğŸ”¬ â“\n\n1. ê°•ì˜ë…¸íŠ¸ ì›ë³¸ ë° ì˜ìƒ ë§í¬\nhttps://guebin.github.io/DL2025/posts/04wk-1.html\n\n\n2. Imports ğŸ“\n\nimport torch\nimport matplotlib.pyplot as plt \nimport pandas as pd\n\n\nplt.rcParams['figure.figsize'] = (4.5, 3.0)\n\n\n\n3. êº½ì¸ì§ì„ ì„ ë§Œë“œëŠ” ë°©ë²• ğŸ“\nì§€ë‚œì‹œê°„ë³µìŠµ\n\n# ì˜¤ëŠ˜ì˜ ì”ì†Œë¦¬.. \n## íšŒê·€(ì¹´í˜ì˜ˆì œ): yhat=ì§ì„ =linr(x), ì •ê·œë¶„í¬, MSEloss\n## ë¡œì§€ìŠ¤í‹±(ìŠ¤í™ê³¼ì·¨ì—…): yhat=ê³¡ì„ =sig(ì§ì„ )=sig(linr(x)), ë² ë¥´ëˆ„ì´, BCELoss\n## ì´ë¦„ì—†ìŒ(ìŠ¤í™ì˜ì—­ì„¤): yhat=êº½ì¸ê³¡ì„ =sig(êº½ì¸ì§ì„ )=sig(??), ë² ë¥´ëˆ„ì´, BCELOss\n\n- ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ì„œëŠ” ì‹œê·¸ëª¨ì´ë“œë¥¼ ì·¨í•˜ê¸° ì „ì— êº½ì¸ ê·¸ë˜í”„ ëª¨ì–‘ì„ ë§Œë“œëŠ” ê¸°ìˆ ì´ ìˆì–´ì•¼ê² ìŒ.\n- ì•„ë˜ì™€ ê°™ì€ ë²¡í„° \\({\\bf x}\\)ë¥¼ ê°€ì •í•˜ì.\n\nx = torch.linspace(-1,1,1001).reshape(-1,1)\nx\n\ntensor([[-1.0000],\n        [-0.9980],\n        [-0.9960],\n        ...,\n        [ 0.9960],\n        [ 0.9980],\n        [ 1.0000]])\n\n\n- ëª©í‘œ: ì•„ë˜ì™€ ê°™ì€ ë²¡í„° \\({\\bf y}\\)ë¥¼ ë§Œë“¤ì–´ë³´ì.\n\\[{\\bf y} = [y_1,y_2,\\dots,y_{n}]^\\top, \\quad y_i = \\begin{cases} 9x_i +4.5& x_i &lt;0 \\\\ -4.5x_i + 4.5& x_i &gt;0 \\end{cases}\\]\n\n\n\n\n\n\nCaution\n\n\n\nì¼ë°˜ì ìœ¼ë¡œ ì œ ê°•ì˜ë…¸íŠ¸ì—ì„œ\n\në…ë¦½ë³€ìˆ˜ = ì„¤ëª…ë³€ìˆ˜ = \\({\\bf x}\\), \\({\\bf X}\\)\nì¢…ì†ë³€ìˆ˜ = ë°˜ì‘ë³€ìˆ˜ = \\({\\bf y}\\)\n\në¥¼ ì˜ë¯¸í•˜ëŠ”ë°ìš”, ì—¬ê¸°ì—ì„œ \\(({\\bf x},{\\bf y})\\) ëŠ” (ë…ë¦½ë³€ìˆ˜,ì¢…ì†ë³€ìˆ˜) í˜¹ì€ (ì„¤ëª…ë³€ìˆ˜,ë°˜ì‘ë³€ìˆ˜) ë¥¼ ì˜ë¯¸í•˜ëŠ”ê²Œ ì•„ë‹™ë‹ˆë‹¤.\n\n\n# ë°©ë²•1 â€“ ìˆ˜ì‹ ê·¸ëŒ€ë¡œ êµ¬í˜„\nğŸ—£ï¸(\n\nplt.plot(x,x,color=\"red\")\nplt.plot(x,9*x+4.5,color=\"blue\")\nplt.plot(x,-4.5*x+4.5,color=\"orange\")\n\n\n\n\n\n\n\n\n\n# (9*x+4.5)[x&lt;0]\n\n\nlen(9*x+4.5)\n\n1001\n\n\n\nlen((9*x+4.5)[x&lt;0])\n\n501\n\n\n)ğŸ—£ï¸\n\nplt.plot(x,9*x+4.5,color=\"blue\",alpha=0.1)\nplt.plot(x[x&lt;0], (9*x+4.5)[x&lt;0],color=\"blue\")\nplt.plot(x,-4.5*x+4.5,color=\"orange\",alpha=0.1)\nplt.plot(x[x&gt;0], (-4.5*x+4.5)[x&gt;0],color=\"orange\")\n\n\n\n\n\n\n\n\n\ny = x*0\ny[x&lt;0] = (9*x+4.5)[x&lt;0]\ny[x&gt;0] = (-4.5*x+4.5)[x&gt;0]\nplt.plot(x,y)\n\n\n\n\n\n\n\n\n#\n# ë°©ë²•2 â€“ ë ë£¨ì´ìš©\nğŸ—£ï¸(\n\nrelu = torch.nn.ReLU()\nplt.plot(x,x,color=\"red\")\nplt.plot(x,relu(x),color=\"blue\")\n\n\n\n\n\n\n\n\n\nxê°€ 0ë³´ë‹¤ ì‘ìœ¼ë©´ yë¥¼ 0ìœ¼ë¡œ ë§Œë“¦\n\n\nrelu = torch.nn.ReLU()\nplt.plot(x,x,color=\"red\")\nplt.plot(x,relu(-x),color=\"blue\")\n\n\n\n\n\n\n\n\n\nyì¶• ëŒ€ì¹­\n\n\nrelu = torch.nn.ReLU()\nplt.plot(x,relu(x),color=\"red\")\nplt.plot(x,relu(-x),color=\"blue\")\n\n\n\n\n\n\n\n\n\nrelu = torch.nn.ReLU()\nplt.plot(x,-relu(x),color=\"red\")\nplt.plot(x,relu(-x),color=\"blue\")\n\n\n\n\n\n\n\n\n\nrelu = torch.nn.ReLU()\nplt.plot(x,-relu(x),color=\"red\")\nplt.plot(x,-relu(-x),color=\"blue\")\n\n\n\n\n\n\n\n\n\níŒŒë€ìƒ‰ì˜ ê¸°ìš¸ê¸°ë¥¼ 9, ë¹¨ê°„ìƒ‰ì˜ ê¸°ìš¸ê¸°ë¥¼ 4.5ë¡œ ë§Œë“¤ë©´\n\n\nrelu = torch.nn.ReLU()\nplt.plot(x,-4.5*relu(x),color=\"red\")\nplt.plot(x,-9*relu(-x),color=\"blue\")\n\n\n\n\n\n\n\n\n\nyì ˆí¸ì´ 4.5ì´ë¯€ë¡œ\n\n\nrelu = torch.nn.ReLU()\n# plt.plot(x,-4.5*relu(x),color=\"red\")\n# plt.plot(x,-9*relu(-x),color=\"blue\")\ny = -4.5*relu(x) + -9*relu(-x)\nplt.plot(x,y)\n\n\n\n\n\n\n\n\n\nrelu = torch.nn.ReLU()\n# plt.plot(x,-4.5*relu(x),color=\"red\")\n# plt.plot(x,-9*relu(-x),color=\"blue\")\ny = -4.5*relu(x) + -9*relu(-x) + 4.5\nplt.plot(x,y)\n\n\n\n\n\n\n\n\n)ğŸ—£ï¸\n\nrelu = torch.nn.ReLU()\n#plt.plot(x,-4.5*relu(x),color=\"red\")\n#plt.plot(x,-9*relu(-x),color=\"blue\")\ny = -4.5*relu(x) + -9*relu(-x) + 4.5\nplt.plot(x,y)\n\n\n\n\n\n\n\n\n- ì¢€ ë” ì¤‘ê°„ê³¼ì •ì„ ì‹œê°í™” â€“ (ê°•ì˜ë•Œ ì„¤ëª…ì•ˆí–ˆìŒ)\n\nfig = plt.figure(figsize=(6, 4))\nspec = fig.add_gridspec(4, 3)\nax1 = fig.add_subplot(spec[:2,0]); ax1.set_title(r'$x$'); ax1.set_ylim(-1,1)\nax2 = fig.add_subplot(spec[2:,0]); ax2.set_title(r'$-x$'); ax2.set_ylim(-1,1)\nax3 = fig.add_subplot(spec[:2,1]); ax3.set_title(r'$relu(x)$'); ax3.set_ylim(-1,1)\nax4 = fig.add_subplot(spec[2:,1]); ax4.set_title(r'$relu(-x)$'); ax4.set_ylim(-1,1)\nax5 = fig.add_subplot(spec[1:3,2]); ax5.set_title(r'$-4.5 relu(x)-9 relu(-x)+4.5$')\n#---#\nax1.plot(x,'--',color='C0')\nax2.plot(-x,'--',color='C1')\nax3.plot(relu(x),'--',color='C0')\nax4.plot(relu(-x),'--',color='C1')\nax5.plot(-4.5*relu(x)-9*relu(-x)+4.5,'--',color='C2')\nfig.tight_layout()\n\n\n\n\n\n\n\n\n#\n# ë°©ë²•3 â€“ reluì˜ ë¸Œë¡œë“œìºìŠ¤íŒ… í™œìš©\nğŸ—£ï¸(\n\ntorch.tensor([[1,2],[2,3],[4,-4]]) \n\ntensor([[ 1,  2],\n        [ 2,  3],\n        [ 4, -4]])\n\n\n\nplt.plot(torch.tensor([[1,2],[2,3],[4,-4]]), '--o') \n\n\n\n\n\n\n\n\n\ncolumnë³„ë¡œ plotì´ ë¨\n\n\ntorch.concat([x,-x], axis=1)\n\ntensor([[-1.0000,  1.0000],\n        [-0.9980,  0.9980],\n        [-0.9960,  0.9960],\n        ...,\n        [ 0.9960, -0.9960],\n        [ 0.9980, -0.9980],\n        [ 1.0000, -1.0000]])\n\n\n\nplt.plot(torch.concat([x,-x], axis=1))\n\n\n\n\n\n\n\n\n\nì—¬ê¸°ì„œ reluë¥¼ í•˜ë©´? reluëŠ” column wiseí•˜ê²Œ ë¸Œë¡œë“œìºìŠ¤íŒ… ë¨\n\n\nplt.plot(relu(torch.concat([x,-x], axis=1)))\n\n\n\n\n\n\n\n\n\nu = torch.concat([x,-x], axis=1)\nv = relu(u)\nplt.plot(v)\n\n\n\n\n\n\n\n\n\nu = torch.concat([x,-x], axis=1)\nv = relu(u)\nv[:,[0]] # ì²«ë²ˆì§¸ ì—´\n\ntensor([[0.0000],\n        [0.0000],\n        [0.0000],\n        ...,\n        [0.9960],\n        [0.9980],\n        [1.0000]])\n\n\n)ğŸ—£ï¸\n- ìš°ë¦¬ê°€ í•˜ê³  ì‹¶ì€ ê²ƒ\n\n# y = -4.5*relu(x) + -9*relu(-x) + 4.5\n\n- ì•„ë˜ì™€ ê°™ì€ ì•„ì´ë””ì–´ë¡œ yë¥¼ ê³„ì‚°í•´ë„ ëœë‹¤.\n\nx, relu ì¤€ë¹„\nu = [x -x]\nv = relu(u) = [relu(x), relu(-x)] = [v1 v2]\ny = -4.5*v1 + -9*v2 + 4.5\n\n\nu = torch.concat([x,-x],axis=1)\nv = relu(u)\nv1 = v[:,[0]]\nv2 = v[:,[1]]\ny = -4.5*v1 -9*v2 + 4.5 \nplt.plot(x,y)\n\n\n\n\n\n\n\n\nğŸ—£ï¸(\n\nBonus\n\n\nv # nx2\n\ntensor([[0.0000, 1.0000],\n        [0.0000, 0.9980],\n        [0.0000, 0.9960],\n        ...,\n        [0.9960, 0.0000],\n        [0.9980, 0.0000],\n        [1.0000, 0.0000]])\n\n\n\nv.T # 2xn, ì¤‘ì²© ë¦¬ìŠ¤íŠ¸ë¡œ í•´ì„ ê°€ëŠ¥\n\ntensor([[0.0000, 0.0000, 0.0000,  ..., 0.9960, 0.9980, 1.0000],\n        [1.0000, 0.9980, 0.9960,  ..., 0.0000, 0.0000, 0.0000]])\n\n\n\nv1, v2 = v.T # ì–¸íŒ¨í‚¹, v1ê³¼ v2ëŠ” length nì¸ vector\ny = -4.5*v1 -9*v2 + 4.5 \ny # y ì—­ì‹œ vector\n\ntensor([-4.5000, -4.4820, -4.4640,  ...,  0.0180,  0.0090,  0.0000])\n\n\n\nyê°€ nx1ì´ ë˜ì–´ì•¼í•˜ë¯€ë¡œ\n\n\nv1, v2 = v.T\ny = -4.5*v1 -9*v2 + 4.5 \ny = y.reshape(-1,1)\nplt.plot(x,y)\n\n\n\n\n\n\n\n\n)ğŸ—£ï¸\n#\n# ë°©ë²•4 â€“ y = linr(v)\nğŸ—£ï¸(\n\nv\n\ntensor([[0.0000, 1.0000],\n        [0.0000, 0.9980],\n        [0.0000, 0.9960],\n        ...,\n        [0.9960, 0.0000],\n        [0.9980, 0.0000],\n        [1.0000, 0.0000]])\n\n\n\nv @ torch.tensor([[-4.5],[-9]])\n\ntensor([[-9.0000],\n        [-8.9820],\n        [-8.9640],\n        ...,\n        [-4.4820],\n        [-4.4910],\n        [-4.5000]])\n\n\n\ny = v @ torch.tensor([[-4.5],[-9]]) + 4.5\nplt.plot(x,y)\n\n\n\n\n\n\n\n\n)ğŸ—£ï¸\n\n# 4. y = -4.5*v1 + -9*v2 + 4.5 = [v1 v2] @ [[-4.5],[-9]] + 4.5 \n# y = -4 + 3*x = [1 x] @ [[-4],[3]]\n\n\nx \nu = torch.concat([x,-x],axis=1)\nv = relu(u) \ny = v @ torch.tensor([[-4.5],[-9]]) + 4.5 \n\n\nplt.plot(x,y)\n\n\n\n\n\n\n\n\n#\n# ë°©ë²•5 â€“ u = linr(x)\nğŸ—£ï¸(\n\n#u = [x -x] = x @ [[1 -1]]\n\n)ğŸ—£ï¸\n\n# x \n# u = torch.concat([x,-x],axis=1)\n# v = relu(u) \n# y = v @ torch.tensor([[-4.5],[-9]]) + 4.5 \n\n\nx \nu = x @ torch.tensor([[1.0, -1.0]])\nv = relu(u) \ny = v @ torch.tensor([[-4.5],[-9]]) + 4.5 \n\n\nplt.plot(x,y)\n\n\n\n\n\n\n\n\n#\n# ë°©ë²•6 â€“ torch.nn.Linear()ë¥¼ ì´ìš©\nğŸ—£ï¸(\n\n# x \n# u = x @ torch.tensor([[1.0, -1.0]]) = linr(x) =&gt; l1(x) \n# v = relu(u) = a1(u)\n# y = v @ torch.tensor([[-4.5],[-9]]) + 4.5 = linr(x) =&gt; l2(v) \n\n\nlì€ linearì˜ ì•½ì, aëŠ” activation functionì˜ ì•½ì\n\n\n# u = l1(x) # l1ì€ x-&gt;uì¸ ì„ í˜•ë³€í™˜: (n,1) -&gt; (n,2) ì¸ ì„ í˜•ë³€í™˜\nl1 = torch.nn.Linear(1,2,bias=False)\nl1.weight.data = torch.tensor([[1.0, -1.0]]).T \nl1(x)\n\ntensor([[-1.0000,  1.0000],\n        [-0.9980,  0.9980],\n        [-0.9960,  0.9960],\n        ...,\n        [ 0.9960, -0.9960],\n        [ 0.9980, -0.9980],\n        [ 1.0000, -1.0000]], grad_fn=&lt;MmBackward0&gt;)\n\n\n\nu\n\ntensor([[-1.0000,  1.0000],\n        [-0.9980,  0.9980],\n        [-0.9960,  0.9960],\n        ...,\n        [ 0.9960, -0.9960],\n        [ 0.9980, -0.9980],\n        [ 1.0000, -1.0000]])\n\n\n\n# u = l1(x) # l1ì€ x-&gt;uì¸ ì„ í˜•ë³€í™˜: (n,1) -&gt; (n,2) ì¸ ì„ í˜•ë³€í™˜\nl1 = torch.nn.Linear(1,2,bias=False)\nl1.weight.data = torch.tensor([[1.0, -1.0]]).T \na1 = relu \nl2 = torch.nn.Linear(2,1,bias=True) # + 4.5 =&gt; bias\nl2.weight.data = torch.tensor([[-4.5],[-9]]).T \nl2.bias.data = torch.tensor([4.5])\nu = l1(x)\nv = a1(u) \ny = l2(v) \n\n\ny\n\ntensor([[-4.5000],\n        [-4.4820],\n        [-4.4640],\n        ...,\n        [ 0.0180],\n        [ 0.0090],\n        [ 0.0000]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\n\nplt.plot(x,y.data)\n\n\n\n\n\n\n\n\n\npiecewise linear function ì •ì˜ (x -&gt; l1 -&gt; a1 -&gt; l2) =&gt; í•œ ë²ˆì— ê·¸ë˜í”„ ê·¸ë¦¬ê¸° ê°€ëŠ¥\n\n\npwlinr = torch.nn.Sequential(l1,a1,l2)\nplt.plot(x,pwlinr(x).data)\n\n\n\n\n\n\n\n\n)ğŸ—£ï¸\n\n# x \n# u = x @ torch.tensor([[1.0, -1.0]]) = l1(x) \n# v = relu(u) = a1(u)\n# y = v @ torch.tensor([[-4.5],[-9]]) + 4.5 = l2(v) \n\n\n# u = l1(x) # l1ì€ x-&gt;uì¸ ì„ í˜•ë³€í™˜: (n,1) -&gt; (n,2) ì¸ ì„ í˜•ë³€í™˜\nl1 = torch.nn.Linear(1,2,bias=False)\nl1.weight.data = torch.tensor([[1.0, -1.0]]).T \na1 = relu \nl2 = torch.nn.Linear(2,1,bias=True)\nl2.weight.data = torch.tensor([[-4.5],[-9]]).T \nl2.bias.data = torch.tensor([4.5])\n#---#\nx\nu = l1(x)\nv = a1(u) \ny = l2(v) \n\n\nplt.plot(x,y.data)\n\n\n\n\n\n\n\n\n\npwlinr = torch.nn.Sequential(l1,a1,l2)\nplt.plot(x,pwlinr(x).data)\n\n\n\n\n\n\n\n\n#\n\n\n\n\n\n\nNote\n\n\n\nìˆ˜ì‹í‘œí˜„\n(1) \\({\\bf X}=\\begin{bmatrix} x_1 \\\\ \\dots \\\\ x_n \\end{bmatrix}\\)\n(2) \\(l_1({\\bf X})={\\bf X}{\\bf W}^{(1)}\\overset{bc}{+} {\\boldsymbol b}^{(1)}=\\begin{bmatrix} x_1 & -x_1 \\\\ x_2 & -x_2 \\\\ \\dots & \\dots \\\\ x_n & -x_n\\end{bmatrix}\\)\n\n\\({\\bf W}^{(1)}=\\begin{bmatrix} 1 & -1 \\end{bmatrix}\\)\n\\({\\boldsymbol b}^{(1)}=\\begin{bmatrix} 0 & 0 \\end{bmatrix}\\)\n\n(3) \\((a_1\\circ l_1)({\\bf X})=\\text{relu}\\big({\\bf X}{\\bf W}^{(1)}\\overset{bc}{+}{\\boldsymbol b}^{(1)}\\big)=\\begin{bmatrix} \\text{relu}(x_1) & \\text{relu}(-x_1) \\\\ \\text{relu}(x_2) & \\text{relu}(-x_2) \\\\ \\dots & \\dots \\\\ \\text{relu}(x_n) & \\text{relu}(-x_n)\\end{bmatrix}\\)\n(4) \\((l_2 \\circ a_1\\circ l_1)({\\bf X})=\\text{relu}\\big({\\bf X}{\\bf W}^{(1)}\\overset{bc}{+}{\\boldsymbol b}^{(1)}\\big){\\bf W}^{(2)}\\overset{bc}{+}b^{(2)}\\)\n\\(\\quad=\\begin{bmatrix} -4.5\\times\\text{relu}(x_1) -9.0 \\times \\text{relu}(-x_1) +4.5 \\\\ -4.5\\times\\text{relu}(x_2) -9.0 \\times\\text{relu}(-x_2) + 4.5 \\\\ \\dots \\\\ -4.5\\times \\text{relu}(x_n) -9.0 \\times\\text{relu}(-x_n)+4.5 \\end{bmatrix}\\)\n\n\\({\\bf W}^{(2)}=\\begin{bmatrix} -4.5 \\\\ -9 \\end{bmatrix}\\)\n\\(b^{(2)}=4.5\\)\n\n(5) \\(\\textup{pwlinr}({\\bf X})=(l_2 \\circ a_1\\circ l_1)({\\bf X})=\\text{relu}\\big({\\bf X}{\\bf W}^{(1)}\\overset{bc}{+}{\\boldsymbol b}^{(1)}\\big){\\bf W}^{(2)}\\overset{bc}{+}b^{(2)}\\)\n\\(\\quad =\\begin{bmatrix} -4.5\\times\\text{relu}(x_1) -9.0 \\times \\text{relu}(-x_1) +4.5 \\\\ -4.5\\times\\text{relu}(x_2) -9.0 \\times\\text{relu}(-x_2) + 4.5 \\\\ \\dots \\\\ -4.5\\times \\text{relu}(x_n) -9.0 \\times\\text{relu}(-x_n)+4.5 \\end{bmatrix}\\)\n\n\n\n\n4. ìŠ¤í™ì˜ì—­ì„¤ ì í•© ğŸ“\n- ë‹¤ì‹œí•œë²ˆ ë°ì´í„° ì •ë¦¬\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DL2025/main/posts/ironyofspec.csv\")\n\nğŸ—£ï¸(\n\ntorch.tensor(df.x)\n\ntensor([-1.0000, -0.9990, -0.9980,  ...,  0.9980,  0.9990,  1.0000],\n       dtype=torch.float64)\n\n\n\ndtype=torch.float64ì„ ë³´ê¸° ì‹«ìœ¼ë©´ ë‹¤ìŒê³¼ ê°™ì´ í•˜ë©´ ë¨ (pytorchëŠ” ê¸°ë³¸ì ìœ¼ë¡œ 32í˜•ìœ¼ë¡œ ì €ì¥ë˜ëŠ” ê²ƒì„ ì›í•¨)\n\n\ntorch.tensor(df.x).float() # vector\n\ntensor([-1.0000, -0.9990, -0.9980,  ...,  0.9980,  0.9990,  1.0000])\n\n\n\nx = torch.tensor(df.x).float().reshape(-1,1)\ny = torch.tensor(df.y).float().reshape(-1,1)\nprob = torch.tensor(df.prob).float().reshape(-1,1)\n\n\nplt.plot(x,y,'.',alpha=0.03)\n\n\n\n\n\n\n\n\n\nprob: ì°¸ê°’, ê´€ì¸¡ ë¶ˆê°€\n\n)ğŸ—£ï¸\n\nx = torch.tensor(df.x).float().reshape(-1,1)\ny = torch.tensor(df.y).float().reshape(-1,1)\nprob = torch.tensor(df.prob).float().reshape(-1,1)\n\n\nplt.plot(x,y,'.',alpha=0.03)\nplt.plot(x,prob,'--')\n\n\n\n\n\n\n\n\n- Step1ì— ëŒ€í•œ ìƒê°: ë„¤íŠ¸ì›Œí¬ë¥¼ ì–´ë–»ê²Œ ë§Œë“¤ê¹Œ? = ì•„í‚¤í…ì²˜ë¥¼ ì–´ë–»ê²Œ ë§Œë“¤ê¹Œ? = ëª¨ë¸ë§\n\\[\\underset{(n,1)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,2)}{\\boldsymbol u^{(1)}} \\overset{a_1}{\\to} \\underset{(n,2)}{\\boldsymbol v^{(1)}} \\overset{l_1}{\\to} \\underset{(n,1)}{\\boldsymbol u^{(2)}} \\overset{a_2}{\\to} \\underset{(n,1)}{\\boldsymbol v^{(2)}}=\\underset{(n,1)}{\\hat{\\boldsymbol y}}\\]\n\n\\(l_1\\): torch.nn.Linear(1,2,bias=False)\n\\(a_1\\): torch.nn.ReLU()\n\\(l_2\\): torch.nn.Linear(2,1,bias=True)\n\\(a_2\\): torch.nn.Sigmoid()\n\nğŸ—£ï¸ l2ê¹Œì§€ëŠ” êº¾ì¸ ì„ \n- Step1-4\n\ntorch.manual_seed(1)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,2,bias=False),\n    torch.nn.ReLU(),\n    torch.nn.Linear(2,1,bias=True),\n    torch.nn.Sigmoid()\n)\nloss_fn = torch.nn.BCELoss() \noptimizr = torch.optim.Adam(net.parameters())\n\nğŸ—£ï¸ lr ë”°ë¡œ ì„¤ì • ì•ˆí•˜ë©´ defaultë¡œ ë“¤ì–´ê°\n\nfor epoc in range(5000):\n    ## step1\n    yhat = net(x)\n    ## step2\n    loss = loss_fn(yhat,y)\n    ## step3\n    loss.backward()\n    ## step4\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'.',alpha=0.03)\nplt.plot(x,prob,'--')\nplt.plot(x,yhat.data,'--')\n\n\n\n\n\n\n\n\ní•œë²ˆë”~\n\nfor epoc in range(5000):\n    ## step1\n    yhat = net(x)\n    ## step2\n    loss = loss_fn(yhat,y)\n    ## step3\n    loss.backward()\n    ## step4\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'.',alpha=0.03)\nplt.plot(x,prob,'--')\nplt.plot(x,yhat.data,'--')\n\n\n\n\n\n\n\n\nğŸ—£ï¸(\n\n???\n\n\nnet\n\nSequential(\n  (0): Linear(in_features=1, out_features=2, bias=False)\n  (1): ReLU()\n  (2): Linear(in_features=2, out_features=1, bias=True)\n  (3): Sigmoid()\n)\n\n\n\nnet[0](x) # ì²˜ìŒ linear transform í†µê³¼\n\ntensor([[-2.8167,  3.9404],\n        [-2.8139,  3.9364],\n        [-2.8111,  3.9325],\n        ...,\n        [ 2.8111, -3.9325],\n        [ 2.8139, -3.9364],\n        [ 2.8167, -3.9404]], grad_fn=&lt;MmBackward0&gt;)\n\n\n\nplt.plot(x,net[0](x).data)\n\n\n\n\n\n\n\n\n\nê¸°ìš¸ê¸° íŠœë‹ì´ ì´ë¯¸ ë˜ì–´ ìˆìŒ (ìƒê°ëŒ€ë¡œë¼ë©´ ë‚˜ì¤‘ì— ë˜ì–´ì•¼ í•¨)\n\n\nplt.plot(x,net[1](net[0](x)).data) # ë ë£¨\n\n\n\n\n\n\n\n\n\nplt.plot(x,net[2](net[1](net[0](x))).data) # 2ë²ˆì§¸ linear transform\n\n\n\n\n\n\n\n\n\nplt.plot(x,net[3](net[2](net[1](net[0](x)))).data) # sigmoid\n\n\n\n\n\n\n\n\n\nì›ë˜ë¼ë©´ u = x @ [1 -1] ì²˜ëŸ¼ ê·¸ë˜í”„ í‹€ì„ ë§ì¶°ë†“ê³  ê¸°ìš¸ê¸°ë¥¼ ë¯¸ì„¸ì¡°ì •í•˜ì˜€ì§€ë§Œ\nê¸°ìš¸ê¸°ë¥¼ ì²˜ìŒë¶€í„° ë¯¸ì„¸ì¡°ì •í•˜ë©´ì„œ í•´ë„ ì˜ ë§ì„ ìˆ˜ ìˆìŒ\n\nì´ ë§ì€ global minì„ í•˜ë‚˜ë§Œ ê°–ëŠ” ê²ƒì´ ì•„ë‹˜ (ì—¬ëŸ¬ ê°œì˜ ìµœì €ê°’ì´ ìˆì„ ìˆ˜ ìˆìŒ)\n\n\n)ğŸ—£ï¸"
  },
  {
    "objectID": "posts/07wk-1.html",
    "href": "posts/07wk-1.html",
    "title": "07wk-1: (í•©ì„±ê³±ì‹ ê²½ë§) â€“ CNN ìë‘, CNN í•µì‹¬ë ˆì´ì–´",
    "section": "",
    "text": "ğŸ“˜ Note Format Guide\nThis format serves as a structured guide for organizing lecture content, personal interpretation, experiments, and study-related questions.\nğŸ“ ğŸ—£ï¸ âœï¸ ğŸ”¬ â“"
  },
  {
    "objectID": "posts/07wk-1.html#a.-ì„±ëŠ¥ì¢‹ìŒ",
    "href": "posts/07wk-1.html#a.-ì„±ëŠ¥ì¢‹ìŒ",
    "title": "07wk-1: (í•©ì„±ê³±ì‹ ê²½ë§) â€“ CNN ìë‘, CNN í•µì‹¬ë ˆì´ì–´",
    "section": "A. ì„±ëŠ¥ì¢‹ìŒ",
    "text": "A. ì„±ëŠ¥ì¢‹ìŒ\nFashion MNIST\n\ntrain_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True)\ntest_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True)\ntrain_dataset = torch.utils.data.Subset(train_dataset, range(5000))\ntest_dataset = torch.utils.data.Subset(test_dataset, range(1000))\nto_tensor = torchvision.transforms.ToTensor()\nX = torch.stack([to_tensor(img) for img, lbl in train_dataset]).to(\"cuda:0\")\ny = torch.tensor([lbl for img, lbl in train_dataset])\ny = torch.nn.functional.one_hot(y).float().to(\"cuda:0\")\nXX = torch.stack([to_tensor(img) for img, lbl in test_dataset]).to(\"cuda:0\")\nyy = torch.tensor([lbl for img, lbl in test_dataset])\nyy = torch.nn.functional.one_hot(yy).float().to(\"cuda:0\")\n\nğŸ—£ï¸(\n\nX.shape # ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë ¤ì„œ ì¤„ì„\n\ntorch.Size([5000, 1, 28, 28])\n\n\n\ny.shape, XX.shape\n\n(torch.Size([5000, 10]), torch.Size([1000, 1, 28, 28]))\n\n\n)ğŸ—£ï¸\në°œì•…ìˆ˜ì¤€ìœ¼ë¡œ ì„¤ê³„í•œ ì‹ ê²½ë§\n\ntorch.manual_seed(0)\nnet = torch.nn.Sequential(\n    torch.nn.Flatten(),\n    torch.nn.Linear(784,2048),\n    torch.nn.ReLU(),\n    torch.nn.Linear(2048,10)\n).to(\"cuda\")\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n\nfor epoc in range(1,500):\n    #1\n    logits = net(X)\n    #2\n    loss = loss_fn(logits, y) \n    #3\n    loss.backward()\n    #4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\n(net(X).argmax(axis=1) == y.argmax(axis=1)).float().mean()\n\ntensor(1., device='cuda:0')\n\n\n\nğŸ—£ï¸ í•™ìŠµìœ¼ë¡œëŠ” ê°œì„ ë  ê²ƒì´ ì—†ìŒ (ì˜¤ë²„í”¼íŒ…ì˜ ë)\n\n\n(net(XX).argmax(axis=1) == yy.argmax(axis=1)).float().mean()\n\ntensor(0.8530, device='cuda:0')\n\n\nëŒ€ì¶©ëŒ€ì¶© ì„¤ê³„í•œ í•©ì„±ê³±ì‹ ê²½ë§\n\ntorch.manual_seed(0)\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,2),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d(2),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2704,10),\n).to(\"cuda\")\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n\nfor epoc in range(1,500):\n    #1\n    logits = net(X)\n    #2\n    loss = loss_fn(logits, y) \n    #3\n    loss.backward()\n    #4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\n(net(X).argmax(axis=1) == y.argmax(axis=1)).float().mean()\n\ntensor(0.9666, device='cuda:0')\n\n\n\n(net(XX).argmax(axis=1) == yy.argmax(axis=1)).float().mean()\n\ntensor(0.8710, device='cuda:0')\n\n\n\nğŸ—£ï¸ ì˜¤ë²„í”¼íŒ…ë„ ì „ë³´ë‹¤ ëœ í•¨, test accë„ ê°œì„ \n\nğŸ—£ï¸(\n\n2704\n\n\nnet\n\nSequential(\n  (0): Conv2d(1, 16, kernel_size=(2, 2), stride=(1, 1))\n  (1): ReLU()\n  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (3): Flatten(start_dim=1, end_dim=-1)\n  (4): Linear(in_features=2704, out_features=10, bias=True)\n)\n\n\n\nnet[:-1]\n\nSequential(\n  (0): Conv2d(1, 16, kernel_size=(2, 2), stride=(1, 1))\n  (1): ReLU()\n  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (3): Flatten(start_dim=1, end_dim=-1)\n)\n\n\n\nnet[:-1](X)\n\ntensor([[0.4116, 0.4116, 0.4116,  ..., 0.6122, 0.5657, 0.2136],\n        [0.4116, 0.4128, 0.4128,  ..., 0.7154, 0.0000, 0.0000],\n        [0.4116, 0.4116, 0.4116,  ..., 0.0000, 0.0000, 0.0000],\n        ...,\n        [0.4116, 0.4116, 0.4116,  ..., 0.0000, 0.0000, 0.0000],\n        [0.4116, 0.4116, 0.4116,  ..., 0.0000, 0.0000, 0.0000],\n        [0.4116, 0.4116, 0.4116,  ..., 0.0000, 0.0000, 0.0000]],\n       device='cuda:0', grad_fn=&lt;ViewBackward0&gt;)\n\n\n\nnet[:-1](X).shape\n\ntorch.Size([5000, 2704])\n\n\n\nì•„ë¬´ê±°ë‚˜ ì¨ë†“ê³  error ë³´ê³  ê³ ì³ë„ ë¨\nì°¸ê³ ) GPU error ë‚˜ë©´ Kernel ì¬ì‹œì‘\n\n)ğŸ—£ï¸"
  },
  {
    "objectID": "posts/07wk-1.html#b.-íŒŒë¼ë©”í„°ì ìŒ",
    "href": "posts/07wk-1.html#b.-íŒŒë¼ë©”í„°ì ìŒ",
    "title": "07wk-1: (í•©ì„±ê³±ì‹ ê²½ë§) â€“ CNN ìë‘, CNN í•µì‹¬ë ˆì´ì–´",
    "section": "B. íŒŒë¼ë©”í„°ì ìŒ",
    "text": "B. íŒŒë¼ë©”í„°ì ìŒ\n\nnet1 = torch.nn.Sequential(\n    torch.nn.Flatten(),\n    torch.nn.Linear(784,2048),\n    torch.nn.ReLU(),\n    torch.nn.Linear(2048,10)\n)\nnet2 = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,2),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d(2),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2704,10),\n)\n\n\nnet1\n\nSequential(\n  (0): Flatten(start_dim=1, end_dim=-1)\n  (1): Linear(in_features=784, out_features=2048, bias=True)\n  (2): ReLU()\n  (3): Linear(in_features=2048, out_features=10, bias=True)\n)\n\n\n\nnet1_params = list(net1.parameters())\nprint(net1_params[0].shape)\nprint(net1_params[1].shape)\nprint(net1_params[2].shape)\nprint(net1_params[3].shape)\n\ntorch.Size([2048, 784])\ntorch.Size([2048])\ntorch.Size([10, 2048])\ntorch.Size([10])\n\n\n\n2048*784 + 2048 + 10*2048 + 10 \n\n1628170\n\n\nğŸ—£ï¸(\n\nnet1.parameters()\n\n&lt;generator object Module.parameters at 0x7f542f6dbcf0&gt;\n\n\n\ngenerator: nextê°€ ë¨ / forë¬¸ì„ ëŒë¦¬ê¸° í¸í•œ list ë¹„ìŠ·í•œ í˜•íƒœ\n\n\nnext(net1.parameters())\n\nParameter containing:\ntensor([[-0.0290,  0.0095, -0.0319,  ..., -0.0157, -0.0290, -0.0092],\n        [-0.0115, -0.0210, -0.0033,  ..., -0.0106, -0.0276, -0.0034],\n        [-0.0270, -0.0204,  0.0075,  ..., -0.0183, -0.0071,  0.0063],\n        ...,\n        [ 0.0150, -0.0349,  0.0023,  ...,  0.0235,  0.0061,  0.0350],\n        [-0.0234, -0.0295, -0.0202,  ..., -0.0353, -0.0169, -0.0149],\n        [-0.0315,  0.0171, -0.0010,  ..., -0.0192, -0.0257,  0.0305]],\n       requires_grad=True)\n\n\n\nlist(net1.parameters())\n\n[Parameter containing:\n tensor([[-0.0290,  0.0095, -0.0319,  ..., -0.0157, -0.0290, -0.0092],\n         [-0.0115, -0.0210, -0.0033,  ..., -0.0106, -0.0276, -0.0034],\n         [-0.0270, -0.0204,  0.0075,  ..., -0.0183, -0.0071,  0.0063],\n         ...,\n         [ 0.0150, -0.0349,  0.0023,  ...,  0.0235,  0.0061,  0.0350],\n         [-0.0234, -0.0295, -0.0202,  ..., -0.0353, -0.0169, -0.0149],\n         [-0.0315,  0.0171, -0.0010,  ..., -0.0192, -0.0257,  0.0305]],\n        requires_grad=True),\n Parameter containing:\n tensor([ 0.0101, -0.0294,  0.0133,  ...,  0.0234,  0.0283, -0.0351],\n        requires_grad=True),\n Parameter containing:\n tensor([[-1.2926e-02, -9.9654e-03, -1.9394e-02,  ..., -4.1905e-05,\n          -1.8466e-02, -1.9714e-02],\n         [-5.3193e-03,  1.1461e-02,  4.5922e-03,  ..., -2.1761e-02,\n           2.1826e-02,  1.1969e-02],\n         [-4.7551e-03, -2.1131e-02, -6.7052e-03,  ...,  2.1758e-02,\n           9.4742e-03, -6.0024e-04],\n         ...,\n         [ 8.4402e-03,  3.9834e-03, -2.1895e-02,  ...,  3.9969e-03,\n           1.6158e-02, -1.3650e-02],\n         [-2.0990e-02,  5.0413e-05,  2.1427e-02,  ..., -1.6232e-02,\n          -5.4801e-03,  9.2220e-04],\n         [ 4.4055e-03,  3.9634e-03,  1.7039e-03,  ...,  1.1729e-02,\n           5.9133e-03, -1.3802e-02]], requires_grad=True),\n Parameter containing:\n tensor([ 0.0117,  0.0112, -0.0121, -0.0206, -0.0110, -0.0080,  0.0154,  0.0140,\n          0.0115,  0.0209], requires_grad=True)]\n\n\n\nnet1_params = list(net1.parameters())\nprint(net1_params[0].shape)\nprint(net1_params[1].shape) # bias\nprint(net1_params[2].shape) # what\nprint(net1_params[3].shape) # bias\n\ntorch.Size([2048, 784])\ntorch.Size([2048])\ntorch.Size([10, 2048])\ntorch.Size([10])\n\n\n)ğŸ—£ï¸\n\nğŸ—£ï¸ netì— parameterê°€ ë§ë‹¤: ë¹„ì‹¸ë‹¤ (netì€ GPUì— ë‹¤ ì˜¬ë¦´ ìˆ˜ ë°–ì— ì—†ìŒ)\n\n\nnet2\n\nSequential(\n  (0): Conv2d(1, 16, kernel_size=(2, 2), stride=(1, 1))\n  (1): ReLU()\n  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (3): Flatten(start_dim=1, end_dim=-1)\n  (4): Linear(in_features=2704, out_features=10, bias=True)\n)\n\n\n\nnet2_params = list(net2.parameters())\nprint(net2_params[0].shape)\nprint(net2_params[1].shape)\nprint(net2_params[2].shape) # what\nprint(net2_params[3].shape) # bias\n\ntorch.Size([16, 1, 2, 2])\ntorch.Size([16])\ntorch.Size([10, 2704])\ntorch.Size([10])\n\n\n\n16*1*2*2 + 16 + 10*2704 + 10 \n\n27130\n\n\n\n27130/1628170\n\n0.01666287918337766\n\n\n\nğŸ—£ï¸ ëŒ€ì¶© ë§Œë“¤ì—ˆëŠ”ë° ì„±ëŠ¥ë„ ì¢‹ìŒ\n\nğŸ—£ï¸(\n\nnet2_params = list(net2.parameters())\nprint(net2_params[0].shape)\n\ntorch.Size([16, 1, 2, 2])\n\n\n\nì°¨ì›ì´ ë§ìŒ\n\n)ğŸ—£ï¸"
  },
  {
    "objectID": "posts/07wk-1.html#c.-ìœ ëª…í•¨",
    "href": "posts/07wk-1.html#c.-ìœ ëª…í•¨",
    "title": "07wk-1: (í•©ì„±ê³±ì‹ ê²½ë§) â€“ CNN ìë‘, CNN í•µì‹¬ë ˆì´ì–´",
    "section": "C. ìœ ëª…í•¨",
    "text": "C. ìœ ëª…í•¨\n- https://brunch.co.kr/@hvnpoet/109\nğŸ—£ï¸(\n\në”¥ëŸ¬ë‹ìŠˆí¼ìŠ¤íƒ€ â€“ íŒíŠ¼, ë¥´ì¿¤, ë²¤ì§€ì˜¤, ì‘\n\níŒíŠ¼ â€“ DBN(ì‚¬ì´ì–¸ìŠ¤) â€”&gt; ê¹Šì€ì‹ ê²½ë§ì„ ë§Œë“¤ì–´ë„ í•™ìŠµí•  ìˆ˜ ìˆë‹¤.\n\nê´€ì‹¬X\n\níŒíŠ¼ ëŒ€í•™ì›ìƒ: ì•Œë ‰ìŠ¤ â€“&gt; CIFAR10(ì´ë¯¸ì§€ ë°ì´í„°)\n\në‹¤ë¥¸ ëŒ€í•™ì›ìƒ: ê³µëª¨ì „ ì œì•ˆ -&gt; ë‚˜ê°”ìŒ\n\n\në‚´ ì»´í“¨í„°ê°€ ë„ˆë¬´ ëŠë¦¼ â€“&gt; GPU\nì˜¤ë²„í”¼íŒ… â€“&gt; ë“œëì•„ì›ƒ\nlocal min, ê¸°ìš¸ê¸°ì†Œë©¸, â€¦ (Adam ê°œë°œ ì „) â€“&gt; ë ë£¨(ë²¤ì§€ì˜¤ ì—°êµ¬ì‹¤ ê°œë°œ) ì‚¬ìš©\n\n1ë“± &lt;â€“ 1%ë§Œ ì˜¬ë ¤ë„ ëŒ€ë‹¨í•œë° 10%ë¥¼ ì˜¬ë¦¼ (2012ë…„)\n2014 &lt;â€“ Adam\n\nìš”ì¦˜ì€ ë” ì¢‹ì€ íŠ¸ëœìŠ¤í¬ë¨¸ê°€ ë‚˜ì˜¤ê¸´ í•¨\n\n)ğŸ—£ï¸"
  },
  {
    "objectID": "posts/07wk-1.html#a.-torch.nn.relu",
    "href": "posts/07wk-1.html#a.-torch.nn.relu",
    "title": "07wk-1: (í•©ì„±ê³±ì‹ ê²½ë§) â€“ CNN ìë‘, CNN í•µì‹¬ë ˆì´ì–´",
    "section": "A. torch.nn.ReLU",
    "text": "A. torch.nn.ReLU\n(ì˜ˆì‹œ1) ì—°ì‚°ë°©ë²•\n\nimg = torch.randn(1,1,4,4) # (4,4) í‘ë°±ì´ë¯¸ì§€ í•œì¥\nrelu = torch.nn.ReLU()\n\nğŸ—£ï¸ (obs, channel, (img size))\n\nimg\n\ntensor([[[[ 1.4381,  0.2449, -0.6420,  2.6874],\n          [ 0.7790,  1.0558,  0.7939,  0.1099],\n          [ 0.3492,  1.7610,  1.6032,  2.4212],\n          [ 0.5416, -0.2153, -1.2772,  0.6885]]]])\n\n\n\nrelu(img)\n\ntensor([[[[1.4381, 0.2449, 0.0000, 2.6874],\n          [0.7790, 1.0558, 0.7939, 0.1099],\n          [0.3492, 1.7610, 1.6032, 2.4212],\n          [0.5416, 0.0000, 0.0000, 0.6885]]]])"
  },
  {
    "objectID": "posts/07wk-1.html#b.-torch.nn.maxpool2d",
    "href": "posts/07wk-1.html#b.-torch.nn.maxpool2d",
    "title": "07wk-1: (í•©ì„±ê³±ì‹ ê²½ë§) â€“ CNN ìë‘, CNN í•µì‹¬ë ˆì´ì–´",
    "section": "B. torch.nn.MaxPool2d",
    "text": "B. torch.nn.MaxPool2d\n(ì˜ˆì‹œ1) ì—°ì‚°ë°©ë²•, kernel_size ì˜ ì˜ë¯¸\n\nimg = torch.rand(1,1,4,4)\nmp = torch.nn.MaxPool2d(kernel_size=2)\n\n\nimg\n\ntensor([[[[0.8921, 0.4222, 0.5778, 0.2707],\n          [0.6921, 0.5627, 0.5356, 0.1048],\n          [0.5356, 0.7699, 0.9047, 0.5911],\n          [0.3617, 0.5345, 0.1218, 0.4772]]]])\n\n\n\nmp(img)\n\ntensor([[[[0.8921, 0.5778],\n          [0.7699, 0.9047]]]])\n\n\nğŸ—£ï¸ 2*2 windowë¥¼ ë§Œë“  ë’¤ max ê°’ì„ ì ìŒ\n(ì˜ˆì‹œ2) ì´ë¯¸ì§€í¬ê¸°ì™€ ë”± ë§ì§€ì•ŠëŠ” ì»¤ë„ì¼ê²½ìš°?\n\nimg = torch.rand(1,1,5,5)\nmp = torch.nn.MaxPool2d(kernel_size=3)\n\n\nimg\n\ntensor([[[[0.9560, 0.4947, 0.1591, 0.2606, 0.9130],\n          [0.0603, 0.1255, 0.6520, 0.2504, 0.8759],\n          [0.7544, 0.5927, 0.5319, 0.2390, 0.2883],\n          [0.9470, 0.8519, 0.3501, 0.0725, 0.3881],\n          [0.7203, 0.0753, 0.8360, 0.1287, 0.9515]]]])\n\n\n\nmp(img)\n\ntensor([[[[0.9560]]]])\n\n\n\nğŸ—£ï¸ versionë§ˆë‹¤ ë‹¤ë¦„\n\npytorchëŠ” ë‚˜ë¨¸ì§€ë¥¼ ê·¸ëƒ¥ ë²„ë¦¼\në‚˜ë¨¸ì§€ ì¤‘ maxë¥¼ ì ê¸°ë„ í•¨\n\n\n(ì˜ˆì‹œ3) ì •ì‚¬ê°í˜•ì´ ì•„ë‹Œ ì»¤ë„\n\nimg = torch.rand(1,1,4,4)\nmp = torch.nn.MaxPool2d(kernel_size=(4,2))\n\n\nimg\n\ntensor([[[[0.4283, 0.9998, 0.3532, 0.3085],\n          [0.3278, 0.8575, 0.3331, 0.9769],\n          [0.0239, 0.2457, 0.8468, 0.8224],\n          [0.9593, 0.1292, 0.5930, 0.3652]]]])\n\n\n\nmp(img)\n\ntensor([[[[0.9998, 0.9769]]]])"
  },
  {
    "objectID": "posts/07wk-1.html#c.-torch.nn.conv2d",
    "href": "posts/07wk-1.html#c.-torch.nn.conv2d",
    "title": "07wk-1: (í•©ì„±ê³±ì‹ ê²½ë§) â€“ CNN ìë‘, CNN í•µì‹¬ë ˆì´ì–´",
    "section": "C. torch.nn.Conv2d",
    "text": "C. torch.nn.Conv2d\n(ì˜ˆì‹œ1) ì—°ì‚°ë°©ë²•, stride=2\n\nimg = torch.rand(1,1,4,4) # (?, in_channels, ?, ?) \nconv = torch.nn.Conv2d(in_channels=1,out_channels=1,kernel_size=2,stride=2) # stride=2: windowë¥¼ 2ì¹¸ ì›€ì§ì´ë¼ëŠ” ëœ» (ë°”ë¡œ ìœ„ì˜ ì˜ˆì‹œì™€ ë¹„ìŠ·)\n\n\nimg\n\ntensor([[[[0.7679, 0.3459, 0.6509, 0.7905],\n          [0.1166, 0.8762, 0.9373, 0.8573],\n          [0.5778, 0.8702, 0.9686, 0.5854],\n          [0.1373, 0.3530, 0.0529, 0.0139]]]])\n\n\n\nconv(img)\n\ntensor([[[[ 0.1106, -0.1898],\n          [ 0.0529, -0.0976]]]], grad_fn=&lt;ConvolutionBackward0&gt;)\n\n\n??\nğŸ—£ï¸ ë°”ë¡œ ìœ ì¶”í•˜ê¸° ì–´ë ¤ì›€\nğŸ—£ï¸(\n\nimg[:, :, :2, :2], img\n\n(tensor([[[[0.7679, 0.3459],\n           [0.1166, 0.8762]]]]),\n tensor([[[[0.7679, 0.3459, 0.6509, 0.7905],\n           [0.1166, 0.8762, 0.9373, 0.8573],\n           [0.5778, 0.8702, 0.9686, 0.5854],\n           [0.1373, 0.3530, 0.0529, 0.0139]]]]))\n\n\n\nimg[:, :, :2, 2:], img\n\n(tensor([[[[0.6509, 0.7905],\n           [0.9373, 0.8573]]]]),\n tensor([[[[0.7679, 0.3459, 0.6509, 0.7905],\n           [0.1166, 0.8762, 0.9373, 0.8573],\n           [0.5778, 0.8702, 0.9686, 0.5854],\n           [0.1373, 0.3530, 0.0529, 0.0139]]]]))\n\n\n\nimg[:, :, 2:, :2], img\n\n(tensor([[[[0.5778, 0.8702],\n           [0.1373, 0.3530]]]]),\n tensor([[[[0.7679, 0.3459, 0.6509, 0.7905],\n           [0.1166, 0.8762, 0.9373, 0.8573],\n           [0.5778, 0.8702, 0.9686, 0.5854],\n           [0.1373, 0.3530, 0.0529, 0.0139]]]]))\n\n\n\nimg[:, :, 2:, 2:], img\n\n(tensor([[[[0.9686, 0.5854],\n           [0.0529, 0.0139]]]]),\n tensor([[[[0.7679, 0.3459, 0.6509, 0.7905],\n           [0.1166, 0.8762, 0.9373, 0.8573],\n           [0.5778, 0.8702, 0.9686, 0.5854],\n           [0.1373, 0.3530, 0.0529, 0.0139]]]]))\n\n\n\nconv(img) # ë¯¸ë¶„ê¼¬ë¦¬í‘œ -&gt; parameter\n\ntensor([[[[ 0.1106, -0.1898],\n          [ 0.0529, -0.0976]]]], grad_fn=&lt;ConvolutionBackward0&gt;)\n\n\n\nconv.weight.data, conv.bias.data\n\n(tensor([[[[-0.0218,  0.2400],\n           [-0.4914,  0.3394]]]]),\n tensor([-0.1958]))\n\n\n\nimg[:, :, :2, :2], img\n\n(tensor([[[[0.7679, 0.3459],\n           [0.1166, 0.8762]]]]),\n tensor([[[[0.7679, 0.3459, 0.6509, 0.7905],\n           [0.1166, 0.8762, 0.9373, 0.8573],\n           [0.5778, 0.8702, 0.9686, 0.5854],\n           [0.1373, 0.3530, 0.0529, 0.0139]]]]))\n\n\n\n-0.0218 * 0.7679\n\n-0.01674022\n\n\n\nimg[:, :, :2, :2]*conv.weight.data, img # í–‰ë ¬ ê³±ì´ ì•„ë‹ˆë¼ ì›ì†Œ ë³„ë¡œ ê³±í•¨ \n\n(tensor([[[[-0.0167,  0.0830],\n           [-0.0573,  0.2974]]]]),\n tensor([[[[0.7679, 0.3459, 0.6509, 0.7905],\n           [0.1166, 0.8762, 0.9373, 0.8573],\n           [0.5778, 0.8702, 0.9686, 0.5854],\n           [0.1373, 0.3530, 0.0529, 0.0139]]]]))\n\n\n\n(img[:, :, :2, :2]*conv.weight.data).sum()+conv.bias.data, conv(img)\n\n(tensor([0.1106]),\n tensor([[[[ 0.1106, -0.1898],\n           [ 0.0529, -0.0976]]]], grad_fn=&lt;ConvolutionBackward0&gt;))\n\n\n\n(img[:, :, :2, 2:]*conv.weight.data).sum()+conv.bias.data, conv(img) # ë‘ë²ˆì§¸ ê°’\n\n(tensor([-0.1898]),\n tensor([[[[ 0.1106, -0.1898],\n           [ 0.0529, -0.0976]]]], grad_fn=&lt;ConvolutionBackward0&gt;))\n\n\n\n(img[:, :, 2:, :2]*conv.weight.data).sum()+conv.bias.data, conv(img) # ì„¸ë²ˆì§¸ ê°’\n\n(tensor([0.0529]),\n tensor([[[[ 0.1106, -0.1898],\n           [ 0.0529, -0.0976]]]], grad_fn=&lt;ConvolutionBackward0&gt;))\n\n\n\n(img[:, :, 2:, 2:]*conv.weight.data).sum()+conv.bias.data, conv(img) # ë„¤ë²ˆì§¸ ê°’\n\n(tensor([-0.0976]),\n tensor([[[[ 0.1106, -0.1898],\n           [ 0.0529, -0.0976]]]], grad_fn=&lt;ConvolutionBackward0&gt;))\n\n\n)ğŸ—£ï¸\n\nconv.weight.data, conv.bias.data\n\n(tensor([[[[ 0.3095,  0.0207],\n           [-0.3130,  0.2836]]]]),\n tensor([-0.2675]))\n\n\n\n(img[:,  :,  :2,  :2] * conv.weight.data).sum()+conv.bias.data, conv(img)\n\n(tensor([-0.3077]),\n tensor([[[[-0.3077, -0.4760],\n           [ 0.0550, -0.0650]]]], grad_fn=&lt;ConvolutionBackward0&gt;))\n\n\n\n(img[:,  :,  :2,  2:] * conv.weight.data).sum()+conv.bias.data, conv(img)\n\n(tensor([-0.4760]),\n tensor([[[[-0.3077, -0.4760],\n           [ 0.0550, -0.0650]]]], grad_fn=&lt;ConvolutionBackward0&gt;))\n\n\n\n(img[:,  :,  2:,  :2] * conv.weight.data).sum()+conv.bias.data, conv(img)\n\n(tensor([0.0550]),\n tensor([[[[-0.3077, -0.4760],\n           [ 0.0550, -0.0650]]]], grad_fn=&lt;ConvolutionBackward0&gt;))\n\n\n\n(img[:,  :,  2:,  2:] * conv.weight.data).sum()+conv.bias.data, conv(img)\n\n(tensor([-0.0650]),\n tensor([[[[-0.3077, -0.4760],\n           [ 0.0550, -0.0650]]]], grad_fn=&lt;ConvolutionBackward0&gt;))"
  },
  {
    "objectID": "posts/06wk-1.html",
    "href": "posts/06wk-1.html",
    "title": "06wk-1: (ì‹ ê²½ë§) â€“ ë°ì´í„°ë¶„ì„ ì½”ë”©íŒ¨í„´",
    "section": "",
    "text": "ğŸ“˜ Note Format Guide\nThis format serves as a structured guide for organizing lecture content, personal interpretation, experiments, and study-related questions.\nğŸ“ ğŸ—£ï¸ âœï¸ ğŸ”¬ â“"
  },
  {
    "objectID": "posts/06wk-1.html#a.-ì¼ë°˜ì ì¸-traintest-ì…‹íŒ…",
    "href": "posts/06wk-1.html#a.-ì¼ë°˜ì ì¸-traintest-ì…‹íŒ…",
    "title": "06wk-1: (ì‹ ê²½ë§) â€“ ë°ì´í„°ë¶„ì„ ì½”ë”©íŒ¨í„´",
    "section": "A. ì¼ë°˜ì ì¸ train/test ì…‹íŒ…",
    "text": "A. ì¼ë°˜ì ì¸ train/test ì…‹íŒ…\n- Step1: ë°ì´í„°ì •ë¦¬\n\ntrain_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True)\ntest_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True)\nto_tensor = torchvision.transforms.ToTensor()\nX0 = torch.stack([to_tensor(img) for img, lbl in train_dataset if lbl==0])\nX1 = torch.stack([to_tensor(img) for img, lbl in train_dataset if lbl==1])\nX = torch.concat([X0,X1],axis=0).reshape(-1,784)\ny = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1)\nXX0 = torch.stack([to_tensor(img) for img, lbl in test_dataset if lbl==0])\nXX1 = torch.stack([to_tensor(img) for img, lbl in test_dataset if lbl==1])\nXX = torch.concat([XX0,XX1],axis=0).reshape(-1,784)\nyy = torch.tensor([0.0]*len(XX0) + [1.0]*len(XX1)).reshape(-1,1)\n\nğŸ—£ï¸(\n\nX[0].shape\n\ntorch.Size([784])\n\n\n\nplt.imshow(X[0].reshape(28,28), cmap=\"gray\")\n\n\n\n\n\n\n\n\n\nprint(y[0]) # labelì´ tensor í˜•íƒœë¡œ ì €ì¥ë˜ì–´ ìˆìŒ\n\ntensor([0.])\n\n\n\nXë§Œ ê°€ì§€ê³  í•™ìŠµì„ í•œ ë’¤ XXë¥¼ ê°€ì§€ê³  í™•ì¸\n\n\nX.shape, y.shape\n\n(torch.Size([12665, 784]), torch.Size([12665, 1]))\n\n\n\nXX.shape, yy.shape\n\n(torch.Size([2115, 784]), torch.Size([2115, 1]))\n\n\n)ğŸ—£ï¸\n- Step2: í•™ìŠµê°€ëŠ¥í•œ ì˜¤ë¸Œì íŠ¸ë“¤ì˜ ì„¤ì • (ëª¨ë¸ë§ê³¼ì • í¬í•¨)\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(784,32),\n    torch.nn.ReLU(), # (n,32)\n    torch.nn.Linear(32,1),\n    torch.nn.Sigmoid() # yëŠ” 0 ë˜ëŠ” 1\n)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD(net.parameters()) # Adamì€ ë„ˆë¬´ ì˜ ë§ì¶°ì„œ SGD\n\n- Step3: í•™ìŠµ (=ì í•©)\n\nfor epoc in range(1,501):\n    #---ì—í­ì‹œì‘---# \n    # 1 \n    yhat = net(X) \n    # 2 \n    loss = loss_fn(yhat,y) \n    # 3 \n    loss.backward()\n    # 4 \n    optimizr.step()\n    optimizr.zero_grad()\n    #---ì—í­ë---# \n    #ì—í­ë§ˆë‹¤ ë‚´ê°€ ë³´ê³ ì‹¶ì€ê²ƒë“¤ì„ ë³´ì—¬ì£¼ëŠ” ì½”ë“œ\n    if (epoc % 50) ==0: # 50ìœ¼ë¡œ ë‚˜ëˆˆ ë‚˜ë¨¸ì§€ = 0 =&gt; 50ì˜ ë°°ìˆ˜\n        acc = ((net(X).data &gt; 0.5) == y).float().mean().item() # item: tensor -&gt; float\n        print(f\"# of epochs={epoc}   \\t train_acc = {acc:.4f}\")\n\n# of epochs=50       train_acc = 0.4677\n# of epochs=100      train_acc = 0.4677\n# of epochs=150      train_acc = 0.4757\n# of epochs=200      train_acc = 0.5295\n# of epochs=250      train_acc = 0.6632\n# of epochs=300      train_acc = 0.7929\n# of epochs=350      train_acc = 0.8731\n# of epochs=400      train_acc = 0.9206\n# of epochs=450      train_acc = 0.9465\n# of epochs=500      train_acc = 0.9634\n\n\nğŸ—£ï¸ ì˜¤ë²„í”¼íŒ… ë¹„íŒ ê°€ëŠ¥ì„± ì¡´ì¬\n- Step4: ì˜ˆì¸¡ & ê²°ê³¼ë¶„ì„\ntrain acc\n\n((net(X).data &gt; 0.5) == y).float().mean()\n\ntensor(0.9634)\n\n\ntest acc\n\n((net(XX).data&gt;0.5) == yy).float().mean()\n\ntensor(0.9749)\n\n\nğŸ—£ï¸ ì‹¤ì „ì—ì„œ ë” ê´œì°®ìŒ\nğŸ—£ï¸ Step4: acc, recall, F1 score, ì‹œê°í™” ë“±\n#ì—í­ë§ˆë‹¤ ë‚´ê°€ ë³´ê³ ì‹¶ì€ê²ƒë“¤ì„ ë³´ì—¬ì£¼ëŠ” ì½”ë“œ\n    if (epoc % 50) ==0: # 50ìœ¼ë¡œ ë‚˜ëˆˆ ë‚˜ë¨¸ì§€ = 0 =&gt; 50ì˜ ë°°ìˆ˜\n        acc = ((net(X).data &gt; 0.5) == y).float().mean().item() # item: tensor -&gt; float\n        Xval --&gt; # train data ìì²´ì—ì„œ test ë°ì´í„°ë¥¼ ë‚˜ëˆ„ê³  ì •í™•ë„ë¥¼ ë¹„êµí•˜ë©° early stopping í•  ìˆ˜ë„ ìˆìŒ (ì˜¤ë²„í”¼íŒ… ë°©ì§€)\n        print(f\"# of epochs={epoc}   \\t train_acc = {acc:.4f}\")"
  },
  {
    "objectID": "posts/06wk-1.html#b.-dropout-ì‚¬ìš©",
    "href": "posts/06wk-1.html#b.-dropout-ì‚¬ìš©",
    "title": "06wk-1: (ì‹ ê²½ë§) â€“ ë°ì´í„°ë¶„ì„ ì½”ë”©íŒ¨í„´",
    "section": "B. Dropout ì‚¬ìš©",
    "text": "B. Dropout ì‚¬ìš©\n- Step1: ë°ì´í„°ì •ë¦¬\n\npass\n\n- Step2: í•™ìŠµê°€ëŠ¥í•œ ì˜¤ë¸Œì íŠ¸ë“¤ì˜ ì„¤ì • (ëª¨ë¸ë§ê³¼ì • í¬í•¨)\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(784,32),\n    torch.nn.Dropout(0.9),\n    torch.nn.ReLU(),\n    torch.nn.Linear(32,1),\n    torch.nn.Sigmoid()\n)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD(net.parameters())\n\nğŸ—£ï¸ ì›ë˜ëŠ” í™œì„±í™” í•¨ìˆ˜ ë‹¤ìŒì´ì§€ë§Œ ReLU í•œì • ì „ì—ë„ ì‚¬ìš© ê°€ëŠ¥\n- Step3: í•™ìŠµ (=ì í•©)\n\nfor epoc in range(1,501):\n    net.train()\n    #---ì—í­ì‹œì‘---# \n    # 1 \n    yhat = net(X) \n    # 2 \n    loss = loss_fn(yhat,y) \n    # 3 \n    loss.backward()\n    # 4 \n    optimizr.step()\n    optimizr.zero_grad()\n    #---ì—í­ë---# \n    net.eval()\n    #ì—í­ë§ˆë‹¤ ë‚´ê°€ ë³´ê³ ì‹¶ì€ê²ƒë“¤ì„ ë³´ì—¬ì£¼ëŠ” ì½”ë“œ\n    if (epoc % 50) ==0:\n        acc = ((net(X).data &gt; 0.5) == y).float().mean().item()\n        print(f\"# of epochs={epoc}   \\t train_acc = {acc:.4f}\")\n\n# of epochs=50       train_acc = 0.4677\n# of epochs=100      train_acc = 0.4677\n# of epochs=150      train_acc = 0.4744\n# of epochs=200      train_acc = 0.5215\n# of epochs=250      train_acc = 0.6435\n# of epochs=300      train_acc = 0.7675\n# of epochs=350      train_acc = 0.8468\n# of epochs=400      train_acc = 0.8978\n# of epochs=450      train_acc = 0.9301\n# of epochs=500      train_acc = 0.9492\n\n\n\n# of epochs=50       train_acc = 0.4677\n# of epochs=100      train_acc = 0.4677\n# of epochs=150      train_acc = 0.4757 # ìœ„ì˜ ê²°ê³¼ì™€ ì‚´ì§ ë‹¤ë¦„\n# of epochs=200      train_acc = 0.5295\n# of epochs=250      train_acc = 0.6632\n# of epochs=300      train_acc = 0.7929\n# of epochs=350      train_acc = 0.8731\n# of epochs=400      train_acc = 0.9206\n# of epochs=450      train_acc = 0.9465\n# of epochs=500      train_acc = 0.9634\n\n- Step4: ì˜ˆì¸¡ & ê²°ê³¼ë¶„ì„\ntrain acc\n\n((net(X).data &gt; 0.5) == y).float().mean()\n\ntensor(0.9492)\n\n\ntest acc\n\n((net(XX).data&gt;0.5) == yy).float().mean()\n\ntensor(0.9626)\n\n\nğŸ—£ï¸ ì‹¤ì „ì—ì„œ ë” ê´œì°®ìŒ"
  },
  {
    "objectID": "posts/06wk-1.html#c.-gpuë„-ì‚¬ìš©",
    "href": "posts/06wk-1.html#c.-gpuë„-ì‚¬ìš©",
    "title": "06wk-1: (ì‹ ê²½ë§) â€“ ë°ì´í„°ë¶„ì„ ì½”ë”©íŒ¨í„´",
    "section": "C. GPUë„ ì‚¬ìš©",
    "text": "C. GPUë„ ì‚¬ìš©\n- Step1: ë°ì´í„°ì •ë¦¬\n\npass\n\n- Step2: í•™ìŠµê°€ëŠ¥í•œ ì˜¤ë¸Œì íŠ¸ë“¤ì˜ ì„¤ì • (ëª¨ë¸ë§ê³¼ì • í¬í•¨)\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(784,32),\n    torch.nn.Dropout(0.9),\n    torch.nn.ReLU(),\n    torch.nn.Linear(32,1),\n    torch.nn.Sigmoid()\n).to(\"cuda:0\")\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD(net.parameters())\n\n- Step3: í•™ìŠµ (=ì í•©)\n\nfor epoc in range(1,501):\n    net.train()\n    #---ì—í­ì‹œì‘---# \n    X = X.to(\"cuda:0\")\n    y = y.to(\"cuda:0\")\n    # 1 \n    yhat = net(X) \n    # 2 \n    loss = loss_fn(yhat,y) \n    # 3 \n    loss.backward()\n    # 4 \n    optimizr.step()\n    optimizr.zero_grad()\n    #---ì—í­ë---# \n    net.eval()\n    #ì—í­ë§ˆë‹¤ ë‚´ê°€ ë³´ê³ ì‹¶ì€ê²ƒë“¤ì„ ë³´ì—¬ì£¼ëŠ” ì½”ë“œ\n    if (epoc % 50) ==0:\n        acc = ((net(X).data &gt; 0.5) == y).float().mean().item()\n        print(f\"# of epochs={epoc}   \\t train_acc = {acc:.4f}\")\n\n# of epochs=50       train_acc = 0.4677\n# of epochs=100      train_acc = 0.4677\n# of epochs=150      train_acc = 0.4745\n# of epochs=200      train_acc = 0.5223\n# of epochs=250      train_acc = 0.6441\n# of epochs=300      train_acc = 0.7686\n# of epochs=350      train_acc = 0.8469\n# of epochs=400      train_acc = 0.8979\n# of epochs=450      train_acc = 0.9302\n# of epochs=500      train_acc = 0.9492\n\n\nğŸ—£ï¸ ë¹ ë¦„\n- Step4: ì˜ˆì¸¡ & ê²°ê³¼ë¶„ì„\ntrain acc\n\n((net(X).data &gt; 0.5) == y).float().mean()\n\ntensor(0.9492, device='cuda:0')\n\n\ntest acc\n\n# ((net(XX).data&gt;0.5) == yy).float().mean() # net(XX)ê°€ ë¬¸ì œ\n\nğŸ—£ï¸ XXë¥¼ GPUì— ì˜¬ë¦¬ë“ ê°€, netë¥¼ CPUì— ë‚´ë¦¬ë“ ê°€\n\nXX = XX.to(\"cuda:0\")\nyy = yy.to(\"cuda:0\") \n\n\n((net(XX).data&gt;0.5) == yy).float().mean()\n\ntensor(0.9626, device='cuda:0')"
  },
  {
    "objectID": "posts/06wk-1.html#d.-ë¯¸ë‹ˆë°°ì¹˜ë„-ì‚¬ìš©",
    "href": "posts/06wk-1.html#d.-ë¯¸ë‹ˆë°°ì¹˜ë„-ì‚¬ìš©",
    "title": "06wk-1: (ì‹ ê²½ë§) â€“ ë°ì´í„°ë¶„ì„ ì½”ë”©íŒ¨í„´",
    "section": "D. ë¯¸ë‹ˆë°°ì¹˜ë„ ì‚¬ìš©",
    "text": "D. ë¯¸ë‹ˆë°°ì¹˜ë„ ì‚¬ìš©\n- Step1: ë°ì´í„°ì •ë¦¬\nğŸ—£ï¸ ë‹¤ì‹œ CPUë¡œ ë‚´ë¦¼\n\nX = X.to(\"cpu\")\ny = y.to(\"cpu\")\nXX = XX.to(\"cpu\")\nyy = yy.to(\"cpu\")\n\nğŸ—£ï¸(\n\nX.shape\n\ntorch.Size([12665, 784])\n\n\n\nds  = torch.utils.data.TensorDataset(X,y)\ndl = torch.utils.data.DataLoader(ds,batch_size = 16) \n\n\n# for Xm, ym in dl: # m: ë¯¸ë‹ˆ ë°°ì¹˜\n#     print(Xm) # í•˜ë‚˜í•˜ë‚˜ê°€ ë¯¸ë‹ˆë°°ì¹˜\n\n\n# for Xm, ym in dl: # m: ë¯¸ë‹ˆ ë°°ì¹˜\n#     print(Xm.shape) # ê²°ê³¼: torch.Size([16, 784])\n\n\nds  = torch.utils.data.TensorDataset(X,y)\ndl = torch.utils.data.DataLoader(ds,batch_size = 16, shuffle=True) \n\n\n# for Xm, ym in dl: # m: ë¯¸ë‹ˆ ë°°ì¹˜\n#     print(ym) # ì„ì—¬ ìˆìŒ\n\n\n# for Xm, ym in dl: # m: ë¯¸ë‹ˆ ë°°ì¹˜\n#     print(ym.shape) # ê²°ê³¼: torch.Size([16, 1])\n\n)ğŸ—£ï¸\n\nds  = torch.utils.data.TensorDataset(X,y)\ndl = torch.utils.data.DataLoader(ds,batch_size = 16, shuffle=True) \n\n- Step2: í•™ìŠµê°€ëŠ¥í•œ ì˜¤ë¸Œì íŠ¸ë“¤ì˜ ì„¤ì • (ëª¨ë¸ë§ê³¼ì • í¬í•¨)\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(784,32),\n    torch.nn.Dropout(0.9),\n    torch.nn.ReLU(),\n    torch.nn.Linear(32,1),\n    torch.nn.Sigmoid()\n).to(\"cuda:0\")\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD(net.parameters())\n\n- Step3: í•™ìŠµ (=ì í•©)\n\nğŸ—£ï¸\n\nX=X.to(â€œcuda:0â€), y=y.to(â€œcuda:0â€)ë¥¼ í•  ìˆ˜ëŠ” ì—†ìœ¼ë¯€ë¡œ ë¯¸ë‹ˆë°°ì¹˜ ë³„ë¡œ GPUì— ì˜¬ë¦¼\nepochì„ 500ë²ˆì”© ëŒë¦´ í•„ìš”ëŠ” ì—†ìœ¼ë¯€ë¡œ 2ë²ˆë§Œ ëŒë¦¼\n\n\n\nfor epoc in range(1,3):\n    net.train()\n    #---ì—í­ì‹œì‘---# \n    for Xm,ym in dl:         \n        Xm = Xm.to(\"cuda:0\")\n        ym = ym.to(\"cuda:0\")\n        # 1 \n        ym_hat = net(Xm) \n        # 2 \n        loss = loss_fn(ym_hat,ym) \n        # 3 \n        loss.backward()\n        # 4 \n        optimizr.step()\n        optimizr.zero_grad()\n    #---ì—í­ë---# \n    net.eval()\n    #ì—í­ë§ˆë‹¤ ë‚´ê°€ ë³´ê³ ì‹¶ì€ê²ƒë“¤ì„ ë³´ì—¬ì£¼ëŠ” ì½”ë“œ\n    s = 0 \n    for Xm, ym in dl:\n        Xm = Xm.to(\"cuda:0\")\n        ym = ym.to(\"cuda:0\")\n        s = s + ((net(Xm) &gt; 0.5) == ym).float().sum()\n    acc = s/12665        \n    print(f\"# of epochs={epoc}   \\t train_acc = {acc:.4f}\")\n\n# of epochs=1        train_acc = 0.9860\n# of epochs=2        train_acc = 0.9931\n\n\nğŸ—£ï¸ ë‹¤ë¥¸ ë°©ë²• (ì´ë ‡ê²Œ í•˜ë©´ ì‰¬ì›€)\nfor epoc in range(1,3):\n    net.train()\n    net.gpu()\n    #---ì—í­ì‹œì‘---# \n    for Xm,ym in dl:         \n        Xm = Xm.to(\"cuda:0\")\n        ym = ym.to(\"cuda:0\")\n        # 1 \n        ym_hat = net(Xm) \n        # 2 \n        loss = loss_fn(ym_hat,ym) \n        # 3 \n        loss.backward()\n        # 4 \n        optimizr.step()\n        optimizr.zero_grad()\n    #---ì—í­ë---# \n    net.eval()\n    net.to(\"cpu\")\n    #ì—í­ë§ˆë‹¤ ë‚´ê°€ ë³´ê³ ì‹¶ì€ê²ƒë“¤ì„ ë³´ì—¬ì£¼ëŠ” ì½”ë“œ\n    acc = ((net(X).data &gt; 0.5) == y).float().mean().item()\n    print(f\"# of epochs={epoc}   \\t train_acc = {acc:.4f}\")\n\nğŸ—£ï¸ ë‹¤ë¥¸ ë°©ë²• (netë¥¼ GPUë¡œ ìœ ì§€í•˜ê³  ì‹¶ìœ¼ë©´) =&gt; ê°•ì˜ì•ˆ ì½”ë“œ\n\nmean ëŒ€ì‹  sum\ní•˜ë‚˜ì˜ ë¯¸ë‹ˆë°°ì¹˜ì—ì„œ ë§ì€ ê²ƒì˜ ê°œìˆ˜ë¥¼ sì— ê³„ì† ëˆ„ì  ì‹œí‚´ (forë¬¸)\nfor ë¬¸ì´ ì¢…ë£Œë˜ê³  së¥¼ ì´ ê°œìˆ˜(X.shape)ë¡œ ë‚˜ëˆ„ë©´ accuracyê°€ ê³„ì‚°ë¨\n\n\n- Step4: ì˜ˆì¸¡ & ê²°ê³¼ë¶„ì„\nğŸ—£ï¸(\n\n# net(X) # error\n\n\nnetì€ cudaì— ìˆê³  XëŠ” cpuì— ìˆìŒ\n\nnetì„ cpuë¡œ ë‚´ë¦´ ê²ƒì¸ì§€, Xë¥¼ cudaë¡œ ì˜¬ë¦´ ê²ƒì¸ì§€ ì„ íƒ\n\nXë¥¼ cudaë¡œ ì˜¬ë¦¬ê¸° ì‹«ì–´ì„œ ë¯¸ë‹ˆë°°ì¹˜ë¥¼ ì‚¬ìš©í•˜ì˜€ìœ¼ë¯€ë¡œ netì„ cpuë¡œ ë‚´ë¦¼\n\n)ğŸ—£ï¸\n\nnet.to(\"cpu\")\n\nSequential(\n  (0): Linear(in_features=784, out_features=32, bias=True)\n  (1): Dropout(p=0.9, inplace=False)\n  (2): ReLU()\n  (3): Linear(in_features=32, out_features=1, bias=True)\n  (4): Sigmoid()\n)\n\n\ntrain acc\n\n((net(X) &gt; 0.5) == y).float().mean()\n\ntensor(0.9931)\n\n\ntest acc\n\n((net(XX) &gt; 0.5) == yy).float().mean()\n\ntensor(0.9967)\n\n\nğŸ—£ï¸ testë„ ì˜ ë‚˜ì˜¤ë¯€ë¡œ ì˜¤ë²„í”¼íŒ… X\n\nì ì  ë¹„ë³¸ì§ˆì ì¸ ì½”ë“œê°€ ëŠ˜ì–´ë‚¨ (=ì½”ë“œê°€ ë“œëŸ½ë‹¤ëŠ” ì†Œë¦¬ì—ìš”) â€“&gt; Trainerì˜ ê°œë… ë“±ì¥\n\n\nğŸ—£ï¸\n\në”¥ëŸ¬ë‹ ê°€ì§€ê³  ë¶„ì„í•˜ë©´ â€“&gt; íŠ¸ë ˆì´ë„ˆê°€ ìˆëŠ” ë‹¤ë¥¸ íŒ¨í‚¤ì§€ë¥¼ ì¨ì•¼í•¨ (í•™ë¶€ ìˆ˜ì¤€)\n\níŒŒì´í† ì¹˜ ë¼ì´íŠ¸ë‹\ní—ˆê¹…í˜ì´ìŠ¤\n\nì—°êµ¬í• ë•ŒëŠ” íŒŒì´í† ì¹˜ ì´í•´í•´ì•¼ í•¨"
  },
  {
    "objectID": "posts/07wk-2.html",
    "href": "posts/07wk-2.html",
    "title": "07wk-2: (í•©ì„±ê³±ì‹ ê²½ë§) â€“ CNN í•µì‹¬ë ˆì´ì–´, CNNì˜ í•™ìŠµì›ë¦¬, FashionMNIST",
    "section": "",
    "text": "ğŸ“˜ Note Format Guide\nThis format serves as a structured guide for organizing lecture content, personal interpretation, experiments, and study-related questions.\nğŸ“ ğŸ—£ï¸ âœï¸ ğŸ”¬ â“"
  },
  {
    "objectID": "posts/07wk-2.html#a.-torch.nn.relu",
    "href": "posts/07wk-2.html#a.-torch.nn.relu",
    "title": "07wk-2: (í•©ì„±ê³±ì‹ ê²½ë§) â€“ CNN í•µì‹¬ë ˆì´ì–´, CNNì˜ í•™ìŠµì›ë¦¬, FashionMNIST",
    "section": "A. torch.nn.ReLU",
    "text": "A. torch.nn.ReLU"
  },
  {
    "objectID": "posts/07wk-2.html#b.-torch.nn.maxpool2d",
    "href": "posts/07wk-2.html#b.-torch.nn.maxpool2d",
    "title": "07wk-2: (í•©ì„±ê³±ì‹ ê²½ë§) â€“ CNN í•µì‹¬ë ˆì´ì–´, CNNì˜ í•™ìŠµì›ë¦¬, FashionMNIST",
    "section": "B. torch.nn.MaxPool2d",
    "text": "B. torch.nn.MaxPool2d"
  },
  {
    "objectID": "posts/07wk-2.html#c.-torch.nn.conv2d",
    "href": "posts/07wk-2.html#c.-torch.nn.conv2d",
    "title": "07wk-2: (í•©ì„±ê³±ì‹ ê²½ë§) â€“ CNN í•µì‹¬ë ˆì´ì–´, CNNì˜ í•™ìŠµì›ë¦¬, FashionMNIST",
    "section": "C. torch.nn.Conv2d",
    "text": "C. torch.nn.Conv2d\n(ì˜ˆì‹œ1) ì—°ì‚°ë°©ë²•, stride=2\n\nimg = torch.rand(1,1,4,4)\nconv = torch.nn.Conv2d(in_channels=1,out_channels=1,kernel_size=2,stride=2)\n\n\nimg\n\ntensor([[[[0.4497, 0.4023, 0.3869, 0.1770],\n          [0.6399, 0.1175, 0.1347, 0.2738],\n          [0.4462, 0.3765, 0.1285, 0.7986],\n          [0.9917, 0.9030, 0.9250, 0.5513]]]])\n\n\n\nconv(img)\n\ntensor([[[[ 0.0084,  0.0557],\n          [-0.1183,  0.1122]]]], grad_fn=&lt;ConvolutionBackward0&gt;)\n\n\n??\n\nconv.weight.data, conv.bias.data\n\n(tensor([[[[-0.0637,  0.4369],\n           [-0.2863, -0.0190]]]]),\n tensor([0.0468]))\n\n\n\n(img[:,  :,  :2,  :2] * conv.weight.data).sum()+conv.bias.data, conv(img)\n\n(tensor([0.0084]),\n tensor([[[[ 0.0084,  0.0557],\n           [-0.1183,  0.1122]]]], grad_fn=&lt;ConvolutionBackward0&gt;))\n\n\n\n(img[:,  :,  :2,  2:] * conv.weight.data).sum()+conv.bias.data, conv(img)\n\n(tensor([0.0557]),\n tensor([[[[ 0.0084,  0.0557],\n           [-0.1183,  0.1122]]]], grad_fn=&lt;ConvolutionBackward0&gt;))\n\n\n\n(img[:,  :,  :2,  2:] * conv.weight.data).sum()+conv.bias.data, conv(img)\n\n(tensor([0.0557]),\n tensor([[[[ 0.0084,  0.0557],\n           [-0.1183,  0.1122]]]], grad_fn=&lt;ConvolutionBackward0&gt;))\n\n\n\n(img[:,  :,  2:,  2:] * conv.weight.data).sum()+conv.bias.data, conv(img)\n\n(tensor([0.1122]),\n tensor([[[[ 0.0084,  0.0557],\n           [-0.1183,  0.1122]]]], grad_fn=&lt;ConvolutionBackward0&gt;))\n\n\n\n\n\n\n\n\nNote\n\n\n\nì…ë ¥ì´ 1ì¥ì˜ í‘ë°±ì´ë¯¸ì§€ì´ê³  ì¶œë ¥ë„ 1ì¥ì˜ í‘ë°±ì´ë¯¸ì§€ì¼ ê²½ìš° ì»¨ë³¼ë£¨ì…˜ ê³„ì‚°ê³¼ì • ìš”ì•½1\n\nìœˆë„ìš°ìƒì„±: kernel_size = (?,?) ì¸ ìœˆë„ìš°ë¥¼ ë§Œë“¦\nsub-imgìƒì„±: ì…ë ¥ ì´ë¯¸ì§€ì— ìœˆë„ìš°ë¥¼ í†µê³¼ì‹œì¼œ (?,?) í¬ê¸°ì˜ sub-imgë¥¼ ë§Œë“¦.\nì—°ì‚°: sub-imgì˜ ê° ì›ì†Œì— conv.weightì˜ ê°’ì„ ì›ì†Œë³„ë¡œ (=element-wisely) ê³±í•˜ê³  ê²°ê³¼ë¥¼ ë”í•¨. (ë§Œì•½ì— conv.biasê°€ ìˆë‹¤ë©´ ìµœì¢…ê²°ê³¼ì— biasë¥¼ ë”í•¨)\nì´ë™&ë°˜ë³µ: ìœˆë„ìš°ë¥¼ stride ë§Œí¼ ì´ë™í•˜ì—¬ ë°˜ë³µ. (stride=1 ì´ë¼ë©´ í•œì¹¸ì”©, stride=2 ë¼ë©´ ë‘ì¹¸ì”© ì´ë™)\n\n\n\n(ì˜ˆì‹œ2) â€“ ì¬í˜„\nâ€œA guide to convolution arithmetic for deep learningâ€ [@dumoulin2016guide] ì— ë‚˜ì˜¨ ê·¸ë¦¼ì¬í˜„\n\nref: https://arxiv.org/abs/1603.07285\n\nğŸ—£ï¸ ìš°ì¸¡ í•˜ë‹¨ ì‘ì€ ìˆ«ì: weight\n\n\n\nFig: conv2d ê³„ì‚°ê³¼ì •ì‹œê°í™”\n\n\n\nimg = torch.tensor([\n    [3,3,2,1,0],\n    [0,0,1,3,1],\n    [3,1,2,2,3],\n    [2,0,0,2,2],\n    [2,0,0,0,1]\n]).reshape(1,1,5,5).float()\nimg\n\ntensor([[[[3., 3., 2., 1., 0.],\n          [0., 0., 1., 3., 1.],\n          [3., 1., 2., 2., 3.],\n          [2., 0., 0., 2., 2.],\n          [2., 0., 0., 0., 1.]]]])\n\n\n\nconv = torch.nn.Conv2d(in_channels=1,out_channels=1,kernel_size=3,bias=False) # bias=0 í•˜ë“œì½”ë”©í•´ë„ ë¨\nconv.weight.data = torch.tensor([[[\n    [ 0.0, 1.0, 2.0],\n    [ 2.0, 2.0, 0.0],\n    [ 0.0, 1.0, 2.0]\n]]])\n\n\nconv(img)\n\ntensor([[[[12., 12., 17.],\n          [10., 17., 19.],\n          [ 9.,  6., 14.]]]], grad_fn=&lt;ConvolutionBackward0&gt;)\n\n\n(ì˜ˆì‹œ3) ì´ë™í‰ê· \n\nimg = torch.arange(1,17).float().reshape(1,1,4,4)\nimg\n\ntensor([[[[ 1.,  2.,  3.,  4.],\n          [ 5.,  6.,  7.,  8.],\n          [ 9., 10., 11., 12.],\n          [13., 14., 15., 16.]]]])\n\n\n\nconv = torch.nn.Conv2d(in_channels=1,out_channels=1,kernel_size=2,stride=1,bias=False)\nconv.weight.data = conv.weight.data*0 + 1/4\nconv.weight.data\n\ntensor([[[[0.2500, 0.2500],\n          [0.2500, 0.2500]]]])\n\n\nğŸ—£ï¸ ì´ë™í•˜ë©´ì„œ í‰ê· ì„ ê³„ì‚°\n\nconv(img)\n\ntensor([[[[ 3.5000,  4.5000,  5.5000],\n          [ 7.5000,  8.5000,  9.5000],\n          [11.5000, 12.5000, 13.5000]]]], grad_fn=&lt;ConvolutionBackward0&gt;)\n\n\n(ì˜ˆì‹œ4) 2ê°œì˜ ì´ë¯¸ì§€\nğŸ—£ï¸ (n,1,?,?), (1,3,?,?) ë“±ë„ ê°€ëŠ¥\n- ê°œë…: (1,1,?,?) \\(\\to\\) (1,1,?,?) ì˜ convë¥¼ observation ë³„ë¡œ ì ìš©\n\nconv ì— í¬í•¨ëœ íŒŒë¼ë©”í„° ìˆ˜ëŠ” (1,1,?,?) \\(\\to\\) (1,1,?,?) ì¸ ê²½ìš°ì™€ (n,1,?,?) \\(\\to\\) (n,1,?,?)ì¸ ê²½ìš°ê°€ ë™ì¼\n\n\nimgs = torch.arange(1,33).float().reshape(2,1,4,4)\nconv = torch.nn.Conv2d(in_channels=1,out_channels=1,kernel_size=2,stride=1,bias=False)\nconv.weight.data = conv.weight.data*0 + 1/4\n\nğŸ—£ï¸ ìœ„ì— ìˆëŠ” ê±° í•œ ì¥, ì•„ë˜ì— ìˆëŠ” ê±° í•œ ì¥\n\nimgs\n\ntensor([[[[ 1.,  2.,  3.,  4.],\n          [ 5.,  6.,  7.,  8.],\n          [ 9., 10., 11., 12.],\n          [13., 14., 15., 16.]]],\n\n\n        [[[17., 18., 19., 20.],\n          [21., 22., 23., 24.],\n          [25., 26., 27., 28.],\n          [29., 30., 31., 32.]]]])\n\n\n\nconv(imgs) # ì´ë™í‰ê· \n\ntensor([[[[ 3.5000,  4.5000,  5.5000],\n          [ 7.5000,  8.5000,  9.5000],\n          [11.5000, 12.5000, 13.5000]]],\n\n\n        [[[19.5000, 20.5000, 21.5000],\n          [23.5000, 24.5000, 25.5000],\n          [27.5000, 28.5000, 29.5000]]]], grad_fn=&lt;ConvolutionBackward0&gt;)\n\n\nğŸ—£ï¸(\n\nconv.weight.shape\n\ntorch.Size([1, 1, 2, 2])\n\n\n\nìˆ«ìê°€ 4ê°œ (2*2)\ní•˜ë‚˜ì˜ convë¥¼ ë‹¤ë¥¸ ì´ë¯¸ì§€ì—ë„ ì ìš©\n\nconv ì— í¬í•¨ëœ íŒŒë¼ë©”í„° ìˆ˜ëŠ” (1,1,?,?) \\(\\to\\) (1,1,?,?) ì¸ ê²½ìš°ì™€ (n,1,?,?) \\(\\to\\) (n,1,?,?)ì¸ ê²½ìš°ê°€ ë™ì¼\n\n\n)ğŸ—£ï¸\n(ì˜ˆì‹œ5) 2ê°œì˜ ì´ë¯¸ì§€, 2ê°œì˜ out_channels\nğŸ—£ï¸ ê°™ì€ ì´ë¯¸ì§€ì— convë¥¼ 2ë²ˆì”© ê±°ëŠ” ê²½ìš°\n- ê°œë…: (1,1,?,?) \\(\\to\\) (1,1,?,?) ì˜ convë¥¼ í•œë²ˆ ì ìš©, ê·¸ê²ƒê³¼ ë³„ê°œë¡œ (1,1,?,?) \\(\\to\\) (1,1,?,?) ì¸ ë‹¤ë¥¸ convë¥¼ ì ìš©í•¨. (ì¦‰ í•˜ë‚˜ì˜ observationë‹¹ 2ë²ˆ convë³€í™˜) ì´ê²ƒì„ observationë³„ë¡œ ë°˜ë³µ\n\n(1,1,?,?) \\(\\to\\) (1,2,?,?) ì¸ ê²½ìš°ëŠ” (1,1,?,?) \\(\\to\\) (1,1,?,?)ì¸ ê²½ìš°ë³´ë‹¤ convì— í¬í•¨ëœ íŒŒë¼ë©”í„° ìˆ˜ê°€ 2ë°° ë§ìŒ\nê·¸ëŸ°ë° (1,1,?,?) \\(\\to\\) (1,2,?,?) ì¸ ê²½ìš°ì™€ (n,1,?,?) \\(\\to\\) (n,2,?,?)ì¸ ê²½ìš°ëŠ” convì— í¬í•¨ëœ íŒŒë¼ë©”í„° ìˆ˜ê°€ ê°™ìŒ.\në”°ë¼ì„œ (n,1,?,?) \\(\\to\\) (n,2,?,?) ì¸ ê²½ìš°ëŠ” (1,1,?,?) \\(\\to\\) (1,1,?,?)ì¸ ê²½ìš°ë³´ë‹¤ convì— í¬í•¨ëœ íŒŒë¼ë©”í„° ìˆ˜ê°€ 2ë°° ë§ìŒ\n\n\nimg = torch.arange(1,33).float().reshape(2,1,4,4)\nconv = torch.nn.Conv2d(in_channels=1,out_channels=2,kernel_size=2,stride=1,bias=False)\n\nğŸ—£ï¸(\n\nconv.weight\n\nParameter containing:\ntensor([[[[-0.0698,  0.1491],\n          [ 0.3877,  0.0913]]],\n\n\n        [[[-0.2105,  0.0241],\n          [-0.4239,  0.2689]]]], requires_grad=True)\n\n\n\nimg.shape # 2ì¥, í‘ë°±, 4*4\n\ntorch.Size([2, 1, 4, 4])\n\n\n\nconv(img).shape\n\ntorch.Size([2, 2, 3, 3])\n\n\n)ğŸ—£ï¸\n\nimg\n\ntensor([[[[ 1.,  2.,  3.,  4.],\n          [ 5.,  6.,  7.,  8.],\n          [ 9., 10., 11., 12.],\n          [13., 14., 15., 16.]]],\n\n\n        [[[17., 18., 19., 20.],\n          [21., 22., 23., 24.],\n          [25., 26., 27., 28.],\n          [29., 30., 31., 32.]]]])\n\n\n\nconv.weight.data[0] = conv.weight.data[0]*0 +1/4 # í‰ê· ì„ ì˜ë¯¸ (bias=False)\nconv.weight.data[1] = conv.weight.data[0]*0\n\n\nconv(img)\n\ntensor([[[[ 3.5000,  4.5000,  5.5000],\n          [ 7.5000,  8.5000,  9.5000],\n          [11.5000, 12.5000, 13.5000]],\n\n         [[ 0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000]]],\n\n\n        [[[19.5000, 20.5000, 21.5000],\n          [23.5000, 24.5000, 25.5000],\n          [27.5000, 28.5000, 29.5000]],\n\n         [[ 0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000]]]], grad_fn=&lt;ConvolutionBackward0&gt;)\n\n\nğŸ—£ï¸ í•˜ë‚˜ì˜ ì´ë¯¸ì§€ê°€ 2ê°œì˜ ì±„ë„ë¡œ ë¶„ë¦¬ ë¨ (í•˜ë‚˜ì˜ ì±„ë„ì€ ì´ë™í‰ê· , ë‹¤ë¥¸ í•˜ë‚˜ëŠ” 0)"
  },
  {
    "objectID": "posts/07wk-2.html#a.-data",
    "href": "posts/07wk-2.html#a.-data",
    "title": "07wk-2: (í•©ì„±ê³±ì‹ ê²½ë§) â€“ CNN í•µì‹¬ë ˆì´ì–´, CNNì˜ í•™ìŠµì›ë¦¬, FashionMNIST",
    "section": "A. data",
    "text": "A. data\nì•„ë˜ì˜ 4ê°œì˜ ì´ë¯¸ì§€ë¥¼ ìƒê°í•˜ì .\n\nimg0 = torch.tensor([\n    [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n    [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n    [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n    [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n    [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n    [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n    [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n    [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n    [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n    [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n    [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n    [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n    [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n    [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n    [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n    [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n]).reshape(1, 1, 16, 16) \nimg1 = 0.1-torch.einsum('nchw-&gt;ncwh', img0.clone())\nimg2 = torch.zeros((1, 1, 16, 16))\nfor i in range(16):\n    for j in range(16):\n        if j &lt;= i:  # ëŒ€ê°ì„  ì•„ë˜ ì‚¼ê°í˜•\n            img2[0, 0, i, j] = 0.1\n# ë¹ˆ ì´ë¯¸ì§€\nimg3 = torch.zeros((1, 1, 16, 16))\nblock_size = 2\n# ë¸”ë¡ ë‹¨ìœ„ë¡œ ì±„ìš°ê¸°\nfor i in range(0, 16, block_size):\n    for j in range(0, 16, block_size):\n        if ((i // block_size) + (j // block_size)) % 2 == 0:\n            img3[0, 0, i:i+block_size, j:j+block_size] = 0.1\n\nğŸ—£ï¸(\n\nimg0.shape # 16*16 í‘ë°± ì´ë¯¸ì§€\n\ntorch.Size([1, 1, 16, 16])\n\n\n\nplt.imshow(img0.reshape(16,16),cmap=\"gray\")\n\n\n\n\n\n\n\n\n\n0.1ì´ í°ìƒ‰, 0ì´ ê²€ì •ìƒ‰\n\n\nplt.imshow(img1.reshape(16,16),cmap=\"gray\")\n\n\n\n\n\n\n\n\n\nplt.imshow(img2.reshape(16,16),cmap=\"gray\")\n\n\n\n\n\n\n\n\n\nplt.imshow(img3.reshape(16,16),cmap=\"gray\")\n\n\n\n\n\n\n\n\n\nimg3.shape\n\ntorch.Size([1, 1, 16, 16])\n\n\n\nimg3.reshape(16,16).shape\n\ntorch.Size([16, 16])\n\n\n\nimg3.squeeze().shape # ì°¨ì›ì´ 1ì¸ê²ƒì€ ì¤„ì–´ë“œëŠ” method\n\ntorch.Size([16, 16])\n\n\n)ğŸ—£ï¸\n\nfig, axs = plt.subplots(2,2)\nfig.set_figheight(8)\nfig.set_figwidth(8)\naxs[0][0].imshow(img0.squeeze(),cmap=\"gray\")\naxs[0][1].imshow(img1.squeeze(),cmap=\"gray\")\naxs[1][0].imshow(img2.squeeze(),cmap=\"gray\")\naxs[1][1].imshow(img3.squeeze(),cmap=\"gray\")\n\n\n\n\n\n\n\n\n\nimgs = torch.concat([img0,img1,img2,img3],axis=0)\nimgs.shape\n\ntorch.Size([4, 1, 16, 16])"
  },
  {
    "objectID": "posts/07wk-2.html#b.-vertical-edge",
    "href": "posts/07wk-2.html#b.-vertical-edge",
    "title": "07wk-2: (í•©ì„±ê³±ì‹ ê²½ë§) â€“ CNN í•µì‹¬ë ˆì´ì–´, CNNì˜ í•™ìŠµì›ë¦¬, FashionMNIST",
    "section": "B. vertical edge",
    "text": "B. vertical edge\nğŸ—£ï¸(\nx1 x2 x3 x4\nx5 x6 x7 x8\nx9 x10 x11 x12\nx13 x14 x15 x16\n\n[-1 1\n -1 1]\n\nx1 x2\nx5 x6\nì— ì ìš©í•˜ë©´\n-x1 x2\n-x5 x6\në‹¤ ë”í•˜ë©´\n-x1 + x2 -x5 + x6\n= (x2-x1) + (x6-x5)\në‘˜ ì”© ë¹„êµí•˜ì—¬ ë³€í™”ê°€ ìˆìœ¼ë©´ 1, ì—†ìœ¼ë©´ 0\n\nì˜ˆì‹œ)\n0 0 1 1\n0 0 1 1\n0 0 1 1\n0 0 1 1\nì ìš© í›„\n0 1 0\n0 1 0\n0 1 0\n\ní™•ì¥)\n0 0 0 1 1 1\n0 0 0 1 1 1\n0 0 0 1 1 1\n0 0 0 1 1 1\n0 0 0 1 1 1\n0 0 0 1 1 1\nì ìš© í›„\n0 0 1 0 0\n0 0 1 0 0\n0 0 1 0 0\n0 0 1 0 0\n0 0 1 0 0\n\nv_conv = torch.nn.Conv2d(\n    in_channels=1,\n    out_channels=1,\n    kernel_size=2,\n    bias=False\n)\n\n\nv_conv.weight.data = torch.tensor([[[\n    [-1.0, 1.0],\n    [-1.0, 1.0]\n]]])\n\n\nplt.imshow(v_conv(img0).squeeze().data, cmap=\"gray\")\n\n\n\n\n\n\n\n\n\nplt.imshow(v_conv(img1).squeeze().data, cmap=\"gray\")\n\n\n\n\n\n\n\n\n\nplt.imshow(v_conv(img2).squeeze().data, cmap=\"gray\")\n\n\n\n\n\n\n\n\n\nplt.imshow(v_conv(img3).squeeze().data, cmap=\"gray\")\n\n\n\n\n\n\n\n\n\níšŒìƒ‰: ê°’ ë³€í™” X, ê²€ì •ìƒ‰: í°ìƒ‰-&gt;ê²€ì •ìƒ‰, í°ìƒ‰: ê²€ì •ìƒ‰-&gt;í°ìƒ‰\nì´ë™í‰ê· ê³¼ kernel_sizeë¥¼ ë§ì¶°ì£¼ë©´ ë°‘ì˜ ì½”ë“œì™€ ê°™ìŒ\n\n)ğŸ—£ï¸\n\nv_conv = torch.nn.Conv2d(\n    in_channels=1,\n    out_channels=1,\n    kernel_size=4,\n    bias=False\n)\n\n\nv_conv.weight.data = torch.tensor([[[\n    [ 0, 0, 0, 0],\n    [ 0, 1.0, -1.0, 0],\n    [0, 1.0, -1.0, 0],\n    [ 0, 0, 0, 0]\n]]])\n\n\nì´ v_convëŠ” ì¢Œìš°ë°©í–¥ì˜ í”½ì…€ë³€í™”, ì¦‰ ìˆ˜ì§ ë°©í–¥ì˜ ì—£ì§€(vertical edge)ë¥¼ ê°ì§€í•˜ëŠ”ë° ì ì ˆí•˜ë‹¤.\n\n\nfig, axs = plt.subplots(2,2)\nfig.set_figheight(8)\nfig.set_figwidth(8)\naxs[0][0].imshow(v_conv(imgs)[0].squeeze().data,cmap=\"gray\")\naxs[0][1].imshow(v_conv(imgs)[1].squeeze().data,cmap=\"gray\")\naxs[1][0].imshow(v_conv(imgs)[2].squeeze().data,cmap=\"gray\")\naxs[1][1].imshow(v_conv(imgs)[3].squeeze().data,cmap=\"gray\")\n\n\n\n\n\n\n\n\nğŸ—£ï¸ ì™¼ìª½ê³¼ ì˜¤ë¥¸ìª½ì„ ë¹„êµí•˜ì—¬ edgeë¥¼ ê±¸ì—ˆë”ë‹ˆ ëª¨ë‘ 0ì´ë©´ ìœ„ì•„ë˜ì„ì„ ì•Œ ìˆ˜ ìˆìŒ"
  },
  {
    "objectID": "posts/07wk-2.html#c.-horizontal-edge",
    "href": "posts/07wk-2.html#c.-horizontal-edge",
    "title": "07wk-2: (í•©ì„±ê³±ì‹ ê²½ë§) â€“ CNN í•µì‹¬ë ˆì´ì–´, CNNì˜ í•™ìŠµì›ë¦¬, FashionMNIST",
    "section": "C. horizontal edge",
    "text": "C. horizontal edge\nâœï¸(\nx1 x2 x3 x4\nx5 x6 x7 x8\nx9 x10 x11 x12\nx13 x14 x15 x16\n\n[-1 -1\n  1 1]\n\nx1 x2\nx5 x6\nì— ì ìš©í•˜ë©´\n-x1 -x2\n x5 x6\në‹¤ ë”í•˜ë©´\n-x1 - x2 + x5 + x6\n= (x5-x1) + (x6-x2)\në‘˜ ì”© ë¹„êµí•˜ì—¬ ë³€í™”ê°€ ìˆìœ¼ë©´ 1, ì—†ìœ¼ë©´ 0\n\nì˜ˆì‹œ)\n1 1 1 1\n1 1 1 1\n0 0 0 0\n0 0 0 0\nì ìš© í›„\n0 0 0\n1 1 1\n0 0 0\n\ní™•ì¥)\n1 1 1 1 1 1\n1 1 1 1 1 1\n1 1 1 1 1 1\n0 0 0 0 0 0\n0 0 0 0 0 0\n0 0 0 0 0 0\nì ìš© í›„\n0 0 0 0 0\n0 0 0 0 0\n1 1 1 1 1\n0 0 0 0 0\n0 0 0 0 0\n)âœï¸\n\nh_conv = torch.nn.Conv2d(\n    in_channels=1,\n    out_channels=1,\n    kernel_size=4,\n    bias=False\n)\n\n\nh_conv.weight.data = torch.tensor([[[\n    [ 0, 0, 0, 0],\n    [ 0, -1.0, -1.0, 0],\n    [0, 1.0, 1.0, 0],\n    [ 0, 0, 0, 0]\n]]])\n\n\nì´ h_convëŠ” ìœ„ì•„ë ˆ ë°©í–¥ì˜ í”½ì…€ë³€í™”, ì¦‰ ìˆ˜í‰ì—£ì§€(horizontal edge)ë¥¼ ê°ì§€í•˜ëŠ”ë° ì ì ˆí•˜ë‹¤.\n\n\nfig, axs = plt.subplots(2,2)\nfig.set_figheight(8)\nfig.set_figwidth(8)\naxs[0][0].imshow(h_conv(imgs)[0].squeeze().data,cmap=\"gray\")\naxs[0][1].imshow(h_conv(imgs)[1].squeeze().data,cmap=\"gray\")\naxs[1][0].imshow(h_conv(imgs)[2].squeeze().data,cmap=\"gray\")\naxs[1][1].imshow(h_conv(imgs)[3].squeeze().data,cmap=\"gray\")\n\n\n\n\n\n\n\n\nğŸ—£ï¸ ìœ„ìª½ê³¼ ì•„ë˜ìª½ì„ ë¹„êµí•˜ì—¬ edgeë¥¼ ê±¸ì—ˆë”ë‹ˆ ëª¨ë‘ 0ì´ë©´ ì¢Œìš°ì„ì„ ì•Œ ìˆ˜ ìˆìŒ"
  },
  {
    "objectID": "posts/07wk-2.html#d.-ì´ë™í‰ê· ",
    "href": "posts/07wk-2.html#d.-ì´ë™í‰ê· ",
    "title": "07wk-2: (í•©ì„±ê³±ì‹ ê²½ë§) â€“ CNN í•µì‹¬ë ˆì´ì–´, CNNì˜ í•™ìŠµì›ë¦¬, FashionMNIST",
    "section": "D. ì´ë™í‰ê· ",
    "text": "D. ì´ë™í‰ê· \nğŸ—£ï¸(\n\nm_conv = torch.nn.Conv2d(\n    in_channels=1,\n    out_channels=1,\n    kernel_size=4,\n)\nm_conv.weight.data = m_conv.weight.data*0 + 1/16 # 16ê°œ pixelì— ëŒ€í•œ í‰ê· \nm_conv.bias.data = m_conv.bias.data*0\n\n\nm_conv(img0).shape\n\ntorch.Size([1, 1, 13, 13])\n\n\n\nm_conv(img0).squeeze().shape\n\ntorch.Size([13, 13])\n\n\n\nplt.imshow(m_conv(img0).squeeze().data, cmap=\"gray\")\n\n\n\n\n\n\n\n\n\ní‰ê· ì€ smoothing í•˜ëŠ” íš¨ê³¼\n\n\nplt.imshow(m_conv(img1).squeeze().data, cmap=\"gray\")\n\n\n\n\n\n\n\n\n\nplt.imshow(m_conv(img2).squeeze().data, cmap=\"gray\")\n\n\n\n\n\n\n\n\n\nplt.imshow(m_conv(img3).squeeze().data, cmap=\"gray\")\n\n\n\n\n\n\n\n\n\ní‰ê· ì€ íšŒìƒ‰ (ê²€ì€ ìƒ‰ìœ¼ë¡œ ë³´ì´ì§€ë§Œ ì•„ë‹˜)\n\n\nm_conv(img3)\n\ntensor([[[[0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500,\n           0.0500, 0.0500, 0.0500, 0.0500, 0.0500],\n          [0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500,\n           0.0500, 0.0500, 0.0500, 0.0500, 0.0500],\n          [0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500,\n           0.0500, 0.0500, 0.0500, 0.0500, 0.0500],\n          [0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500,\n           0.0500, 0.0500, 0.0500, 0.0500, 0.0500],\n          [0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500,\n           0.0500, 0.0500, 0.0500, 0.0500, 0.0500],\n          [0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500,\n           0.0500, 0.0500, 0.0500, 0.0500, 0.0500],\n          [0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500,\n           0.0500, 0.0500, 0.0500, 0.0500, 0.0500],\n          [0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500,\n           0.0500, 0.0500, 0.0500, 0.0500, 0.0500],\n          [0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500,\n           0.0500, 0.0500, 0.0500, 0.0500, 0.0500],\n          [0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500,\n           0.0500, 0.0500, 0.0500, 0.0500, 0.0500],\n          [0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500,\n           0.0500, 0.0500, 0.0500, 0.0500, 0.0500],\n          [0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500,\n           0.0500, 0.0500, 0.0500, 0.0500, 0.0500],\n          [0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500,\n           0.0500, 0.0500, 0.0500, 0.0500, 0.0500]]]],\n       grad_fn=&lt;ConvolutionBackward0&gt;)\n\n\n\n# plt.imshow(m_conv(img3).squeeze().data*0, cmap=\"gray\") # *0ì„ í•´ë„ ê²€ì€ ìƒ‰ìœ¼ë¡œ ë³´ì„\n\n\nplt.imshow(m_conv(img3).squeeze().data, cmap=\"gray\", vmin=-1, vmax=1) # ì‹¤ì§ˆì ìœ¼ë¡œ ì–´ë–¤ ìƒ‰ì¸ì§€ ì•Œë ¤ë©´\n\n\n\n\n\n\n\n\n\nplt.imshow(m_conv(img3).squeeze().data*0, cmap=\"gray\", vmin=-1, vmax=1) # ì‹¤ì§ˆì ìœ¼ë¡œ ì–´ë–¤ ìƒ‰ì¸ì§€ ì•Œë ¤ë©´\n\n\n\n\n\n\n\n\n\nplt.imshow(m_conv(img3).squeeze().data*0-1, cmap=\"gray\", vmin=-1, vmax=1) # ê²€ì •ìƒ‰\n\n\n\n\n\n\n\n\n\nplt.imshow(m_conv(img3).squeeze().data*0+1, cmap=\"gray\", vmin=-1, vmax=1) # í°ìƒ‰\n\n\n\n\n\n\n\n\n\nimgs.shape\n\ntorch.Size([4, 1, 16, 16])\n\n\n\nm_conv(imgs).shape\n\ntorch.Size([4, 1, 13, 13])\n\n\n)ğŸ—£ï¸\n\nm_conv = torch.nn.Conv2d(\n    in_channels=1,\n    out_channels=1,\n    kernel_size=4,\n)\nm_conv.weight.data = m_conv.weight.data*0 + 1/16\nm_conv.bias.data = m_conv.bias.data*0 - 0.05 # ì§„ì§œ 0ì´ ë¨\n\n\nfig, axs = plt.subplots(2,2)\nfig.set_figheight(8)\nfig.set_figwidth(8)\naxs[0][0].imshow(m_conv(imgs)[0].squeeze().data,cmap=\"gray\")\naxs[0][1].imshow(m_conv(imgs)[1].squeeze().data,cmap=\"gray\")\naxs[1][0].imshow(m_conv(imgs)[2].squeeze().data,cmap=\"gray\")\naxs[1][1].imshow(m_conv(imgs)[3].squeeze().data,cmap=\"gray\")\n\n\n\n\n\n\n\n\n\nğŸ—£ï¸\n\nì´ë™í‰ê· ì„ í–ˆë”ë‹ˆ 0ìœ¼ë¡œ ë‚˜ì˜¤ë©´ ì²´í¬ë¬´ëŠ¬ì¸ ê²ƒì„ ì•Œ ìˆ˜ ìˆìŒ\n3ê°œì˜ í•„í„°ì— ëª¨ë‘ 0ì´ ì•ˆë˜ë©´ ëŒ€ê°ì„ ì¸ ê²ƒì„ ì•Œ ìˆ˜ ìˆìŒ\n\n\nğŸ—£ï¸(\n\në§¤ìš° ë‹¤ì–‘í•œ ì¡°í•© ê°€ëŠ¥\n\n\nm_conv = torch.nn.Conv2d(\n    in_channels=1,\n    out_channels=1,\n    kernel_size=6, # 6ìœ¼ë¡œ ë³€ê²½\n)\nm_conv.weight.data = m_conv.weight.data*0 + 1/16\nm_conv.bias.data = m_conv.bias.data*0 - 0.05\n\n\nfig, axs = plt.subplots(2,2)\nfig.set_figheight(8)\nfig.set_figwidth(8)\naxs[0][0].imshow(m_conv(imgs)[0].squeeze().data,cmap=\"gray\")\naxs[0][1].imshow(m_conv(imgs)[1].squeeze().data,cmap=\"gray\")\naxs[1][0].imshow(m_conv(imgs)[2].squeeze().data,cmap=\"gray\")\naxs[1][1].imshow(m_conv(imgs)[3].squeeze().data,cmap=\"gray\")\n\n\n\n\n\n\n\n\n\nfig, axs = plt.subplots(2,2)\nfig.set_figheight(8)\nfig.set_figwidth(8)\naxs[0][0].imshow(m_conv(imgs)[0].squeeze().data,cmap=\"gray\")\naxs[0][1].imshow(m_conv(imgs)[1].squeeze().data,cmap=\"gray\")\naxs[1][0].imshow(m_conv(imgs)[2].squeeze().data,cmap=\"gray\")\naxs[1][1].imshow(m_conv(m_conv(imgs))[3].squeeze().data,cmap=\"gray\") # í•œ ë²ˆ ë” ê±¸ë©´\n\n\n\n\n\n\n\n\n\nfig, axs = plt.subplots(2,2)\nfig.set_figheight(8)\nfig.set_figwidth(8)\naxs[0][0].imshow(m_conv(imgs)[0].squeeze().data,cmap=\"gray\")\naxs[0][1].imshow(m_conv(imgs)[1].squeeze().data,cmap=\"gray\")\naxs[1][0].imshow(m_conv(m_conv(imgs))[2].squeeze().data,cmap=\"gray\") # ì´ê²ƒë„ í•œ ë²ˆ ë”\naxs[1][1].imshow(m_conv(m_conv(imgs))[3].squeeze().data,cmap=\"gray\")\n\n\n\n\n\n\n\n\n)ğŸ—£ï¸"
  },
  {
    "objectID": "posts/07wk-2.html#e.-cde-relu-mp",
    "href": "posts/07wk-2.html#e.-cde-relu-mp",
    "title": "07wk-2: (í•©ì„±ê³±ì‹ ê²½ë§) â€“ CNN í•µì‹¬ë ˆì´ì–´, CNNì˜ í•™ìŠµì›ë¦¬, FashionMNIST",
    "section": "E. (C,D,E) + relu + mp",
    "text": "E. (C,D,E) + relu + mp\nğŸ—£ï¸(\n\nm_conv = torch.nn.Conv2d(\n    in_channels=1,\n    out_channels=1,\n    kernel_size=4,\n)\nm_conv.weight.data = m_conv.weight.data*0 + 1/16\nm_conv.bias.data = m_conv.bias.data*0 - 0.05 # ì§„ì§œ 0ì´ ë¨\n\n\nm_conv(imgs).shape\n\ntorch.Size([4, 1, 13, 13])\n\n\n\nmp = torch.nn.MaxPool2d(kernel_size=13) # 13*13 ì¤‘ì— ê°€ì¥ í° ê°’ì„ ë½‘ìŒ\n\n\nmp(m_conv(imgs)) # ë§ˆì§€ë§‰ë§Œ 0ì´ ë¨\n\ntensor([[[[5.0000e-02]]],\n\n\n        [[[5.0000e-02]]],\n\n\n        [[[5.0000e-02]]],\n\n\n        [[[9.3132e-10]]]], grad_fn=&lt;MaxPool2DWithIndicesBackward0&gt;)\n\n\n\nmp(h_conv(imgs)) # ì²˜ìŒë§Œ 0ì´ ë¨\n\ntensor([[[[0.0000]]],\n\n\n        [[[0.2000]]],\n\n\n        [[[0.1000]]],\n\n\n        [[[0.2000]]]], grad_fn=&lt;MaxPool2DWithIndicesBackward0&gt;)\n\n\n\nmp(v_conv(imgs)) # ë‘ë²ˆì§¸ë§Œ 0ì´ ë¨\n\ntensor([[[[0.2000]]],\n\n\n        [[[0.0000]]],\n\n\n        [[[0.1000]]],\n\n\n        [[[0.2000]]]], grad_fn=&lt;MaxPool2DWithIndicesBackward0&gt;)\n\n\n\nì—¬ê¸°ì„œëŠ” í° ì˜ë¯¸ëŠ” ì—†ì§€ë§Œ reluë„ ê°™ì´ í†µê³¼ë¥¼ ì‹œí‚¤ë©´ ê²°ê³¼ê°€ ë‹¤ì–‘í•´ì§\n\n\nrelu = torch.nn.ReLU()\n\n\nfig, axs = plt.subplots(2,2)\nfig.set_figheight(8)\nfig.set_figwidth(8)\naxs[0][0].imshow(relu(h_conv(imgs))[0].squeeze().data,cmap=\"gray\")\naxs[0][1].imshow(relu(h_conv(imgs))[1].squeeze().data,cmap=\"gray\")\naxs[1][0].imshow(relu(h_conv(imgs))[2].squeeze().data,cmap=\"gray\")\naxs[1][1].imshow(relu(h_conv(imgs))[3].squeeze().data,cmap=\"gray\")\n\n\n\n\n\n\n\n\n)ğŸ—£ï¸\n\nrelu = torch.nn.ReLU()\nmp = torch.nn.MaxPool2d(kernel_size=13)\n\n\nmp(relu(v_conv(imgs)))\n\ntensor([[[[0.2000]]],\n\n\n        [[[0.0000]]],\n\n\n        [[[0.1000]]],\n\n\n        [[[0.2000]]]], grad_fn=&lt;MaxPool2DWithIndicesBackward0&gt;)\n\n\n\nmp(relu(h_conv(imgs)))\n\ntensor([[[[0.0000]]],\n\n\n        [[[0.2000]]],\n\n\n        [[[0.1000]]],\n\n\n        [[[0.2000]]]], grad_fn=&lt;MaxPool2DWithIndicesBackward0&gt;)\n\n\n\nmp(relu(m_conv(imgs)))\n\ntensor([[[[5.0000e-02]]],\n\n\n        [[[5.0000e-02]]],\n\n\n        [[[5.0000e-02]]],\n\n\n        [[[9.3132e-10]]]], grad_fn=&lt;MaxPool2DWithIndicesBackward0&gt;)\n\n\nğŸ—£ï¸ ë§¤ìš° ë§ì€ ìˆ«ìë“¤ì´ êµ¬ë¶„í•˜ê¸° ìš©ì´í•œ ìˆ«ìë“¤ë¡œ ìš”ì•½ë¨"
  },
  {
    "objectID": "posts/07wk-2.html#f.-ëŒ€ì¶©-ì´ëŸ°-êµ¬ì¡°",
    "href": "posts/07wk-2.html#f.-ëŒ€ì¶©-ì´ëŸ°-êµ¬ì¡°",
    "title": "07wk-2: (í•©ì„±ê³±ì‹ ê²½ë§) â€“ CNN í•µì‹¬ë ˆì´ì–´, CNNì˜ í•™ìŠµì›ë¦¬, FashionMNIST",
    "section": "F. ëŒ€ì¶© ì´ëŸ° êµ¬ì¡°",
    "text": "F. ëŒ€ì¶© ì´ëŸ° êµ¬ì¡°\nğŸ—£ï¸(\nì´ë¯¸ì§€1-&gt;h-&gt;r-&gt;mp-&gt;0,??,??\n      -&gt;v-&gt;r-&gt;mp-&gt;\n      -&gt;m-&gt;r-&gt;mp\n\nì´ë¯¸ì§€1-&gt;(h,v,m)-&gt;r-&gt;mp-&gt;0,??,??\nì´ë¯¸ì§€2-&gt;(h,v,m)-&gt;r-&gt;mp-&gt;0,??,??\nì´ë¯¸ì§€3-&gt;(h,v,m)-&gt;r-&gt;mp-&gt;0,??,??\nì´ë¯¸ì§€4-&gt;(h,v,m)-&gt;r-&gt;mp-&gt;0,??,??\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(in_channels=1,out_channels=3,kernel_size=4), # bias ì‚¬ìš©\n    torch.nn.ReLU(), # ë‹¤ì–‘í•´ì§\n    torch.nn.MaxPool2d(kernel_size=13),\n    torch.nn.Flatten()\n)\n\n\nnet[0].weight.shape\n\ntorch.Size([3, 1, 4, 4])\n\n\n1,1,4,4 --&gt; m_conv\n1,1,4,4 --&gt; h_conv\n1,1,4,4 --&gt; v_conv\n3,1,4,4 --&gt; conv\n\nnet[0].weight.data\n\ntensor([[[[-0.0833, -0.0289, -0.2051, -0.2370],\n          [-0.1144, -0.0212, -0.0276,  0.1908],\n          [-0.1374, -0.1581, -0.0825, -0.1222],\n          [-0.2006, -0.1606, -0.1211,  0.0583]]],\n\n\n        [[[ 0.2180, -0.1371, -0.0198, -0.1251],\n          [-0.0198, -0.0584,  0.1258,  0.0500],\n          [ 0.0558, -0.0605, -0.1558,  0.2098],\n          [ 0.0355, -0.0771,  0.0439,  0.0692]]],\n\n\n        [[[ 0.2496, -0.0280, -0.1691, -0.2486],\n          [-0.1302,  0.1365, -0.0934, -0.2321],\n          [-0.0914,  0.2089, -0.1784,  0.1483],\n          [ 0.1498,  0.0237,  0.2464,  0.0540]]]])\n\n\n\ntorch.concat(\n    [v_conv.weight.data,\n     h_conv.weight.data,\n     m_conv.weight.data],axis=0).shape\n\ntorch.Size([3, 1, 4, 4])\n\n\n\nnet[0].weight.data = torch.concat(\n    [v_conv.weight.data,\n     h_conv.weight.data,\n     m_conv.weight.data],axis=0)\nnet[0].weight.data\n\ntensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  1.0000, -1.0000,  0.0000],\n          [ 0.0000,  1.0000, -1.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  0.0000]]],\n\n\n        [[[ 0.0000,  0.0000,  0.0000,  0.0000],\n          [ 0.0000, -1.0000, -1.0000,  0.0000],\n          [ 0.0000,  1.0000,  1.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  0.0000]]],\n\n\n        [[[ 0.0625,  0.0625,  0.0625,  0.0625],\n          [ 0.0625,  0.0625,  0.0625,  0.0625],\n          [ 0.0625,  0.0625,  0.0625,  0.0625],\n          [ 0.0625,  0.0625,  0.0625,  0.0625]]]])\n\n\n\nnet[0].bias.data\n\ntensor([ 0.1274,  0.2041, -0.0989])\n\n\n\nnet[0].bias.data = torch.tensor([0.0,0.0, -0.05])\n\n\nnet(imgs)\n\ntensor([[2.0000e-01, 0.0000e+00, 5.0000e-02],\n        [0.0000e+00, 2.0000e-01, 5.0000e-02],\n        [1.0000e-01, 1.0000e-01, 5.0000e-02],\n        [2.0000e-01, 2.0000e-01, 9.3132e-10]], grad_fn=&lt;ViewBackward0&gt;)\n\n\në‘ë²ˆì§¸ 0\nì²«ë²ˆì§¸ 0\n0 ì—†ìŒ\në„¤ë²ˆì§¸ 0\n)ğŸ—£ï¸\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(in_channels=1,out_channels=3,kernel_size=4), # bias ì‚¬ìš©\n    torch.nn.ReLU(), # ë‹¤ì–‘í•´ì§\n    torch.nn.MaxPool2d(kernel_size=13),\n    torch.nn.Flatten()\n)\nnet[0].weight.data = torch.concat(\n    [v_conv.weight.data,\n     h_conv.weight.data,\n     m_conv.weight.data],axis=0)\nnet[0].bias.data = torch.tensor([0.0,0.0, -0.05])\n\n\nplt.matshow(net(imgs).data,cmap=\"gray\")\n\n\n\n\n\n\n\n\n\nnet(imgs).shape\n\ntorch.Size([4, 3])\n\n\nğŸ—£ï¸ 4ê°œì˜ ì´ë¯¸ì§€ë¥¼ ê°ê° 3ê°œë¡œ íŠ¹ì§•ì„ ìš”ì•½\n\nì¶œë ¥ì€ (n,3)ìœ¼ë¡œ ì •ë¦¬ë˜ì–´ì„œ ë‚˜ì˜¨ë‹¤. ì´ ì‹œì ë¶€í„°ëŠ” ë” ì´ìƒ ì´ë¯¸ì§€ê°€ ì…ë ¥ì´ë¼ê³  ìƒê°í•˜ì§€ ì•Šì•„ë„ ë˜ê³ , ë‹¨ìˆœíˆ (n, 3) í¬ê¸°ì˜ ìˆ«ì ë°ì´í„°ê°€ ì…ë ¥ìœ¼ë¡œ ì£¼ì–´ì§„ ê²ƒì²˜ëŸ¼ ë³´ë©´ ëœë‹¤. ì¦‰ ì´ì œë¶€í„°ëŠ” ì´ (n,3) ë°ì´í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ëŠ” ì‹ ê²½ë§ì„ ì„¤ê³„í•˜ë©´ ëœë‹¤."
  },
  {
    "objectID": "posts/07wk-2.html#g.-mpì˜-ì—­í• ",
    "href": "posts/07wk-2.html#g.-mpì˜-ì—­í• ",
    "title": "07wk-2: (í•©ì„±ê³±ì‹ ê²½ë§) â€“ CNN í•µì‹¬ë ˆì´ì–´, CNNì˜ í•™ìŠµì›ë¦¬, FashionMNIST",
    "section": "G. mpì˜ ì—­í• ?",
    "text": "G. mpì˜ ì—­í• ?\n- ìƒ˜í”Œì´ë¯¸ì§€\n\nimg = torch.zeros((1, 1, 16, 16))\ntriangle_size = 4\nfor i in range(triangle_size):\n    for j in range(triangle_size):\n        if j &lt;= i:  # ì•„ë˜ ë°©í–¥ ì§ê°ì‚¼ê°í˜• (ì™¼ìª½ ìœ„ ê¼­ì§“ì  ê¸°ì¤€)\n            img[0, 0, i, j] = 1.0\n\n\nplt.imshow(img.squeeze(),cmap=\"gray\")\n\n\n\n\n\n\n\n\n- mp1 íšŒ\n\nmp = torch.nn.MaxPool2d(kernel_size=2)\nplt.imshow(mp(img).squeeze(),cmap=\"gray\")\n\n\n\n\n\n\n\n\nğŸ—£ï¸ ì´ë¯¸ì§€ê°€ 1/4ë¡œ ì¤„ì–´ë“¦\n- mp 2~4íšŒ\n\nmp = torch.nn.MaxPool2d(kernel_size=2)\nplt.imshow(mp(mp(img)).squeeze(),cmap=\"gray\")\n\n\n\n\n\n\n\n\n\nmp = torch.nn.MaxPool2d(kernel_size=2)\nplt.imshow(mp(mp(mp(img))).squeeze(),cmap=\"gray\")\n\n\n\n\n\n\n\n\n- maxpoolingì€ ì´ë¯¸ì§€ë¥¼ â€œìºë¦¬ì»¤ì²˜í™”â€ í•œë‹¤ê³  ë¹„ìœ í•  ìˆ˜ ìˆìŒ. ë””í…Œì¼ì€ ë²„ë¦¬ê³ , ì¤‘ìš”í•œ íŠ¹ì§•ë§Œ ë½‘ì•„ì„œ ê³¼ì¥ë˜ê²Œ ìš”ì•½í•œë‹¤.\n\nğŸ—£ï¸ ì‚¬ì´ì¦ˆë¥¼ ì¤„ì´ë ¤ê³  ì‚¬ìš©, ì¤‘ìš”í•œ ì •ë³´ë„ ì†ì‹¤ X\n\nCNN\n--&gt; 2d // flatten (conv(íŠ¹ì§•)-relu(íŠ¹ì§•ë‹¤ë³€í™”)-maxpooling(ìš”ì•½))\n--&gt; 1d // ë‹¨ìˆœì‹ ê²½ë§"
  },
  {
    "objectID": "posts/07wk-2.html#footnotes",
    "href": "posts/07wk-2.html#footnotes",
    "title": "07wk-2: (í•©ì„±ê³±ì‹ ê²½ë§) â€“ CNN í•µì‹¬ë ˆì´ì–´, CNNì˜ í•™ìŠµì›ë¦¬, FashionMNIST",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nì…ë ¥shape=(1,1,?,?) ì´ê³  ì¶œë ¥ì˜shape=(1,1,?,?)ì¼ ê²½ìš°â†©ï¸"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deep Learning",
    "section": "",
    "text": "Based on: https://guebin.github.io/DL2025/\n\n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJun 14, 2025\n\n\n15wk-1: (ê°•í™”í•™ìŠµ) â€“ LunarLander\n\n\nsw1kwon \n\n\n\n\nJun 9, 2025\n\n\n14wk-2: (ê°•í™”í•™ìŠµ) â€“ 4x4 Grid World q_table, Appedix B\n\n\nsw1kwon \n\n\n\n\nJun 4, 2025\n\n\n14wk-1: (ê°•í™”í•™ìŠµ) â€“ 4x4 Grid World í™˜ê²½ì˜ ì´í•´\n\n\nsw1kwon \n\n\n\n\nJun 2, 2025\n\n\n13wk-2: (ê°•í™”í•™ìŠµ) â€“ Bandit í™˜ê²½ ì„¤ê³„ ë° í’€ì´, 4x4 Grid World ê²Œì„ì„¤ëª…, í™˜ê²½êµ¬í˜„, ì—ì´ì „íŠ¸(ëœë¤)êµ¬í˜„\n\n\nsw1kwon \n\n\n\n\nMay 28, 2025\n\n\n13wk-1: (ê°•í™”í•™ìŠµ) â€“ ê°•í™”í•™ìŠµ Intro, Bandit ê²Œì„ ì„¤ëª…, Bandit í™˜ê²½ ì„¤ê³„ ë° í’€ì´\n\n\nsw1kwon \n\n\n\n\nMay 14, 2025\n\n\n11wk-1: (ì¶”ì²œì‹œìŠ¤í…œ) â€“ Embedding ë ˆì´ì–´, ì‚¬ìš©ìì •ì˜ ë„¤íŠ¸ì›Œí¬, MF-based ì¶”ì²œì‹œìŠ¤í…œì„ ë„˜ì–´ì„œ\n\n\nsw1kwon \n\n\n\n\nMay 12, 2025\n\n\n10wk-2: (ì¶”ì²œì‹œìŠ¤í…œ) â€“ optimizer ì‚¬ìš© ê³ ê¸‰, ëª¨ë¸ë§ ì „ëµ, MF-based ì¶”ì²œì‹œìŠ¤í…œ\n\n\nsw1kwon \n\n\n\n\nApr 21, 2025\n\n\n07wk-2: (í•©ì„±ê³±ì‹ ê²½ë§) â€“ CNN í•µì‹¬ë ˆì´ì–´, CNNì˜ í•™ìŠµì›ë¦¬, FashionMNIST\n\n\nsw1kwon \n\n\n\n\nApr 16, 2025\n\n\n07wk-1: (í•©ì„±ê³±ì‹ ê²½ë§) â€“ CNN ìë‘, CNN í•µì‹¬ë ˆì´ì–´\n\n\nsw1kwon \n\n\n\n\nApr 14, 2025\n\n\n06wk-2: (ì‹ ê²½ë§) â€“ ë‹¤í•­ë¶„ë¥˜, FashionMNIST\n\n\nsw1kwon \n\n\n\n\nApr 9, 2025\n\n\n06wk-1: (ì‹ ê²½ë§) â€“ ë°ì´í„°ë¶„ì„ ì½”ë”©íŒ¨í„´\n\n\nsw1kwon \n\n\n\n\nApr 7, 2025\n\n\n05wk-2: (ì‹ ê²½ë§) â€“ ì‹ ê²½ë§ì˜ í‘œí˜„, GPUì‚¬ìš©ë²•, í™•ë¥ ì ê²½ì‚¬í•˜ê°•ë²•\n\n\nsw1kwon \n\n\n\n\nApr 2, 2025\n\n\n05wk-1: (ì‹ ê²½ë§) â€“ ì˜ˆì¸¡, ì‹œë²¤ì½”ì •ë¦¬ì˜ ì´ë©´, ë“œëì•„ì›ƒ\n\n\nsw1kwon \n\n\n\n\nMar 31, 2025\n\n\n04wk-2: (ì‹ ê²½ë§) â€“ êº½ì¸ê·¸ë˜í”„ì˜ í•œê³„(?), ì‹œë²¤ì½”ì •ë¦¬, MNIST\n\n\nsw1kwon \n\n\n\n\nMar 26, 2025\n\n\n04wk-1: (ì‹ ê²½ë§) â€“ ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„ ê·¹ë³µ\n\n\nsw1kwon \n\n\n\n\nMar 24, 2025\n\n\n03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„\n\n\nsw1kwon \n\n\n\n\nMar 19, 2025\n\n\n03wk-1: (íšŒê·€, ë¡œì§€ìŠ¤í‹±) â€“ íŒŒì´í† ì¹˜ì‹ ì½”ë”©íŒ¨í„´ (2), ë¡œì§€ìŠ¤í‹± ëª¨í˜•\n\n\nsw1kwon \n\n\n\n\nMar 17, 2025\n\n\n02wk-2: (íšŒê·€) â€“ íŒŒë¼ë©”í„°ì˜ í•™ìŠµê³¼ì • ìŒë¯¸, MSE, íŒŒì´í† ì¹˜ì‹ ì½”ë”©íŒ¨í„´ (1)\n\n\nsw1kwon \n\n\n\n\nMar 10, 2025\n\n\n01wk-2, 02wk-1: (íšŒê·€) â€“ íšŒê·€ëª¨í˜•, ì†ì‹¤í•¨ìˆ˜, íŒŒì´í† ì¹˜ë¥¼ ì´ìš©í•œ ì¶”ì •\n\n\nsw1kwon \n\n\n\n\nMar 5, 2025\n\n\n01wk-1: (í† ì¹˜) â€“ ê°•ì˜ì†Œê°œ, íŒŒì´í† ì¹˜ ê¸°ë³¸\n\n\nsw1kwon \n\n\n\n\nJan 1, 2025\n\n\nA1: Exercise â€“ ver. 0505-1 #1\n\n\nsw1kwon \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/03wk-1.html",
    "href": "posts/03wk-1.html",
    "title": "03wk-1: (íšŒê·€, ë¡œì§€ìŠ¤í‹±) â€“ íŒŒì´í† ì¹˜ì‹ ì½”ë”©íŒ¨í„´ (2), ë¡œì§€ìŠ¤í‹± ëª¨í˜•",
    "section": "",
    "text": "ğŸ“˜ Note Format Guide\nThis format serves as a structured guide for organizing lecture content, personal interpretation, experiments, and study-related questions.\nğŸ“ ğŸ—£ï¸ âœï¸ ğŸ”¬ â“"
  },
  {
    "objectID": "posts/03wk-1.html#a.-biasì˜-ì‚¬ìš©",
    "href": "posts/03wk-1.html#a.-biasì˜-ì‚¬ìš©",
    "title": "03wk-1: (íšŒê·€, ë¡œì§€ìŠ¤í‹±) â€“ íŒŒì´í† ì¹˜ì‹ ì½”ë”©íŒ¨í„´ (2), ë¡œì§€ìŠ¤í‹± ëª¨í˜•",
    "section": "A. biasì˜ ì‚¬ìš©",
    "text": "A. biasì˜ ì‚¬ìš©\nğŸ—£ï¸(\n\nì €ë²ˆ ì‹œê°„ ì½”ë“œ\n\n\nnet = torch.nn.Linear(2, 1, bias=False)\nnet.weight.data = torch.tensor([[-5.0, 10.0]])\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD(net.parameters(), lr=0.1) # lr: learning rate\n\n# step 1~4\nfor epoc in range(30):\n    # 1\n    yhat = net(X)\n    # 2\n    loss = loss_fn(yhat,y)\n    # 3\n    loss.backward()\n    # 4\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nnet.weight # ì§€ë‚œ ì‹œê°„ ê²°ê³¼ì™€ ë™ì¼\n\nParameter containing:\ntensor([[2.4290, 4.0144]], requires_grad=True)\n\n\n\nì´ì œ bias=True\n\n\n# net(X) = X@net.weight.T # í˜„ì¬ ì´ë ‡ê²Œ ì•Œê³  ìˆìœ¼ë‚˜ ì‚¬ì‹¤ì€ ì•„ë‹˜\n\n\nnet.weight\n\nParameter containing:\ntensor([[2.4290, 4.0144]], requires_grad=True)\n\n\n\nprint(net.bias) # í˜„ì¬ëŠ” bias=False\n\nNone\n\n\n\n# net(X) = X@net.weight.T + net.bias # ì‚¬ì‹¤ì€ ì´ê²Œ ë§ìŒ\n\n\në‘˜ì€ ë™ì¼\n\ny = X@W + Ïµ # y = net(X) + Ïµ\ny = w0hat + x*w1hat + Ïµ # y = net(x) + Ïµ\n\nnet(X) = X@net.weight.T + net.bias ì—ì„œ Xê°€ xë¡œ ë°”ë€Œë©´\n\nnet(x) = x@net.weight.T + net.bias\nnet(x) = w0hat + x*w1hat ì´ë¯€ë¡œ\nnet.biasì— í•´ë‹¹í•˜ëŠ” ê²ƒì€ w0hat\nnet.weight.Tì— í•´ë‹¹í•˜ëŠ” ê²ƒì€ w1hat ìœ¼ë¡œ ìƒê° ê°€ëŠ¥\n\nìœ„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ net(x)ë¥¼ ë§Œë“¤ë©´\n\nxëŠ” (n,1)ì´ë¯€ë¡œ input ì°¨ì›ì€ 1\n\n\n\nnet = torch.nn.Linear(1,1,bias=True)\nnet\n\nLinear(in_features=1, out_features=1, bias=True)\n\n\n\nnet.weight # 1x1 matrix\n\nParameter containing:\ntensor([[0.3480]], requires_grad=True)\n\n\n\nnet.bias # length 1ì¸ vector\n\nParameter containing:\ntensor([0.7757], requires_grad=True)\n\n\n\nnet.weight.T # net(x) = x@net.weight.T + net.bias ì—ì„œ net.weight.TëŠ” w1hat\n\ntensor([[0.3480]], grad_fn=&lt;PermuteBackward0&gt;)\n\n\n\nnet.weight.data = torch.tensor([[10.0]])\nnet.weight.data\n\ntensor([[10.]])\n\n\n\nnet.bias.data = torch.tensor([[-5.0]]) # net(x) = x@net.weight.T + net.bias ì—ì„œ net.biasëŠ” w0hat\nnet.bias.data\n\ntensor([[-5.]])\n\n\n\nìœ„ì˜ ë‚´ìš©ì„ ì €ë²ˆ ì‹œê°„ ì½”ë“œì— ë°˜ì˜í•˜ë©´\n\nnet ìˆ˜ì •, weight ë° bias ê°’ ìˆ˜ì •\nnet(X) -&gt; net(x)\n\n\n\nnet = torch.nn.Linear(1, 1, bias=True)\nnet.weight.data = torch.tensor([[10.0]])\nnet.bias.data = torch.tensor([[-5.0]])\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD(net.parameters(), lr=0.1) # lr: learning rate\n\n# step 1~4\nfor epoc in range(30):\n    # 1\n    yhat = net(x)\n    # 2\n    loss = loss_fn(yhat,y)\n    # 3\n    loss.backward()\n    # 4\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nnet.weight\n\nParameter containing:\ntensor([[4.0144]], requires_grad=True)\n\n\n\nnet.bias\n\nParameter containing:\ntensor([[2.4290]], requires_grad=True)\n\n\n\nì €ë²ˆ ì‹œê°„ ê²°ê³¼ì™€ ë™ì¼\n\n)ğŸ—£ï¸\nnetì—ì„œ biasë¥¼ ì‚¬ìš©\n\n# step1ì„ ìœ„í•œ ì‚¬ì „ì¤€ë¹„\nnet = torch.nn.Linear(\n    in_features=1,\n    out_features=1,\n    bias=True\n) # net(x) = x@net.weight.T + net.bias \nnet.bias.data = torch.tensor([-5.0])\nnet.weight.data = torch.tensor([[10.0]])\n# step2ë¥¼ ìœ„í•œ ì‚¬ì „ì¤€ë¹„\nloss_fn = torch.nn.MSELoss()\n# step4ë¥¼ ìœ„í•œ ì‚¬ì „ì¤€ë¹„ \noptimizr = torch.optim.SGD(net.parameters(),lr=0.1)\nfor epoc in range(30):\n    # step1: yhat \n    yhat = net(x)\n    # step2: loss\n    loss = loss_fn(yhat,y)\n    # step3: ë¯¸ë¶„\n    loss.backward()\n    # step4: update\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nnet.bias.data, net.weight.data\n\n(tensor([2.4290]), tensor([[4.0144]]))\n\n\n#"
  },
  {
    "objectID": "posts/03wk-1.html#b.-ì˜ëª»ëœ-ì½”ë“œ",
    "href": "posts/03wk-1.html#b.-ì˜ëª»ëœ-ì½”ë“œ",
    "title": "03wk-1: (íšŒê·€, ë¡œì§€ìŠ¤í‹±) â€“ íŒŒì´í† ì¹˜ì‹ ì½”ë”©íŒ¨í„´ (2), ë¡œì§€ìŠ¤í‹± ëª¨í˜•",
    "section": "B. ì˜ëª»ëœ(?) ì½”ë“œ",
    "text": "B. ì˜ëª»ëœ(?) ì½”ë“œ\nğŸ—£ï¸ biasì˜ defaultëŠ” Trueì´ë¯€ë¡œ ì €ë²ˆ ì‹œê°„ ì½”ë“œì—ì„œ biasë¥¼ ì§€ìš°ë©´ bias=Trueê°€ ë¨\n\n# step1ì„ ìœ„í•œ ì‚¬ì „ì¤€ë¹„\nnet = torch.nn.Linear(\n    in_features=2,\n    out_features=1,\n)\nnet.weight.data = torch.tensor([[-5.0,  10.0]])\n# step2ë¥¼ ìœ„í•œ ì‚¬ì „ì¤€ë¹„\nloss_fn = torch.nn.MSELoss()\n# step4ë¥¼ ìœ„í•œ ì‚¬ì „ì¤€ë¹„ \noptimizr = torch.optim.SGD(net.parameters(),lr=0.1)\nfor epoc in range(30):\n    # step1: yhat \n    yhat = net(X)\n    # step2: loss\n    loss = loss_fn(yhat,y)\n    # step3: ë¯¸ë¶„\n    loss.backward()\n    # step4: update\n    optimizr.step()\n    optimizr.zero_grad()\n\nğŸ—£ï¸(\n\nnet.weight # ê²°ê³¼ê°€ ë§ì´ ë‹¬ë¼ì§\n\nParameter containing:\ntensor([[-1.1114,  4.0080]], requires_grad=True)\n\n\n\nplt.plot(x,y,'o')\nplt.plot(x,net(X).data, '--')\n\n\n\n\n\n\n\n\n\nê·¸ëŸ°ë° ê²°ê³¼ë¥¼ ì‹œê°í™”í•´ë³´ë©´ ë‚˜ì˜ì§€ ì•ŠìŒ\n\n)ğŸ—£ï¸\n- ê²°ê³¼ì‹œê°í™”\n\nplt.plot(x,y,'o')\nplt.plot(x,yhat.data,'--')\nplt.title(f'net.weight={net.weight.data.reshape(-1)}');\n\n\n\n\n\n\n\n\n- ë‚˜ì˜ì§€ ì•Šì€ ì´ìœ ?\nâœï¸ ë°”ë¡œ ë°‘ì˜ ì½”ë“œëŠ” í¸ì˜ìƒ ì‹¤í–‰ X\n# step1ì„ ìœ„í•œ ì‚¬ì „ì¤€ë¹„\nnet = torch.nn.Linear(\n    in_features=2,\n    out_features=1,\n)\nyhat = net(X) = X@net.weight.T + net.bias\n\nnet.weight\n\nParameter containing:\ntensor([[-1.1114,  4.0080]], requires_grad=True)\n\n\n\nnet.bias\n\nParameter containing:\ntensor([3.5562], requires_grad=True)\n\n\nğŸ—£ï¸(\n\nì›ë˜ëŒ€ë¡œë¼ë©´ ì ˆí¸, ê¸°ìš¸ê¸° ì´ 2ê°œì˜ parameterë§Œ í•™ìŠµí•´ì•¼í•˜ëŠ”ë° ìœ„ì˜ ê²°ê³¼ëŠ” 3ê°œë¥¼ í•™ìŠµí•¨\nyhat ê³„ì‚° ê³¼ì •ì„ ì‚´í´ë³´ë©´\n\n\nX[[0],:] # nx2 martixì—ì„œ ì²« ë²ˆì§¸ observationë§Œ ë½‘ìŒ\n\ntensor([[ 1.0000, -2.4821]])\n\n\n\nyhat[:1] # ì´ yhatì´ ì–´ë–»ê²Œ ë‚˜ì™”ëŠ”ì§€ ë³´ë©´\n\ntensor([[-7.5063]], grad_fn=&lt;SliceBackward0&gt;)\n\n\n\nX[[0],:] @ net.weight.T + net.bias\n\n\n-1.1114 * 1.0000 + 4.0080 * (-2.4821) + 3.5562 # ì•½ê°„ì˜ ì°¨ì´ëŠ” ì†Œìˆ˜ì  ì°¨ì´\n\n-7.503456799999999\n\n\n\n-2.4821ì€ x, ë‹¤ìŒê³¼ ê°™ì´ ì •ë¦¬í•˜ë©´\n\n\n-1.1114 * 1.0000 + 3.5562\n\n2.4448\n\n\n\nì ˆí¸ì— ëŒ€í•œ True ê°’: 2.5, ê¸°ìš¸ê¸°ì— ëŒ€í•œ True ê°’: 4\n\nì¦‰, ì ˆí¸ì„ 2ê°œë¡œ ë‚˜ëˆ ì„œ í•™ìŠµí•¨ (ë¹„íš¨ìœ¨ì )\n\nê·¸ëŸ¬ë©´ ì´ê²Œ í‹€ë¦° ê²ƒì¸ê°€?\n\níšŒê·€ë¶„ì„ì—ì„œ ì´ë ‡ê²Œ ëª¨ë¸ë§í•˜ë©´ í‹€ë¦¼ (í†µê³„í•™ì  ê´€ì )\ní•˜ì§€ë§Œ í•™ìŠµ ê²°ê³¼ ìì²´ëŠ” ë§ìŒ (ë¹„íš¨ìœ¨ì ì¼ë¿)\nAIë‚˜ DL ê´€ì ì—ì„œëŠ” ìµœì ì˜ parameter ê°œìˆ˜ê°€ ì •í•´ì§€ì§€ ì•Šì€ ê²½ìš°ê°€ ë§ì•„ì„œ\në¹„íš¨ìœ¨ì ì´ê¸´í•´ë„ ì˜ëª»ìœ¼ë¡œ ê¹Œì§€ëŠ” ìƒê° X\n\n\n)ğŸ—£ï¸"
  },
  {
    "objectID": "posts/03wk-1.html#a.-hatbf-y",
    "href": "posts/03wk-1.html#a.-hatbf-y",
    "title": "03wk-1: (íšŒê·€, ë¡œì§€ìŠ¤í‹±) â€“ íŒŒì´í† ì¹˜ì‹ ì½”ë”©íŒ¨í„´ (2), ë¡œì§€ìŠ¤í‹± ëª¨í˜•",
    "section": "A. \\(\\hat{\\bf y} = ??\\)",
    "text": "A. \\(\\hat{\\bf y} = ??\\)\nğŸ—£ï¸(\n\nì¼ë°˜ì ìœ¼ë¡œ íšŒê·€ë¶„ì„ì—ì„œ ì„¤ëª… ë³€ìˆ˜, ë°˜ì‘ ë³€ìˆ˜ ëª¨ë‘ ì—°ì†í˜• ë³€ìˆ˜ì´ì§€ë§Œ,\nyê°€ ìƒíƒœë¥¼ ì˜ë¯¸í•  ë•Œê°€ ìˆìŒ (ex. X = ì ìˆ˜, y = í•©ê²©/ë¶ˆí•©ê²©)\n\ní•©ê²©ì„ 1, ë¶ˆí•©ê²©ì„ 0ìœ¼ë¡œ ìˆ«ìí™”í•˜ë©´\nyëŠ” 0 ë˜ëŠ” 1ë§Œ ê°€ì§\n\nì´ëŸ¬í•œ ìë£ŒëŠ” ë§¤ìš° ë§ìŒ\n\n)ğŸ—£ï¸\n- \\({\\bf X}\\)ë¥¼ ê°€ì§€ê³  \\({\\bf y}\\)ë¥¼ ë§ì¶”ëŠ” ì•„ë˜ì™€ ê°™ì€ ë¬¸ì œ\n\nx = torch.tensor([-6,-5,-4,-3,-2,-1, 0, 1, 2, 3, 4, 5, 6.0]).reshape(-1,1)\ny = torch.tensor([ 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1]).reshape(-1,1)\nplt.plot(x,y,'o')\n\n\n\n\n\n\n\n\nğŸ—£ï¸(\n\nxê°€ ì¦ê°€í•¨ì— ë”°ë¼ yê°€ 1ì´ ë  ê²ƒ ê°™ê³ , xê°€ ê°ì†Œí•¨ì— ë”°ë¼ yê°€ 0ë  ê²ƒ ê°™ìŒ\nëª¨ë¸ë§ì„ ì–´ë–»ê²Œ?\nëª¨ë¸ë§: observed dataë¥¼ ë³´ê³  error-freeí•œ structureë¥¼ ì°¾ëŠ” ê²ƒ\nì—¬ê¸°ì„œ error-freeí•œ structureëŠ”?\n\nerror-free: ìš´ì ì¸ ìš”ì†Œê°€ ì—†ìŒ\n\nìš´ì ì¸ ìš”ì†Œ?\n\nì´ ê²½ìš° 0ì ì¸ë° í•©ê²©, 1ì ì¸ë° ë¶ˆí•©ê²©\në„ì €íˆ ë°›ì•„ë“¤ì´ì§€ ëª»í•  ìˆ˜ ìˆìŒ\n(ì´ë ‡ê²Œ ê²½ê³„ì— ìˆëŠ”ë° ìš´ì ì¸ ìš”ì†Œë¡œ ê²°ì •ë˜ëŠ” ê²½ìš°)\n\nì´ê²ƒì„ ì¼ë°˜ì ì¸ íšŒê·€ë¶„ì„ì²˜ëŸ¼ underlying(error-free)ì´ ìˆê³  ì˜¤ì°¨í•­ì„ ì •ê·œë¶„í¬ì—ì„œ errorë¥¼ ë½‘ì€ ê²ƒìœ¼ë¡œ ì„¤ëª…í•˜ë©´ X\n\nì´ì „ì˜ cafe ë°ì´í„°ëŠ” ì´ë ‡ê²Œ ì„¤ëª… ê°€ëŠ¥\n\nì°¨ë¼ë¦¬ underlyingì—ì„œ xê°’ì— ëŒ€ì‘í•˜ëŠ” yê°’ì„ ì„±ê³µ í™•ë¥ ë¡œ í•˜ëŠ” ë² ë¥´ëˆ„ì´ ì‹œí–‰ìœ¼ë¡œ ì„¤ëª…í•˜ë©´ ê·¸ëŸ´ë“¯ í•¨\n\nunderlying: ì—¬ê¸°ì„œëŠ” ê´€ì¸¡ê°’ì´ ì•„ë‹ˆê³  í™•ë¥ ì„ ì˜ë¯¸í•˜ëŠ” ê³¡ì„ ìœ¼ë¡œ í•´ì„\nì„±ê³µ í™•ë¥ ì´ 0.9ì¸ ë² ë¥´ëˆ„ì´ ì‹œí–‰ì„ í–ˆëŠ”ë° 0.1ì¸ í™•ë¥ ì˜ ê²°ê³¼ê°€ ë‚˜ì™€ë„ ì–´ì©” ìˆ˜ ì—†ìŒ (ìš´ì ì¸ ìš”ì†Œ)\nì˜¤ì°¨: ë² ë¥´ëˆ„ì´ ì‹œí–‰ì— ì˜í•´ ìƒì„±ë˜ëŠ” ëœë¤ì„±\n\ní†µê³„í•™ê³¼ì‹ ëª¨ë¸ë§\n\nstructure(error-free)ë¿ë§Œ ì•„ë‹ˆë¼ (ì´ê²ƒë„ ì–´ë ¤ì›€, ì—¬ê¸°ê¹Œì§€ëŠ” ë¹„í†µê³„í•™ê³¼ì‹)\nê´€ì¸¡ì¹˜ë¥¼ error termì„ ì´ìš©í•´ ì„¤ëª… (ìš´ì ì¸ ìš”ì†Œê°€ ì–´ë–»ê²Œ ì‘ìš©í•˜ëŠ”ì§€)\n\nyhat\n\nunderlying\nyê°€ 0 ë˜ëŠ” 1ë§Œ ê°€ì§€ë¯€ë¡œ yhatë„ ê·¸ë˜ì•¼í•˜ë‚˜ ì‹¶ì§€ë§Œ X (íšŒê·€ë¶„ì„ì—ì„œ ì˜¤ì°¨í•­ì´ í¬í•¨ëœ ê´€ì¸¡ì¹˜ë¥¼ ë”°ë¼ê°€ëŠ” ê²ƒê³¼ ë™ì¼)\nyhatì€ 0ê³¼ 1ì‚¬ì´ì˜ ìˆ«ì (ëª¨ë¸ë§ ëŒ€ìƒ: ê´€ì¸¡ì¹˜ê°€ ì•„ë‹ˆë¼ ì¶”ì„¸ì„ )\n\n\në‹¤ìŒê³¼ ê°™ì´ ëª¨ë¸ë§ì„ í•´ë³´ë©´\n\n\nprob = torch.exp(x) / (torch.exp(x) + 1)\nplt.plot(x,y,'o')\nplt.plot(x,prob,'--')\n\n\n\n\n\n\n\n\n\n\\(\\frac{e^x}{e^x + 1}\\)\n\n\\(x\\)ê°€ ì»¤ì§€ë©´ 1ì— ê°€ê¹Œì›Œì§€ê³ \n\\(x=0\\)ì´ë©´ 1/2\n\\(x\\)ê°€ ì‘ì•„ì§€ë©´ 0ì— ê°€ê¹Œì›Œì§\n\ní•˜ì§€ë§Œ ì´ ìˆ˜ì‹ì€ ì´ ê²½ìš°ì—ë§Œ ë§ê³  í™•ì¥ì„±ì´ ë–¨ì–´ì§\n\n)ğŸ—£ï¸\n- ì•„ë˜ì™€ ê°™ì´ ëª¨í˜•í™” í•˜ë©´?\n\nplt.plot(x,y,'o', label=r\"observed data (with error) = $(x_i,y_i)$\")\nplt.plot(x,torch.exp(x)/(1+torch.exp(x)),'o--', label = \"underlying (without error)\")\nplt.legend()"
  },
  {
    "objectID": "posts/03wk-1.html#b.-hatbf-y-fracexptextlinrbf-x1exptextlinrbf-x",
    "href": "posts/03wk-1.html#b.-hatbf-y-fracexptextlinrbf-x1exptextlinrbf-x",
    "title": "03wk-1: (íšŒê·€, ë¡œì§€ìŠ¤í‹±) â€“ íŒŒì´í† ì¹˜ì‹ ì½”ë”©íŒ¨í„´ (2), ë¡œì§€ìŠ¤í‹± ëª¨í˜•",
    "section": "B. \\(\\hat{\\bf y} = \\frac{\\exp(\\text{linr}({\\bf X}))}{1+\\exp(\\text{linr}({\\bf X}))}\\)",
    "text": "B. \\(\\hat{\\bf y} = \\frac{\\exp(\\text{linr}({\\bf X}))}{1+\\exp(\\text{linr}({\\bf X}))}\\)\n- ê±±ì •: ì‚°ì ë„ê°€ ê¼­ ì•„ë˜ì™€ ê°™ì€ ë°©ì‹ì´ ì•„ë‹ˆë¼ë©´ ì–´ì©Œì§€?\n\nplt.plot(x,y,'o')\n\n\n\n\n\n\n\n\n\n\\(x\\)ê°€ ì¦ê°€í• ìˆ˜ë¡ \\(y\\)ê°€ 0ì´ ëœë‹¤ë©´?\n0ê·¼ì²˜ì—ì„œ ë³€í™”ê°€ ì¼ì–´ë‚˜ì§€ ì•Šê³  2ê·¼ì²˜ì—ì„œ ë³€í™”ê°€ ì¼ì–´ë‚œë‹¤ë©´?\në³€í™”ê°€ ì¢€ ë” ê¸‰í•˜ê²Œ (í˜¹ì€ ì™„ë§Œí•˜ê²Œ ì¼ì–´ë‚œë‹¤ë©´?)\n\nğŸ—£ï¸(\n\n\\(\\frac{e^{-x}}{e^{-x} + 1}\\)\ní•©ê²©ë¥ ì´ ë‚®ì€ ê²½ìš°\nstrictí•˜ê²Œ ê²°ê³¼ê°€ ë‚˜ë‰˜ëŠ” ê²½ìš°(ex. ì¥í•™ê¸ˆ)\n\n\nplt.plot(x,y,'o', label=r\"observed data (with error) = $(x_i,y_i)$\")\nplt.plot(x,torch.exp(-x)/(1+torch.exp(-x)),'o--', label = \"underlying (without error)\")\nplt.legend()\n\n\n\n\n\n\n\n\n\nplt.plot(x,y,'o', label=r\"observed data (with error) = $(x_i,y_i)$\")\nplt.plot(x,torch.exp(-x+3)/(1+torch.exp(-x+3)),'o--', label = \"underlying (without error)\")\nplt.legend()\n\n\n\n\n\n\n\n\n\nplt.plot(x,y,'o', label=r\"observed data (with error) = $(x_i,y_i)$\")\nplt.plot(x,torch.exp(5*x+3)/(1+torch.exp(5*x+3)),'o--', label = \"underlying (without error)\")\nplt.legend()\n\n\n\n\n\n\n\n\n\nì´ëŸ¬í•œ 5*x+3 ë“±ì„ ì¼ë°˜í™”í•˜ë©´\n\n5x+3 = w0hat + w1hat  x : íšŒê·€ë¶„ì„ ì„ í˜• ëª¨í˜•\n= w0hat + w1hat * x = linr(x) # xë¥¼ linear transformì‹œí‚´\n\n\nğŸ”¬ 0ê·¼ì²˜ì—ì„œ ë³€í™”ê°€ ì¼ì–´ë‚˜ì§€ ì•Šê³  2ê·¼ì²˜ì—ì„œ ë³€í™”ê°€ ì¼ì–´ë‚œë‹¤ë©´?\n\nplt.plot(x,y,'o', label=r\"observed data (with error) = $(x_i,y_i)$\")\nplt.plot(x,torch.exp(x-2)/(1+torch.exp(x-2)),'o--', label = \"underlying (without error)\")\nplt.legend()\n\n\n\n\n\n\n\n\nğŸ”¬ ë³€í™”ê°€ ì¢€ ë” ê¸‰í•˜ê²Œ ì¼ì–´ë‚œë‹¤ë©´?\n\nplt.plot(x,y,'o', label=r\"observed data (with error) = $(x_i,y_i)$\")\nplt.plot(x,torch.exp(3*x)/(1+torch.exp(3*x)),'o--', label = \"underlying (without error)\")\nplt.legend()\n\n\n\n\n\n\n\n\nğŸ”¬ ë³€í™”ê°€ ì¢€ ë” ì™„ë§Œí•˜ê²Œ ì¼ì–´ë‚œë‹¤ë©´?\n\nplt.plot(x,y,'o', label=r\"observed data (with error) = $(x_i,y_i)$\")\nplt.plot(x,torch.exp(x/3)/(1+torch.exp(x/3)),'o--', label = \"underlying (without error)\")\nplt.legend()\n\n\n\n\n\n\n\n\n)ğŸ—£ï¸\n\nplt.plot(x,y,'o', label=r\"observed data (with error) = $(x_i,y_i)$\")\nplt.plot(x,torch.exp(5*x+3)/(1+torch.exp(5*x+3)),'o--', label = \"underlying (without error)\")\nplt.legend()\n\n\n\n\n\n\n\n\n- ê±±ì •í•´ê²°\n\n#plt.plot(x,y,'o', label=r\"observed data (with error) = $(x_i,y_i)$\")\nplt.plot(x,torch.exp(x)/(1+torch.exp(x)),'o--', label = \"underlying type1 (without error)\", color=\"C1\")\nplt.plot(x,torch.exp(5*x)/(1+torch.exp(5*x)),'o--', label = \"underlying type2 (without error)\", color=\"C2\")\nplt.legend()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\níšŒê·€ vs ë¡œì§€ìŠ¤í‹±\n\n\\({\\bf X} \\to {\\bf y}\\) ì— ëŒ€í•œ íŒ¨í„´ì´ \\(\\text{linr}({\\bf X}) \\approx {\\bf y}\\) ì´ë¼ë©´ íšŒê·€!\n\\({\\bf X} \\to {\\bf y}\\) ì— ëŒ€í•œ íŒ¨í„´ì´ \\(\\frac{\\exp(\\text{linr}({\\bf X}))}{1+\\exp(\\text{linr}({\\bf X}))} \\approx {\\bf y}\\) ì´ë¼ë©´ ë¡œì§€ìŠ¤í‹±!\n\n\n\nğŸ—£ï¸(\n\nXë¥¼ linear transformí–ˆë”ë‹ˆ ì„  ìì²´ê°€ yì™€ ë¹„ìŠ· =&gt; íšŒê·€\nìœ„ì˜ ê·¸ë˜í”„ë¥¼ ê·¸ë¦¬ëŠ” ì‹ìœ¼ë¡œ í–ˆë”ë‹ˆ yì™€ ë¹„ìŠ· =&gt; ë¡œì§€ìŠ¤í‹±\n\nì •í™•íˆëŠ” í™•ë¥ ì´ yì™€ ë¹„ìŠ·í•˜ë‹¤ë©´ (y ìì²´ëŠ” 0 ë˜ëŠ” 1)\n\n\n)ğŸ—£ï¸"
  },
  {
    "objectID": "posts/03wk-1.html#c.-ë¡œì§€ìŠ¤í‹±-ëª¨í˜•",
    "href": "posts/03wk-1.html#c.-ë¡œì§€ìŠ¤í‹±-ëª¨í˜•",
    "title": "03wk-1: (íšŒê·€, ë¡œì§€ìŠ¤í‹±) â€“ íŒŒì´í† ì¹˜ì‹ ì½”ë”©íŒ¨í„´ (2), ë¡œì§€ìŠ¤í‹± ëª¨í˜•",
    "section": "C. ë¡œì§€ìŠ¤í‹± ëª¨í˜•",
    "text": "C. ë¡œì§€ìŠ¤í‹± ëª¨í˜•\n- \\(x\\)ê°€ ì»¤ì§ˆìˆ˜ë¡ (í˜¹ì€ ì‘ì•„ì§ˆìˆ˜ë¡) \\(y=1\\)ì´ ì˜ë‚˜ì˜¤ëŠ” ëª¨í˜•ì€ ì•„ë˜ì™€ ê°™ì´ ì„¤ê³„í•  ìˆ˜ ìˆìŒ &lt;â€” ì™¸ìš°ì„¸ìš”!!!\n\n\\(y_i \\sim {\\cal B}(\\pi_i),\\quad\\) where \\(\\pi_i = \\frac{\\exp(w_0+w_1x_i)}{1+\\exp(w_0+w_1x_i)} = \\frac{1}{1+\\exp(-w_0-w_1x_i)}\\)\n\\(\\hat{y}_i= \\frac{\\exp(\\hat{w}_0+\\hat{w}_1x_i)}{1+\\exp(\\hat{w}_0+\\hat{w}_1x_i)}=\\frac{1}{1+\\exp(-\\hat{w}_0-\\hat{w}_1x_i)}\\)\nğŸ—£ï¸\n\n\\(\\pi_i\\)ëŠ” í™•ë¥ ì„ ì˜ë¯¸\n\\(\\frac{e^{x}}{1 + e^{x}}\\) = \\(\\frac{1}{e^{-x} + 1}\\) ì—ì„œ \\(x\\) ëŒ€ì‹  \\(w_0+w_1x_i\\)\nì±… ë§ˆë‹¤ ë‹¤ë¥´ì§€ë§Œ ì˜¤ë¥¸ìª½ì²˜ëŸ¼ ë§ì´ ì”€\n\n\n- íšŒê·€ëª¨í˜•ê³¼ ë¡œì§€ìŠ¤í‹± ëª¨í˜•ì˜ ë¹„êµ\n\níšŒê·€ëª¨í˜•: \\(y_i \\sim {\\cal N}(w_0+w_1x_i, \\sigma^2)\\)1\në¡œì§€ìŠ¤í‹±: \\(y_i \\sim {\\cal B}\\big(\\frac{\\exp(w_0+w_1x_i)}{1+\\exp(w_0+w_1x_i)}\\big)\\)\nğŸ—£ï¸\n\níšŒê·€ëª¨í˜•: ì˜¤ì°¨í•­ì˜ ê´€ì ì—ì„œ í•´ì„\në¡œì§€ìŠ¤í‹±(yê°€ 0 ë˜ëŠ” 1): ìœ„ì˜ ê³¡ì„ ì„ ë‚˜íƒ€ë‚´ëŠ” ì¼ë°˜ì ì¸ ìˆ˜ì‹\n\n=&gt; ì´ ìˆ˜ì‹ê°’ì„ í† ëŒ€ë¡œ ë² ë¥´ëˆ„ì´ ì‹œí–‰ì„ í•˜ë©´ ì˜¤ì°¨í•­ê¹Œì§€ ì„¤ëª… ê°€ëŠ¥í•œ ëª¨ë¸ì´ ë¨\n\n\n\n- ìš°ë¦¬ê°€ ì˜ˆì¸¡í•˜ê³  ì‹¶ì€ê²ƒ\n\níšŒê·€ëª¨í˜•: ì •ê·œë¶„í¬ì˜ í‰ê· ì„ ì˜ˆì¸¡í•˜ê³  ì‹¶ìŒ. ì¦‰ \\(w_0+w_1x_i\\)ë¥¼ ì˜ˆì¸¡í•˜ê³  ì‹¶ìŒ. ì˜ˆì¸¡ê°’ìœ¼ë¡œëŠ” \\(\\hat{w}_0 + \\hat{w}_1x_i\\)ë¥¼ ì‚¬ìš©!\në¡œì§€ìŠ¤í‹±: ë² ë¥´ëˆ„ì´ì˜ í‰ê· ì„ ì˜ˆì¸¡í•˜ê³  ì‹¶ìŒ. ì¦‰ \\(\\frac{\\exp(w_0+w_1x_i)}{1+\\exp(w_0+w_1x_i)}\\)ë¥¼ ì˜ˆì¸¡í•˜ê³  ì‹¶ìŒ. ì˜ˆì¸¡ê°’ìœ¼ë¡œëŠ” \\(\\frac{\\exp(\\hat{w}_0+\\hat{w}_1x_i)}{1+\\exp(\\hat{w}_0+\\hat{w}_1x_i)}\\)ë¥¼ ì‚¬ìš©!\nğŸ—£ï¸\n\në‘˜ ë‹¤ \\(\\hat{w}_0\\), \\(\\hat{w}_1\\)ë¥¼ ì¶”ì •í•˜ë©´ ê°ê° ì§ì„ ê³¼ ê³¡ì„ ì´ ê²°ì •ë¨\në² ë¥´ëˆ„ì´ì˜ í‰ê· ì€ \\(p\\)\n\nì¦‰, í™•ë¥ ì„ ì˜ˆì¸¡í•˜ê³  ì‹¶ìŒ"
  },
  {
    "objectID": "posts/03wk-1.html#footnotes",
    "href": "posts/03wk-1.html#footnotes",
    "title": "03wk-1: (íšŒê·€, ë¡œì§€ìŠ¤í‹±) â€“ íŒŒì´í† ì¹˜ì‹ ì½”ë”©íŒ¨í„´ (2), ë¡œì§€ìŠ¤í‹± ëª¨í˜•",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nì›ë˜ëŠ” ì´ë ‡ê²Œ ì¼ì—ˆì§€.. \\(y_i = w_0 + w_1x_i + \\epsilon_i \\quad \\epsilon_i \\sim {\\cal N}(0,\\sigma^2)\\)â†©ï¸"
  },
  {
    "objectID": "posts/13wk-2.html",
    "href": "posts/13wk-2.html",
    "title": "13wk-2: (ê°•í™”í•™ìŠµ) â€“ Bandit í™˜ê²½ ì„¤ê³„ ë° í’€ì´, 4x4 Grid World ê²Œì„ì„¤ëª…, í™˜ê²½êµ¬í˜„, ì—ì´ì „íŠ¸(ëœë¤)êµ¬í˜„",
    "section": "",
    "text": "ğŸ“˜ Note Format Guide\nThis format serves as a structured guide for organizing lecture content, personal interpretation, experiments, and study-related questions.\nğŸ“ ğŸ—£ï¸ âœï¸ ğŸ”¬ â“"
  },
  {
    "objectID": "posts/13wk-2.html#a.-ëŒ€ì¶©-ê°œë…ë§Œ-ì‹¤ìŠµ",
    "href": "posts/13wk-2.html#a.-ëŒ€ì¶©-ê°œë…ë§Œ-ì‹¤ìŠµ",
    "title": "13wk-2: (ê°•í™”í•™ìŠµ) â€“ Bandit í™˜ê²½ ì„¤ê³„ ë° í’€ì´, 4x4 Grid World ê²Œì„ì„¤ëª…, í™˜ê²½êµ¬í˜„, ì—ì´ì „íŠ¸(ëœë¤)êµ¬í˜„",
    "section": "A. ëŒ€ì¶© ê°œë…ë§Œ ì‹¤ìŠµ",
    "text": "A. ëŒ€ì¶© ê°œë…ë§Œ ì‹¤ìŠµ\n\naction_space = [0,1] \nactions_deque = collections.deque(maxlen=500)\nrewards_deque =  collections.deque(maxlen=500)\n#---#\n\n\nfor _ in range(10):\n    action = np.random.choice(action_space)\n    if action == 1:\n        reward = 10 \n    else:\n        reward = 1\n    actions_deque.append(action)\n    rewards_deque.append(reward)\n\n\nactions_deque\n\ndeque([np.int64(1),\n       np.int64(0),\n       np.int64(0),\n       np.int64(1),\n       np.int64(0),\n       np.int64(0),\n       np.int64(0),\n       np.int64(1),\n       np.int64(1),\n       np.int64(0)],\n      maxlen=500)\n\n\n\nrewards_deque\n\ndeque([10, 1, 1, 10, 1, 1, 1, 10, 10, 1], maxlen=500)\n\n\n\nactions_numpy = np.array(actions_deque)\nrewards_numpy = np.array(rewards_deque)\n\n\nq0 = rewards_numpy[actions_numpy == 0].mean()\nq1 = rewards_numpy[actions_numpy == 1].mean()\nq_table = np.array([q0,q1])\nq_table\n\narray([ 1., 10.])\n\n\n\naction = q_table.argmax()\n\n\nfor _ in range(5):\n    action = q_table.argmax()\n    if action == 1:\n        reward = 10 \n    else:\n        reward = 1\n    actions_deque.append(action)\n    rewards_deque.append(reward)\n    actions_numpy = np.array(actions_deque)\n    rewards_numpy = np.array(rewards_deque)    \n    q0 = rewards_numpy[actions_numpy == 0].mean()\n    q1 = rewards_numpy[actions_numpy == 1].mean()\n    q_table = np.array([q0,q1])\n\n\nactions_numpy\n\narray([1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1])\n\n\n\nrewards_numpy\n\narray([10,  1,  1, 10,  1,  1,  1, 10, 10,  1, 10, 10, 10, 10, 10])"
  },
  {
    "objectID": "posts/13wk-2.html#b.-í´ë˜ìŠ¤ë¥¼-ì´ìš©í•œ-êµ¬í˜„",
    "href": "posts/13wk-2.html#b.-í´ë˜ìŠ¤ë¥¼-ì´ìš©í•œ-êµ¬í˜„",
    "title": "13wk-2: (ê°•í™”í•™ìŠµ) â€“ Bandit í™˜ê²½ ì„¤ê³„ ë° í’€ì´, 4x4 Grid World ê²Œì„ì„¤ëª…, í™˜ê²½êµ¬í˜„, ì—ì´ì „íŠ¸(ëœë¤)êµ¬í˜„",
    "section": "B. í´ë˜ìŠ¤ë¥¼ ì´ìš©í•œ êµ¬í˜„",
    "text": "B. í´ë˜ìŠ¤ë¥¼ ì´ìš©í•œ êµ¬í˜„\n\nclass Bandit:\n    def __init__(self):\n        self.reward = None \n    def step(self,action):\n        if action == 0:\n            self.reward = 1\n        else: \n            self.reward = 10 \n        return self.reward \n\n\nclass Agent:\n    def __init__(self):\n        pass \n    def act(self):\n        # ë§Œì•½ì— ê²½í—˜ì´ 20ë³´ë‹¤ ì‘ìŒ --&gt; ëœë¤ì•¡ì…˜ \n        # ê²½í—˜ì´ 20ë³´ë‹¤ í¬ë©´ --&gt; action = q_table.argmax()\n        pass \n    def save_experience(self):\n        # ë°ì´í„° ì €ì¥ \n        pass \n    def learn(self):\n        # q_table ì„ ì—…ë°ì´íŠ¸í•˜ëŠ” ê³¼ì • \n        pass\n\nğŸ—£ï¸(\nì—ì´ì „íŠ¸(=í”Œë ˆì´ì–´) ì•¡ì…˜ â€“&gt; ë³´ìƒ, ë‹¤ìŒìƒíƒœ\n\nenv = Bandit()\n\n\nenv.step(1)\n\n10\n\n\n\nclass Agent:\n    def __init__(self):\n        self.action = None\n        self.action_space = [0,1]\n        self.q_table = None\n    def act(self):\n        if n_experience &lt; 20:\n            self.action = np.random.choice(self.action_space)\n        else:\n            self.action = self.q_table.argmax()\n        # ë§Œì•½ì— ê²½í—˜ì´ 20ë³´ë‹¤ ì‘ìŒ --&gt; ëœë¤ì•¡ì…˜ \n        # ê²½í—˜ì´ 20ë³´ë‹¤ í¬ë©´ --&gt; action = q_table.argmax()\n        pass \n    def save_experience(self):\n        # ë°ì´í„° ì €ì¥ \n        pass \n    def learn(self):\n        # q_table ì„ ì—…ë°ì´íŠ¸í•˜ëŠ” ê³¼ì • \n        pass\n\n\nì•„ë˜ë¥¼ ë¨¼ì € ì±„ìš°ê³  ê·¸ì— ë§ì¶° ìœ„ì˜ __init__ì„ ì±„ì›€\n\n\nclass Agent:\n    def __init__(self):\n        self.action = None\n        self.action_space = [0,1]\n        self.q_table = None\n        self.n_experience = 0\n    def act(self):\n        if self.n_experience &lt; 20:\n            self.action = np.random.choice(self.action_space)\n        else:\n            self.action = self.q_table.argmax() \n    def save_experience(self):\n        # ë°ì´í„° ì €ì¥ \n        pass \n    def learn(self):\n        # q_table ì„ ì—…ë°ì´íŠ¸í•˜ëŠ” ê³¼ì • \n        pass\n\n\nclass Agent:\n    def __init__(self):\n        self.action = None\n        self.reward = None\n        self.actions = collections.deque(maxlen=500)\n        self.rewards = collections.deque(maxlen=500)\n        self.action_space = [0,1]\n        self.q_table = None\n        self.n_experience = 0\n    def act(self):\n        if self.n_experience &lt; 20:\n            self.action = np.random.choice(self.action_space)\n        else:\n            self.action = self.q_table.argmax() \n    def save_experience(self):\n        self.actions.append(self.action)\n        self.rewards.append(self.reward)\n        self.n_experience = self.n_experience + 1 \n    def learn(self):\n        # q_table ì„ ì—…ë°ì´íŠ¸í•˜ëŠ” ê³¼ì •\n        q0 = rewards[actions == 0].mean() # í–‰ë™0ì„ í–ˆì„ ë•Œ ì–»ëŠ” ë³´ìƒì˜ í‰ê· ê°’\n        q1 = rewards[actions == 1].mean() # í–‰ë™1ì„ í–ˆì„ ë•Œ ì–»ëŠ” ë³´ìƒì˜ í‰ê· ê°’\n        self.q_table = np.array([q0, q1])\n\n\nclass Agent:\n    def __init__(self):\n        self.action = None\n        self.reward = None\n        self.actions = collections.deque(maxlen=500)\n        self.rewards = collections.deque(maxlen=500)\n        self.action_space = [0,1]\n        self.q_table = None\n        self.n_experience = 0\n    def act(self):\n        if self.n_experience &lt; 20:\n            self.action = np.random.choice(self.action_space)\n        else:\n            self.action = self.q_table.argmax() \n    def save_experience(self):\n        self.actions.append(self.action)\n        self.rewards.append(self.reward)\n        self.n_experience = self.n_experience + 1 \n    def learn(self):\n        if self.n_experience &lt; 20:\n            pass\n        else:\n            # q_table ì„ ì—…ë°ì´íŠ¸í•˜ëŠ” ê³¼ì •\n            actions = np.array(self.actions)\n            rewards = np.array(self.rewards)\n            q0 = rewards[actions == 0].mean() # í–‰ë™0ì„ í–ˆì„ ë•Œ ì–»ëŠ” ë³´ìƒì˜ í‰ê· ê°’\n            q1 = rewards[actions == 1].mean() # í–‰ë™1ì„ í–ˆì„ ë•Œ ì–»ëŠ” ë³´ìƒì˜ í‰ê· ê°’\n            self.q_table = np.array([q0, q1])\n\n\nenv = Bandit()\nplayer = Agent()\n\n\nplayer.act()\n\n\nplayer.action\n\nnp.int64(1)\n\n\n\ní–‰ë™ì„ í•˜ê¸°ëŠ” í•˜ë‚˜ ì–´ë–¤ í–‰ë™ì„ ë³´ê¸°ê°€ ì§ê´€ì ì´ì§€ ì•ŠìŒ\n\n\nclass Agent:\n    def __init__(self):\n        self.action = None\n        self.reward = None\n        self.actions = collections.deque(maxlen=500)\n        self.rewards = collections.deque(maxlen=500)\n        self.action_space = [0,1]\n        self.q_table = None\n        self.n_experience = 0\n    def act(self):\n        if self.n_experience &lt; 20:\n            self.action = np.random.choice(self.action_space)\n        else:\n            self.action = self.q_table.argmax()\n        print(f\"ë²„íŠ¼{self.action}ëˆ„ë¦„!\")\n    def save_experience(self):\n        self.actions.append(self.action)\n        self.rewards.append(self.reward)\n        self.n_experience = self.n_experience + 1 \n    def learn(self):\n        if self.n_experience &lt; 20:\n            pass\n        else:\n            # q_table ì„ ì—…ë°ì´íŠ¸í•˜ëŠ” ê³¼ì •\n            actions = np.array(self.actions)\n            rewards = np.array(self.rewards)\n            q0 = rewards[actions == 0].mean() # í–‰ë™0ì„ í–ˆì„ ë•Œ ì–»ëŠ” ë³´ìƒì˜ í‰ê· ê°’\n            q1 = rewards[actions == 1].mean() # í–‰ë™1ì„ í–ˆì„ ë•Œ ì–»ëŠ” ë³´ìƒì˜ í‰ê· ê°’\n            self.q_table = np.array([q0, q1])\n\n\nenv = Bandit()\nplayer = Agent()\n\n\nplayer.act()\n\në²„íŠ¼1ëˆ„ë¦„!\n\n\n\nplayer.action\n\nnp.int64(1)\n\n\n\nì‹¤ì§ˆì  ë™ì‘\n\n\nplayer.act()\nprint(player.action)\n\në²„íŠ¼1ëˆ„ë¦„!\n1\n\n\n\nplayer.act()\nenv.step(player.action)\n\në²„íŠ¼1ëˆ„ë¦„!\n\n\n10\n\n\n\nplayer.act()\nplayer.reward = env.step(player.action)\nprint(player.action, player.reward)\n\në²„íŠ¼0ëˆ„ë¦„!\n0 1\n\n\n\nplayer.actions # historyê°€ ìˆì„ ì¤„ ì•Œì•˜ëŠ”ë° ì—†ìŒ\n\ndeque([], maxlen=500)\n\n\n\nplayer.act()\nplayer.reward = env.step(player.action)\nplayer.save_experience()\nprint(player.action, player.reward)\n\në²„íŠ¼0ëˆ„ë¦„!\n0 1\n\n\n\nplayer.actions\n\ndeque([np.int64(0)], maxlen=500)\n\n\n\nplayer.rewards\n\ndeque([1], maxlen=500)\n\n\n\nì €ì¥ëœ ê²ƒì„ ë°”íƒ•ìœ¼ë¡œ q_tableì„ ë§Œë“¤ì–´ì•¼ í•¨\n\n\nplayer.act()\nplayer.reward = env.step(player.action)\nplayer.save_experience() # ë°ì´í„°ë¥¼ ì €ì¥\nplayer.learn() # ì €ì¥ëœ ë°ì´í„°ë¥¼ í•™ìŠµ\n\në²„íŠ¼1ëˆ„ë¦„!\n\n\n\nplayer.actions\n\ndeque([np.int64(0), np.int64(1), np.int64(1)], maxlen=500)\n\n\n\nplayer.rewards\n\ndeque([1, 10, 10], maxlen=500)\n\n\n\nplayer.q_table # ì—†ëŠ” ì´ìœ : n_experience &lt; 20\n\n\nenv = Bandit()\nplayer = Agent()\n\n\nfor _ in range(19):\n    player.act()\n    player.reward = env.step(player.action)\n    player.save_experience() # ë°ì´í„°ë¥¼ ì €ì¥\n    player.learn() # ì €ì¥ëœ ë°ì´í„°ë¥¼ í•™ìŠµ\n\në²„íŠ¼0ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼0ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼0ëˆ„ë¦„!\në²„íŠ¼0ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼0ëˆ„ë¦„!\në²„íŠ¼0ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼0ëˆ„ë¦„!\në²„íŠ¼0ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼0ëˆ„ë¦„!\në²„íŠ¼0ëˆ„ë¦„!\në²„íŠ¼0ëˆ„ë¦„!\n\n\n\nplayer.actions\n\ndeque([np.int64(0),\n       np.int64(1),\n       np.int64(0),\n       np.int64(1),\n       np.int64(0),\n       np.int64(0),\n       np.int64(1),\n       np.int64(0),\n       np.int64(0),\n       np.int64(1),\n       np.int64(1),\n       np.int64(1),\n       np.int64(0),\n       np.int64(0),\n       np.int64(1),\n       np.int64(1),\n       np.int64(0),\n       np.int64(0),\n       np.int64(0)],\n      maxlen=500)\n\n\n\nplayer.rewards\n\ndeque([1, 10, 1, 10, 1, 1, 10, 1, 1, 10, 10, 10, 1, 1, 10, 10, 1, 1, 1],\n      maxlen=500)\n\n\n\nplayer.q_table # ì—†ëŠ” ì´ìœ : n_experience &lt; 20\n\n\nenv = Bandit()\nplayer = Agent()\n\n\nfor _ in range(40):\n    player.act()\n    player.reward = env.step(player.action)\n    player.save_experience() # ë°ì´í„°ë¥¼ ì €ì¥\n    player.learn() # ì €ì¥ëœ ë°ì´í„°ë¥¼ í•™ìŠµ\n\në²„íŠ¼0ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼0ëˆ„ë¦„!\në²„íŠ¼0ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼0ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼0ëˆ„ë¦„!\në²„íŠ¼0ëˆ„ë¦„!\në²„íŠ¼0ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼0ëˆ„ë¦„!\në²„íŠ¼0ëˆ„ë¦„!\në²„íŠ¼0ëˆ„ë¦„!\në²„íŠ¼0ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼0ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\n\n\n\nplayer.actions\n\ndeque([np.int64(0),\n       np.int64(1),\n       np.int64(0),\n       np.int64(0),\n       np.int64(1),\n       np.int64(0),\n       np.int64(1),\n       np.int64(0),\n       np.int64(0),\n       np.int64(0),\n       np.int64(1),\n       np.int64(1),\n       np.int64(0),\n       np.int64(0),\n       np.int64(0),\n       np.int64(0),\n       np.int64(1),\n       np.int64(1),\n       np.int64(1),\n       np.int64(0),\n       np.int64(1),\n       np.int64(1),\n       np.int64(1),\n       np.int64(1),\n       np.int64(1),\n       np.int64(1),\n       np.int64(1),\n       np.int64(1),\n       np.int64(1),\n       np.int64(1),\n       np.int64(1),\n       np.int64(1),\n       np.int64(1),\n       np.int64(1),\n       np.int64(1),\n       np.int64(1),\n       np.int64(1),\n       np.int64(1),\n       np.int64(1),\n       np.int64(1)],\n      maxlen=500)\n\n\n\nplayer.rewards\n\ndeque([1,\n       10,\n       1,\n       1,\n       10,\n       1,\n       10,\n       1,\n       1,\n       1,\n       10,\n       10,\n       1,\n       1,\n       1,\n       1,\n       10,\n       10,\n       10,\n       1,\n       10,\n       10,\n       10,\n       10,\n       10,\n       10,\n       10,\n       10,\n       10,\n       10,\n       10,\n       10,\n       10,\n       10,\n       10,\n       10,\n       10,\n       10,\n       10,\n       10],\n      maxlen=500)\n\n\n\nplayer.q_table\n\narray([ 1., 10.])\n\n\n\nê²Œì„ ì¢…ë£Œ ì¡°ê±´ì´ í•„ìš”í•  ê²ƒ ê°™ìŒ\n\n\nnp.array(player.rewards)[-20:] # ìµœê·¼ 20ë²ˆì§¸\n\narray([10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n       10, 10, 10])\n\n\n\nnp.array(player.rewards)[-20:].mean() &gt; 9.5\n\nnp.True_\n\n\n\nenv = Bandit()\nplayer = Agent()\n\n\nfor _ in range(100):\n    player.act()\n    player.reward = env.step(player.action)\n    player.save_experience() # ë°ì´í„°ë¥¼ ì €ì¥\n    player.learn() # ì €ì¥ëœ ë°ì´í„°ë¥¼ í•™ìŠµ\n    if np.array(player.rewards)[-20:].mean() &gt; 9.5:\n        print(\"---ê²Œì„í´ë¦¬ì–´---\")\n        break\n\në²„íŠ¼0ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼0ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼0ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼0ëˆ„ë¦„!\në²„íŠ¼0ëˆ„ë¦„!\në²„íŠ¼0ëˆ„ë¦„!\në²„íŠ¼0ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼0ëˆ„ë¦„!\në²„íŠ¼0ëˆ„ë¦„!\në²„íŠ¼0ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼0ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\n---ê²Œì„í´ë¦¬ì–´---\n\n\n\nfor _ in range(100):\n    player.act()\n    player.reward = env.step(player.action)\n    player.save_experience() # ë°ì´í„°ë¥¼ ì €ì¥\n    player.learn() # ì €ì¥ëœ ë°ì´í„°ë¥¼ í•™ìŠµ\n    if np.array(player.rewards)[-20:].mean() &gt; 9.5:\n        print(\"---ê²Œì„í´ë¦¬ì–´---\")\n        break\n\në²„íŠ¼1ëˆ„ë¦„!\n---ê²Œì„í´ë¦¬ì–´---\n\n\n\nìœ„ì™€ ê°™ì€ ê²½ìš°ë¥¼ í”¼í•˜ê¸° ìœ„í•´ì„œ\n\n\nenv = Bandit()\nplayer = Agent()\n\n\nfor _ in range(100):\n    player.act()\n    player.reward = env.step(player.action)\n    player.save_experience() # ë°ì´í„°ë¥¼ ì €ì¥\n    player.learn() # ì €ì¥ëœ ë°ì´í„°ë¥¼ í•™ìŠµ\n    if player.n_experience &lt; 20:\n        pass\n    else:\n        if np.array(player.rewards)[-20:].mean() &gt; 9.5:\n            print(\"---ê²Œì„í´ë¦¬ì–´---\")\n            break\n\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼0ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼0ëˆ„ë¦„!\në²„íŠ¼0ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼0ëˆ„ë¦„!\në²„íŠ¼0ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼0ëˆ„ë¦„!\në²„íŠ¼0ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼0ëˆ„ë¦„!\në²„íŠ¼0ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼0ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\n---ê²Œì„í´ë¦¬ì–´---\n\n\n\nì •ë¦¬\n\nenv = Bandit()\nplayer = Agent()\nfor _ in range(100):\n    # step1: agent action\n    player.act()\n    # step2: action --&gt; state, reward\n    player.reward = env.step(player.action)\n    # step3: agentê°€ ë°ì´í„°ë¥¼ ì¶•ì í•˜ê³  í•™ìŠµ\n    player.save_experience() # ë°ì´í„°ë¥¼ ì €ì¥\n    player.learn() # ì €ì¥ëœ ë°ì´í„°ë¥¼ í•™ìŠµ\n    # --- ê°•í™”í•™ìŠµì˜ ì¢…ë£Œë¥¼ ê²°ì • --- #\n    if player.n_experience &lt; 20:\n        pass\n    else:\n        if np.array(player.rewards)[-20:].mean() &gt; 9.5:\n            print(\"---ê²Œì„í´ë¦¬ì–´---\")\n            break\n)ğŸ—£ï¸\n\n\nclass Agent:\n    def __init__(self):\n        self.action = None \n        self.reward = None \n        self.actions = collections.deque(maxlen=500)\n        self.rewards = collections.deque(maxlen=500)\n        self.action_space = [0,1] \n        self.q_table = None \n        self.n_experience = 0\n    def act(self):\n        if self.n_experience &lt; 20:\n            self.action = np.random.choice(self.action_space)\n        else: \n            self.action = self.q_table.argmax()\n        print(f\"ë²„íŠ¼{self.action}ëˆ„ë¦„!\")\n    def save_experience(self):\n        self.actions.append(self.action)\n        self.rewards.append(self.reward)\n        self.n_experience = self.n_experience + 1\n    def learn(self):\n        if self.n_experience &lt; 20:\n            pass\n        else:\n            # q_table ì„ ì—…ë°ì´íŠ¸í•˜ëŠ” ê³¼ì • \n            actions = np.array(self.actions)\n            rewards = np.array(self.rewards)\n            q0 = rewards[actions == 0].mean() # í–‰ë™0ì„í–ˆì„ë•Œ ì–»ëŠ” ë³´ìƒì˜ í‰ê· ê°’\n            q1 = rewards[actions == 1].mean()# í–‰ë™1ì„í–ˆì„ë•Œ ì–»ëŠ” ë³´ìƒì˜ í‰ê· ê°’\n            self.q_table = np.array([q0,q1])\n\n\nenv = Bandit()\nplayer = Agent()\nfor _ in range(100):\n    # step1: agent action \n    player.act()\n    # step2: action --&gt; state, reward\n    player.reward = env.step(player.action)\n    # step3: agentê°€ ë°ì´í„°ë¥¼ ì¶•ì í•˜ê³  í•™ìŠµ\n    player.save_experience() # ë°ì´í„°ë¥¼ ì €ì¥\n    player.learn() #ì €ì¥ëœ ë°ì´í„°ë¥¼ í•™ìŠµ \n    #---ê°•í™”í•™ìŠµì˜ ì¢…ë£Œë¥¼ ê²°ì •--#\n    if player.n_experience &lt; 20:\n        pass \n    else: \n        if np.array(player.rewards)[-20:].mean() &gt; 9.5:\n            print(\"---ê²Œì„í´ë¦¬ì–´---\")\n            break\n\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼0ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼0ëˆ„ë¦„!\në²„íŠ¼0ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼0ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼0ëˆ„ë¦„!\në²„íŠ¼0ëˆ„ë¦„!\në²„íŠ¼0ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼0ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼0ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼0ëˆ„ë¦„!\në²„íŠ¼0ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\në²„íŠ¼1ëˆ„ë¦„!\n---ê²Œì„í´ë¦¬ì–´---"
  },
  {
    "objectID": "posts/13wk-2.html#a.-ê²Œì„ì„¤ëª…",
    "href": "posts/13wk-2.html#a.-ê²Œì„ì„¤ëª…",
    "title": "13wk-2: (ê°•í™”í•™ìŠµ) â€“ Bandit í™˜ê²½ ì„¤ê³„ ë° í’€ì´, 4x4 Grid World ê²Œì„ì„¤ëª…, í™˜ê²½êµ¬í˜„, ì—ì´ì „íŠ¸(ëœë¤)êµ¬í˜„",
    "section": "A. ê²Œì„ì„¤ëª…",
    "text": "A. ê²Œì„ì„¤ëª…\n- ë¬¸ì œì„¤ëª…: 4x4 ê·¸ë¦¬ë“œì›”ë“œì—ì„œ ìƒí•˜ì¢Œìš°ë¡œ ì›€ì§ì´ëŠ” ì—ì´ì „íŠ¸ê°€ ëª©í‘œì ì— ë„ë‹¬í•˜ë„ë¡ í•˜ëŠ” ê²Œì„\n\në°±ë¬¸ì´ ë¶ˆì—¬ì¼ê²¬: https://claude.ai/public/artifacts/76e13820-2b51-4e7e-a514-00190de17c45 (ì¶œì²˜: í´ë¡œë“œ)\n\nğŸ—£ï¸(\n\nì—ì´ì „íŠ¸ í™˜ê²½\nì—ì´ì „íŠ¸ í–‰ë™ - ìƒí•˜ì¢Œìš°ë¡œ ì´ë™ â€“&gt; 4ê°œì˜ í–‰ë™ = 0,1,2,3\ní™˜ê²½ì€ ë³´ìƒì„ ì¤Œ -&gt; -1, -10, +100 ì¤‘ í•˜ë‚˜ë¥¼ ì¤Œ\n\n-1: ê²©ì ì•ˆì— ì—ì´ì „íŠ¸ê°€ ìˆìŒ & ì—ì´ì „íŠ¸ì˜ ìœ„ì¹˜ê°€ (3,3)ì´ ì•„ë‹ ë•Œ\n+100: (ê²©ì ì•ˆì— ì—ì´ì „íŠ¸ê°€ ìˆìŒ &) ì—ì´ì „íŠ¸ì˜ ìœ„ì¹˜ê°€ (3,3)ì¼ ë•Œ\n-10: ì—ì´ì „íŠ¸ê°€ ê²©ì ì•ˆì— ìˆì§€ ì•ŠìŒ\n\nì—ì´ì „íŠ¸ &lt;â€”&gt; í™˜ê²½\n\nì—ì´ì „íŠ¸ â€“(action)â€“&gt; í™˜ê²½\nì—ì´ì „íŠ¸ &lt;â€“(reward, state)â€“ í™˜ê²½\n\n\n)ğŸ—£ï¸\n- GridWorldì—ì„œ ì‚¬ìš©ë˜ëŠ” ì£¼ìš”ë³€ìˆ˜\n\nState: ê° ê²©ì ì…€ì´ í•˜ë‚˜ì˜ ìƒíƒœì´ë©°, ì—ì´ì „íŠ¸ëŠ” ì´ëŸ¬í•œ ìƒíƒœ ì¤‘ í•˜ë‚˜ì— ìˆì„ ìˆ˜ ìˆìŒ.\nAction: ì—ì´ì „íŠ¸ëŠ” í˜„ì¬ìƒíƒœì—ì„œ ë‹¤ìŒìƒíƒœë¡œ ì´ë™í•˜ê¸° ìœ„í•´ ìƒ,í•˜,ì¢Œ,ìš° ì¤‘ í•˜ë‚˜ì˜ í–‰ë™ì„ ì·¨í•  ìˆ˜ ìˆìŒ.\nReward: ì—ì´ì „íŠ¸ê°€ í˜„ì¬ìƒíƒœì—ì„œ íŠ¹ì • actionì„ í•˜ë©´ ì–»ì–´ì§€ëŠ” ë³´ìƒ.\nTerminated: í•˜ë‚˜ì˜ ì—í”¼ì†Œë“œê°€ ì¢…ë£Œë˜ì—ˆìŒì„ ë‚˜íƒ€ë‚´ëŠ” ìƒíƒœ."
  },
  {
    "objectID": "posts/13wk-2.html#b.-ì‹œê°í™”",
    "href": "posts/13wk-2.html#b.-ì‹œê°í™”",
    "title": "13wk-2: (ê°•í™”í•™ìŠµ) â€“ Bandit í™˜ê²½ ì„¤ê³„ ë° í’€ì´, 4x4 Grid World ê²Œì„ì„¤ëª…, í™˜ê²½êµ¬í˜„, ì—ì´ì „íŠ¸(ëœë¤)êµ¬í˜„",
    "section": "B. ì‹œê°í™”",
    "text": "B. ì‹œê°í™”\n\ndef show(states):\n    fig = plt.Figure()\n    ax = fig.subplots()\n    ax.matshow(np.zeros([4,4]), cmap='bwr',alpha=0.0)\n    sc = ax.scatter(0, 0, color='red', s=500)  \n    ax.text(0, 0, 'start', ha='center', va='center')\n    ax.text(3, 3, 'end', ha='center', va='center')\n    # Adding grid lines to the plot\n    ax.set_xticks(np.arange(-.5, 4, 1), minor=True)\n    ax.set_yticks(np.arange(-.5, 4, 1), minor=True)\n    ax.grid(which='minor', color='black', linestyle='-', linewidth=2)\n    state_space = gym.spaces.MultiDiscrete([4,4])\n    def update(t):\n        if states[t] in state_space:\n            s1,s2 = states[t]\n            states[t] = [s2,s1]\n            sc.set_offsets(states[t])\n        else:\n            s1,s2 = states[t]\n            s1 = s1 + 0.5 if s1 &lt; 0 else (s1 - 0.5 if s1 &gt; 3 else s1)\n            s2 = s2 + 0.5 if s2 &lt; 0 else (s2 - 0.5 if s2 &gt; 3 else s2)\n            states[t] = [s2,s1]       \n            sc.set_offsets(states[t])\n    ani = FuncAnimation(fig,update,frames=len(states))\n    display(IPython.display.HTML(ani.to_jshtml()))\n\nğŸ—£ï¸(\n\nìœ„ ì½”ë“œëŠ” ê³µë¶€í•  í•„ìš” ì—†ê³ , ì‚¬ìš© ë°©ë²•ì„ ì•Œë©´ ë¨\n\n\nshow(\n    [[0,0],[0,1]]\n)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\nshow(\n    [[0,0],[0,1],[0,2],[0,3],[0,4]]\n) # ë°–ìœ¼ë¡œ ë‚˜ê°\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n)ğŸ—£ï¸\n\nshow([[0,0],[1,0],[2,0],[3,0],[4,0]]) # show ì‚¬ìš©ë°©ë²•\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/exercise-f1.html",
    "href": "posts/exercise-f1.html",
    "title": "A1: Exercise â€“ ver. 0505-1 #1",
    "section": "",
    "text": "ë¬¸ì œí’€ì´ì— í•„ìš”í•œ ëª¨ë“ˆì€ ìŠ¤ìŠ¤ë¡œ import í•  ê²ƒ\nimport torch"
  },
  {
    "objectID": "posts/exercise-f1.html#ë²¡í„°ì™€-í–‰ë ¬",
    "href": "posts/exercise-f1.html#ë²¡í„°ì™€-í–‰ë ¬",
    "title": "A1: Exercise â€“ ver. 0505-1 #1",
    "section": "$. ë²¡í„°ì™€ í–‰ë ¬",
    "text": "$. ë²¡í„°ì™€ í–‰ë ¬\n(1) ì•„ë˜ì™€ ê°™ì´ length 5 ì¸ vectorë¥¼ torch.tensorë¡œ ì„ ì–¸í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•˜ë¼.\n\\[{\\bf x} = [1,2,3,4,5]\\]\n(í’€ì´)\n\nx = torch.tensor([1,2,3,4,5])\nx\n\ntensor([1, 2, 3, 4, 5])\n\n\n(2) ì•„ë˜ì™€ ê°™ì€ 2x2 matrix ë¥¼ torch.tensorë¡œ ì„ ì–¸í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•˜ë¼.\n\\[{\\bf A} = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\\]\n(í’€ì´?)\n\nA = torch.tensor([[1,2],[3,4]])\nA\n\ntensor([[1, 2],\n        [3, 4]])\n\n\n(3) ì•„ë˜ì™€ ê°™ì€ matrix ë¥¼ torch.tensorë¡œ ì„ ì–¸í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•˜ë¼.\n\\[{\\bf W} = \\begin{bmatrix} 2.5  \\\\  4 \\end{bmatrix}\\]\n(í’€ì´?)\n\nW = torch.tensor([[2.5],[4]])\nW\n\ntensor([[2.5000],\n        [4.0000]])\n\n\n(4) ì•„ë˜ì™€ ê°™ì€ matrix ë¥¼ torch.tensorë¡œ ì„ ì–¸í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•˜ë¼.\n\\[{\\bf x} = \\begin{bmatrix} 2.5  & 4 \\end{bmatrix}\\]\n(í’€ì´?)\n\nx = torch.tensor([[2.5, 4]])\nx\n\ntensor([[2.5000, 4.0000]])"
  },
  {
    "objectID": "posts/exercise-f1.html#concat-stack",
    "href": "posts/exercise-f1.html#concat-stack",
    "title": "A1: Exercise â€“ ver. 0505-1 #1",
    "section": "$. concat, stack",
    "text": "$. concat, stack\na,bê°€ ì•„ë˜ì™€ ê°™ì´ ì£¼ì–´ì¡Œë‹¤ê³  í•˜ì.\n\na = torch.tensor([1]*10)\nb = torch.tensor([2]*10)\n\n(ê´€ì°°?)\n\na\n\ntensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n\n\n\na.shape\n\ntorch.Size([10])\n\n\n\nb\n\ntensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n\n\n(ì‹¤í—˜?)\n\ntorch.tensor([[1]*10])\n\ntensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n\n\n\ntorch.tensor([[1]*10]).shape\n\ntorch.Size([1, 10])\n\n\nì•„ë˜ë¥¼ ì˜ ì½ê³  ë¬¼ìŒì— ë‹µí•˜ë¼.\n(1) ì£¼ì–´ì§„ a,bì™€ torch.concatë¥¼ ì´ìš©í•˜ì—¬ ì•„ë˜ì™€ ê°™ì€ ë°°ì—´ì„ ë§Œë“¤ì–´ë¼.\ntensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n\ntorch.concat([a.reshape(-1,1), b.reshape(-1,1)])\n\ntensor([[1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [2],\n        [2],\n        [2],\n        [2],\n        [2],\n        [2],\n        [2],\n        [2],\n        [2],\n        [2]])\n\n\n(í’€ì´?)\n\ntorch.concat([a,b])\n\ntensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n\n\n(2) ì£¼ì–´ì§„ a,b ì™€ torch.concat,.reshapeë¥¼ ì´ìš©í•˜ì—¬ ì•„ë˜ì™€ ê°™ì€ ë°°ì—´ì„ ë§Œë“¤ì–´ë¼.\ntensor([[1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [2],\n        [2],\n        [2],\n        [2],\n        [2],\n        [2],\n        [2],\n        [2],\n        [2],\n        [2]])\n(í’€ì´)\n\ntorch.concat([a.reshape(-1,1), b.reshape(-1,1)])\n\ntensor([[1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [2],\n        [2],\n        [2],\n        [2],\n        [2],\n        [2],\n        [2],\n        [2],\n        [2],\n        [2]])\n\n\n(ê´€ì°°?)\n\na.reshape(-1,1)\n\ntensor([[1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1]])\n\n\n\na\n\ntensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n\n\n(3) ì£¼ì–´ì§„ a,b ì™€ torch.concat,.reshapeë¥¼ ì´ìš©í•˜ì—¬ ì•„ë˜ì™€ ê°™ì€ ë°°ì—´ì„ ë§Œë“¤ì–´ë¼.\ntensor([[1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2]])\n(í’€ì´?)\n\ntorch.concat([a,b])\n\ntensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n\n\n\ntorch.concat([a,b]).reshape(-1,2)\n\ntensor([[1, 1],\n        [1, 1],\n        [1, 1],\n        [1, 1],\n        [1, 1],\n        [2, 2],\n        [2, 2],\n        [2, 2],\n        [2, 2],\n        [2, 2]])\n\n\n\ntorch.concat([a.reshape(-1,1),b.reshape(-1,1)], axis=1)\n\ntensor([[1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2]])\n\n\n(4) ì£¼ì–´ì§„ a,bì™€ torch.stack ì„ ì´ìš©í•˜ì—¬ ì•„ë˜ì™€ ê°™ì€ ë°°ì—´ì„ ë§Œë“¤ì–´ë¼.\ntensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n        [2, 2, 2, 2, 2, 2, 2, 2, 2, 2]]\n(í’€ì´?)\n\ntorch.stack([a,b])\n\ntensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n        [2, 2, 2, 2, 2, 2, 2, 2, 2, 2]])\n\n\n(ê´€ì°°?)\n\ntorch.stack([a,b], axis=1)\n\ntensor([[1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2]])\n\n\n(5) ì£¼ì–´ì§„ a,bì™€ torch.stackì„ ì´ìš©í•˜ì—¬ ì•„ë˜ì™€ ê°™ì€ ë°°ì—´ì„ ë§Œë“¤ì–´ë¼.\ntensor([[1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2]])\n(í’€ì´?)\n\ntorch.stack([a,b], axis=1)\n\ntensor([[1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2],\n        [1, 2]])"
  },
  {
    "objectID": "posts/exercise-f1.html#í–‰ë ¬ê³±",
    "href": "posts/exercise-f1.html#í–‰ë ¬ê³±",
    "title": "A1: Exercise â€“ ver. 0505-1 #1",
    "section": "$. í–‰ë ¬ê³±",
    "text": "$. í–‰ë ¬ê³±\n(1) ì•„ë˜ì™€ ê°™ì€ í…ì„œë¥¼ ê³ ë ¤í•˜ì.\n\na = torch.tensor([1,2,3,4,5]).reshape(-1,1)\nb = torch.tensor([3,2,1,1,2]).reshape(-1,1)\n\n@ ì—°ì‚°ìë¥¼ ì´ìš©í•˜ì—¬ \\(\\sum_{i=1}^{5}a_ib_i\\)ë¥¼ ê³„ì‚°í•˜ë¼.\n(í’€ì´)\n\na.T @ b\n\ntensor([[24]])\n\n\n(2) ì•„ë˜ì™€ ê°™ì€ í…ì„œë¥¼ ê³ ë ¤í•˜ì.\n\ntorch.manual_seed(0)\nx = torch.randn(100).reshape(-1,1)\n\n@ì—°ì‚°ìë¥¼ ì´ìš©í•˜ì—¬ \\(\\sum_{i=1}^{100}x_i^2\\)ì„ ê³„ì‚°í•˜ë¼.\n(í’€ì´?)\n\nx.T @ x\n\ntensor([[105.0856]])\n\n\n\ny = x.squeeze()\ny @ y\n\ntensor(105.0856)"
  },
  {
    "objectID": "posts/exercise-f1.html#ì¸ë±ì‹±",
    "href": "posts/exercise-f1.html#ì¸ë±ì‹±",
    "title": "A1: Exercise â€“ ver. 0505-1 #1",
    "section": "$. ì¸ë±ì‹±",
    "text": "$. ì¸ë±ì‹±\nì•„ë˜ì™€ ê°™ì€ ë°°ì—´ì„ ì„ ì–¸í•˜ë¼.\n\ntorch.manual_seed(1)\nx = torch.randn(12).reshape(3,4)\nx\n\ntensor([[ 0.6614,  0.2669,  0.0617,  0.6213],\n        [-0.4519, -0.1661, -1.5228,  0.3817],\n        [-1.0276, -0.5631, -0.8923, -0.0583]])\n\n\n(1) 1ì—´ì„ ì¶”ì¶œí•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•˜ë¼. ì¦‰ ê²°ê³¼ê°€ ì•„ë˜ì™€ ê°™ì´ ë‚˜ì˜¤ë„ë¡ í•˜ë¼.\ntensor([[ 0.6614],\n        [-0.4519],\n        [-1.0276]])\n(í’€ì´?)\n\nx[:,0]\n\ntensor([ 0.6614, -0.4519, -1.0276])\n\n\n\nx[:,0].reshape(-1,1)\n\ntensor([[ 0.6614],\n        [-0.4519],\n        [-1.0276]])\n\n\n\nx[:,0].unsqueeze(1)\n\ntensor([[ 0.6614],\n        [-0.4519],\n        [-1.0276]])\n\n\n(2) 2-3ì—´ì„ ì¶”ì¶œí•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•˜ë¼. ì¦‰ ê²°ê³¼ê°€ ì•„ë˜ì™€ ê°™ì´ ë‚˜ì˜¤ë„ë¡ í•˜ë¼.\ntensor([[ 0.2669,  0.0617],\n        [-0.1661, -1.5228],\n        [-0.5631, -0.8923]])\n(í’€ì´?)\n\nx[:,1:3]\n\ntensor([[ 0.2669,  0.0617],\n        [-0.1661, -1.5228],\n        [-0.5631, -0.8923]])\n\n\n\nx[:,[1,2]]\n\ntensor([[ 0.2669,  0.0617],\n        [-0.1661, -1.5228],\n        [-0.5631, -0.8923]])\n\n\n(3) 2-3í–‰ì„ ì¶”ì¶œí•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•˜ë¼. ì¦‰ ê²°ê³¼ê°€ ì•„ë˜ì™€ ê°™ì´ ë‚˜ì˜¤ë„ë¡ í•˜ë¼.\ntensor([[-0.4519, -0.1661, -1.5228,  0.3817],\n        [-1.0276, -0.5631, -0.8923, -0.0583]])\n\n(í’€ì´?)\n\nx[1:,:]\n\ntensor([[-0.4519, -0.1661, -1.5228,  0.3817],\n        [-1.0276, -0.5631, -0.8923, -0.0583]])\n\n\n\nx[1:3,:]\n\ntensor([[-0.4519, -0.1661, -1.5228,  0.3817],\n        [-1.0276, -0.5631, -0.8923, -0.0583]])\n\n\n\nx[[1,2],:]\n\ntensor([[-0.4519, -0.1661, -1.5228,  0.3817],\n        [-1.0276, -0.5631, -0.8923, -0.0583]])\n\n\n(ê´€ì°°?)\n\nx[1:2,:]\n\ntensor([[-0.4519, -0.1661, -1.5228,  0.3817]])"
  },
  {
    "objectID": "posts/exercise-f1.html#torch.einsum",
    "href": "posts/exercise-f1.html#torch.einsum",
    "title": "A1: Exercise â€“ ver. 0505-1 #1",
    "section": "$. torch.einsum",
    "text": "$. torch.einsum\n(1) ì•„ë˜ì— ì½”ë“œì¤‘ X.t()ì— ëŒ€ì‘í•˜ëŠ” ì½”ë“œë¥¼ torch.einsumìœ¼ë¡œ êµ¬í˜„í•˜ë¼.\n\nX = torch.randn(5,2)\nX.t()\n\ntensor([[-0.1955,  0.4224, -0.4212, -1.5727,  3.5870],\n        [-0.9656,  0.2673, -0.5107, -0.1232, -1.8313]])\n\n\n(í’€ì´?)\n\ntorch.einsum('ij-&gt;ji', X)\n\ntensor([[-0.1955,  0.4224, -0.4212, -1.5727,  3.5870],\n        [-0.9656,  0.2673, -0.5107, -0.1232, -1.8313]])\n\n\n(2) ì•„ë˜ì— ì½”ë“œì¤‘ X@bì— ëŒ€ì‘í•˜ëŠ” ì½”ë“œë¥¼ torch.einsumìœ¼ë¡œ êµ¬í˜„í•˜ë¼.\n\nX = torch.randn(5,2)\nb = torch.randn(2,1)\nX@b\n\ntensor([[ 3.0610],\n        [ 1.1183],\n        [-5.5600],\n        [ 2.2772],\n        [-1.3251]])\n\n\n(í’€ì´?)\n\ntorch.einsum('ij,jk-&gt;ik', X, b)\n\ntensor([[ 3.0610],\n        [ 1.1183],\n        [-5.5600],\n        [ 2.2772],\n        [-1.3251]])\n\n\n(3) ì•„ë˜ì— ì½”ë“œì¤‘ linr(X)ì— ëŒ€ì‘í•˜ëŠ” ì½”ë“œë¥¼ torch.einsumìœ¼ë¡œ êµ¬í˜„í•˜ë¼.\n\nX =  torch.randn(5,2)\nlinr = torch.nn.Linear(2,1,bias=False)\nlinr(X)\n\ntensor([[-0.1186],\n        [-0.1857],\n        [ 0.1783],\n        [ 0.2444],\n        [ 0.4162]], grad_fn=&lt;MmBackward0&gt;)\n\n\n(í’€ì´?)\n\nW = linr.weight\n\n\nW.shape\n\ntorch.Size([1, 2])\n\n\n\ntorch.einsum('ij,kj-&gt;ik', X, W)   # shape: (5,1)\n\ntensor([[-0.1186],\n        [-0.1857],\n        [ 0.1783],\n        [ 0.2444],\n        [ 0.4162]], grad_fn=&lt;ViewBackward0&gt;)"
  },
  {
    "objectID": "posts/10wk-2.html",
    "href": "posts/10wk-2.html",
    "title": "10wk-2: (ì¶”ì²œì‹œìŠ¤í…œ) â€“ optimizer ì‚¬ìš© ê³ ê¸‰, ëª¨ë¸ë§ ì „ëµ, MF-based ì¶”ì²œì‹œìŠ¤í…œ",
    "section": "",
    "text": "ğŸ“˜ Note Format Guide\nThis format serves as a structured guide for organizing lecture content, personal interpretation, experiments, and study-related questions.\nğŸ“ ğŸ—£ï¸ âœï¸ ğŸ”¬ â“"
  },
  {
    "objectID": "posts/10wk-2.html#a.-optimizer-ì‚¬ìš©-ê³ ê¸‰",
    "href": "posts/10wk-2.html#a.-optimizer-ì‚¬ìš©-ê³ ê¸‰",
    "title": "10wk-2: (ì¶”ì²œì‹œìŠ¤í…œ) â€“ optimizer ì‚¬ìš© ê³ ê¸‰, ëª¨ë¸ë§ ì „ëµ, MF-based ì¶”ì²œì‹œìŠ¤í…œ",
    "section": "A. optimizer ì‚¬ìš© ê³ ê¸‰",
    "text": "A. optimizer ì‚¬ìš© ê³ ê¸‰\n# íšŒê·€ë¶„ì„ â€“ ì•ˆì•Œë ¤ì¤¬ë˜ ê¸°ìˆ ..\nì£¼ì–´ì§„ ìë£Œê°€ ì•„ë˜ì™€ ê°™ë‹¤ê³  í•˜ì.\n\ntorch.manual_seed(43052)\nx,_ = torch.randn(100).sort()\nx = x.reshape(-1,1)\nones= torch.ones(100).reshape(-1,1)\nX = torch.concat([ones,x],axis=-1)\nÏµ = torch.randn(100).reshape(-1,1)*0.5\ny = 2.5+ 4*x + Ïµ\n\n\nplt.plot(x,y,'o')\n\n\n\n\n\n\n\n\n\nw = torch.tensor(10.0,requires_grad=True)\nb = torch.tensor(-5.0,requires_grad=True)\n\n\nplt.plot(x,y,'o')\nplt.plot(x,(x*w + b).data,'--')\n\n\n\n\n\n\n\n\ntorch.optim.SGDë¥¼ ì´ìš©í•˜ì—¬ Whatì„ updateí•˜ë¼. í•™ìŠµë¥ ì€ 0.1ë¡œ ì„¤ì •í•˜ê³  30íšŒ updateí•˜ë¼.\n(í’€ì´)\nğŸ—£ï¸(\n\nloss_fn = torch.nn.MSELoss()\n# optimizr = torch.optim.SGD(net.parameters())\nfor epoc in range(30):\n    yhat = x*w + b\n    loss = loss_fn(yhat,y)\n    loss.backward()\n    w.data = w.data - 0.1*w.grad\n    b.data = b.data - 0.1*b.grad\n    w.grad=None\n    b.grad=None\n\n\nyhat = x*w + b ì´ëŸ° ì‹ì´ë¼ optimizr ì‚¬ìš© ë¶ˆê°€\n\n\nw,b\n\n(tensor(4.0144, requires_grad=True), tensor(2.4290, requires_grad=True))\n\n\n\nplt.plot(x,y,'o')\nplt.plot(x,(x*w + b).data,'--')\n\n\n\n\n\n\n\n\n\nì´ë ‡ê²Œ í•˜ë ¤ë©´ update ê³µì‹ì„ ì „ë¶€ ì•Œê³  ìˆì–´ì•¼ í•˜ë¯€ë¡œ optimizerë¥¼ ì‚¬ìš©í•˜ê³  ì‹¶ìŒ (SGDë§Œ ë°”ê¿”ì„œ)\n\n\n# torch.optim.SGD?\n\nInit signature:\ntorch.optim.SGD(\n    params: Union[Iterable[torch.Tensor], Iterable[Dict[str, Any]], Iterable[Tuple[str, torch.Tensor]]],\n    lr: Union[float, torch.Tensor] = 0.001,\n\nIterable ì˜ˆì‹œ: ë¦¬ìŠ¤íŠ¸\n\ní•™ìŠµí•˜ê³  ì‹¶ì€ parameterë“¤ì´ network í˜•íƒœë¡œ ì§œì ¸ ìˆì§€ ì•Šì•„ë„ ë¦¬ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ì „ë‹¬í•´ì£¼ë©´ ë¨\n\n\n\nw = torch.tensor(10.0,requires_grad=True)\nb = torch.tensor(-5.0,requires_grad=True)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD([w,b],lr=0.1)\nfor epoc in range(30):\n    yhat = x*w +b \n    loss = loss_fn(yhat,y) \n    loss.backward()\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nw,b\n\n(tensor(4.0144, requires_grad=True), tensor(2.4290, requires_grad=True))\n\n\n\nplt.plot(x,y,'o')\nplt.plot(x,(x*w + b).data,'--')\n\n\n\n\n\n\n\n\n)ğŸ—£ï¸\n\nw = torch.tensor(10.0,requires_grad=True)\nb = torch.tensor(-5.0,requires_grad=True)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD([w,b],lr=0.1)\nfor epoc in range(30):\n    yhat = x*w +b \n    loss = loss_fn(yhat,y) \n    loss.backward()\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nw,b\n\n(tensor(4.0144, requires_grad=True), tensor(2.4290, requires_grad=True))\n\n\n\nplt.plot(x,y,'o')\nplt.plot(x,(x*w + b).data,'--')\n\n\n\n\n\n\n\n\n#\n# 2025-ì¤‘ê°„ê³ ì‚¬ 3ë²ˆ\n\ntorch.manual_seed(43052)\ndist = torch.distributions.Exponential(1/2)\nx = dist.sample((10000,1))\n\nì£¼ì–´ì§„ ìë£Œ \\(x_i\\)ì— ëŒ€í•˜ì—¬ í•¨ìˆ˜ \\(l(\\lambda)\\)ë¥¼ ìµœëŒ€í™”í•˜ëŠ” \\(\\lambda\\)ë¥¼ ê²½ì‚¬í•˜ê°•ë²• ê¸°ë°˜ì˜ ì•Œê³ ë¦¬ì¦˜ì„ ì´ìš©í•˜ì—¬ ì¶”ì •í•˜ë¼. ë‹¨ ì´ë•Œ \\(\\lambda\\)ì˜ ì´ˆê¸° ì¶”ì •ê°’ì€ 1ë¡œ ì„¤ì •í•˜ë¼.\n\\[\nl(\\lambda) =\\frac{1}{n} \\sum_{i=1}^{n}\\log f(x_i), \\quad f(x_i) = \\frac{1}{\\lambda} e^{-\\frac{x_i}{\\lambda}}, \\quad x_i \\geq 0\n\\]\nhint\n\n\\(l(\\lambda)\\)ë¥¼ ìµœëŒ€í™”í•˜ëŠ” \\(\\lambda\\)ëŠ” \\(-l(\\lambda)\\)ë¥¼ ìµœì†Œí™”í•©ë‹ˆë‹¤.\nì´ë¡ ì ìœ¼ë¡œëŠ” \\(l(\\lambda)\\)ë¥¼ ìµœëŒ€í™”í•˜ëŠ” \\(\\lambda\\)ëŠ” x.mean()ì…ë‹ˆë‹¤. ì¦‰ ì œëŒ€ë¡œ \\(\\lambda\\)ë¥¼ ì¶”ì •í•œë‹¤ë©´ x.mean()ì´ ë‚˜ì˜¤ë„ë¡ ë˜ì–´ìˆìŠµë‹ˆë‹¤.\nì €ëŠ” ê²½ì‚¬í•˜ê°•ë²•ì„ ì´ìš©í–ˆê³  í•™ìŠµë¥ ì€ 0.05ë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤. 1000íšŒ updateí•˜ë‹ˆê¹Œ ì˜ ìˆ˜ë ´í–ˆìŠµë‹ˆë‹¤.\n\n(í’€ì´)\nğŸ—£ï¸(\n\nlamb = torch.tensor(1.0, requires_grad=True) # matrix [[1.0]]ê°€ ì•„ë‹ˆë¼ scalarì—¬ë„ ìƒê´€ X\n\n\n-x\n\ntensor([[-0.9071],\n        [-1.0760],\n        [-0.0572],\n        ...,\n        [-0.3747],\n        [-1.2479],\n        [-0.9108]])\n\n\n\n-x/lamb\n\ntensor([[-0.9071],\n        [-1.0760],\n        [-0.0572],\n        ...,\n        [-0.3747],\n        [-1.2479],\n        [-0.9108]], grad_fn=&lt;DivBackward0&gt;)\n\n\n\ntorch.exp(-x/lamb)\n\ntensor([[0.4037],\n        [0.3410],\n        [0.9444],\n        ...,\n        [0.6875],\n        [0.2871],\n        [0.4022]], grad_fn=&lt;ExpBackward0&gt;)\n\n\n\nfx = torch.exp(-x/lamb)/lamb\nl = torch.log(fx).mean()\n\n\nfor i in range(1000):\n    fx = torch.exp(-x/lamb)/lamb\n    l = torch.log(fx).mean()\n    (-l).backward()\n    lamb.data = lamb.data - 0.1*lamb.grad\n    lamb.grad = None\n\n\nlamb\n\ntensor(1.9874, requires_grad=True)\n\n\n\nx.mean()\n\ntensor(1.9874)\n\n\n\në‹¤ë¥¸ ë°©ë²• (ë¬¸ë²•ì´ ì¤‘ìš”, iterableë¡œ ì…ë ¥)\n\n\nlamb = torch.tensor(1.0, requires_grad=True)\noptimizr = torch.optim.SGD([lamb], lr=0.1)\n\n\nfor i in range(1000):\n    fx = torch.exp(-x/lamb)/lamb\n    l = torch.log(fx).mean()\n    (-l).backward()\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nlamb\n\ntensor(1.9874, requires_grad=True)\n\n\n)ğŸ—£ï¸\n\nlamb = torch.tensor(1.0,requires_grad=True)\noptimizr = torch.optim.SGD([lamb],  lr =0.05)\n\n\nfor i in range(1000):\n    fx = torch.exp(-x/lamb)/lamb\n    l = torch.log(fx).mean()\n    (-l).backward()\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nlamb\n\ntensor(1.9874, requires_grad=True)\n\n\n#"
  },
  {
    "objectID": "posts/10wk-2.html#b.-ëª¨ë¸ë§-ì „ëµ",
    "href": "posts/10wk-2.html#b.-ëª¨ë¸ë§-ì „ëµ",
    "title": "10wk-2: (ì¶”ì²œì‹œìŠ¤í…œ) â€“ optimizer ì‚¬ìš© ê³ ê¸‰, ëª¨ë¸ë§ ì „ëµ, MF-based ì¶”ì²œì‹œìŠ¤í…œ",
    "section": "B. ëª¨ë¸ë§ ì „ëµ",
    "text": "B. ëª¨ë¸ë§ ì „ëµ\n# 2025-ì¤‘ê°„ê³ ì‚¬ 4ë²ˆ â€“ ììœ  ë‚™í•˜ ìš´ë™ì´ë€ ì–´ë–¤ ë¬¼ì²´ê°€ ì¼ì •í•œ ë†’ì´ì—ì„œ ë–¨ì–´ì ¸ ì§€ë©´ì— ë„ë‹¬í•˜ê¸° ê¹Œì§€ ê±¸ë¦¬ëŠ” ì‹œê°„ì„ ë‹¤ë£¨ëŠ” ë¬¼ë¦¬í•™ ê°œë…ì´ë‹¤. ë‹¤ìŒì€ ë¬¼ë¦¬í•™ì˜ ììœ  ë‚™í•˜ ìš´ë™ì—ì„œ ì°©ì•ˆí•˜ì—¬ ìƒì„±í•œ ë°ì´í„°ì´ë‹¤.\n\ntorch.manual_seed(43052)\nh = torch.rand(100)*100\nh,_ = h.sort()\nh = h.reshape(100,1)\nt = torch.sqrt(2*h/9.8) + torch.randn([100,1])*0.1\n\nì—¬ê¸°ì—ì„œ \\(h\\)ëŠ” ë‚™í•˜ì „ì˜ ë†’ì´(ë‹¨ìœ„: m), \\(t\\)ëŠ” í•´ë‹¹ë†’ì´ì—ì„œ ë¬¼ì¹˜ê°€ ì§€ë©´ì— ë„ë‹¬í•˜ê¸° ê¹Œì§€ ê±¸ë¦¬ëŠ” ì‹œê°„(ë‹¨ìœ„:ì´ˆ)ì„ ì˜ë¯¸í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì•„ë˜ì˜ ìë£ŒëŠ” \\(h=99.3920, t=4.4583\\)ë¥¼ ì˜ë¯¸í•˜ëŠ”ë°\n\nh[-1], t[-1]\n\n(tensor([99.3920]), tensor([4.4583]))\n\n\nì´ê²ƒì€ ë†’ì´ \\(99.3920\\)mì—ì„œ ë‚™í•˜í•œ ë¬¼ì²´ê°€ ì•½ \\(4.4583\\)ì´ˆë§Œì— ì§€ë©´ì— ë„ë‹¬í–ˆìŒì„ ì˜ë¯¸í•œë‹¤. ì•„ë˜ì˜ ê·¸ë¦¼ì€ \\(x\\)ì¶•ì— \\(h\\), \\(y\\)ì¶•ì— \\(t\\)ë¥¼ ë‘ê³  í•´ë‹¹ ë°ì´í„°ë¥¼ ì‚°ì ë„ë¡œ ì‹œê°í™” í•œ ê²ƒì´ë‹¤.\n\nplt.plot(h,t,'o',alpha=0.5)\nplt.xlabel('Height (m)')\nplt.ylabel('Time to fall (sec)')\nplt.title('Free Fall Time vs Height')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nê·¸ë˜í”„ë¥¼ ë³´ë©´ ë†’ì´ê°€ ë†’ì„ ìˆ˜ë¡ ë‚™í•˜ì‹œê°„ë„ ê¸¸ì–´ì§€ëŠ” ê²½í–¥ì´ ê´€ì°°ëœë‹¤. ë‹¤ë§Œ ë™ì¼í•œ ë†’ì´ë¼ í•˜ë”ë¼ë„ ë‚™í•˜ì‹œê°„ì´ ì¡°ê¸ˆì”© ì°¨ì´ë‚˜ëŠ” ê²½ìš°ê°€ ìˆëŠ”ë°, ì´ëŠ” ì‚¬ëŒì´ ì‹œê°„ì¸¡ì •ì„ ìˆ˜ë™ìœ¼ë¡œ í•˜ë©° ë°œìƒí•˜ëŠ” ì‹¤í—˜ì˜¤ì°¨ ë•Œë¬¸ì´ë‹¤. ì´ëŸ¬í•œ ì˜¤ì°¨ì—ë„ ë¶ˆêµ¬í•˜ê³  \\(h\\)ì™€ \\(t\\)ì‚¬ì´ì—ëŠ” ì¼ì •í•œ ê·œì¹™ì´ ì¡´ì¬í•˜ëŠ”ë“¯ í•˜ë‹¤. ë¬¼ë¦¬í•™ê³¼ êµìˆ˜ë‹˜ê»˜ ìë¬¸ì„ ìš”ì²­í•œ ê²°ê³¼ ììœ ë‚™í•˜ì— ê±¸ë¦¬ëŠ” ì‹œê°„ì€ \\(\\sqrt{h}\\)ì— ë¹„ë¡€í•¨ì„ ì•Œ ìˆ˜ ìˆì—ˆê³  ì´ë¥¼ ê·¼ê±°ë¡œ ì•„ë˜ì™€ ê°™ì€ ëª¨í˜•ì„ ì„¤ê³„í•˜ì˜€ë‹¤.\n\\[t_i = \\beta_0 + \\beta_1 \\sqrt{h_i}+\\epsilon_i, \\quad \\epsilon_i \\sim {\\cal N}(0,\\sigma^2)\\]\nìœ„ì˜ ëª¨í˜•ì„ í™œìš©í•˜ì—¬ ë†’ì´ \\(h\\)ë¡œë¶€í„° ë‚™í•˜ì‹œê°„ \\(t\\)ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ì‹ ê²½ë§ ëª¨ë¸ì„ ì„¤ê³„í•˜ê³  í•™ìŠµí•˜ë¼. í•™ìŠµí•œ ì‹ ê²½ë§ ëª¨ë¸ì„ í™œìš©í•˜ì—¬ ë†’ì´ 40m,60m,80m ì—ì„œ ë¬¼ì²´ë¥¼ ììœ ë‚™í•˜ ì‹œì¼°ì„ë•Œ ì§€ë©´ì— ë„ë‹¬í•˜ê¸°ê¹Œì§€ ê±¸ë¦¬ëŠ” ì‹œê°„ì„ ê°ê° ì˜ˆì¸¡í•˜ë¼.\nhint\n\n\\(y_i = t_i\\) ë¡œ ìƒê°í•˜ì‹œê³  \\(x_i= \\sqrt{h}_i\\)ë¡œ ìƒê°í•˜ì‹œë©´ ê·¸ëƒ¥ íšŒê·€ëª¨í˜•ì´ì£ ?\në‹µì€ \\(2.8571\\)ì´ˆ, \\(3.4493\\)ì´ˆ, \\(4.0406\\)ì´ˆ ê·¼ì²˜ë¡œ ë‚˜ì˜¤ë©´ ë©ë‹ˆë‹¤.\nì œì‹œëœ ëª¨í˜•(\\(t_i = \\beta_0 + \\beta_1 \\sqrt{h_i}+\\epsilon_i\\))ì„ ë¬´ì‹œí•˜ê³  04wk-2ì™€ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ì‹ ê²½ë§ì„ ì„¤ê³„í•˜ê³  í‘¸ì…”ë„ ë§Œì ìœ¼ë¡œ ì¸ì •í•©ë‹ˆë‹¤.\n\nğŸ—£ï¸(\n\nh -&gt; t\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,32),\n    torch.nn.ReLU(),\n    torch.nn.Linear(32,1)\n)\noptimizr = torch.optim.Adam(net.parameters())\nloss_fn = torch.nn.MSELoss()\n#--#\nfor epoc in range(100):\n    # 1\n    that = net(h)\n    # 2\n    loss = loss_fn(that,t)\n    # 3\n    loss.backward()\n    # 4\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(h,t,'o',alpha=0.5)\nplt.xlabel('Height (m)')\nplt.ylabel('Time to fall (sec)')\nplt.title('Free Fall Time vs Height')\nplt.grid(True)\nplt.plot(h,that.data,'--')\nplt.show()\n\n\n\n\n\n\n\n\n\nepochì„ 2,000ìœ¼ë¡œ ì¦ê°€\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,32),\n    torch.nn.ReLU(),\n    torch.nn.Linear(32,1)\n)\noptimizr = torch.optim.Adam(net.parameters())\nloss_fn = torch.nn.MSELoss()\n#--#\nfor epoc in range(2000):\n    # 1\n    that = net(h)\n    # 2\n    loss = loss_fn(that,t)\n    # 3\n    loss.backward()\n    # 4\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(h,t,'o',alpha=0.5)\nplt.xlabel('Height (m)')\nplt.ylabel('Time to fall (sec)')\nplt.title('Free Fall Time vs Height')\nplt.grid(True)\nplt.plot(h,that.data,'--')\nplt.show()\n\n\n\n\n\n\n\n\n\nhh = torch.tensor([40,60,80]).float().reshape(3,1)\nnet(hh)\n\ntensor([[2.9389],\n        [3.4704],\n        [4.0018]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\n\në‹¤ë¥´ê²Œ í‘¼ë‹¤ë©´\n\nlinr1 = torch.nn.Linear(1,32)\nrelu = torch.nn.ReLU()\nlinr2 = torch.nn.Linear(32,1)\n\n#---#\nfor epoc in range(2000):\n    #1\n    that = linr2(relu(linr1(h)))\n    #2\n    loss = loss_fn(that,t)\n    #3\n    loss.backward()\n    #4 \n    optimizr.step()\n    optimizr.zero_grad()\n\noptimizrëŠ” ì–´ë–»ê²Œ?\n\n\nnet.parameters()\n\n&lt;generator object Module.parameters at 0x7f7f45c5d510&gt;\n\n\n\në¦¬ìŠ¤íŠ¸ëŠ” ì•„ë‹ˆì§€ë§Œ generator\n\n\n# list(net.parameters())\n\n\në¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“¤ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì´ì „ ì½”ë“œì— ë¦¬ìŠ¤íŠ¸ë§Œ í•´ì¤˜ë„ ë‹¤ ëŒì•„ê°\noptimizr = torch.optim.Adam(list(net.parameters())) ë„ ë˜ì§€ë§Œ ì•„ë˜ì™€ ê°™ì´ í•  ìˆ˜ë„ ìˆìŒ\n\nlinr1 = torch.nn.Linear(1,32)\nrelu = torch.nn.ReLU() # parameter ì—†ìŒ\nlinr2 = torch.nn.Linear(32,1)\nlist(linr1.parameters()) + list(linr2.parameters())\n\nlinr1 = torch.nn.Linear(1,32)\nrelu = torch.nn.ReLU()\nlinr2 = torch.nn.Linear(32,1)\noptimizr = torch.optim.Adam(list(linr1.parameters()) + list(linr2.parameters()))\nloss_fn = torch.nn.MSELoss()\n#---#\nfor epoc in range(2000):\n    #1\n    that = linr2(relu(linr1(h)))\n    #2\n    loss = loss_fn(that,t)\n    #3\n    loss.backward()\n    #4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(h,t,'o',alpha=0.5)\nplt.plot(h,that.data,'--')\n\n\n\n\n\n\n\n\n\nNetworkë¥¼ ì“°ë©´ í¸í•˜ì§€ë§Œ ê´œíˆ ì´ë ‡ê²Œ í’€ì–´ë„ ë˜ê¸´ í•¨\nì§€ê¸ˆê¹Œì§€ëŠ”\n\ntorch.nn.Sequential : ìˆœë°©í–¥ìœ¼ë¡œ íë¦„\ní•˜ì§€ë§Œ ê²½ìš°ì— ë”°ë¼ Networkê°€ ê·¸ë ‡ê²Œ ì•ˆ ë  ìˆ˜ë„ ìˆìŒ\nê·¸ëŸ° ê²½ìš°ëŠ” ì—¬ëŸ¬ Networkë¥¼ ì§œì„œ ì¡°í•© (ìœ„ì™€ ê°™ì´)\n\n\n)ğŸ—£ï¸\n(í’€ì´)\nğŸ—£ï¸(\n\nì›ë˜ í’€ì´ (ê¸°ì¡´ì˜ ë¬¼ë¦¬ ì´ë¡  í™œìš©)\n\n\nx = torch.sqrt(h)\ny = t\nnet = torch.nn.Linear(1,1)\noptimizr = torch.optim.Adam(net.parameters())\n#---#\nfor epoc in range(6000):\n    #1\n    yhat = net(x)\n    #2\n    loss = loss_fn(yhat,y)\n    #3\n    loss.backward()\n    #4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(h,t,'o',alpha=0.5)\nplt.plot(h,yhat.data,'--')\n\n\n\n\n\n\n\n\n\nhh = torch.tensor([40,60,80]).float().reshape(3,1)\nxx = torch.sqrt(hh)\nnet(xx)\n\ntensor([[2.8532],\n        [3.4889],\n        [4.0249]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\n\nëª¨ë¸ë§ì— ìˆì–´ ì •ë‹µì´ ì—†ì–´ì§€ëŠ” ì¶”ì„¸..\n\n)ğŸ—£ï¸\n\nlinr1 = torch.nn.Linear(1,32)\nrelu = torch.nn.ReLU()\nlinr2 = torch.nn.Linear(32,1)\noptimizr = torch.optim.Adam(list(linr1.parameters()) + list(linr2.parameters()))\nloss_fn = torch.nn.MSELoss()\n#---#\nfor epoc in range(2000):\n    #1\n    that = linr2(relu(linr1(h)))\n    #2\n    loss = loss_fn(that,t)\n    #3\n    loss.backward()\n    #4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(h,t,'o',alpha=0.5)\nplt.plot(h,that.data,'--')\n\n\n\n\n\n\n\n\n\nhh = torch.tensor([40,60,80]).float().reshape(3,1)\nnet(hh)\n\ntensor([[2.8631],\n        [3.5162],\n        [4.0079]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\n(í’€ì´2)\n\nx = torch.sqrt(h)\ny = t \nnet = torch.nn.Linear(1,1)\noptimizr = torch.optim.Adam(net.parameters())\n#---#\nfor epoc in range(6000):\n    #1\n    yhat = net(x) \n    #2\n    loss = loss_fn(yhat,y)\n    #3\n    loss.backward()\n    #4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(h,t,'o',alpha=0.5)\nplt.plot(h,yhat.data,'--')\n\n\n\n\n\n\n\n\n\nhh = torch.tensor([40,60,80]).float().reshape(3,1)\nxx = torch.sqrt(hh)\nnet(xx)\n\ntensor([[2.8613],\n        [3.4889],\n        [4.0180]], grad_fn=&lt;AddmmBackward0&gt;)"
  },
  {
    "objectID": "posts/10wk-2.html#a.-data-ë‚˜ëŠ”-solo",
    "href": "posts/10wk-2.html#a.-data-ë‚˜ëŠ”-solo",
    "title": "10wk-2: (ì¶”ì²œì‹œìŠ¤í…œ) â€“ optimizer ì‚¬ìš© ê³ ê¸‰, ëª¨ë¸ë§ ì „ëµ, MF-based ì¶”ì²œì‹œìŠ¤í…œ",
    "section": "A. Data: ë‚˜ëŠ” SOLO",
    "text": "A. Data: ë‚˜ëŠ” SOLO\n- Data\n\ndf_view = pd.read_csv('https://raw.githubusercontent.com/guebin/DL2025/main/posts/iamsolo.csv',index_col=0)\ndf_view\n\n\n\n\n\n\n\n\nì˜ì‹(IN)\nì˜ì² (IN)\nì˜í˜¸(IS)\nê´‘ìˆ˜(IS)\nìƒì² (EN)\nì˜ìˆ˜(EN)\nê·œë¹ˆ(ES)\në‹¤í˜¸(ES)\n\n\n\n\nì˜¥ìˆœ(IN)\nNaN\n4.02\n3.45\n3.42\n0.84\n1.12\n0.43\n0.49\n\n\nì˜ì(IN)\n3.93\n3.99\n3.63\n3.43\n0.98\n0.96\n0.52\nNaN\n\n\nì •ìˆ™(IS)\n3.52\n3.42\n4.05\n4.06\n0.39\nNaN\n0.93\n0.99\n\n\nì˜ìˆ™(IS)\n3.43\n3.57\nNaN\n3.95\n0.56\n0.52\n0.89\n0.89\n\n\nìˆœì(EN)\n1.12\nNaN\n0.59\n0.43\n4.01\n4.16\n3.52\n3.38\n\n\ní˜„ìˆ™(EN)\n0.94\n1.05\n0.32\n0.45\n4.02\n3.78\nNaN\n3.54\n\n\nì„œì—°(ES)\n0.51\n0.56\n0.88\n0.89\n3.50\n3.64\n4.04\n4.10\n\n\në³´ëŒ(ES)\n0.48\n0.51\n1.03\nNaN\n3.52\n4.00\n3.82\nNaN\n\n\ní•˜ë‹ˆ(I)\n4.85\n4.82\nNaN\n4.98\n4.53\n4.39\n4.45\n4.52\n\n\n\n\n\n\n\n- ë°ì´í„°ë¥¼ ì´í•´í•  ë•Œ í•„ìš”í•œ ê°€ì •ë“¤ â€“ ì œê°€ ë§ˆìŒëŒ€ë¡œ ì„¤ì •í–ˆì–´ìš”..\n\nê¶í•©ì´ ì˜ë§ìœ¼ë©´ 5ì , ì˜ ì•ˆë§ìœ¼ë©´ 0ì  ì´ë‹¤.\nMBTI ì„±í–¥ì— ë”°ë¼ì„œ ê¶í•¨ì˜ ì •ë„ê°€ ë‹¤ë¥´ë‹¤. íŠ¹íˆ I/Eì˜ ì„±í–¥ì¼ì¹˜ê°€ ì¤‘ìš”í•˜ë‹¤.\ní•˜ë‹ˆëŠ” ëª¨ë“  ì‚¬ëŒë“¤ê³¼ ëŒ€ì²´ë¡œ ê¶í•©ì´ ì˜ ë§ëŠ”ë‹¤."
  },
  {
    "objectID": "posts/10wk-2.html#b.-ì•„ì´ë””ì–´",
    "href": "posts/10wk-2.html#b.-ì•„ì´ë””ì–´",
    "title": "10wk-2: (ì¶”ì²œì‹œìŠ¤í…œ) â€“ optimizer ì‚¬ìš© ê³ ê¸‰, ëª¨ë¸ë§ ì „ëµ, MF-based ì¶”ì²œì‹œìŠ¤í…œ",
    "section": "B. ì•„ì´ë””ì–´",
    "text": "B. ì•„ì´ë””ì–´\n- ëª©í‘œ: NaNì„ ì¶”ì •\n- ìˆ˜ë™ì¶”ë¡ : ê·¸ëŸ´ë“¯í•œ ìˆ«ìë¥¼ ì¶”ì •í•´ë³´ì.\n\nì˜¥ìˆœ(IN),ì˜ì‹(IN)ì˜ ê¶í•©ì€? \\(\\to\\) ì˜ ë§ì„ë“¯\nì„œì—°(ES),ê·œë¹ˆ(ES)ì˜ ê¶í•©ì€? \\(\\to\\) ì˜ ë§ì„ë“¯\nì˜ì(IN),ë‹¤í˜¸(ES)ì˜ ê¶í•©ì€? \\(\\to\\) ì˜ ì•ˆë§ì„ë“¯\ní•˜ë‹ˆ(I),ì˜í˜¸(IS)ì˜ ê¶í•©ì€? \\(\\to\\)ì—„ì²­ ì˜ ë§ì„ë“¯\n\n- ì¢€ ë” ì²´ê³„ì ì¸ ì¶”ë¡  ì „ëµ\n\nsig = torch.nn.Sigmoid()\n\n(1) ì˜¥ìˆœ(IN)ê³¼ ì˜ì‹(IN)ì˜ ê¶í•© \\(\\approx\\) ì˜¥ìˆœì˜I/Eì„±í–¥\\(\\times\\)ì˜ì‹ì˜I/Eì„±í–¥ \\(+\\) ì˜¥ìˆœì˜N/Sì„±í–¥\\(\\times\\)ì˜ì‹ì˜N/Sì„±í–¥\nğŸ—£ï¸(\n\nIì— ê°€ê¹Œìš¸ìˆ˜ë¡ 2ì— ê°€ê¹ê³  Eì— ê°€ê¹Œìš¸ìˆ˜ë¡ -2ì— ê°€ê¹ë‹¤ê³  ê°€ì •\nNì— ê°€ê¹Œìš¸ìˆ˜ë¡ 2ì— ê°€ê¹ê³  Sì— ê°€ê¹Œìš¸ìˆ˜ë¡ -2ì— ê°€ê¹ë‹¤ê³  ê°€ì •\n\n\nì˜¥ìˆœì„±í–¥ =  torch.tensor([1.9, 1.9])\nì˜ì‹ì„±í–¥ =  torch.tensor([1.9, 1.9])\n\n\n(ì˜¥ìˆœì„±í–¥ * ì˜ì‹ì„±í–¥)\n\ntensor([3.6100, 3.6100])\n\n\n\n(ì˜¥ìˆœì„±í–¥ * ì˜ì‹ì„±í–¥).sum()\n\ntensor(7.2200)\n\n\n\nsig = torch.nn.Sigmoid()\n\n\nsig((ì˜¥ìˆœì„±í–¥ * ì˜ì‹ì„±í–¥).sum()) # í™•ë¥ ì´ ì•„ë‹ˆë¼ í‰ì ìœ¼ë¡œ ìƒê° (0~1)\n\ntensor(0.9993)\n\n\n\nsig((ì˜¥ìˆœì„±í–¥ * ì˜ì‹ì„±í–¥).sum())*5\n\ntensor(4.9963)\n\n\n\në‚˜ë¦„ ê·¼ê±°ëŠ” ìˆìŒ\n\n\nì˜¥ìˆœì„±í–¥ =  torch.tensor([0, 1.9])\nì˜ì‹ì„±í–¥ =  torch.tensor([1.9, 1.9])\nsig((ì˜¥ìˆœì„±í–¥ * ì˜ì‹ì„±í–¥).sum())*5 # ì¡°ê¸ˆ ê°ì†Œ\n\ntensor(4.8683)\n\n\n\nì˜¥ìˆœì„±í–¥ =  torch.tensor([1.9, 1.9])\nì˜ì‹ì„±í–¥ =  torch.tensor([-1.9, -1.9])\n(ì˜¥ìˆœì„±í–¥ * ì˜ì‹ì„±í–¥).sum()\n\ntensor(-7.2200)\n\n\n\nsig((ì˜¥ìˆœì„±í–¥ * ì˜ì‹ì„±í–¥).sum())*5\n\ntensor(0.0037)\n\n\n\nì˜¥ìˆœì„±í–¥ =  torch.tensor([1.9, 1.9])\nì˜ì‹ì„±í–¥ =  torch.tensor([1.9, -1.9])\nsig((ì˜¥ìˆœì„±í–¥ * ì˜ì‹ì„±í–¥).sum())*5\n\ntensor(2.5000)\n\n\n)ğŸ—£ï¸\n\nì˜¥ìˆœì„±í–¥ =  torch.tensor([1.9, 1.9])\nì˜ì‹ì„±í–¥ =  torch.tensor([1.9, 1.9])\nsig((ì˜¥ìˆœì„±í–¥ *  ì˜ì‹ì„±í–¥).sum())*5\n\ntensor(4.9963)\n\n\n(2) ì„œì—°(ES)ê³¼ ê·œë¹ˆ(ES)ì˜ ê¶í•© \\(\\approx\\) ì„œì—°ì˜I/Eì„±í–¥\\(\\times\\)ê·œë¹ˆì˜I/Eì„±í–¥ \\(+\\) ì„œì—°ì˜N/Sì„±í–¥\\(\\times\\)ê·œë¹ˆì˜N/Sì„±í–¥\n\nì„œì—°ì„±í–¥ =  torch.tensor([-1.9, -1.9])\nê·œë¹ˆì„±í–¥ =  torch.tensor([-1.9, -1.9])\nsig((ì„œì—°ì„±í–¥ *  ê·œë¹ˆì„±í–¥).sum())*5\n\ntensor(4.9963)\n\n\n(3) ì˜ì(IN)ì™€ ë‹¤í˜¸(ES)ì˜ ê¶í•© \\(\\approx\\) ì˜ìI/Eì„±í–¥\\(\\times\\)ë‹¤í˜¸I/Eì„±í–¥ \\(+\\) ì˜ìN/Sì„±í–¥\\(\\times\\)ë‹¤í˜¸ì˜N/Sì„±í–¥\n\nì˜ìì„±í–¥ =  torch.tensor([1.9, 1.9])\në‹¤í˜¸ì„±í–¥ =  torch.tensor([-1.9, -1.9])\nsig((ì˜ìì„±í–¥ *  ë‹¤í˜¸ì„±í–¥).sum())*5\n\ntensor(0.0037)\n\n\n(4) í•˜ë‹ˆ(I)ì™€ ì˜í˜¸(IS)ì˜ ê¶í•© \\(\\approx\\) í•˜ë‹ˆI/Eì„±í–¥\\(\\times\\)ì˜í˜¸I/Eì„±í–¥ \\(+\\) í•˜ë‹ˆN/Sì„±í–¥\\(\\times\\)ì˜í˜¸ì˜N/Sì„±í–¥ \\(+\\) í•˜ë‹ˆì˜ë§¤ë ¥\nğŸ—£ï¸(\n\ní•˜ë‹ˆì„±í–¥ =  torch.tensor([1.9, 0])\nì˜í˜¸ì„±í–¥ =  torch.tensor([1.9, -1.9])\nsig((í•˜ë‹ˆì„±í–¥ * ì˜í˜¸ì„±í–¥).sum())*5\n\ntensor(4.8683)\n\n\n\ní•˜ë‹ˆì„±í–¥ =  torch.tensor([1.9, 0])\nì˜í˜¸ì„±í–¥ =  torch.tensor([1.9, -1.9])\nsig((í•˜ë‹ˆì„±í–¥ * ì˜í˜¸ì„±í–¥).sum())*5\n\ntensor(4.8683)\n\n\n\ní•˜ë‹ˆì„±í–¥ =  torch.tensor([1.9, 0])\nì˜í˜¸ì„±í–¥ =  torch.tensor([-1.9, -1.9])\nsig((í•˜ë‹ˆì„±í–¥ * ì˜í˜¸ì„±í–¥).sum())*5\n\ntensor(0.1317)\n\n\n\ní•˜ë‹ˆëŠ” ë‹¤ ë§ëŠ”ë‹¤ê³  ê°€ì •í•˜ì˜€ìœ¼ë¯€ë¡œ ì¼ê´„ì ìœ¼ë¡œ ìƒìˆ˜ë¥¼ ë”í•  í•„ìš”ê°€ ìˆìŒ\n\n\ní•˜ë‹ˆì„±í–¥ =  torch.tensor([1.9, 0])\nì˜í˜¸ì„±í–¥ =  torch.tensor([-1.9, -1.9])\nsig((í•˜ë‹ˆì„±í–¥ * ì˜í˜¸ì„±í–¥).sum() + 5)*5\n\ntensor(4.0030)\n\n\n\ní•˜ë‹ˆì„±í–¥ =  torch.tensor([1.9, 0])\ní•˜ë‹ˆë§¤ë ¥ =  torch.tensor(5)\nì˜í˜¸ì„±í–¥ =  torch.tensor([-1.9, -1.9])\nsig((í•˜ë‹ˆì„±í–¥ *  ì˜í˜¸ì„±í–¥).sum() + í•˜ë‹ˆë§¤ë ¥)*5\n\ntensor(4.0030)\n\n\n\ní•˜ë‹ˆì„±í–¥ =  torch.tensor([1.9, 0])\ní•˜ë‹ˆë§¤ë ¥ =  torch.tensor(5)\nì˜í˜¸ì„±í–¥ =  torch.tensor([1.9, -1.9])\nsig((í•˜ë‹ˆì„±í–¥ *  ì˜í˜¸ì„±í–¥).sum() + í•˜ë‹ˆë§¤ë ¥)*5 # ì„±í–¥ì´ ë§ëŠ”ë‹¤ë©´ 5 ê·¼ì²˜\n\ntensor(4.9991)\n\n\n)ğŸ—£ï¸\n\ní•˜ë‹ˆì„±í–¥ =  torch.tensor([1.9, 0])\ní•˜ë‹ˆë§¤ë ¥ =  torch.tensor(5)\nì˜í˜¸ì„±í–¥ =  torch.tensor([1.9, -1.9])\nsig((í•˜ë‹ˆì„±í–¥ *  ì˜í˜¸ì„±í–¥).sum() + í•˜ë‹ˆë§¤ë ¥)*5\n\ntensor(4.9991)\n\n\n- ì „ì²´ ì‚¬ìš©ìì˜ ì„¤ì •ê°’\nğŸ—£ï¸(\n\nI, N = 1.8, 1.8\n\n\nì˜¥ìˆœì„±í–¥  = torch.tensor([I,N])\nì˜ìì„±í–¥ = torch.tensor([I,N])\nì •ìˆ™ì„±í–¥ = torch.tensor([I,-N])\nì˜ìˆ™ì„±í–¥ = torch.tensor([I,-N])\nìˆœìì„±í–¥ = torch.tensor([-I,N])\ní˜„ìˆ™ì„±í–¥ = torch.tensor([-I,N])\nì„œì—°ì„±í–¥ = torch.tensor([-I,-N])\në³´ëŒì„±í–¥ = torch.tensor([-I,-N])\ní•˜ë‹ˆì„±í–¥ = torch.tensor([I,0])\n\n\n[ì˜¥ìˆœì„±í–¥,ì˜ìì„±í–¥,ì •ìˆ™ì„±í–¥,ì˜ìˆ™ì„±í–¥,ìˆœìì„±í–¥,í˜„ìˆ™ì„±í–¥,ì„œì—°ì„±í–¥,ë³´ëŒì„±í–¥,í•˜ë‹ˆì„±í–¥] # torch ë“¤ì„ ë¬¶ì€ ë¦¬ìŠ¤íŠ¸\n\n[tensor([1.8000, 1.8000]),\n tensor([1.8000, 1.8000]),\n tensor([ 1.8000, -1.8000]),\n tensor([ 1.8000, -1.8000]),\n tensor([-1.8000,  1.8000]),\n tensor([-1.8000,  1.8000]),\n tensor([-1.8000, -1.8000]),\n tensor([-1.8000, -1.8000]),\n tensor([1.8000, 0.0000])]\n\n\n\ntorch.stack([ì˜¥ìˆœì„±í–¥,ì˜ìì„±í–¥,ì •ìˆ™ì„±í–¥,ì˜ìˆ™ì„±í–¥,ìˆœìì„±í–¥,í˜„ìˆ™ì„±í–¥,ì„œì—°ì„±í–¥,ë³´ëŒì„±í–¥,í•˜ë‹ˆì„±í–¥]) # matrix\n\ntensor([[ 1.8000,  1.8000],\n        [ 1.8000,  1.8000],\n        [ 1.8000, -1.8000],\n        [ 1.8000, -1.8000],\n        [-1.8000,  1.8000],\n        [-1.8000,  1.8000],\n        [-1.8000, -1.8000],\n        [-1.8000, -1.8000],\n        [ 1.8000,  0.0000]])\n\n\n\ntorch.stack([ì˜¥ìˆœì„±í–¥,ì˜ìì„±í–¥,ì •ìˆ™ì„±í–¥,ì˜ìˆ™ì„±í–¥,ìˆœìì„±í–¥,í˜„ìˆ™ì„±í–¥,ì„œì—°ì„±í–¥,ë³´ëŒì„±í–¥,í•˜ë‹ˆì„±í–¥]).shape\n\ntorch.Size([9, 2])\n\n\n\n9: ì¶œì—°ìì˜ ìˆ˜ / 2: ì„±í–¥ì˜ ìˆ˜\n\n\nW = torch.stack([ì˜¥ìˆœì„±í–¥,ì˜ìì„±í–¥,ì •ìˆ™ì„±í–¥,ì˜ìˆ™ì„±í–¥,ìˆœìì„±í–¥,í˜„ìˆ™ì„±í–¥,ì„œì—°ì„±í–¥,ë³´ëŒì„±í–¥,í•˜ë‹ˆì„±í–¥])\n\n\ní•˜ë‹ˆì˜ ë§¤ë ¥ ì ìˆ˜ë„ ë§Œë“¤ê³  ì‹¶ìŒ\n\n\ntorch.tensor([0,0,0,0,0,0,0,0,5])\n\ntensor([0, 0, 0, 0, 0, 0, 0, 0, 5])\n\n\n\ntorch.tensor([0,0,0,0,0,0,0,0,5]).reshape(-1,1)\n\ntensor([[0],\n        [0],\n        [0],\n        [0],\n        [0],\n        [0],\n        [0],\n        [0],\n        [5]])\n\n\n\nì˜¥ìˆœì„±í–¥  = torch.tensor([I,N])\nì˜ìì„±í–¥ = torch.tensor([I,N])\nì •ìˆ™ì„±í–¥ = torch.tensor([I,-N])\nì˜ìˆ™ì„±í–¥ = torch.tensor([I,-N])\nìˆœìì„±í–¥ = torch.tensor([-I,N])\ní˜„ìˆ™ì„±í–¥ = torch.tensor([-I,N])\nì„œì—°ì„±í–¥ = torch.tensor([-I,-N])\në³´ëŒì„±í–¥ = torch.tensor([-I,-N])\ní•˜ë‹ˆì„±í–¥ = torch.tensor([I,0])\nW = torch.stack([ì˜¥ìˆœì„±í–¥,ì˜ìì„±í–¥,ì •ìˆ™ì„±í–¥,ì˜ìˆ™ì„±í–¥,ìˆœìì„±í–¥,í˜„ìˆ™ì„±í–¥,ì„œì—°ì„±í–¥,ë³´ëŒì„±í–¥,í•˜ë‹ˆì„±í–¥])\nb1 = torch.tensor([0,0,0,0,0,0,0,0,5]).reshape(-1,1).float()\nW,b1\n\n(tensor([[ 1.8000,  1.8000],\n         [ 1.8000,  1.8000],\n         [ 1.8000, -1.8000],\n         [ 1.8000, -1.8000],\n         [-1.8000,  1.8000],\n         [-1.8000,  1.8000],\n         [-1.8000, -1.8000],\n         [-1.8000, -1.8000],\n         [ 1.8000,  0.0000]]),\n tensor([[0.],\n         [0.],\n         [0.],\n         [0.],\n         [0.],\n         [0.],\n         [0.],\n         [0.],\n         [5.]]))\n\n\n\nì˜ì‹ì„±í–¥  = torch.tensor([I,N])\nì˜ì² ì„±í–¥ = torch.tensor([I,N])\nì˜í˜¸ì„±í–¥ = torch.tensor([I,-N])\nê´‘ìˆ˜ì„±í–¥ = torch.tensor([I,-N])\nìƒì² ì„±í–¥ = torch.tensor([-I,N])\nì˜ìˆ˜ì„±í–¥ = torch.tensor([-I,N])\nê·œë¹ˆì„±í–¥ = torch.tensor([-I,-N])\në‹¤í˜¸ì„±í–¥ = torch.tensor([-I,-N])\nM = torch.stack([ì˜ì‹ì„±í–¥,ì˜ì² ì„±í–¥,ì˜í˜¸ì„±í–¥,ê´‘ìˆ˜ì„±í–¥,ìƒì² ì„±í–¥,ì˜ìˆ˜ì„±í–¥,ê·œë¹ˆì„±í–¥,ë‹¤í˜¸ì„±í–¥])\nb2 = torch.tensor([0,0,0,0,0,0,0,0]).reshape(-1,1).float()\n\n\nW.shape, M.shape, b1.shape, b2.shape\n\n(torch.Size([9, 2]),\n torch.Size([8, 2]),\n torch.Size([9, 1]),\n torch.Size([8, 1]))\n\n\n)ğŸ—£ï¸\n\nI, N = 1.8, 0.9\n\n\nì˜¥ìˆœì„±í–¥  = torch.tensor([I,N])\nì˜ìì„±í–¥ = torch.tensor([I,N])\nì •ìˆ™ì„±í–¥ = torch.tensor([I,-N])\nì˜ìˆ™ì„±í–¥ = torch.tensor([I,-N])\nìˆœìì„±í–¥ = torch.tensor([-I,N])\ní˜„ìˆ™ì„±í–¥ = torch.tensor([-I,N])\nì„œì—°ì„±í–¥ = torch.tensor([-I,-N])\në³´ëŒì„±í–¥ = torch.tensor([-I,-N])\ní•˜ë‹ˆì„±í–¥ = torch.tensor([I,0])\nW = torch.stack([ì˜¥ìˆœì„±í–¥,ì˜ìì„±í–¥,ì •ìˆ™ì„±í–¥,ì˜ìˆ™ì„±í–¥,ìˆœìì„±í–¥,í˜„ìˆ™ì„±í–¥,ì„œì—°ì„±í–¥,ë³´ëŒì„±í–¥,í•˜ë‹ˆì„±í–¥])\nb1 = torch.tensor([0,0,0,0,0,0,0,0,5]).reshape(-1,1).float()\nW,b1\n\n(tensor([[ 1.8000,  0.9000],\n         [ 1.8000,  0.9000],\n         [ 1.8000, -0.9000],\n         [ 1.8000, -0.9000],\n         [-1.8000,  0.9000],\n         [-1.8000,  0.9000],\n         [-1.8000, -0.9000],\n         [-1.8000, -0.9000],\n         [ 1.8000,  0.0000]]),\n tensor([[0.],\n         [0.],\n         [0.],\n         [0.],\n         [0.],\n         [0.],\n         [0.],\n         [0.],\n         [5.]]))\n\n\n\nì˜ì‹ì„±í–¥  = torch.tensor([I,N])\nì˜ì² ì„±í–¥ = torch.tensor([I,N])\nì˜í˜¸ì„±í–¥ = torch.tensor([I,-N])\nê´‘ìˆ˜ì„±í–¥ = torch.tensor([I,-N])\nìƒì² ì„±í–¥ = torch.tensor([-I,N])\nì˜ìˆ˜ì„±í–¥ = torch.tensor([-I,N])\nê·œë¹ˆì„±í–¥ = torch.tensor([-I,-N])\në‹¤í˜¸ì„±í–¥ = torch.tensor([-I,-N])\nM = torch.stack([ì˜ì‹ì„±í–¥,ì˜ì² ì„±í–¥,ì˜í˜¸ì„±í–¥,ê´‘ìˆ˜ì„±í–¥,ìƒì² ì„±í–¥,ì˜ìˆ˜ì„±í–¥,ê·œë¹ˆì„±í–¥,ë‹¤í˜¸ì„±í–¥])\nb2 = torch.tensor([0,0,0,0,0,0,0,0]).reshape(-1,1).float()\n\n\nW.shape, M.shape, b1.shape, b2.shape\n\n(torch.Size([9, 2]),\n torch.Size([8, 2]),\n torch.Size([9, 1]),\n torch.Size([8, 1]))\n\n\n- ì•„ë˜ì˜ í–‰ë ¬ê³± ê´€ì°°\nğŸ—£ï¸(\n\nW.shape, M.shape, b1.shape, b2.shape\n\n(torch.Size([9, 2]),\n torch.Size([8, 2]),\n torch.Size([9, 1]),\n torch.Size([8, 1]))\n\n\n\ndf_view\n\n\n\n\n\n\n\n\nì˜ì‹(IN)\nì˜ì² (IN)\nì˜í˜¸(IS)\nê´‘ìˆ˜(IS)\nìƒì² (EN)\nì˜ìˆ˜(EN)\nê·œë¹ˆ(ES)\në‹¤í˜¸(ES)\n\n\n\n\nì˜¥ìˆœ(IN)\nNaN\n4.02\n3.45\n3.42\n0.84\n1.12\n0.43\n0.49\n\n\nì˜ì(IN)\n3.93\n3.99\n3.63\n3.43\n0.98\n0.96\n0.52\nNaN\n\n\nì •ìˆ™(IS)\n3.52\n3.42\n4.05\n4.06\n0.39\nNaN\n0.93\n0.99\n\n\nì˜ìˆ™(IS)\n3.43\n3.57\nNaN\n3.95\n0.56\n0.52\n0.89\n0.89\n\n\nìˆœì(EN)\n1.12\nNaN\n0.59\n0.43\n4.01\n4.16\n3.52\n3.38\n\n\ní˜„ìˆ™(EN)\n0.94\n1.05\n0.32\n0.45\n4.02\n3.78\nNaN\n3.54\n\n\nì„œì—°(ES)\n0.51\n0.56\n0.88\n0.89\n3.50\n3.64\n4.04\n4.10\n\n\në³´ëŒ(ES)\n0.48\n0.51\n1.03\nNaN\n3.52\n4.00\n3.82\nNaN\n\n\ní•˜ë‹ˆ(I)\n4.85\n4.82\nNaN\n4.98\n4.53\n4.39\n4.45\n4.52\n\n\n\n\n\n\n\n\nb1 + b2.T # (9,1) + (1,8) ì•ˆë˜ì•¼í•˜ëŠ”ë° ë¨(ë¸Œë¡œë“œìºìŠ¤íŒ…)\n\ntensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0.],\n        [5., 5., 5., 5., 5., 5., 5., 5.]])\n\n\n[a        [ c d .... 0 ]       a+c  a+d  ... 0\n b                             b+c  b+d  ... 0 \n ...  +                   =    ...\n 5]                            5  5  5  ...  5  \n\nW@M.T\n\ntensor([[ 6.4800,  6.4800,  0.0000,  0.0000,  0.0000,  0.0000, -6.4800, -6.4800],\n        [ 6.4800,  6.4800,  0.0000,  0.0000,  0.0000,  0.0000, -6.4800, -6.4800],\n        [ 0.0000,  0.0000,  6.4800,  6.4800, -6.4800, -6.4800,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  6.4800,  6.4800, -6.4800, -6.4800,  0.0000,  0.0000],\n        [ 0.0000,  0.0000, -6.4800, -6.4800,  6.4800,  6.4800,  0.0000,  0.0000],\n        [ 0.0000,  0.0000, -6.4800, -6.4800,  6.4800,  6.4800,  0.0000,  0.0000],\n        [-6.4800, -6.4800,  0.0000,  0.0000,  0.0000,  0.0000,  6.4800,  6.4800],\n        [-6.4800, -6.4800,  0.0000,  0.0000,  0.0000,  0.0000,  6.4800,  6.4800],\n        [ 3.2400,  3.2400,  3.2400,  3.2400, -3.2400, -3.2400, -3.2400, -3.2400]])\n\n\n[ a b    [ e g ...          [ae+bf ag+bh ...\n  c d      f h ... ]       \n  ...\n  ... ]\n\nsig(W @ M.T + b1 @ b2.T) * 5\n\ntensor([[4.9923, 4.9923, 2.5000, 2.5000, 2.5000, 2.5000, 0.0077, 0.0077],\n        [4.9923, 4.9923, 2.5000, 2.5000, 2.5000, 2.5000, 0.0077, 0.0077],\n        [2.5000, 2.5000, 4.9923, 4.9923, 0.0077, 0.0077, 2.5000, 2.5000],\n        [2.5000, 2.5000, 4.9923, 4.9923, 0.0077, 0.0077, 2.5000, 2.5000],\n        [2.5000, 2.5000, 0.0077, 0.0077, 4.9923, 4.9923, 2.5000, 2.5000],\n        [2.5000, 2.5000, 0.0077, 0.0077, 4.9923, 4.9923, 2.5000, 2.5000],\n        [0.0077, 0.0077, 2.5000, 2.5000, 2.5000, 2.5000, 4.9923, 4.9923],\n        [0.0077, 0.0077, 2.5000, 2.5000, 2.5000, 2.5000, 4.9923, 4.9923],\n        [4.8116, 4.8116, 4.8116, 4.8116, 0.1884, 0.1884, 0.1884, 0.1884]])\n\n\n\ndf_view\n\n\n\n\n\n\n\n\nì˜ì‹(IN)\nì˜ì² (IN)\nì˜í˜¸(IS)\nê´‘ìˆ˜(IS)\nìƒì² (EN)\nì˜ìˆ˜(EN)\nê·œë¹ˆ(ES)\në‹¤í˜¸(ES)\n\n\n\n\nì˜¥ìˆœ(IN)\nNaN\n4.02\n3.45\n3.42\n0.84\n1.12\n0.43\n0.49\n\n\nì˜ì(IN)\n3.93\n3.99\n3.63\n3.43\n0.98\n0.96\n0.52\nNaN\n\n\nì •ìˆ™(IS)\n3.52\n3.42\n4.05\n4.06\n0.39\nNaN\n0.93\n0.99\n\n\nì˜ìˆ™(IS)\n3.43\n3.57\nNaN\n3.95\n0.56\n0.52\n0.89\n0.89\n\n\nìˆœì(EN)\n1.12\nNaN\n0.59\n0.43\n4.01\n4.16\n3.52\n3.38\n\n\ní˜„ìˆ™(EN)\n0.94\n1.05\n0.32\n0.45\n4.02\n3.78\nNaN\n3.54\n\n\nì„œì—°(ES)\n0.51\n0.56\n0.88\n0.89\n3.50\n3.64\n4.04\n4.10\n\n\në³´ëŒ(ES)\n0.48\n0.51\n1.03\nNaN\n3.52\n4.00\n3.82\nNaN\n\n\ní•˜ë‹ˆ(I)\n4.85\n4.82\nNaN\n4.98\n4.53\n4.39\n4.45\n4.52\n\n\n\n\n\n\n\n\nê²½í–¥ì€ ë§ëŠ” ê²ƒ ê°™ì€ë° ê°€ìš´ë° ë¶€ë¶„ì€ ì¡°ì •ì´ í•„ìš”í•´ ë³´ì„\n\n\nI, N = 1.8, 0.9\n\n\nì˜¥ìˆœì„±í–¥  = torch.tensor([I,N])\nì˜ìì„±í–¥ = torch.tensor([I,N])\nì •ìˆ™ì„±í–¥ = torch.tensor([I,-N])\nì˜ìˆ™ì„±í–¥ = torch.tensor([I,-N])\nìˆœìì„±í–¥ = torch.tensor([-I,N])\ní˜„ìˆ™ì„±í–¥ = torch.tensor([-I,N])\nì„œì—°ì„±í–¥ = torch.tensor([-I,-N])\në³´ëŒì„±í–¥ = torch.tensor([-I,-N])\ní•˜ë‹ˆì„±í–¥ = torch.tensor([I,0])\nW = torch.stack([ì˜¥ìˆœì„±í–¥,ì˜ìì„±í–¥,ì •ìˆ™ì„±í–¥,ì˜ìˆ™ì„±í–¥,ìˆœìì„±í–¥,í˜„ìˆ™ì„±í–¥,ì„œì—°ì„±í–¥,ë³´ëŒì„±í–¥,í•˜ë‹ˆì„±í–¥])\nb1 = torch.tensor([0,0,0,0,0,0,0,0,5]).reshape(-1,1).float()\nW,b1\n\n(tensor([[ 1.8000,  0.9000],\n         [ 1.8000,  0.9000],\n         [ 1.8000, -0.9000],\n         [ 1.8000, -0.9000],\n         [-1.8000,  0.9000],\n         [-1.8000,  0.9000],\n         [-1.8000, -0.9000],\n         [-1.8000, -0.9000],\n         [ 1.8000,  0.0000]]),\n tensor([[0.],\n         [0.],\n         [0.],\n         [0.],\n         [0.],\n         [0.],\n         [0.],\n         [0.],\n         [5.]]))\n\n\n\nì˜ì‹ì„±í–¥  = torch.tensor([I,N])\nì˜ì² ì„±í–¥ = torch.tensor([I,N])\nì˜í˜¸ì„±í–¥ = torch.tensor([I,-N])\nê´‘ìˆ˜ì„±í–¥ = torch.tensor([I,-N])\nìƒì² ì„±í–¥ = torch.tensor([-I,N])\nì˜ìˆ˜ì„±í–¥ = torch.tensor([-I,N])\nê·œë¹ˆì„±í–¥ = torch.tensor([-I,-N])\në‹¤í˜¸ì„±í–¥ = torch.tensor([-I,-N])\nM = torch.stack([ì˜ì‹ì„±í–¥,ì˜ì² ì„±í–¥,ì˜í˜¸ì„±í–¥,ê´‘ìˆ˜ì„±í–¥,ìƒì² ì„±í–¥,ì˜ìˆ˜ì„±í–¥,ê·œë¹ˆì„±í–¥,ë‹¤í˜¸ì„±í–¥])\nb2 = torch.tensor([0,0,0,0,0,0,0,0]).reshape(-1,1).float()\n\n\nsig(W @ M.T + b1 @ b2.T) * 5\n\ntensor([[4.9144, 4.9144, 4.5954, 4.5954, 0.4046, 0.4046, 0.0856, 0.0856],\n        [4.9144, 4.9144, 4.5954, 4.5954, 0.4046, 0.4046, 0.0856, 0.0856],\n        [4.5954, 4.5954, 4.9144, 4.9144, 0.0856, 0.0856, 0.4046, 0.4046],\n        [4.5954, 4.5954, 4.9144, 4.9144, 0.0856, 0.0856, 0.4046, 0.4046],\n        [0.4046, 0.4046, 0.0856, 0.0856, 4.9144, 4.9144, 4.5954, 4.5954],\n        [0.4046, 0.4046, 0.0856, 0.0856, 4.9144, 4.9144, 4.5954, 4.5954],\n        [0.0856, 0.0856, 0.4046, 0.4046, 4.5954, 4.5954, 4.9144, 4.9144],\n        [0.0856, 0.0856, 0.4046, 0.4046, 4.5954, 4.5954, 4.9144, 4.9144],\n        [4.8116, 4.8116, 4.8116, 4.8116, 0.1884, 0.1884, 0.1884, 0.1884]])\n\n\n\ndf_view\n\n\n\n\n\n\n\n\nì˜ì‹(IN)\nì˜ì² (IN)\nì˜í˜¸(IS)\nê´‘ìˆ˜(IS)\nìƒì² (EN)\nì˜ìˆ˜(EN)\nê·œë¹ˆ(ES)\në‹¤í˜¸(ES)\n\n\n\n\nì˜¥ìˆœ(IN)\nNaN\n4.02\n3.45\n3.42\n0.84\n1.12\n0.43\n0.49\n\n\nì˜ì(IN)\n3.93\n3.99\n3.63\n3.43\n0.98\n0.96\n0.52\nNaN\n\n\nì •ìˆ™(IS)\n3.52\n3.42\n4.05\n4.06\n0.39\nNaN\n0.93\n0.99\n\n\nì˜ìˆ™(IS)\n3.43\n3.57\nNaN\n3.95\n0.56\n0.52\n0.89\n0.89\n\n\nìˆœì(EN)\n1.12\nNaN\n0.59\n0.43\n4.01\n4.16\n3.52\n3.38\n\n\ní˜„ìˆ™(EN)\n0.94\n1.05\n0.32\n0.45\n4.02\n3.78\nNaN\n3.54\n\n\nì„œì—°(ES)\n0.51\n0.56\n0.88\n0.89\n3.50\n3.64\n4.04\n4.10\n\n\në³´ëŒ(ES)\n0.48\n0.51\n1.03\nNaN\n3.52\n4.00\n3.82\nNaN\n\n\ní•˜ë‹ˆ(I)\n4.85\n4.82\nNaN\n4.98\n4.53\n4.39\n4.45\n4.52\n\n\n\n\n\n\n\n\nì™„ì „íˆ ë§ì§€ëŠ” ì•Šì§€ë§Œ ì´ëŸ° ì‹ìœ¼ë¡œ í•˜ë©´ ë  ê²ƒ ê°™ìŒ\n\nì •í™•í•œ ìˆ˜ì¹˜ëŠ” optimizerê°€ ì°¾ê²Œ\n\n\n)ğŸ—£ï¸\n\nsig(W @ M.T + b1 + b2.T)*5\n\ntensor([[4.9144, 4.9144, 4.5954, 4.5954, 0.4046, 0.4046, 0.0856, 0.0856],\n        [4.9144, 4.9144, 4.5954, 4.5954, 0.4046, 0.4046, 0.0856, 0.0856],\n        [4.5954, 4.5954, 4.9144, 4.9144, 0.0856, 0.0856, 0.4046, 0.4046],\n        [4.5954, 4.5954, 4.9144, 4.9144, 0.0856, 0.0856, 0.4046, 0.4046],\n        [0.4046, 0.4046, 0.0856, 0.0856, 4.9144, 4.9144, 4.5954, 4.5954],\n        [0.4046, 0.4046, 0.0856, 0.0856, 4.9144, 4.9144, 4.5954, 4.5954],\n        [0.0856, 0.0856, 0.4046, 0.4046, 4.5954, 4.5954, 4.9144, 4.9144],\n        [0.0856, 0.0856, 0.4046, 0.4046, 4.5954, 4.5954, 4.9144, 4.9144],\n        [4.9987, 4.9987, 4.9987, 4.9987, 4.2660, 4.2660, 4.2660, 4.2660]])\n\n\n\ndf_view\n\n\n\n\n\n\n\n\nì˜ì‹(IN)\nì˜ì² (IN)\nì˜í˜¸(IS)\nê´‘ìˆ˜(IS)\nìƒì² (EN)\nì˜ìˆ˜(EN)\nê·œë¹ˆ(ES)\në‹¤í˜¸(ES)\n\n\n\n\nì˜¥ìˆœ(IN)\nNaN\n4.02\n3.45\n3.42\n0.84\n1.12\n0.43\n0.49\n\n\nì˜ì(IN)\n3.93\n3.99\n3.63\n3.43\n0.98\n0.96\n0.52\nNaN\n\n\nì •ìˆ™(IS)\n3.52\n3.42\n4.05\n4.06\n0.39\nNaN\n0.93\n0.99\n\n\nì˜ìˆ™(IS)\n3.43\n3.57\nNaN\n3.95\n0.56\n0.52\n0.89\n0.89\n\n\nìˆœì(EN)\n1.12\nNaN\n0.59\n0.43\n4.01\n4.16\n3.52\n3.38\n\n\ní˜„ìˆ™(EN)\n0.94\n1.05\n0.32\n0.45\n4.02\n3.78\nNaN\n3.54\n\n\nì„œì—°(ES)\n0.51\n0.56\n0.88\n0.89\n3.50\n3.64\n4.04\n4.10\n\n\në³´ëŒ(ES)\n0.48\n0.51\n1.03\nNaN\n3.52\n4.00\n3.82\nNaN\n\n\ní•˜ë‹ˆ(I)\n4.85\n4.82\nNaN\n4.98\n4.53\n4.39\n4.45\n4.52\n\n\n\n\n\n\n\n- ëª¨ë¸ë§\n\\[{\\tt df\\_view} \\approx sig\\left({\\bf W}@{\\bf M}^\\top + bias \\right) \\times 5\\]\n- ìë£Œë¥¼ ì•„ë˜ì™€ ê°™ì´ ì •ë¦¬í•œë‹¤ë©´?\nğŸ—£ï¸(\n\ndf_view.stack()\n\nì˜¥ìˆœ(IN)  ì˜ì² (IN)    4.02\n        ì˜í˜¸(IS)    3.45\n        ê´‘ìˆ˜(IS)    3.42\n        ìƒì² (EN)    0.84\n        ì˜ìˆ˜(EN)    1.12\n                  ... \ní•˜ë‹ˆ(I)   ê´‘ìˆ˜(IS)    4.98\n        ìƒì² (EN)    4.53\n        ì˜ìˆ˜(EN)    4.39\n        ê·œë¹ˆ(ES)    4.45\n        ë‹¤í˜¸(ES)    4.52\nLength: 63, dtype: float64\n\n\n\ndf_view.stack().reset_index()\n\n\n\n\n\n\n\n\nlevel_0\nlevel_1\n0\n\n\n\n\n0\nì˜¥ìˆœ(IN)\nì˜ì² (IN)\n4.02\n\n\n1\nì˜¥ìˆœ(IN)\nì˜í˜¸(IS)\n3.45\n\n\n2\nì˜¥ìˆœ(IN)\nê´‘ìˆ˜(IS)\n3.42\n\n\n3\nì˜¥ìˆœ(IN)\nìƒì² (EN)\n0.84\n\n\n4\nì˜¥ìˆœ(IN)\nì˜ìˆ˜(EN)\n1.12\n\n\n...\n...\n...\n...\n\n\n58\ní•˜ë‹ˆ(I)\nê´‘ìˆ˜(IS)\n4.98\n\n\n59\ní•˜ë‹ˆ(I)\nìƒì² (EN)\n4.53\n\n\n60\ní•˜ë‹ˆ(I)\nì˜ìˆ˜(EN)\n4.39\n\n\n61\ní•˜ë‹ˆ(I)\nê·œë¹ˆ(ES)\n4.45\n\n\n62\ní•˜ë‹ˆ(I)\në‹¤í˜¸(ES)\n4.52\n\n\n\n\n63 rows Ã— 3 columns\n\n\n\nìœ„ì˜ ë°ì´í„°ë¥¼ ê°€ì§€ê³ \n  ì˜¥ìˆœ     ì˜ì²   \na1 a2 a' b1 b2 b' =&gt; a1b1 + a2b2 + a'+b' = yhat ì˜ ë„¤íŠ¸ì›Œí¬ë¥¼ ì§œê³  ì‹¶ìŒ\n\ndf_train = df_view.stack().reset_index().set_axis(['ì—¬ì„±ì¶œì—°ì','ë‚¨ì„±ì¶œì—°ì','ê¶í•©ì ìˆ˜'],axis=1)\ndf_train\n\n\n\n\n\n\n\n\nì—¬ì„±ì¶œì—°ì\në‚¨ì„±ì¶œì—°ì\nê¶í•©ì ìˆ˜\n\n\n\n\n0\nì˜¥ìˆœ(IN)\nì˜ì² (IN)\n4.02\n\n\n1\nì˜¥ìˆœ(IN)\nì˜í˜¸(IS)\n3.45\n\n\n2\nì˜¥ìˆœ(IN)\nê´‘ìˆ˜(IS)\n3.42\n\n\n3\nì˜¥ìˆœ(IN)\nìƒì² (EN)\n0.84\n\n\n4\nì˜¥ìˆœ(IN)\nì˜ìˆ˜(EN)\n1.12\n\n\n...\n...\n...\n...\n\n\n58\ní•˜ë‹ˆ(I)\nê´‘ìˆ˜(IS)\n4.98\n\n\n59\ní•˜ë‹ˆ(I)\nìƒì² (EN)\n4.53\n\n\n60\ní•˜ë‹ˆ(I)\nì˜ìˆ˜(EN)\n4.39\n\n\n61\ní•˜ë‹ˆ(I)\nê·œë¹ˆ(ES)\n4.45\n\n\n62\ní•˜ë‹ˆ(I)\në‹¤í˜¸(ES)\n4.52\n\n\n\n\n63 rows Ã— 3 columns\n\n\n\n\nìˆ«ìí™”ë¥¼ ì‹œì¼œì•¼ í•¨\n\n\nset(df_train.ì—¬ì„±ì¶œì—°ì)\n\n{'ë³´ëŒ(ES)',\n 'ì„œì—°(ES)',\n 'ìˆœì(EN)',\n 'ì˜ìˆ™(IS)',\n 'ì˜ì(IN)',\n 'ì˜¥ìˆœ(IN)',\n 'ì •ìˆ™(IS)',\n 'í•˜ë‹ˆ(I)',\n 'í˜„ìˆ™(EN)'}\n\n\n\n{name: i for i,name in enumerate(set(df_train.ì—¬ì„±ì¶œì—°ì))}\n\n{'ì •ìˆ™(IS)': 0,\n 'ì˜ìˆ™(IS)': 1,\n 'í•˜ë‹ˆ(I)': 2,\n 'ì„œì—°(ES)': 3,\n 'ë³´ëŒ(ES)': 4,\n 'í˜„ìˆ™(EN)': 5,\n 'ìˆœì(EN)': 6,\n 'ì˜¥ìˆœ(IN)': 7,\n 'ì˜ì(IN)': 8}\n\n\n\ndf_train.ì—¬ì„±ì¶œì—°ì.map({name: i for i,name in enumerate(set(df_train.ì—¬ì„±ì¶œì—°ì))})\n\n0     7\n1     7\n2     7\n3     7\n4     7\n     ..\n58    2\n59    2\n60    2\n61    2\n62    2\nName: ì—¬ì„±ì¶œì—°ì, Length: 63, dtype: int64\n\n\n\ntorch.tensor(df_train.ì—¬ì„±ì¶œì—°ì.map({name: i for i,name in enumerate(set(df_train.ì—¬ì„±ì¶œì—°ì))})) # 63ê°œ\n\ntensor([7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n        1, 1, 1, 1, 6, 6, 6, 6, 6, 6, 6, 5, 5, 5, 5, 5, 5, 5, 3, 3, 3, 3, 3, 3,\n        3, 3, 4, 4, 4, 4, 4, 4, 2, 2, 2, 2, 2, 2, 2])\n\n\n\nëª¨í˜•ì— ëŒë¦¬ë ¤ë©´ one hot encoding í•˜ëŠ” ê²ƒì´ í¸í•¨\n\n\n# torch.nn.functional.one_hot(torch.tensor(df_train.ì—¬ì„±ì¶œì—°ì.map({name: i for i,name in enumerate(set(df_train.ì—¬ì„±ì¶œì—°ì))})))\n\ntensor([[0, 0, 0, 0, 0, 0, 0, 1, 0],\n        [0, 0, 0, 0, 0, 0, 0, 1, 0],\n\n9ì°¨ì›ì„ ì†ì„± ê°œìˆ˜ 2ê°œ(2ì°¨ì›), 1ê°œ(0ì°¨ì›)ë¡œ ë§ì¶”ë ¤ë©´ linear transform í•˜ë©´ ë  ê²ƒ ê°™ìŒ\n\nX1 = torch.nn.functional.one_hot(torch.tensor(df_train.ì—¬ì„±ì¶œì—°ì.map({name: i for i,name in enumerate(set(df_train.ì—¬ì„±ì¶œì—°ì))}))).float()\nX2 = torch.nn.functional.one_hot(torch.tensor(df_train.ë‚¨ì„±ì¶œì—°ì.map({name: i for i,name in enumerate(set(df_train.ë‚¨ì„±ì¶œì—°ì))}))).float()\n\n\nX1.shape, X2.shape\n\n(torch.Size([63, 9]), torch.Size([63, 8]))\n\n\n\n# 63,9 --&gt; 63,2\nl1 = torch.nn.Linear(9,2, bias=False) # biasëŠ” í•„ìš” ì—†ì„ ê²ƒ ê°™ìŒ\nl1(X1).shape\n\ntorch.Size([63, 2])\n\n\n\n# 63,9 --&gt; 63,2\nl1 = torch.nn.Linear(9,2, bias=False) # biasëŠ” í•„ìš” ì—†ì„ ê²ƒ ê°™ìŒ\nl1(X1).shape\n\ntorch.Size([63, 2])\n\n\n\nl1 = torch.nn.Linear(9,2, bias=False)\nb1 = torch.nn.Linear(9,1, bias=False) # ì ˆëŒ€ì ì¸ ë§¤ë ¥\nb1(X1).shape\n\ntorch.Size([63, 1])\n\n\n\në‚¨ì„±ê¹Œì§€ ë°˜ì˜\n\n\nl1 = torch.nn.Linear(9,2,bias=False)\nb1 = torch.nn.Linear(9,1,bias=False)\nl2 = torch.nn.Linear(8,2,bias=False)\nb2 = torch.nn.Linear(8,1,bias=False)\n\n\nl1(X1)[:3], l2(X2)[:3]\n\n(tensor([[ 0.2745, -0.1854],\n         [ 0.2745, -0.1854],\n         [ 0.2745, -0.1854]], grad_fn=&lt;SliceBackward0&gt;),\n tensor([[ 0.3284,  0.1675],\n         [-0.1163,  0.2470],\n         [-0.2877,  0.2392]], grad_fn=&lt;SliceBackward0&gt;))\n\n\n\nl1(X1) * l2(X2) # 63,2 (I/E, N/S)\n(l1(X1) * l2(X2)).sum(axis=1) # 63,1\n\ntensor([ 0.0591, -0.0777, -0.1233, -0.0217, -0.1010,  0.0349, -0.0662,  0.0569,\n         0.1020, -0.0365, -0.0898, -0.0431, -0.0752,  0.0590,  0.0728,  0.1109,\n        -0.0117, -0.0624, -0.0485,  0.0639, -0.0644,  0.0530,  0.0608,  0.0078,\n        -0.0285,  0.0046,  0.0347, -0.0204, -0.0223, -0.0714, -0.0934,  0.0005,\n        -0.0755,  0.0055, -0.0362,  0.0099,  0.0120,  0.0039, -0.0002, -0.0056,\n        -0.0005, -0.0047, -0.0469, -0.0373, -0.0532, -0.0510,  0.0196, -0.0401,\n        -0.0209, -0.0038,  0.0261, -0.0131,  0.0900,  0.0006,  0.0963, -0.0085,\n        -0.0034,  0.0039, -0.0213, -0.0008, -0.0173,  0.0024, -0.0090],\n       grad_fn=&lt;SumBackward1&gt;)\n\n\n\n# (l1(X1) * l2(X2)).sum(axis=1).reshape(-1,1) # vector\n\n\n# (l1(X1) * l2(X2)).sum(axis=1).reshape(-1,1) + b1(X1) + b2(X2) # ê°ê°ì˜ ë§¤ë ¥ ì ìˆ˜ë¥¼ ë”í•¨\n\nsig((l1(X1) * l2(X2)).sum(axis=1).reshape(-1,1) + b1(X1) + b2(X2))*5 # 0~5\n# ê¶í•© matrixë¡œ ì í•©í•  ìˆ˜ ìˆëŠ” ìµœì´ˆì˜ ì§ì„ \n\nyhat = sig((l1(X1) * l2(X2)).sum(axis=1).reshape(-1,1) + b1(X1) + b2(X2))*5\n\n)ğŸ—£ï¸\n\ndf_train = df_view.stack().reset_index().set_axis(['ì—¬ì„±ì¶œì—°ì','ë‚¨ì„±ì¶œì—°ì','ê¶í•©ì ìˆ˜'],axis=1)\ndf_train\n\n\n\n\n\n\n\n\nì—¬ì„±ì¶œì—°ì\në‚¨ì„±ì¶œì—°ì\nê¶í•©ì ìˆ˜\n\n\n\n\n0\nì˜¥ìˆœ(IN)\nì˜ì² (IN)\n4.02\n\n\n1\nì˜¥ìˆœ(IN)\nì˜í˜¸(IS)\n3.45\n\n\n2\nì˜¥ìˆœ(IN)\nê´‘ìˆ˜(IS)\n3.42\n\n\n3\nì˜¥ìˆœ(IN)\nìƒì² (EN)\n0.84\n\n\n4\nì˜¥ìˆœ(IN)\nì˜ìˆ˜(EN)\n1.12\n\n\n...\n...\n...\n...\n\n\n58\ní•˜ë‹ˆ(I)\nê´‘ìˆ˜(IS)\n4.98\n\n\n59\ní•˜ë‹ˆ(I)\nìƒì² (EN)\n4.53\n\n\n60\ní•˜ë‹ˆ(I)\nì˜ìˆ˜(EN)\n4.39\n\n\n61\ní•˜ë‹ˆ(I)\nê·œë¹ˆ(ES)\n4.45\n\n\n62\ní•˜ë‹ˆ(I)\në‹¤í˜¸(ES)\n4.52\n\n\n\n\n63 rows Ã— 3 columns\n\n\n\n\n{name: i for i,name in enumerate(set(df_train.ì—¬ì„±ì¶œì—°ì))}\n\n{'ì˜ìˆ™(IS)': 0,\n 'ì˜ì(IN)': 1,\n 'ì •ìˆ™(IS)': 2,\n 'í˜„ìˆ™(EN)': 3,\n 'í•˜ë‹ˆ(I)': 4,\n 'ìˆœì(EN)': 5,\n 'ë³´ëŒ(ES)': 6,\n 'ì˜¥ìˆœ(IN)': 7,\n 'ì„œì—°(ES)': 8}\n\n\n\nX1 = torch.nn.functional.one_hot(torch.tensor(df_train.ì—¬ì„±ì¶œì—°ì.map({name: i for i,name in enumerate(set(df_train.ì—¬ì„±ì¶œì—°ì))}))).float()\nX2 = torch.nn.functional.one_hot(torch.tensor(df_train.ë‚¨ì„±ì¶œì—°ì.map({name: i for i,name in enumerate(set(df_train.ë‚¨ì„±ì¶œì—°ì))}))).float()\n\n\nl1 = torch.nn.Linear(9,2,bias=False)\nb1 = torch.nn.Linear(9,1,bias=False)\nl2 = torch.nn.Linear(8,2,bias=False)\nb2 = torch.nn.Linear(8,1,bias=False)\n\n\nyhat = sig((l1(X1) * l2(X2)).sum(axis=1).reshape(-1,1) + b1(X1) + b2(X2))*5"
  },
  {
    "objectID": "posts/10wk-2.html#c.-í•™ìŠµ",
    "href": "posts/10wk-2.html#c.-í•™ìŠµ",
    "title": "10wk-2: (ì¶”ì²œì‹œìŠ¤í…œ) â€“ optimizer ì‚¬ìš© ê³ ê¸‰, ëª¨ë¸ë§ ì „ëµ, MF-based ì¶”ì²œì‹œìŠ¤í…œ",
    "section": "C. í•™ìŠµ",
    "text": "C. í•™ìŠµ\nğŸ—£ï¸(\n\n# torch.tensor(df_train.ê¶í•©ì ìˆ˜).reshape(-1,1)\n\nì°¸ê³ )\n        [4.9800],\n        [4.5300],\n        [4.3900],\n        [4.4500],\n        [4.5200]], dtype=torch.float64)\n.float()ì„ ë¶™ì´ë©´ , dtype=torch.float64 ê¼¬ë¦¬í‘œ ì‚¬ë¼ì§\n\ndf_train = df_view.stack().reset_index().set_axis(['ì—¬ì„±ì¶œì—°ì','ë‚¨ì„±ì¶œì—°ì','ê¶í•©ì ìˆ˜'],axis=1)\nì—¬ì„±ì¸ë±ìŠ¤ = {'ì˜¥ìˆœ(IN)':0, 'ì˜ì(IN)':1, 'ì •ìˆ™(IS)':2, 'ì˜ìˆ™(IS)':3, 'ìˆœì(EN)':4, 'í˜„ìˆ™(EN)':5, 'ì„œì—°(ES)':6, 'ë³´ëŒ(ES)':7, 'í•˜ë‹ˆ(I)':8}\në‚¨ì„±ì¸ë±ìŠ¤ = {'ì˜ì‹(IN)':0, 'ì˜ì² (IN)':1, 'ì˜í˜¸(IS)':2, 'ê´‘ìˆ˜(IS)':3, 'ìƒì² (EN)':4, 'ì˜ìˆ˜(EN)':5, 'ê·œë¹ˆ(ES)':6, 'ë‹¤í˜¸(ES)':7}\nX1 = torch.nn.functional.one_hot(torch.tensor(df_train.ì—¬ì„±ì¶œì—°ì.map(ì—¬ì„±ì¸ë±ìŠ¤))).float()\nX2 = torch.nn.functional.one_hot(torch.tensor(df_train.ë‚¨ì„±ì¶œì—°ì.map(ë‚¨ì„±ì¸ë±ìŠ¤))).float()\ny = torch.tensor(df_train.ê¶í•©ì ìˆ˜).reshape(-1,1).float()\n\n\nX1.shape, X2.shape, y.shape\n\n(torch.Size([63, 9]), torch.Size([63, 8]), torch.Size([63, 1]))\n\n\n\nyhatê³¼ yë¥¼ ë¹„ìŠ·í•˜ê²Œ í•˜ë ¤ë©´ loss í•„ìš”\n\nlossëŠ” MSEë¡œ\nfloatì´ ë‚˜ì˜¤ë¯€ë¡œ ì—”íŠ¸ë¡œí”¼ë¥¼ ì“¸ ìˆ˜ëŠ” ì—†ìŒ\n\nì°¸ê³ \n\nMSE LossëŠ” ë³´í†µ ì‹¤ìˆ˜ ì „ì—­ì„ ê°€ì •í•˜ì§€ë§Œ ì´ ê²½ìš°(0~5)ì—ë„ ê²°ê³¼ëŠ” ì¢‹ìŒ\nactivation functionì¸ sigmoidì— 5ë¥¼ ê³±í•´ë„ ê²°ê³¼ê°€ ì¢‹ìŒ\n\n\n\nloss_fn = torch.nn.MSELoss() \nl1 = torch.nn.Linear(9,2,bias=False)\nl2 = torch.nn.Linear(8,2,bias=False)\nW_features = l1(X1) \nM_features = l2(X2)\n\n\n(W_features * M_features).shape\n\ntorch.Size([63, 2])\n\n\n\n(W_features * M_features).sum(axis=1).shape # ê¶í•©ì´ ì˜ ë§ìœ¼ë©´ ê°’ì´ ì»¤ì§\n\ntorch.Size([63])\n\n\n\n(W_features * M_features).sum(axis=1).reshape(-1,1).shape # 63,1 ê¶í•©ì´ ì˜ ë§ìœ¼ë©´ ê°’ì´ ì»¤ì§\n\ntorch.Size([63, 1])\n\n\n\nb1 = torch.nn.Linear(9,1,bias=False) # ë§¤ë ¥ network\nb2 = torch.nn.Linear(8,1,bias=False)\nW_bias = b1(X1)\nM_bias = b2(X2)\n\n\n((W_features * M_features).sum(axis=1).reshape(-1,1) + W_bias + M_bias).shape # ê¶í•©ì´ ì˜ ë§ìœ¼ë©´ ê°’ì´ ì»¤ì§\n\ntorch.Size([63, 1])\n\n\n\n# (W_features * M_features).sum(axis=1).reshape(-1,1) + W_bias + M_bias # ê¶í•©ì´ ì˜ ë§ìœ¼ë©´ ê°’ì´ ì»¤ì§\n\n\nsig = torch.nn.Sigmoid()\n# sig((W_features * M_features).sum(axis=1).reshape(-1,1) + W_bias + M_bias)*5\n\n\nì •ë¦¬\n\ndf_train = df_view.stack().reset_index().set_axis(['ì—¬ì„±ì¶œì—°ì','ë‚¨ì„±ì¶œì—°ì','ê¶í•©ì ìˆ˜'],axis=1)\nì—¬ì„±ì¸ë±ìŠ¤ = {'ì˜¥ìˆœ(IN)':0, 'ì˜ì(IN)':1, 'ì •ìˆ™(IS)':2, 'ì˜ìˆ™(IS)':3, 'ìˆœì(EN)':4, 'í˜„ìˆ™(EN)':5, 'ì„œì—°(ES)':6, 'ë³´ëŒ(ES)':7, 'í•˜ë‹ˆ(I)':8}\në‚¨ì„±ì¸ë±ìŠ¤ = {'ì˜ì‹(IN)':0, 'ì˜ì² (IN)':1, 'ì˜í˜¸(IS)':2, 'ê´‘ìˆ˜(IS)':3, 'ìƒì² (EN)':4, 'ì˜ìˆ˜(EN)':5, 'ê·œë¹ˆ(ES)':6, 'ë‹¤í˜¸(ES)':7}\nX1 = torch.nn.functional.one_hot(torch.tensor(df_train.ì—¬ì„±ì¶œì—°ì.map(ì—¬ì„±ì¸ë±ìŠ¤))).float()\nX2 = torch.nn.functional.one_hot(torch.tensor(df_train.ë‚¨ì„±ì¶œì—°ì.map(ë‚¨ì„±ì¸ë±ìŠ¤))).float()\ny = torch.tensor(df_train.ê¶í•©ì ìˆ˜).reshape(-1,1).float()\nloss_fn = torch.nn.MSELoss() \nl1 = torch.nn.Linear(9,2,bias=False)\nl2 = torch.nn.Linear(8,2,bias=False)\nb1 = torch.nn.Linear(9,1,bias=False)\nb2 = torch.nn.Linear(8,1,bias=False)\nsig = torch.nn.Sigmoid()\nW_features = l1(X1) \nM_features = l2(X2)\nW_bias = b1(X1)\nM_bias = b2(X2)\nyhat = sig((W_features * M_features).sum(axis=1).reshape(-1,1) + W_bias + M_bias)*5\nloss = loss_fn(yhat,y)\nloss.backward()\n\nupdateë¥¼ í•´ì•¼í•˜ëŠ”ë° parameterê°€ l1,l2,b1,b2ì— ëª¨ë‘ ìˆìŒ =&gt; optimizerë¥¼ ì“°ê³  ì‹¶ìŒ\n\ndf_train = df_view.stack().reset_index().set_axis(['ì—¬ì„±ì¶œì—°ì','ë‚¨ì„±ì¶œì—°ì','ê¶í•©ì ìˆ˜'],axis=1)\nì—¬ì„±ì¸ë±ìŠ¤ = {'ì˜¥ìˆœ(IN)':0, 'ì˜ì(IN)':1, 'ì •ìˆ™(IS)':2, 'ì˜ìˆ™(IS)':3, 'ìˆœì(EN)':4, 'í˜„ìˆ™(EN)':5, 'ì„œì—°(ES)':6, 'ë³´ëŒ(ES)':7, 'í•˜ë‹ˆ(I)':8}\në‚¨ì„±ì¸ë±ìŠ¤ = {'ì˜ì‹(IN)':0, 'ì˜ì² (IN)':1, 'ì˜í˜¸(IS)':2, 'ê´‘ìˆ˜(IS)':3, 'ìƒì² (EN)':4, 'ì˜ìˆ˜(EN)':5, 'ê·œë¹ˆ(ES)':6, 'ë‹¤í˜¸(ES)':7}\nX1 = torch.nn.functional.one_hot(torch.tensor(df_train.ì—¬ì„±ì¶œì—°ì.map(ì—¬ì„±ì¸ë±ìŠ¤))).float()\nX2 = torch.nn.functional.one_hot(torch.tensor(df_train.ë‚¨ì„±ì¶œì—°ì.map(ë‚¨ì„±ì¸ë±ìŠ¤))).float()\ny = torch.tensor(df_train.ê¶í•©ì ìˆ˜).reshape(-1,1).float()\n#----#\nloss_fn = torch.nn.MSELoss() \nl1 = torch.nn.Linear(9,2,bias=False)\nl2 = torch.nn.Linear(8,2,bias=False)\nb1 = torch.nn.Linear(9,1,bias=False)\nb2 = torch.nn.Linear(8,1,bias=False)\nparams = list(l1.parameters()) + list(l2.parameters())  + list(b1.parameters()) + list(b2.parameters())\noptimizr = torch.optim.Adam(params) \nsig = torch.nn.Sigmoid()\n#----#\nW_features = l1(X1) \nM_features = l2(X2)\nW_bias = b1(X1)\nM_bias = b2(X2)\nyhat = sig((W_features * M_features).sum(axis=1).reshape(-1,1) + W_bias + M_bias)*5\nloss = loss_fn(yhat,y)\nloss.backward()\n\ní•™ìŠµ ì‹œì‘\n\n\ndf_train = df_view.stack().reset_index().set_axis(['ì—¬ì„±ì¶œì—°ì','ë‚¨ì„±ì¶œì—°ì','ê¶í•©ì ìˆ˜'],axis=1)\nì—¬ì„±ì¸ë±ìŠ¤ = {'ì˜¥ìˆœ(IN)':0, 'ì˜ì(IN)':1, 'ì •ìˆ™(IS)':2, 'ì˜ìˆ™(IS)':3, 'ìˆœì(EN)':4, 'í˜„ìˆ™(EN)':5, 'ì„œì—°(ES)':6, 'ë³´ëŒ(ES)':7, 'í•˜ë‹ˆ(I)':8}\në‚¨ì„±ì¸ë±ìŠ¤ = {'ì˜ì‹(IN)':0, 'ì˜ì² (IN)':1, 'ì˜í˜¸(IS)':2, 'ê´‘ìˆ˜(IS)':3, 'ìƒì² (EN)':4, 'ì˜ìˆ˜(EN)':5, 'ê·œë¹ˆ(ES)':6, 'ë‹¤í˜¸(ES)':7}\nX1 = torch.nn.functional.one_hot(torch.tensor(df_train.ì—¬ì„±ì¶œì—°ì.map(ì—¬ì„±ì¸ë±ìŠ¤))).float()\nX2 = torch.nn.functional.one_hot(torch.tensor(df_train.ë‚¨ì„±ì¶œì—°ì.map(ë‚¨ì„±ì¸ë±ìŠ¤))).float()\ny = torch.tensor(df_train.ê¶í•©ì ìˆ˜).reshape(-1,1).float()\n#----#\nloss_fn = torch.nn.MSELoss() \nl1 = torch.nn.Linear(9,2,bias=False)\nl2 = torch.nn.Linear(8,2,bias=False)\nb1 = torch.nn.Linear(9,1,bias=False)\nb2 = torch.nn.Linear(8,1,bias=False)\nparams = list(l1.parameters()) + list(l2.parameters())  + list(b1.parameters()) + list(b2.parameters())\noptimizr = torch.optim.Adam(params) \nsig = torch.nn.Sigmoid()\n#----#\nfor epoc in range(100):\n    #step1\n    W_features = l1(X1) \n    M_features = l2(X2)\n    W_bias = b1(X1)\n    M_bias = b2(X2)\n    yhat = sig((W_features * M_features).sum(axis=1).reshape(-1,1) + W_bias + M_bias)*5\n    #step2\n    loss = loss_fn(yhat,y)\n    #stpe3\n    loss.backward()\n    #step4\n    optimizr.step()\n    optimizr.zero_grad()\n\n\ny[:5], yhat[:5] # ë¹„ìŠ·í•´ì•¼í•˜ëŠ”ë° ì•ˆ ë¹„ìŠ·í•¨\n\n(tensor([[4.0200],\n         [3.4500],\n         [3.4200],\n         [0.8400],\n         [1.1200]]),\n tensor([[2.7421],\n         [2.5621],\n         [2.9185],\n         [2.5075],\n         [2.8614]], grad_fn=&lt;SliceBackward0&gt;))\n\n\n\nepochì„ 3,000 ìœ¼ë¡œ\n\n\ndf_train = df_view.stack().reset_index().set_axis(['ì—¬ì„±ì¶œì—°ì','ë‚¨ì„±ì¶œì—°ì','ê¶í•©ì ìˆ˜'],axis=1)\nì—¬ì„±ì¸ë±ìŠ¤ = {'ì˜¥ìˆœ(IN)':0, 'ì˜ì(IN)':1, 'ì •ìˆ™(IS)':2, 'ì˜ìˆ™(IS)':3, 'ìˆœì(EN)':4, 'í˜„ìˆ™(EN)':5, 'ì„œì—°(ES)':6, 'ë³´ëŒ(ES)':7, 'í•˜ë‹ˆ(I)':8}\në‚¨ì„±ì¸ë±ìŠ¤ = {'ì˜ì‹(IN)':0, 'ì˜ì² (IN)':1, 'ì˜í˜¸(IS)':2, 'ê´‘ìˆ˜(IS)':3, 'ìƒì² (EN)':4, 'ì˜ìˆ˜(EN)':5, 'ê·œë¹ˆ(ES)':6, 'ë‹¤í˜¸(ES)':7}\nX1 = torch.nn.functional.one_hot(torch.tensor(df_train.ì—¬ì„±ì¶œì—°ì.map(ì—¬ì„±ì¸ë±ìŠ¤))).float()\nX2 = torch.nn.functional.one_hot(torch.tensor(df_train.ë‚¨ì„±ì¶œì—°ì.map(ë‚¨ì„±ì¸ë±ìŠ¤))).float()\ny = torch.tensor(df_train.ê¶í•©ì ìˆ˜).reshape(-1,1).float()\n#----#\nloss_fn = torch.nn.MSELoss() \nl1 = torch.nn.Linear(9,2,bias=False)\nl2 = torch.nn.Linear(8,2,bias=False)\nb1 = torch.nn.Linear(9,1,bias=False)\nb2 = torch.nn.Linear(8,1,bias=False)\nparams = list(l1.parameters()) + list(l2.parameters())  + list(b1.parameters()) + list(b2.parameters())\noptimizr = torch.optim.Adam(params) \nsig = torch.nn.Sigmoid()\n#----#\nfor epoc in range(3000):\n    #step1\n    W_features = l1(X1) \n    M_features = l2(X2)\n    W_bias = b1(X1)\n    M_bias = b2(X2)\n    yhat = sig((W_features * M_features).sum(axis=1).reshape(-1,1) + W_bias + M_bias)*5\n    #step2\n    loss = loss_fn(yhat,y)\n    #stpe3\n    loss.backward()\n    #step4\n    optimizr.step()\n    optimizr.zero_grad()\n\n\ny[:5], yhat[:5]\n\n(tensor([[4.0200],\n         [3.4500],\n         [3.4200],\n         [0.8400],\n         [1.1200]]),\n tensor([[3.6914],\n         [3.5355],\n         [3.6526],\n         [0.6727],\n         [0.8341]], grad_fn=&lt;SliceBackward0&gt;))\n\n\n\nëœ ê²ƒ ê°™ìŒ, 5,000ìœ¼ë¡œ\n\n\ndf_train = df_view.stack().reset_index().set_axis(['ì—¬ì„±ì¶œì—°ì','ë‚¨ì„±ì¶œì—°ì','ê¶í•©ì ìˆ˜'],axis=1)\nì—¬ì„±ì¸ë±ìŠ¤ = {'ì˜¥ìˆœ(IN)':0, 'ì˜ì(IN)':1, 'ì •ìˆ™(IS)':2, 'ì˜ìˆ™(IS)':3, 'ìˆœì(EN)':4, 'í˜„ìˆ™(EN)':5, 'ì„œì—°(ES)':6, 'ë³´ëŒ(ES)':7, 'í•˜ë‹ˆ(I)':8}\në‚¨ì„±ì¸ë±ìŠ¤ = {'ì˜ì‹(IN)':0, 'ì˜ì² (IN)':1, 'ì˜í˜¸(IS)':2, 'ê´‘ìˆ˜(IS)':3, 'ìƒì² (EN)':4, 'ì˜ìˆ˜(EN)':5, 'ê·œë¹ˆ(ES)':6, 'ë‹¤í˜¸(ES)':7}\nX1 = torch.nn.functional.one_hot(torch.tensor(df_train.ì—¬ì„±ì¶œì—°ì.map(ì—¬ì„±ì¸ë±ìŠ¤))).float()\nX2 = torch.nn.functional.one_hot(torch.tensor(df_train.ë‚¨ì„±ì¶œì—°ì.map(ë‚¨ì„±ì¸ë±ìŠ¤))).float()\ny = torch.tensor(df_train.ê¶í•©ì ìˆ˜).reshape(-1,1).float()\n#----#\nloss_fn = torch.nn.MSELoss() \nl1 = torch.nn.Linear(9,2,bias=False)\nl2 = torch.nn.Linear(8,2,bias=False)\nb1 = torch.nn.Linear(9,1,bias=False)\nb2 = torch.nn.Linear(8,1,bias=False)\nparams = list(l1.parameters()) + list(l2.parameters())  + list(b1.parameters()) + list(b2.parameters())\noptimizr = torch.optim.Adam(params) \nsig = torch.nn.Sigmoid()\n#----#\nfor epoc in range(5000):\n    #step1\n    W_features = l1(X1) \n    M_features = l2(X2)\n    W_bias = b1(X1)\n    M_bias = b2(X2)\n    yhat = sig((W_features * M_features).sum(axis=1).reshape(-1,1) + W_bias + M_bias)*5\n    #step2\n    loss = loss_fn(yhat,y)\n    #stpe3\n    loss.backward()\n    #step4\n    optimizr.step()\n    optimizr.zero_grad()\n\n\ny[:5], yhat[:5] # ë¹„ìŠ·í•¨\n\n(tensor([[4.0200],\n         [3.4500],\n         [3.4200],\n         [0.8400],\n         [1.1200]]),\n tensor([[4.0586],\n         [3.4643],\n         [3.4175],\n         [0.8539],\n         [0.9135]], grad_fn=&lt;SliceBackward0&gt;))\n\n\n)ğŸ—£ï¸\n\n#df_view\ndf_train = df_view.stack().reset_index().set_axis(['ì—¬ì„±ì¶œì—°ì','ë‚¨ì„±ì¶œì—°ì','ê¶í•©ì ìˆ˜'],axis=1)\nì—¬ì„±ì¸ë±ìŠ¤ = {'ì˜¥ìˆœ(IN)':0, 'ì˜ì(IN)':1, 'ì •ìˆ™(IS)':2, 'ì˜ìˆ™(IS)':3, 'ìˆœì(EN)':4, 'í˜„ìˆ™(EN)':5, 'ì„œì—°(ES)':6, 'ë³´ëŒ(ES)':7, 'í•˜ë‹ˆ(I)':8}\në‚¨ì„±ì¸ë±ìŠ¤ = {'ì˜ì‹(IN)':0, 'ì˜ì² (IN)':1, 'ì˜í˜¸(IS)':2, 'ê´‘ìˆ˜(IS)':3, 'ìƒì² (EN)':4, 'ì˜ìˆ˜(EN)':5, 'ê·œë¹ˆ(ES)':6, 'ë‹¤í˜¸(ES)':7}\nX1 = torch.nn.functional.one_hot(torch.tensor(df_train.ì—¬ì„±ì¶œì—°ì.map(ì—¬ì„±ì¸ë±ìŠ¤))).float()\nX2 = torch.nn.functional.one_hot(torch.tensor(df_train.ë‚¨ì„±ì¶œì—°ì.map(ë‚¨ì„±ì¸ë±ìŠ¤))).float()\ny = torch.tensor(df_train.ê¶í•©ì ìˆ˜).reshape(-1,1).float()\n#----#\nloss_fn = torch.nn.MSELoss() \nl1 = torch.nn.Linear(9,2,bias=False)\nl2 = torch.nn.Linear(8,2,bias=False)\nb1 = torch.nn.Linear(9,1,bias=False)\nb2 = torch.nn.Linear(8,1,bias=False)\nparams = list(l1.parameters()) + list(l2.parameters())  + list(b1.parameters()) + list(b2.parameters())\noptimizr = torch.optim.Adam(params)\nsig = torch.nn.Sigmoid()\n#----#\nfor epoc in range(5000):\n    #step1\n    W_features = l1(X1) \n    M_features = l2(X2) \n    W_bias = b1(X1)\n    M_bais = b2(X2)\n    yhat = sig((W_features * M_features).sum(axis=1).reshape(-1,1) + W_bias + M_bais)*5\n    #step2\n    loss = loss_fn(yhat,y)\n    #step3\n    loss.backward()\n    #step4\n    optimizr.step()\n    optimizr.zero_grad()\n\n\ny[:5], yhat[:5]\n\n(tensor([[4.0200],\n         [3.4500],\n         [3.4200],\n         [0.8400],\n         [1.1200]]),\n tensor([[4.0370],\n         [3.4701],\n         [3.4334],\n         [0.8374],\n         [0.9061]], grad_fn=&lt;SliceBackward0&gt;))"
  },
  {
    "objectID": "posts/10wk-2.html#d.-ì˜ˆì¸¡",
    "href": "posts/10wk-2.html#d.-ì˜ˆì¸¡",
    "title": "10wk-2: (ì¶”ì²œì‹œìŠ¤í…œ) â€“ optimizer ì‚¬ìš© ê³ ê¸‰, ëª¨ë¸ë§ ì „ëµ, MF-based ì¶”ì²œì‹œìŠ¤í…œ",
    "section": "D. ì˜ˆì¸¡",
    "text": "D. ì˜ˆì¸¡\nğŸ—£ï¸(\n\ndf_test = pd.DataFrame({'ì—¬ì„±ì¶œì—°ì':['ì˜¥ìˆœ(IN)','í•˜ë‹ˆ(I)'],'ë‚¨ì„±ì¶œì—°ì':['ì˜ì‹(IN)','ì˜í˜¸(IS)']})\ndf_test\n\n\n\n\n\n\n\n\nì—¬ì„±ì¶œì—°ì\në‚¨ì„±ì¶œì—°ì\n\n\n\n\n0\nì˜¥ìˆœ(IN)\nì˜ì‹(IN)\n\n\n1\ní•˜ë‹ˆ(I)\nì˜í˜¸(IS)\n\n\n\n\n\n\n\n\ndf_train[:2] # ê¶í•© ì ìˆ˜ê°€ ê¶ê¸ˆí•¨\n\n\n\n\n\n\n\n\nì—¬ì„±ì¶œì—°ì\në‚¨ì„±ì¶œì—°ì\nê¶í•©ì ìˆ˜\n\n\n\n\n0\nì˜¥ìˆœ(IN)\nì˜ì² (IN)\n4.02\n\n\n1\nì˜¥ìˆœ(IN)\nì˜í˜¸(IS)\n3.45\n\n\n\n\n\n\n\n\ndf_test.ì—¬ì„±ì¶œì—°ì.map(ì—¬ì„±ì¸ë±ìŠ¤)\n\n0    0\n1    8\nName: ì—¬ì„±ì¶œì—°ì, dtype: int64\n\n\n\ntorch.tensor(df_test.ì—¬ì„±ì¶œì—°ì.map(ì—¬ì„±ì¸ë±ìŠ¤))\n\ntensor([0, 8])\n\n\n\ntorch.nn.functional.one_hot(torch.tensor(df_test.ì—¬ì„±ì¶œì—°ì.map(ì—¬ì„±ì¸ë±ìŠ¤)))\n\ntensor([[1, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 1]])\n\n\n\nXX1 = torch.nn.functional.one_hot(torch.tensor(df_test.ì—¬ì„±ì¶œì—°ì.map(ì—¬ì„±ì¸ë±ìŠ¤)))\n\n\nXX1.shape, X1.shape\n\n(torch.Size([2, 9]), torch.Size([63, 9]))\n\n\n\nXX2 = torch.nn.functional.one_hot(torch.tensor(df_test.ë‚¨ì„±ì¶œì—°ì.map(ë‚¨ì„±ì¸ë±ìŠ¤)))\n\n\nXX2.shape # ì°¨ì›ì´ ì´ìƒí•¨\n\ntorch.Size([2, 3])\n\n\n\nXX2\n\ntensor([[1, 0, 0],\n        [0, 0, 1]])\n\n\n\ndf_test.ë‚¨ì„±ì¶œì—°ì\n\n0    ì˜ì‹(IN)\n1    ì˜í˜¸(IS)\nName: ë‚¨ì„±ì¶œì—°ì, dtype: object\n\n\n\në‚¨ì„±ì¸ë±ìŠ¤\n\n{'ì˜ì‹(IN)': 0,\n 'ì˜ì² (IN)': 1,\n 'ì˜í˜¸(IS)': 2,\n 'ê´‘ìˆ˜(IS)': 3,\n 'ìƒì² (EN)': 4,\n 'ì˜ìˆ˜(EN)': 5,\n 'ê·œë¹ˆ(ES)': 6,\n 'ë‹¤í˜¸(ES)': 7}\n\n\n\ntorch.tensor(df_test.ë‚¨ì„±ì¶œì—°ì.map(ë‚¨ì„±ì¸ë±ìŠ¤))\n\ntensor([0, 2])\n\n\n\ntorch.nn.functional.one_hot(torch.tensor(df_test.ë‚¨ì„±ì¶œì—°ì.map(ë‚¨ì„±ì¸ë±ìŠ¤)))\n\ntensor([[1, 0, 0],\n        [0, 0, 1]])\n\n\n\n0,1,2 ì´ 3ê°œì˜ classë§Œ ìˆë‹¤ê³  íŒë‹¨í•œ ê²ƒ ê°™ìŒ\n\n\nXX1 = torch.nn.functional.one_hot(torch.tensor(df_test.ì—¬ì„±ì¶œì—°ì.map(ì—¬ì„±ì¸ë±ìŠ¤)),num_classes=9).float() # ìš´ ì¢‹ê²Œ í•˜ë‹ˆê°€ ë§ˆì§€ë§‰ ì°¸ê°€ìë¼ ì•„ê¹ŒëŠ” ì´ìƒ ì—†ì—ˆìŒ\nXX2 = torch.nn.functional.one_hot(torch.tensor(df_test.ë‚¨ì„±ì¶œì—°ì.map(ë‚¨ì„±ì¸ë±ìŠ¤)),num_classes=8).float()\n\n\nl1(XX1) # í•™ìŠµëœ parameter\n\ntensor([[-0.1969, -1.1841],\n        [ 0.7690, -1.0187]], grad_fn=&lt;MmBackward0&gt;)\n\n\n\ndf_test\n\n\n\n\n\n\n\n\nì—¬ì„±ì¶œì—°ì\në‚¨ì„±ì¶œì—°ì\n\n\n\n\n0\nì˜¥ìˆœ(IN)\nì˜ì‹(IN)\n\n\n1\ní•˜ë‹ˆ(I)\nì˜í˜¸(IS)\n\n\n\n\n\n\n\n\n(l1(XX1) * l2(XX2)).sum(axis=1).reshape(2,1)\n\ntensor([[1.9070],\n        [0.4191]], grad_fn=&lt;ViewBackward0&gt;)\n\n\n\ní•˜ë‹ˆì™€ ì˜í˜¸ëŠ” Ië§Œ ë§ì•„ì„œ ê¶í•©ì´ ì˜¤íˆë ¤ ì•ˆ ì¢‹ê²Œ ë‚˜ì˜´\n\n\nb1(XX1) # ì—¬ì„± ì°¸ê°€ìì˜ ë§¤ë ¥ (í•˜ë‹ˆëŠ” ì ìˆ˜ê°€ ë†’ìŒ)\n\ntensor([[-1.3766],\n        [ 1.2688]], grad_fn=&lt;MmBackward0&gt;)\n\n\n\nb2(XX2) # ë‚¨ì„± ì°¸ê°€ìì˜ ë§¤ë ¥\n\ntensor([[0.8753],\n        [0.9267]], grad_fn=&lt;MmBackward0&gt;)\n\n\n\nìµœì¢… ì ìˆ˜ëŠ”\n\n\nsig((l1(XX1) * l2(XX2)).sum(axis=1).reshape(2,1) + b1(XX1) + b2(XX2))*5 # ê·¸ëŸ´ë“¯ í•¨\n\ntensor([[4.0154],\n        [4.6590]], grad_fn=&lt;MulBackward0&gt;)\n\n\n)ğŸ—£ï¸\n\ndf_view\n\n\n\n\n\n\n\n\nì˜ì‹(IN)\nì˜ì² (IN)\nì˜í˜¸(IS)\nê´‘ìˆ˜(IS)\nìƒì² (EN)\nì˜ìˆ˜(EN)\nê·œë¹ˆ(ES)\në‹¤í˜¸(ES)\n\n\n\n\nì˜¥ìˆœ(IN)\nNaN\n4.02\n3.45\n3.42\n0.84\n1.12\n0.43\n0.49\n\n\nì˜ì(IN)\n3.93\n3.99\n3.63\n3.43\n0.98\n0.96\n0.52\nNaN\n\n\nì •ìˆ™(IS)\n3.52\n3.42\n4.05\n4.06\n0.39\nNaN\n0.93\n0.99\n\n\nì˜ìˆ™(IS)\n3.43\n3.57\nNaN\n3.95\n0.56\n0.52\n0.89\n0.89\n\n\nìˆœì(EN)\n1.12\nNaN\n0.59\n0.43\n4.01\n4.16\n3.52\n3.38\n\n\ní˜„ìˆ™(EN)\n0.94\n1.05\n0.32\n0.45\n4.02\n3.78\nNaN\n3.54\n\n\nì„œì—°(ES)\n0.51\n0.56\n0.88\n0.89\n3.50\n3.64\n4.04\n4.10\n\n\në³´ëŒ(ES)\n0.48\n0.51\n1.03\nNaN\n3.52\n4.00\n3.82\nNaN\n\n\ní•˜ë‹ˆ(I)\n4.85\n4.82\nNaN\n4.98\n4.53\n4.39\n4.45\n4.52\n\n\n\n\n\n\n\n\ndf_train[:2]\n\n\n\n\n\n\n\n\nì—¬ì„±ì¶œì—°ì\në‚¨ì„±ì¶œì—°ì\nê¶í•©ì ìˆ˜\n\n\n\n\n0\nì˜¥ìˆœ(IN)\nì˜ì² (IN)\n4.02\n\n\n1\nì˜¥ìˆœ(IN)\nì˜í˜¸(IS)\n3.45\n\n\n\n\n\n\n\nì í•©ëœ ë„¤íŠ¸ì›Œí¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì•„ë˜ì˜ ê°’ì— ëŒ€í•œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ë¼.\n\ndf_test = pd.DataFrame({'ì—¬ì„±ì¶œì—°ì':['ì˜¥ìˆœ(IN)','í•˜ë‹ˆ(I)'],'ë‚¨ì„±ì¶œì—°ì':['ì˜ì‹(IN)','ì˜í˜¸(IS)']})\ndf_test\n\n\n\n\n\n\n\n\nì—¬ì„±ì¶œì—°ì\në‚¨ì„±ì¶œì—°ì\n\n\n\n\n0\nì˜¥ìˆœ(IN)\nì˜ì‹(IN)\n\n\n1\ní•˜ë‹ˆ(I)\nì˜í˜¸(IS)\n\n\n\n\n\n\n\n\nXX1 = torch.nn.functional.one_hot(torch.tensor(df_test.ì—¬ì„±ì¶œì—°ì.map(ì—¬ì„±ì¸ë±ìŠ¤)),num_classes=9).float()\nXX2 = torch.nn.functional.one_hot(torch.tensor(df_test.ë‚¨ì„±ì¶œì—°ì.map(ë‚¨ì„±ì¸ë±ìŠ¤)),num_classes=8).float()\n\n\nsig((l1(XX1) * l2(XX2)).sum(axis=1).reshape(2,1) + b1(XX1) + b2(XX2))*5\n\ntensor([[3.9640],\n        [4.6568]], grad_fn=&lt;MulBackward0&gt;)"
  },
  {
    "objectID": "posts/14wk-1.html",
    "href": "posts/14wk-1.html",
    "title": "14wk-1: (ê°•í™”í•™ìŠµ) â€“ 4x4 Grid World í™˜ê²½ì˜ ì´í•´",
    "section": "",
    "text": "ğŸ“˜ Note Format Guide\nThis format serves as a structured guide for organizing lecture content, personal interpretation, experiments, and study-related questions.\nğŸ“ ğŸ—£ï¸ âœï¸ ğŸ”¬ â“"
  },
  {
    "objectID": "posts/14wk-1.html#a.-ë°ì´í„°-ì¶•ì ",
    "href": "posts/14wk-1.html#a.-ë°ì´í„°-ì¶•ì ",
    "title": "14wk-1: (ê°•í™”í•™ìŠµ) â€“ 4x4 Grid World í™˜ê²½ì˜ ì´í•´",
    "section": "A. ë°ì´í„° ì¶•ì ",
    "text": "A. ë°ì´í„° ì¶•ì \n- ëœë¤ì—ì´ì „íŠ¸ë¥¼ ì´ìš©í•´ ë¬´ì‘ìœ„ë¡œ 100,000 ì—í”¼ì†Œë“œë¥¼ ì§„í–‰í•´ë³´ì.\n\nplayer = RandomAgent()\nenv = GridWorld()\nscores = [] \nscore = 0 \n#\nfor e in range(1,100000):\n    #---ì—í”¼ì†Œë“œì‹œì‘---#\n    while True:\n        # step1 -- ì•¡ì…˜ì„ íƒ\n        player.act()\n        # step2 -- í™˜ê²½ë°˜ì‘ \n        player.next_state, player.reward, player.terminated = env.step(player.action)\n        # step3 -- ê²½í—˜ê¸°ë¡ & í•™ìŠµ \n        player.save_experience()\n        player.learn()\n        # step4 --ì¢…ë£Œ ì¡°ê±´ ì²´í¬ & í›„ì† ì²˜ë¦¬\n        if env.terminated:\n            score = score + player.reward\n            scores.append(score)\n            score = 0 \n            player.state = env.reset() \n            break\n        else: \n            score = score + player.reward            \n            player.state = player.next_state\n\n\n\n\n\n\n\nImportant\n\n\n\nê°•ì˜ë…¸íŠ¸ ìˆ˜ì • 2025-06-12\në…¸ê·œí˜¸í•™ìƒì˜ ë„ì›€ìœ¼ë¡œ ì˜ˆì „ê°•ì˜ì˜ ì˜¤ë¥˜ë¥¼ ë°œê²¬í•˜ì—¬ ìˆ˜ì •í•˜ì˜€ìŠµë‹ˆë‹¤.\n# ìˆ˜ì •ì „\n...\n        if env.terminated:\n            ...\n        else: \n            score = score + player.reward\n            scores.append(score)            \n            player.state = player.next_state\n            \n# ìˆ˜ì •í›„\n        if env.terminated:\n            ...\n        else: \n            score = score + player.reward\n#            scores.append(score)            ### &lt;--- ì—¬ê¸°ë¥¼ ì£¼ì„ì²˜ë¦¬í•´ì•¼í•¨!! \n            player.state = player.next_state\n:::\n\n::: {#ef29fdf9-b027-4d88-8bd7-1e5c6d3b08d9 .cell tags='[]' execution_count=5}\n``` {.python .cell-code}\nplayer.n_experience\n\n326581\n\n\n\n\nğŸ—£ï¸\n\ní•œ ì—í”¼ì†Œë“œ ë‹¹ í‰ê·  3ë²ˆ ì •ë„"
  },
  {
    "objectID": "posts/14wk-1.html#b.-ì²«ë²ˆì§¸-q_table",
    "href": "posts/14wk-1.html#b.-ì²«ë²ˆì§¸-q_table",
    "title": "14wk-1: (ê°•í™”í•™ìŠµ) â€“ 4x4 Grid World í™˜ê²½ì˜ ì´í•´",
    "section": "B. ì²«ë²ˆì§¸ q_table",
    "text": "B. ì²«ë²ˆì§¸ q_table\n- ë°´ë”§ê²Œì„ì—ì„œëŠ” \\(q(a)\\) ë¥¼ ì •ì˜í–ˆì—ˆìŒ.\n\n\\(q(0) = 1\\)\n\\(q(1) = 10\\)\n\n- ì—¬ê¸°ì—ì„œëŠ” \\(q(s_1,s_2,a)\\)ë¥¼ ì •ì˜í•´ì•¼í•¨!\n\n\n\n\n\n\nNote\n\n\n\nì§ê´€ì ìœ¼ë¡œ ì•„ë˜ì˜ ê·¸ë¦¼ì´ ë– ì˜¤ë¦„ \nê·¸ë¦¼ì— ëŒ€ì‘í•˜ëŠ” \\(q(s_1,s_2,a)\\)ì˜ ê°’ì€ ì•„ë˜ì™€ ê°™ìŒ\n\n\\(a=0\\)\\(a=1\\)\\(a=2\\)\\(a=3\\)\n\n\n\\(a=0 \\Leftrightarrow \\text{\\tt action=right}\\)\n\\[ \\begin{bmatrix}\nq(0,0,0) & q(0,1,0) & q(0,2,0) & q(0,3,0) \\\\\nq(1,0,0) & q(1,1,0) & q(1,2,0) & q(1,3,0) \\\\\nq(2,0,0) & q(2,1,0) & q(2,2,0) & q(2,3,0) \\\\\nq(3,0,0) & q(3,1,0) &q(3,2,0) & q(3,3,0) \\\\\n\\end{bmatrix} =  \\begin{bmatrix}\n-1 & -1 & -1 & -10 \\\\\n-1 & -1 & -1 & -10 \\\\\n-1 & -1 & -1 & -10 \\\\\n-1 & -1 & 100 &  \\text{-} \\\\\n\\end{bmatrix}\n\\]\n\n\n\\(a=1 \\Leftrightarrow \\text{\\tt action=left}\\)\n\\[ \\begin{bmatrix}\nq(0,0,1) & q(0,1,1) & q(0,2,1) & q(0,3,1) \\\\\nq(1,0,1) & q(1,1,1) & q(1,2,1) & q(1,3,1) \\\\\nq(2,0,1) & q(2,1,1) & q(2,2,1) & q(2,3,1) \\\\\nq(3,0,1) & q(3,1,1) &q(3,2,1) & q(3,3,1) \\\\\n\\end{bmatrix} = \\begin{bmatrix}\n-10 & -1 & -1 & -1 \\\\\n-10& -1 & -1 & -1 \\\\\n-10 & -1 & -1 & -1 \\\\\n-10 & -1 & -1 &  \\text{-} \\\\\n\\end{bmatrix}\n\\]\n\n\n\\(a=2 \\Leftrightarrow \\text{\\tt action=down}\\)\n\\[  \\begin{bmatrix}\nq(0,0,2) & q(0,1,2) & q(0,2,2) & q(0,3,2) \\\\\nq(1,0,2) & q(1,1,2) & q(1,2,2) & q(1,3,2) \\\\\nq(2,0,2) & q(2,1,2) & q(2,2,2) & q(2,3,2) \\\\\nq(3,0,2) & q(3,1,2) &q(3,2,2) & q(3,3,2) \\\\\n\\end{bmatrix} = \\begin{bmatrix}\n-1 & -1 & -1 & -1 \\\\\n-1& -1 & -1 & -1 \\\\\n-1 & -1 & -1 & 100\\\\\n-10 & -10 & -10 &  \\text{-} \\\\\n\\end{bmatrix}\n\\]\n\n\n\\(a=3 \\Leftrightarrow \\text{\\tt action=up}\\)\n\\[  \\begin{bmatrix}\nq(0,0,3) & q(0,1,3) & q(0,2,3) & q(0,3,3) \\\\\nq(1,0,3) & q(1,1,3) & q(1,2,3) & q(1,3,3) \\\\\nq(2,0,3) & q(2,1,3) & q(2,2,3) & q(2,3,3) \\\\\nq(3,0,3) & q(3,1,3) &q(3,2,3) & q(3,3,3) \\\\\n\\end{bmatrix} =\\begin{bmatrix}\n-10 & -10 & -10 & -10\\\\\n-1& -1 & -1 & -1 \\\\\n-1 & -1 & -1 & -1 \\\\\n-1 & -1 & -1 &  \\text{-} \\\\\n\\end{bmatrix}\n\\]\n\n\n\n\n\n- ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ \\(q(s_1,s_2,a)\\)ë¥¼ êµ¬í•´ë³´ì.\nğŸ—£ï¸(\n\nplayer.states[0], player.actions[0], player.rewards[0]\n\n(array([0, 0]), np.int64(1), -10)\n\n\nq(0, 0, 1) = -10\nâœï¸ ì´ ê²½ìš°ì— ê²Œì„ì´ ëë‚¬ìŒ\n\nplayer.states[1], player.actions[1], player.rewards[1]\n\n(array([0, 0]), np.int64(2), -1)\n\n\nq(0, 0, 2) = -1\n\nplayer.states[2], player.actions[2], player.rewards[2]\n\n(array([1, 0]), np.int64(1), -10)\n\n\n\nq_table ë§Œë“¤ê¸°\n\n\nq_table = np.zeros([4,4,4])\nq_table\n\narray([[[0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.]],\n\n       [[0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.]],\n\n       [[0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.]],\n\n       [[0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.]]])\n\n\n\nfor (s1,s2), a, r in zip(player.states, player.actions, player.rewards):\n    q_table[s1,s2,a] = q_table[s1,s2,a] + r # [s1,s2,s3]ì—ì„œ ë°›ì€ ë³´ìƒ(r)ì„ ê³„ì† ë”í•¨\n\n\nq_table[:,:,0]\n\narray([[-29987., -10133.,  -3694., -12710.],\n       [-10197.,  -7066.,  -3646., -14840.],\n       [ -3761.,  -3648.,  -2312.,  -9980.],\n       [ -1361.,  -1446.,  91000.,      0.]])\n\n\nì•„ë˜ì™€ ê°™ì€ ê²½ìš°ë¥¼ ê¸°ëŒ€í•˜ì˜€ìœ¼ë‚˜ ê·¸ë ‡ì§€ ì•ŠìŒ\n-1 -1 -1 -10\n-1 -1 -1 -10\n-1 -1 -1 -10\n-1 -1 100 -\ncountë¥¼ ê³„ì‚°í•˜ì—¬ í‰ê· ì„ êµ¬í•´ì•¼ í•  ê²ƒ ê°™ìŒ\n\nq_table = np.zeros([4,4,4])\ncount = np.zeros([4,4,4])\n\n\nfor (s1,s2), a, r in zip(player.states, player.actions, player.rewards):\n    q_table[s1,s2,a] = q_table[s1,s2,a] + r\n    count[s1,s2,a] = count[s1,s2,a] + 1\n\n\nq_table[:,:,0]\n\narray([[-29987., -10133.,  -3694., -12710.],\n       [-10197.,  -7066.,  -3646., -14840.],\n       [ -3761.,  -3648.,  -2312.,  -9980.],\n       [ -1361.,  -1446.,  91000.,      0.]])\n\n\n\ncount[:,:,0]\n\narray([[29987., 10133.,  3694.,  1271.],\n       [10197.,  7066.,  3646.,  1484.],\n       [ 3761.,  3648.,  2312.,   998.],\n       [ 1361.,  1446.,   910.,     0.]])\n\n\n\nq_table[...,0] # ë™ì¼ ì½”ë“œ\n\narray([[-29987., -10133.,  -3694., -12710.],\n       [-10197.,  -7066.,  -3646., -14840.],\n       [ -3761.,  -3648.,  -2312.,  -9980.],\n       [ -1361.,  -1446.,  91000.,      0.]])\n\n\n\ncount[...,0] # ë™ì¼ ì½”ë“œ\n\narray([[29987., 10133.,  3694.,  1271.],\n       [10197.,  7066.,  3646.,  1484.],\n       [ 3761.,  3648.,  2312.,   998.],\n       [ 1361.,  1446.,   910.,     0.]])\n\n\n\nq_table[...,1]\n\narray([[-301600.,  -10110.,   -3881.,   -1306.],\n       [-101040.,   -6958.,   -3589.,   -1430.],\n       [ -38240.,   -3584.,   -2295.,    -913.],\n       [ -13030.,   -1492.,    -936.,       0.]])\n\n\n\ncount[...,1]\n\narray([[30160., 10110.,  3881.,  1306.],\n       [10104.,  6958.,  3589.,  1430.],\n       [ 3824.,  3584.,  2295.,   913.],\n       [ 1303.,  1492.,   936.,     0.]])\n\n\nq_tableì„ countë¡œ ë‚˜ëˆ„ë©´ ì•„ë˜ì™€ ê°™ì€ ê²°ê³¼ê°€ ë‚˜ì˜¬ ê²ƒ ê°™ìŒ\n-10 -1 -1 -1\n-10 -1 -1 -1\n-10 -1 -1 -1\n-10 -1 -1 -\n\nq_table / count\n\n/tmp/ipykernel_2313654/1678126644.py:1: RuntimeWarning: invalid value encountered in divide\n  q_table / count\n\n\narray([[[ -1., -10.,  -1., -10.],\n        [ -1.,  -1.,  -1., -10.],\n        [ -1.,  -1.,  -1., -10.],\n        [-10.,  -1.,  -1., -10.]],\n\n       [[ -1., -10.,  -1.,  -1.],\n        [ -1.,  -1.,  -1.,  -1.],\n        [ -1.,  -1.,  -1.,  -1.],\n        [-10.,  -1.,  -1.,  -1.]],\n\n       [[ -1., -10.,  -1.,  -1.],\n        [ -1.,  -1.,  -1.,  -1.],\n        [ -1.,  -1.,  -1.,  -1.],\n        [-10.,  -1., 100.,  -1.]],\n\n       [[ -1., -10., -10.,  -1.],\n        [ -1.,  -1., -10.,  -1.],\n        [100.,  -1., -10.,  -1.],\n        [ nan,  nan,  nan,  nan]]])\n\n\n\nì—†ëŠ” ê°’ì´ ì¡´ì¬í•¨\n\n\n(q_table / count)[...,0]\n\n/tmp/ipykernel_2313654/878353018.py:1: RuntimeWarning: invalid value encountered in divide\n  (q_table / count)[...,0]\n\n\narray([[ -1.,  -1.,  -1., -10.],\n       [ -1.,  -1.,  -1., -10.],\n       [ -1.,  -1.,  -1., -10.],\n       [ -1.,  -1., 100.,  nan]])\n\n\n\nnan ìœ„ì¹˜ì˜ ê°’ì€ ì›ë˜ ì—†ìœ¼ë¯€ë¡œ countê°€ ì•ˆ ë¨\n\n\ncount[...,0]\n\narray([[29987., 10133.,  3694.,  1271.],\n       [10197.,  7066.,  3646.,  1484.],\n       [ 3761.,  3648.,  2312.,   998.],\n       [ 1361.,  1446.,   910.,     0.]])\n\n\n\n0ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì´ ë¬¸ì œê°€ ë˜ë¯€ë¡œ ë‹¤ìŒê³¼ ê°™ì´ í•˜ë©´\n\n\ncount[count == 0] = 0.000001\nq_table = q_table / count\n\n\nq_table[...,0]\n\narray([[ -1.,  -1.,  -1., -10.],\n       [ -1.,  -1.,  -1., -10.],\n       [ -1.,  -1.,  -1., -10.],\n       [ -1.,  -1., 100.,   0.]])\n\n\n)ğŸ—£ï¸\n\nplayer.states[0], player.actions[0], player.rewards[0]\n\n(array([0, 0]), 0, -1)\n\n\n\nq_table = np.zeros((4,4,4))\ncount = np.zeros((4,4,4))\n\n\nmemory =  zip(player.states, player.actions, player.rewards)\nfor (s1,s2), a, r in memory:\n    q_table[s1,s2,a] = q_table[s1,s2,a] + r\n    count[s1,s2,a] = count[s1,s2,a] + 1 \n\n\ncount[count==0] = 0.001 \n\n\nq_table = q_table / count\n\n\nq_table[...,0], q_table[...,1], q_table[...,2], q_table[...,3]\n\n(array([[ -1.,  -1.,  -1., -10.],\n        [ -1.,  -1.,  -1., -10.],\n        [ -1.,  -1.,  -1., -10.],\n        [ -1.,  -1., 100.,   0.]]),\n array([[-10.,  -1.,  -1.,  -1.],\n        [-10.,  -1.,  -1.,  -1.],\n        [-10.,  -1.,  -1.,  -1.],\n        [-10.,  -1.,  -1.,   0.]]),\n array([[ -1.,  -1.,  -1.,  -1.],\n        [ -1.,  -1.,  -1.,  -1.],\n        [ -1.,  -1.,  -1., 100.],\n        [-10., -10., -10.,   0.]]),\n array([[-10., -10., -10., -10.],\n        [ -1.,  -1.,  -1.,  -1.],\n        [ -1.,  -1.,  -1.,  -1.],\n        [ -1.,  -1.,  -1.,   0.]]))\n\n\n- countë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ë°©ë²•ì€ ì—†ì„ê¹Œ? â€“ í…Œí¬ë‹‰\nğŸ—£ï¸(\n\në¶€ìŠ¤íŒ… ì•Œê³ ë¦¬ì¦˜ì„ ì˜ ì•Œë©´ ì´í•´í•˜ê¸° ì‰¬ì›€\n\nì˜ˆì‹œ)\nì–´ë–¤ ì•¡ì…˜ì„ í–ˆì„ ë•Œ ë°›ëŠ” ë³´ìƒ 80 / í•™ìŠµë¥ : 0.1 / ì´ˆê¸°ê°’ 50\n80 - 50 = 30\n50 + 3 = 53\n80 - 53 = 27\n53 + 2.7 = 55.7\n...\n80 ê·¼ì²˜ë¡œ ê°€ê²Œ ë¨\n\nq_table = np.zeros([4,4,4])\nfor (s1,s2), a, r in zip(player.states, player.actions, player.rewards):\n    qhat = q_table[s1,s2,a]\n    q = r\n    diff = q - qhat\n    q_table[s1,s2,a] = q_table[s1,s2,a] + 0.01*diff\n\n\nq_table\n\narray([[[ -1.        , -10.        ,  -1.        , -10.        ],\n        [ -1.        ,  -1.        ,  -1.        , -10.        ],\n        [ -1.        ,  -1.        ,  -1.        , -10.        ],\n        [ -9.99997166,  -0.99999801,  -0.99999585,  -9.99998066]],\n\n       [[ -1.        , -10.        ,  -1.        ,  -1.        ],\n        [ -1.        ,  -1.        ,  -1.        ,  -1.        ],\n        [ -1.        ,  -1.        ,  -1.        ,  -1.        ],\n        [ -9.99999667,  -0.99999943,  -0.99999928,  -0.9999994 ]],\n\n       [[ -1.        , -10.        ,  -1.        ,  -1.        ],\n        [ -1.        ,  -1.        ,  -1.        ,  -1.        ],\n        [ -1.        ,  -1.        ,  -1.        ,  -1.        ],\n        [ -9.99955952,  -0.9998965 ,  99.99218879,  -0.99983567]],\n\n       [[ -0.99999885,  -9.99997946,  -9.99998584,  -0.99999546],\n        [ -0.99999951,  -0.99999969,  -9.9999924 ,  -0.99999986],\n        [ 99.98933337,  -0.99991786,  -9.99916195,  -0.99991703],\n        [  0.        ,   0.        ,   0.        ,   0.        ]]])\n\n\n\nq_table[...,0]\n\narray([[-1.        , -1.        , -1.        , -9.99997166],\n       [-1.        , -1.        , -1.        , -9.99999667],\n       [-1.        , -1.        , -1.        , -9.99955952],\n       [-0.99999885, -0.99999951, 99.98933337,  0.        ]])\n\n\n\nq_table[...,0].round(2)\n\narray([[ -1.  ,  -1.  ,  -1.  , -10.  ],\n       [ -1.  ,  -1.  ,  -1.  , -10.  ],\n       [ -1.  ,  -1.  ,  -1.  , -10.  ],\n       [ -1.  ,  -1.  ,  99.99,   0.  ]])\n\n\n)ğŸ—£ï¸\n\nq_table = np.zeros((4,4,4))\nmemory =  zip(player.states, player.actions, player.rewards)\nfor (s1,s2), a, r in memory:\n    qhat = q_table[s1,s2,a] # ë‚´ê°€ ìƒê°í–ˆë˜ê°“\n    q = r # ì‹¤ì œê°’\n    diff = q-qhat # ì°¨ì´\n    q_table[s1,s2,a] = q_table[s1,s2,a]  + 0.01*diff# update\n\n\nq_table.round(2)\n\narray([[[ -1.  , -10.  ,  -1.  , -10.  ],\n        [ -1.  ,  -1.  ,  -1.  , -10.  ],\n        [ -1.  ,  -1.  ,  -1.  , -10.  ],\n        [-10.  ,  -1.  ,  -1.  , -10.  ]],\n\n       [[ -1.  , -10.  ,  -1.  ,  -1.  ],\n        [ -1.  ,  -1.  ,  -1.  ,  -1.  ],\n        [ -1.  ,  -1.  ,  -1.  ,  -1.  ],\n        [-10.  ,  -1.  ,  -1.  ,  -1.  ]],\n\n       [[ -1.  , -10.  ,  -1.  ,  -1.  ],\n        [ -1.  ,  -1.  ,  -1.  ,  -1.  ],\n        [ -1.  ,  -1.  ,  -1.  ,  -1.  ],\n        [-10.  ,  -1.  ,  99.99,  -1.  ]],\n\n       [[ -1.  , -10.  , -10.  ,  -1.  ],\n        [ -1.  ,  -1.  , -10.  ,  -1.  ],\n        [ 99.99,  -1.  , -10.  ,  -1.  ],\n        [  0.  ,   0.  ,   0.  ,   0.  ]]])"
  },
  {
    "objectID": "posts/14wk-1.html#c.-ì²«ë²ˆì§¸-q_tableë³´ë‹¤-ë‚˜ì€-ê²ƒ",
    "href": "posts/14wk-1.html#c.-ì²«ë²ˆì§¸-q_tableë³´ë‹¤-ë‚˜ì€-ê²ƒ",
    "title": "14wk-1: (ê°•í™”í•™ìŠµ) â€“ 4x4 Grid World í™˜ê²½ì˜ ì´í•´",
    "section": "C. ì²«ë²ˆì§¸ q_tableë³´ë‹¤ ë‚˜ì€ ê²ƒ?",
    "text": "C. ì²«ë²ˆì§¸ q_tableë³´ë‹¤ ë‚˜ì€ ê²ƒ?\n- ì²«ë²ˆì§¸ q_tableì„ ì•Œê³ ìˆë‹¤ê³  ê°€ì •í•˜ì.\n\n- ì •ì±…ì‹œê°í™” (í•©ë¦¬ì ì¸ í–‰ë™)\n\n- ì´ê²Œ ìµœì„ ì˜ ì •ì±…ì¼ê¹Œ?\n\nğŸ—£ï¸\n\ní° ìˆ«ì ë”°ë¼ê°€ê¸°?\n(2,2)ì—ì„œ ì˜¤ë¥¸ìª½ê³¼ ì•„ë˜ëŠ” -1 ì´ ì•„ë‹ˆë¼ 99ë¡œ ìˆ˜ì •?\n\nì˜¤ë¥¸ìª½ì´ë‚˜ ì•„ë˜ë¡œ ê°€ë©´ 100ì´ ë³´ì¥ì´ ë˜ì–´ ìˆìŒ\n\n(1,2)ì—ì„œ ì™¼ìª½ê³¼ ì•„ë˜ìª½ì€ rewardê°€ ë˜‘ê°™ì€ ê²ƒì´ ë§ëŠ”ê°€?\n\nì•„ë˜ë¡œ ê°€ë©´ 99ê°€ ë³´ì¥ì´ ë˜ì–´ ìˆìœ¼ë¯€ë¡œ 98ë¡œ ìˆ˜ì •?\n\nì´ëŸ° ì‹ìœ¼ë¡œ 100ì ì„ ê¸°ì¤€ìœ¼ë¡œ ì ì°¨ ì¬ì¡°ì •ì´ ë˜ì–´ì•¼ í•  ê²ƒ ê°™ìŒ"
  },
  {
    "objectID": "posts/11wk-1.html",
    "href": "posts/11wk-1.html",
    "title": "11wk-1: (ì¶”ì²œì‹œìŠ¤í…œ) â€“ Embedding ë ˆì´ì–´, ì‚¬ìš©ìì •ì˜ ë„¤íŠ¸ì›Œí¬, MF-based ì¶”ì²œì‹œìŠ¤í…œì„ ë„˜ì–´ì„œ",
    "section": "",
    "text": "ğŸ“˜ Note Format Guide\nThis format serves as a structured guide for organizing lecture content, personal interpretation, experiments, and study-related questions.\nğŸ“ ğŸ—£ï¸ âœï¸ ğŸ”¬ â“"
  },
  {
    "objectID": "posts/11wk-1.html#a.-ì„ë² ë”©ë ˆì´ì–´",
    "href": "posts/11wk-1.html#a.-ì„ë² ë”©ë ˆì´ì–´",
    "title": "11wk-1: (ì¶”ì²œì‹œìŠ¤í…œ) â€“ Embedding ë ˆì´ì–´, ì‚¬ìš©ìì •ì˜ ë„¤íŠ¸ì›Œí¬, MF-based ì¶”ì²œì‹œìŠ¤í…œì„ ë„˜ì–´ì„œ",
    "section": "A. ì„ë² ë”©ë ˆì´ì–´",
    "text": "A. ì„ë² ë”©ë ˆì´ì–´\n- ëª¨í‹°ë¸Œ: torch.nn.functional.one_hot + torch.nn.Linear ë¥¼ ë§¤ë²ˆ ì“°ëŠ”ê±´ ë„ˆë¬´ ê·€ì°®ì§€ ì•Šì–´?\nğŸ—£ï¸(\n\ntorch.manual_seed(43052)\n#x  = ['ì˜¥ìˆœ', 'ì˜ìˆ™', 'í•˜ë‹ˆ', 'ì˜¥ìˆœ', 'ì˜ìˆ™'] \nx = torch.tensor([0,1,2,0,1])\nlinr = torch.nn.Linear(3,1,bias=False)\nlinr(torch.nn.functional.one_hot(x).float())\n\ntensor([[-0.2002],\n        [-0.4890],\n        [ 0.2081],\n        [-0.2002],\n        [-0.4890]], grad_fn=&lt;MmBackward0&gt;)\n\n\n\nì§€ë‚œ ì‹œê°„ê¹Œì§€ëŠ” ìœ„ì²˜ëŸ¼ í–ˆìŒ\n\n\ntorch.manual_seed(43052)\n#x  = ['ì˜¥ìˆœ', 'ì˜ìˆ™', 'í•˜ë‹ˆ', 'ì˜¥ìˆœ', 'ì˜ìˆ™'] \nx = torch.tensor([0,1,2,0,1])\nX = torch.nn.functional.one_hot(x).float()\nlinr = torch.nn.Linear(3,1,bias=False)\nlinr(X)\n\ntensor([[-0.2002],\n        [-0.4890],\n        [ 0.2081],\n        [-0.2002],\n        [-0.4890]], grad_fn=&lt;MmBackward0&gt;)\n\n\n\nx, X\n\n(tensor([0, 1, 2, 0, 1]),\n tensor([[1., 0., 0.],\n         [0., 1., 0.],\n         [0., 0., 1.],\n         [1., 0., 0.],\n         [0., 1., 0.]]))\n\n\n\nlinr.weight\n\nParameter containing:\ntensor([[-0.2002, -0.4890,  0.2081]], requires_grad=True)\n\n\n\nX @ linr.weight.T\n\ntensor([[-0.2002],\n        [-0.4890],\n        [ 0.2081],\n        [-0.2002],\n        [-0.4890]], grad_fn=&lt;MmBackward0&gt;)\n\n\n)ğŸ—£ï¸\n\ntorch.manual_seed(43052)\n#x  = ['ì˜¥ìˆœ', 'ì˜ìˆ™', 'í•˜ë‹ˆ', 'ì˜¥ìˆœ', 'ì˜ìˆ™'] \nx = torch.tensor([0,1,2,0,1])\nX = torch.nn.functional.one_hot(x).float()\nlinr = torch.nn.Linear(3,1,bias=False)\nlinr(X)\n\ntensor([[-0.2002],\n        [-0.4890],\n        [ 0.2081],\n        [-0.2002],\n        [-0.4890]], grad_fn=&lt;MmBackward0&gt;)\n\n\n- ê³„ì‚°ë°©ì‹\n\n\\({\\boldsymbol x}= \\begin{bmatrix} 0 \\\\ 1 \\\\ 2 \\\\ 0 \\\\ 1 \\end{bmatrix} \\Longrightarrow {\\bf X}= \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{bmatrix}\\)\n\\(\\text{linr}({\\bf X})= \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{bmatrix}\\begin{bmatrix} -0.2002 \\\\ -0.4890 \\\\ 0.2081 \\end{bmatrix} = \\begin{bmatrix} -0.2002 \\\\ -0.4890 \\\\ 0.2081 \\\\ -0.2002 \\\\ -0.4890 \\end{bmatrix}\\)\n\n- torch.nn.functional.one_hot + torch.nn.Linear ë¥¼ í•¨ê»˜ì²˜ë¦¬í•´ì£¼ëŠ” ë ˆì´ì–´ torch.nn.Embedding ì¡´ì¬\nğŸ—£ï¸(\n\në‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ í•  ìˆ˜ë„ ìˆìŒ\n\n\ntorch.manual_seed(43052)\n#x  = ['ì˜¥ìˆœ', 'ì˜ìˆ™', 'í•˜ë‹ˆ', 'ì˜¥ìˆœ', 'ì˜ìˆ™'] \nx = torch.tensor([0,1,2,0,1])\nebdd = torch.nn.Embedding(3,1)\nebdd(x) # ì´ë ‡ê²Œ í•˜ë©´ ìˆ«ìë“¤ì´ ë‚˜ì˜´ (ìœ„ì™€ëŠ” ë‹¤ë¦„)\n\ntensor([[-0.8178],\n        [-0.7052],\n        [-0.5843],\n        [-0.8178],\n        [-0.7052]], grad_fn=&lt;EmbeddingBackward0&gt;)\n\n\n\n#x  = ['ì˜¥ìˆœ', 'ì˜ìˆ™', 'í•˜ë‹ˆ', 'ì˜¥ìˆœ', 'ì˜ìˆ™'] \nx = torch.tensor([0,1,2,0,1])\nebdd = torch.nn.Embedding(3,1)\nebdd(x)\n\ntensor([[-0.6179],\n        [ 1.9949],\n        [-0.4724],\n        [-0.6179],\n        [ 1.9949]], grad_fn=&lt;EmbeddingBackward0&gt;)\n\n\n\nebdd.weight\n\nParameter containing:\ntensor([[-0.6179],\n        [ 1.9949],\n        [-0.4724]], requires_grad=True)\n\n\n\nlinr.weight\n\nParameter containing:\ntensor([[-0.2002, -0.4890,  0.2081]], requires_grad=True)\n\n\nebdd.weight.data = torch.tensor([[-0.2002],[-0.4890],[0.2081]]) # ì´ì „ linrì˜ parameterë¡œ ë®ì–´ì“°ë©´\n\ntorch.manual_seed(43052)\n#x  = ['ì˜¥ìˆœ', 'ì˜ìˆ™', 'í•˜ë‹ˆ', 'ì˜¥ìˆœ', 'ì˜ìˆ™'] \nx = torch.tensor([0,1,2,0,1])\nebdd = torch.nn.Embedding(3,1)\nebdd.weight.data = torch.tensor([[-0.2002],[-0.4890],[0.2081]])\nebdd(x)\n\ntensor([[-0.2002],\n        [-0.4890],\n        [ 0.2081],\n        [-0.2002],\n        [-0.4890]], grad_fn=&lt;EmbeddingBackward0&gt;)\n\n\n\nlinr(X) # ìœ„ì™€ ë™ì¼\n\ntensor([[-0.2002],\n        [-0.4890],\n        [ 0.2081],\n        [-0.2002],\n        [-0.4890]], grad_fn=&lt;MmBackward0&gt;)\n\n\n\nEmbeddingì—ëŠ” one hot encoding í›„ linr ì„ í•  ë•Œì˜ ì°¨ì›ì„ ë„£ì–´ì£¼ë©´ ë¨\n\n)ğŸ—£ï¸\n\n#x  = ['ì˜¥ìˆœ', 'ì˜ìˆ™', 'í•˜ë‹ˆ', 'ì˜¥ìˆœ', 'ì˜ìˆ™'] \nx = torch.tensor([0,1,2,0,1])\nebdd = torch.nn.Embedding(3,1)\nebdd.weight.data = torch.tensor([[-0.2002],[-0.4890],[0.2081]])\nebdd(x)\n\ntensor([[-0.2002],\n        [-0.4890],\n        [ 0.2081],\n        [-0.2002],\n        [-0.4890]], grad_fn=&lt;EmbeddingBackward0&gt;)\n\n\n\n\\(\\text{ebdd}({\\boldsymbol x})= \\text{linr}\\big(\\text{onehot}({\\boldsymbol x})\\big) = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{bmatrix}\\begin{bmatrix} -0.2002 \\\\ -0.4890 \\\\ 0.2081 \\end{bmatrix} = \\begin{bmatrix} -0.2002 \\\\ -0.4890 \\\\ 0.2081 \\\\ -0.2002 \\\\ -0.4890 \\end{bmatrix}\\)\nìš°ë¦¬ê°€ ì´ì „ì— êµ¬í˜„í–ˆë˜ ì½”ë“œ â€œonehot + linrâ€ ì™€ â€œebddâ€ëŠ” ì •í™•í•˜ê²Œ ë™ì¼í•œ ë™ì‘ì„ ìˆ˜í–‰í•¨.\n\n- ê²°ë¡ : ì•„ë˜ì˜ ë‘ê°œì˜ ì½”ë“œëŠ” ê°™ë‹¤.\nx= torch.tensor([0,1,2,0,1])\n\n## ì½”ë“œ1 \nlinr = torch.nn.Linear(3,1) \nlinr(torch.nn.functional.one_hot(x))\n\n## ì½”ë“œ2 \nebdd = torch.nn.Embedding(3,1)\nebdd(x)"
  },
  {
    "objectID": "posts/11wk-1.html#b.-mf-based-ì¶”ì²œì‹œìŠ¤í…œ-ì¬ì„¤ê³„",
    "href": "posts/11wk-1.html#b.-mf-based-ì¶”ì²œì‹œìŠ¤í…œ-ì¬ì„¤ê³„",
    "title": "11wk-1: (ì¶”ì²œì‹œìŠ¤í…œ) â€“ Embedding ë ˆì´ì–´, ì‚¬ìš©ìì •ì˜ ë„¤íŠ¸ì›Œí¬, MF-based ì¶”ì²œì‹œìŠ¤í…œì„ ë„˜ì–´ì„œ",
    "section": "B. MF-based ì¶”ì²œì‹œìŠ¤í…œ ì¬ì„¤ê³„",
    "text": "B. MF-based ì¶”ì²œì‹œìŠ¤í…œ ì¬ì„¤ê³„\nì•„ë˜ì˜ ìë£Œë¥¼ í™œìš©í•˜ì—¬ ì¶”ì²œì‹œìŠ¤í…œì„ ì„¤ê³„í•˜ê³ ìí•œë‹¤.\n\ndf_view = pd.read_csv('https://raw.githubusercontent.com/guebin/DL2025/main/posts/iamsolo.csv',index_col=0)\ndf_view\n\n\n\n\n\n\n\n\nì˜ì‹(IN)\nì˜ì² (IN)\nì˜í˜¸(IS)\nê´‘ìˆ˜(IS)\nìƒì² (EN)\nì˜ìˆ˜(EN)\nê·œë¹ˆ(ES)\në‹¤í˜¸(ES)\n\n\n\n\nì˜¥ìˆœ(IN)\nNaN\n4.02\n3.45\n3.42\n0.84\n1.12\n0.43\n0.49\n\n\nì˜ì(IN)\n3.93\n3.99\n3.63\n3.43\n0.98\n0.96\n0.52\nNaN\n\n\nì •ìˆ™(IS)\n3.52\n3.42\n4.05\n4.06\n0.39\nNaN\n0.93\n0.99\n\n\nì˜ìˆ™(IS)\n3.43\n3.57\nNaN\n3.95\n0.56\n0.52\n0.89\n0.89\n\n\nìˆœì(EN)\n1.12\nNaN\n0.59\n0.43\n4.01\n4.16\n3.52\n3.38\n\n\ní˜„ìˆ™(EN)\n0.94\n1.05\n0.32\n0.45\n4.02\n3.78\nNaN\n3.54\n\n\nì„œì—°(ES)\n0.51\n0.56\n0.88\n0.89\n3.50\n3.64\n4.04\n4.10\n\n\në³´ëŒ(ES)\n0.48\n0.51\n1.03\nNaN\n3.52\n4.00\n3.82\nNaN\n\n\ní•˜ë‹ˆ(I)\n4.85\n4.82\nNaN\n4.98\n4.53\n4.39\n4.45\n4.52\n\n\n\n\n\n\n\n\ndf_train = df_view.stack().reset_index().set_axis(['W','M','y'],axis=1)\nì—¬ì„±ì¸ë±ìŠ¤ = {'ì˜¥ìˆœ(IN)':0, 'ì˜ì(IN)':1, 'ì •ìˆ™(IS)':2, 'ì˜ìˆ™(IS)':3, 'ìˆœì(EN)':4, 'í˜„ìˆ™(EN)':5, 'ì„œì—°(ES)':6, 'ë³´ëŒ(ES)':7, 'í•˜ë‹ˆ(I)':8}\në‚¨ì„±ì¸ë±ìŠ¤ = {'ì˜ì‹(IN)':0, 'ì˜ì² (IN)':1, 'ì˜í˜¸(IS)':2, 'ê´‘ìˆ˜(IS)':3, 'ìƒì² (EN)':4, 'ì˜ìˆ˜(EN)':5, 'ê·œë¹ˆ(ES)':6, 'ë‹¤í˜¸(ES)':7}\nx1 = torch.tensor(df_train['W'].map(ì—¬ì„±ì¸ë±ìŠ¤)) # length-n int vector \nx2 = torch.tensor(df_train['M'].map(ë‚¨ì„±ì¸ë±ìŠ¤)) # length-n int vector \ny = torch.tensor(df_train['y']).float().reshape(-1,1) # (n,1) float vector\n\nì„ë² ë”©ë ˆì´ì–´ë¥¼ í™œìš©í•˜ì—¬ MF-based ì¶”ì²œì‹œìŠ¤í…œì„ ì„¤ê³„í•˜ë¼.\n(í’€ì´)\nğŸ—£ï¸(\n# ì§€ë‚œ ì‹œê°„ì—ëŠ” x1ì´ one hot encoding ëœ í˜•íƒœ\nX1 = torch.nn.functional.one_hot(x1)\n\nx1 # ì´ì œëŠ” ì´ ìƒíƒœì—ì„œ ë°”ë¡œ í•™ìŠµì„ ì‹œí‚¤ë ¤ê³  í•¨ (ì„ë² ë”© ë ˆì´ì–´ í™œìš©)\n\ntensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3,\n        3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6,\n        6, 6, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8])\n\n\n\nì´ì „ ì‹œê°„ ì½”ë“œë¥¼ ì°¸ê³ í•˜ì—¬ ìˆ˜ì •í•˜ë©´\n\n#----#\nloss_fn = torch.nn.MSELoss() \nl1 = torch.nn.Linear(9,2,bias=False)\nl2 = torch.nn.Linear(8,2,bias=False)\nb1 = torch.nn.Linear(9,1,bias=False)\nb2 = torch.nn.Linear(8,1,bias=False)\nparams = list(l1.parameters()) + list(l2.parameters())  + list(b1.parameters()) + list(b2.parameters())\noptimizr = torch.optim.Adam(params) \nsig = torch.nn.Sigmoid()\n#----#\nfor epoc in range(100):\n    #step1\n    W_features = l1(X1) \n    M_features = l2(X2)\n    W_bias = b1(X1)\n    M_bias = b2(X2)\n    yhat = sig((W_features * M_features).sum(axis=1).reshape(-1,1) + W_bias + M_bias)*5\n    #step2\n    loss = loss_fn(yhat,y)\n    #stpe3\n    loss.backward()\n    #step4\n    optimizr.step()\n    optimizr.zero_grad()\n\n#df_view\nloss_fn = torch.nn.MSELoss() \nebdd1 = torch.nn.Embedding(9,2)\nebdd2 = torch.nn.Embedding(8,2)\nb1 = torch.nn.Embedding(9,1)\nb2 = torch.nn.Embedding(8,1)\nparams = list(ebdd1.parameters()) + list(ebdd2.parameters())  + list(b1.parameters()) + list(b2.parameters())\noptimizr = torch.optim.Adam(params)\nsig = torch.nn.Sigmoid()\n#----#\nfor epoc in range(5000):\n    #step1\n    W_features = ebdd1(x1) \n    M_features = ebdd2(x2) \n    W_bias = b1(x1)\n    M_bais = b2(x2)\n    yhat = sig((W_features * M_features).sum(axis=1).reshape(-1,1) + W_bias + M_bais)*5\n    #step2\n    loss = loss_fn(yhat,y)\n    #step3\n    loss.backward()\n    #step4\n    optimizr.step()\n    optimizr.zero_grad()\n\n\ny[:5], yhat[:5] # ê·¸ëŸ­ì €ëŸ­ ì˜ ë§ì¶”ê³  ìˆìŒ\n\n(tensor([[4.0200],\n         [3.4500],\n         [3.4200],\n         [0.8400],\n         [1.1200]]),\n tensor([[3.6066],\n         [3.6236],\n         [3.6976],\n         [0.7890],\n         [0.8583]], grad_fn=&lt;SliceBackward0&gt;))\n\n\n)ğŸ—£ï¸\n\n#df_view\nloss_fn = torch.nn.MSELoss() \nebdd1 = torch.nn.Embedding(9,2)\nebdd2 = torch.nn.Embedding(8,2)\nb1 = torch.nn.Embedding(9,1)\nb2 = torch.nn.Embedding(8,1)\nparams = list(ebdd1.parameters()) + list(ebdd2.parameters())  + list(b1.parameters()) + list(b2.parameters())\noptimizr = torch.optim.Adam(params)\nsig = torch.nn.Sigmoid()\n#----#\nfor epoc in range(5000):\n    #step1\n    W_features = ebdd1(x1) \n    M_features = ebdd2(x2) \n    W_bias = b1(x1)\n    M_bais = b2(x2)\n    yhat = sig((W_features * M_features).sum(axis=1).reshape(-1,1) + W_bias + M_bais)*5\n    #step2\n    loss = loss_fn(yhat,y)\n    #step3\n    loss.backward()\n    #step4\n    optimizr.step()\n    optimizr.zero_grad()\n\n\ny[:5], yhat[:5]\n\n(tensor([[4.0200],\n         [3.4500],\n         [3.4200],\n         [0.8400],\n         [1.1200]]),\n tensor([[3.9809],\n         [3.4865],\n         [3.4730],\n         [0.8163],\n         [0.9715]], grad_fn=&lt;SliceBackward0&gt;))"
  },
  {
    "objectID": "posts/11wk-1.html#a.-ì‚¬ìš©ìì •ì˜-ë„¤íŠ¸ì›Œí¬-ì‚¬ìš©ë²•",
    "href": "posts/11wk-1.html#a.-ì‚¬ìš©ìì •ì˜-ë„¤íŠ¸ì›Œí¬-ì‚¬ìš©ë²•",
    "title": "11wk-1: (ì¶”ì²œì‹œìŠ¤í…œ) â€“ Embedding ë ˆì´ì–´, ì‚¬ìš©ìì •ì˜ ë„¤íŠ¸ì›Œí¬, MF-based ì¶”ì²œì‹œìŠ¤í…œì„ ë„˜ì–´ì„œ",
    "section": "A. ì‚¬ìš©ìì •ì˜ ë„¤íŠ¸ì›Œí¬ ì‚¬ìš©ë²•",
    "text": "A. ì‚¬ìš©ìì •ì˜ ë„¤íŠ¸ì›Œí¬ ì‚¬ìš©ë²•\n# ì˜ˆë¹„í•™ìŠµ1: net(X)ì™€ ì‚¬ì‹¤ net.forward(X)ëŠ” ê°™ë‹¤.\nğŸ—£ï¸(\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n)\n\n\nX = torch.randn(5,1)\nnet(X)\n\ntensor([[0.5705],\n        [0.7858],\n        [0.8796],\n        [0.8451],\n        [0.8138]], grad_fn=&lt;SigmoidBackward0&gt;)\n\n\n\nnet.forward(X)\n\ntensor([[0.5705],\n        [0.7858],\n        [0.8796],\n        [0.8451],\n        [0.8138]], grad_fn=&lt;SigmoidBackward0&gt;)\n\n\n\nnet(X)ê³¼ net.forward(X)ëŠ” ë™ì¼\nnet.forward(X)ëŠ” functionì´ì–´ì„œ ì´ë¥¼ í†µí•´ nex(X)ë¥¼ ë‹¤ì‹œ ì •ì˜í•  ìˆ˜ ìˆìŒ\n\n\ndef func(x):\n    return \"ë©”ë¡±\"\n\n\nfunc(33)\n\n'ë©”ë¡±'\n\n\n\nfunc('sss')\n\n'ë©”ë¡±'\n\n\n\nnet.forward = func\n\n\nnet.forward(X)\n\n'ë©”ë¡±'\n\n\n\nnet(X)\n\n'ë©”ë¡±'\n\n\n)ğŸ—£ï¸\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n)\nX = torch.randn(5,1)\n\n\nnet(X)\n#net.forward(X)\n\ntensor([[0.3340],\n        [0.4480],\n        [0.3143],\n        [0.2375],\n        [0.2066]], grad_fn=&lt;SigmoidBackward0&gt;)\n\n\nê·¸ë˜ì„œ net.forwardë¥¼ ì¬ì •ì˜í•˜ë©´ net(X)ì˜ ê¸°ëŠ¥ì„ ì¬ì •ì˜ í•  ìˆ˜ ìˆë‹¤.\n\ndef func(x):\n    return \"ë©”ë¡±\"\n\n\nnet.forward = func \n\n\nnet.forward(X) \n\n'ë©”ë¡±'\n\n\n\nnet(X)\n\n'ë©”ë¡±'\n\n\n#\n# ì˜ˆë¹„í•™ìŠµ2: torch.nn.Moduleì„ ìƒì†ë°›ì•„ì„œ ë„¤íŠ¸ì›Œí¬ë¥¼ ë§Œë“¤ë©´ (= â€œclass XXX(torch.nn.Module):â€ ì™€ ê°™ì€ ë°©ì‹ìœ¼ë¡œ í´ë˜ìŠ¤ë¥¼ ì„ ì–¸í•˜ë©´) ì•½ì†ëœ ì•„í‚¤í…ì²˜ë¥¼ ê°€ì§„ ë„¤íŠ¸ì›Œí¬ë¥¼ ì°ì–´ë‚´ëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“¤ ìˆ˜ ìˆë‹¤.\n\n\n\n\n\n\nNote\n\n\n\ní´ë˜ìŠ¤ì˜ ê¸°ì´ˆê°€ ë¶€ì¡±í•œ ë¶„ë“¤ì€ ì•„ë˜ì˜ ë§í¬ì—ì„œ\n\nhttps://guebin.github.io/PP2024/\n\n11wk-2, 12wk-2, 13wk-2, 14wk-2 ì— ëŒ€í•œë‚´ìš©ì„ í•™ìŠµí•˜ì‹œê¸¸ ë°”ëë‹ˆë‹¤.\n\n\n\nğŸ—£ï¸ ì´ì „ì— ë§Œë“  ì‚¬ìš©ì ì •ì˜ ë ˆì´ì–´ì™€ ë°©ì‹ ë¹„ìŠ·\n\n(ì˜ˆì œ1) â€“ torch.nn.Moduleì˜ ìƒì†ì„ ì´ìš©í•˜ì—¬ ì•„ë˜ì™€ ë™ì¼í•œ ì•„í‚¤í…ì²˜ë¥¼ ê°€ì§€ëŠ” ë„¤íŠ¸ì›Œí¬ë¥¼ ì„¤ê³„í•˜ë¼.\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=1,bias=True), # linr1\n    torch.nn.Sigmoid(),#sig\n    torch.nn.Linear(in_features=1,out_features=1,bias=False) # linr2 \n)\n\n\nx = torch.tensor([[1.0]])\n\n\nnet(x)\n\ntensor([[-0.5737]], grad_fn=&lt;MmBackward0&gt;)\n\n\n(í’€ì´)\nğŸ—£ï¸(\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=1,bias=True), # linr1\n    torch.nn.Sigmoid(),#sig\n    torch.nn.Linear(in_features=1,out_features=1,bias=False) # linr2 \n)\n\n\nx = torch.tensor([[1.0]])\n\n\nnet(x)\n\ntensor([[-0.3695]], grad_fn=&lt;MmBackward0&gt;)\n\n\n\nclass MyNet1(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linr1 = torch.nn.Linear(in_features=1,out_features=1,bias=True)\n        self.sig = torch.nn.Sigmoid()\n        self.linr2 = torch.nn.Linear(in_features=1,out_features=1,bias=False)\n    def forward(self,x):\n        #net(x): x --&gt; linr --&gt; sig --&gt; linr\n        out = self.linr2(self.sig(self.linr1(x))) # êµ¬ë¶„ì„ ìœ„í•´ linr1 linr2\n        return out\n\n\nnet = MyNet1()\n\n\nnet(x) # ê°’ì€ ë‹¬ë¼ì§€ì§€ë§Œ ë™ì¼í•œ ê¸°ëŠ¥ì„ ìˆ˜í–‰\n\ntensor([[-0.0929]], grad_fn=&lt;MmBackward0&gt;)\n\n\n)ğŸ—£ï¸\n\nclass MyNet1(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linr1 = torch.nn.Linear(in_features=1,out_features=1,bias=True)\n        self.sig = torch.nn.Sigmoid()\n        self.linr2 = torch.nn.Linear(in_features=1,out_features=1,bias=False)\n    def forward(self,x):\n        out = self.linr2(self.sig(self.linr1(x)))\n        return out \n\n(ì˜ˆì‹œ2) â€“ torch.nn.Moduleì˜ ìƒì†ì„ ì´ìš©í•˜ì—¬ ì•„ë˜ì™€ ë™ì¼í•œ ë™ì‘ì„ í•˜ëŠ” ë„¤íŠ¸ì›Œí¬ë¥¼ ì„¤ê³„í•˜ë¼.\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=1,bias=True),\n    torch.nn.ReLU(),\n    torch.nn.Linear(in_features=1,out_features=1,bias=False)\n)\n\n(í’€ì´)\n\nclass MyNet2(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linr1 = torch.nn.Linear(in_features=1,out_features=1,bias=True)\n        self.relu = torch.nn.ReLU()\n        self.linr2 = torch.nn.Linear(in_features=1,out_features=1,bias=False)\n    def forward(self,X):\n        netout = self.linr2(self.relu(self.linr1(x)))\n        return netout\n\nì‚¬ìš©ì ì •ì˜ ë„¤íŠ¸ì›Œí¬ë¥¼ ë§Œë“œëŠ” ë°©ë²•\nstep1: ì•„ë˜ì™€ ì½”ë“œë¥¼ ë³µì‚¬í•˜ì—¬ í‹€ì„ ë§Œë“ ë‹¤. (ì´ê±´ ë¬´ì¡°ê±´ ê³ ì •ì„, XXXX ìë¦¬ëŠ” ì›í•˜ëŠ” ì´ë¦„ì„ ë„£ëŠ”ë‹¤)\nclass XXXX(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        ## ìš°ë¦¬ê°€ netoutì„ êµ¬í• ë•Œ ì‚¬ìš©í•  ë ˆì´ì–´ë¥¼ ì •ì˜ \n        \n        ## ì •ì˜ ë\n    def forward(self,X):\n        ## netoutì„ ì–´ë–»ê²Œ êµ¬í• ê²ƒì¸ì§€ ì •ì˜ \n        \n        ## ì •ì˜ ë\n        return netout\n\nforwardì˜ ì…ë ¥: XëŠ” net(X)ì— ì‚¬ìš©í•˜ëŠ” Xì„\nforwardì˜ ì¶œë ¥: netoutì€ net.forward(X) í•¨ìˆ˜ì˜ ë¦¬í„´ê°’ì„\në‹¹ì—°íˆ X/netoutì€ ë‹¤ë¥¸ ë³€ìˆ˜ë¡œ ì¨ë„ ë¬´ë°© (ì˜ˆë¥¼ë“¤ë©´ input/output ì´ë¼ë“ ì§€)\n\nstep2: def __init__(self):ì— yhatì„ êµ¬í•˜ê¸° ìœ„í•´ í•„ìš”í•œ ì¬ë£Œë¥¼ ë ˆì´ì–´ë¥¼ ì •ì˜í•˜ê³  ì´ë¦„ì„ ë¶™ì¸ë‹¤. ì´ë¦„ì€ í•­ìƒ self.xxx ì™€ ê°™ì€ ì‹ìœ¼ë¡œ ì •ì˜í•œë‹¤.\nclass XXXX(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        ## ìš°ë¦¬ê°€ netoutì„ êµ¬í• ë•Œ ì‚¬ìš©í•  ë ˆì´ì–´ë¥¼ ì •ì˜ \n        self.xxx1 = torch.nn.Linear(in_features=1,out_features=1,bias=True)\n        self.xxx2 = torch.nn.Sigmoid()\n        self.xxx3 = torch.nn.Linear(in_features=1,out_features=1,bias=True)\n        ## ì •ì˜ ë\n    def forward(self,X):\n        ## netoutì„ ì–´ë–»ê²Œ êµ¬í• ê²ƒì¸ì§€ ì •ì˜ \n        \n        ## ì •ì˜ ë\n        return netout\nstep3: def forward:ì— â€œX â€“&gt; netoutâ€ ìœ¼ë¡œ ê°€ëŠ” ê³¼ì •ì„ ë¬˜ì‚¬í•œ ì½”ë“œë¥¼ ì‘ì„±í•˜ê³  netoutì„ ë¦¬í„´í•˜ë„ë¡ í•œë‹¤.\nclass XXXX(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        ## ìš°ë¦¬ê°€ netout êµ¬í• ë•Œ ì‚¬ìš©í•  ë ˆì´ì–´ë¥¼ ì •ì˜ \n        self.xxx1 = torch.nn.Linear(in_features=1,out_features=1,bias=True)\n        self.xxx2 = torch.nn.Sigmoid()\n        self.xxx3 = torch.nn.Linear(in_features=1,out_features=1,bias=True)\n        ## ì •ì˜ ë\n    def forward(self,X):\n        ## netoutì„ ì–´ë–»ê²Œ êµ¬í• ê²ƒì¸ì§€ ì •ì˜ \n        u = self.xxx1(X) \n        v = self.xxx2(u)\n        netout = self.xxx3(v) \n        ## ì •ì˜ ë\n        return netout\n#\n# ì‹¤ìŠµ(2025-ì¤‘ê°„ê³ ì‚¬ 4ë²ˆ): ììœ  ë‚™í•˜ ìš´ë™ì´ë€ ì–´ë–¤ ë¬¼ì²´ê°€ ì¼ì •í•œ ë†’ì´ì—ì„œ ë–¨ì–´ì ¸ ì§€ë©´ì— ë„ë‹¬í•˜ê¸° ê¹Œì§€ ê±¸ë¦¬ëŠ” ì‹œê°„ì„ ë‹¤ë£¨ëŠ” ë¬¼ë¦¬í•™ ê°œë…ì´ë‹¤. ë‹¤ìŒì€ ë¬¼ë¦¬í•™ì˜ ììœ  ë‚™í•˜ ìš´ë™ì—ì„œ ì°©ì•ˆí•˜ì—¬ ìƒì„±í•œ ë°ì´í„°ì´ë‹¤.\n\ntorch.manual_seed(43052)\nh = torch.rand(100)*100\nh,_ = h.sort()\nh = h.reshape(100,1)\nt = torch.sqrt(2*h/9.8) + torch.randn([100,1])*0.1\n\nì—¬ê¸°ì—ì„œ \\(h\\)ëŠ” ë‚™í•˜ì „ì˜ ë†’ì´(ë‹¨ìœ„: m), \\(t\\)ëŠ” í•´ë‹¹ë†’ì´ì—ì„œ ë¬¼ì¹˜ê°€ ì§€ë©´ì— ë„ë‹¬í•˜ê¸° ê¹Œì§€ ê±¸ë¦¬ëŠ” ì‹œê°„(ë‹¨ìœ„:ì´ˆ)ì„ ì˜ë¯¸í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì•„ë˜ì˜ ìë£ŒëŠ” \\(h=99.3920, t=4.4583\\)ë¥¼ ì˜ë¯¸í•˜ëŠ”ë°\n\nh[-1], t[-1]\n\n(tensor([99.3920]), tensor([4.4583]))\n\n\nì´ê²ƒì€ ë†’ì´ \\(99.3920\\)mì—ì„œ ë‚™í•˜í•œ ë¬¼ì²´ê°€ ì•½ \\(4.4583\\)ì´ˆë§Œì— ì§€ë©´ì— ë„ë‹¬í–ˆìŒì„ ì˜ë¯¸í•œë‹¤. ì•„ë˜ì˜ ê·¸ë¦¼ì€ \\(x\\)ì¶•ì— \\(h\\), \\(y\\)ì¶•ì— \\(t\\)ë¥¼ ë‘ê³  í•´ë‹¹ ë°ì´í„°ë¥¼ ì‚°ì ë„ë¡œ ì‹œê°í™” í•œ ê²ƒì´ë‹¤.\n\nplt.plot(h,t,'o',alpha=0.5)\nplt.xlabel('Height (m)')\nplt.ylabel('Time to fall (sec)')\nplt.title('Free Fall Time vs Height')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nê·¸ë˜í”„ë¥¼ ë³´ë©´ ë†’ì´ê°€ ë†’ì„ ìˆ˜ë¡ ë‚™í•˜ì‹œê°„ë„ ê¸¸ì–´ì§€ëŠ” ê²½í–¥ì´ ê´€ì°°ëœë‹¤. ë‹¤ë§Œ ë™ì¼í•œ ë†’ì´ë¼ í•˜ë”ë¼ë„ ë‚™í•˜ì‹œê°„ì´ ì¡°ê¸ˆì”© ì°¨ì´ë‚˜ëŠ” ê²½ìš°ê°€ ìˆëŠ”ë°, ì´ëŠ” ì‚¬ëŒì´ ì‹œê°„ì¸¡ì •ì„ ìˆ˜ë™ìœ¼ë¡œ í•˜ë©° ë°œìƒí•˜ëŠ” ì‹¤í—˜ì˜¤ì°¨ ë•Œë¬¸ì´ë‹¤. ì´ëŸ¬í•œ ì˜¤ì°¨ì—ë„ ë¶ˆêµ¬í•˜ê³  \\(h\\)ì™€ \\(t\\)ì‚¬ì´ì—ëŠ” ì¼ì •í•œ ê·œì¹™ì´ ì¡´ì¬í•˜ëŠ”ë“¯ í•˜ë‹¤. ë¬¼ë¦¬í•™ê³¼ êµìˆ˜ë‹˜ê»˜ ìë¬¸ì„ ìš”ì²­í•œ ê²°ê³¼ ììœ ë‚™í•˜ì— ê±¸ë¦¬ëŠ” ì‹œê°„ì€ \\(\\sqrt{h}\\)ì— ë¹„ë¡€í•¨ì„ ì•Œ ìˆ˜ ìˆì—ˆê³  ì´ë¥¼ ê·¼ê±°ë¡œ ì•„ë˜ì™€ ê°™ì€ ëª¨í˜•ì„ ì„¤ê³„í•˜ì˜€ë‹¤.\n\\[t_i = \\beta_0 + \\beta_1 \\sqrt{h_i}+\\epsilon_i, \\quad \\epsilon_i \\sim {\\cal N}(0,\\sigma^2)\\]\nìœ„ì˜ ëª¨í˜•ì„ í™œìš©í•˜ì—¬ ë†’ì´ \\(h\\)ë¡œë¶€í„° ë‚™í•˜ì‹œê°„ \\(t\\)ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ì‹ ê²½ë§ ëª¨ë¸ì„ ì„¤ê³„í•˜ê³  í•™ìŠµí•˜ë¼. í•™ìŠµí•œ ì‹ ê²½ë§ ëª¨ë¸ì„ í™œìš©í•˜ì—¬ ë†’ì´ 40m,60m,80m ì—ì„œ ë¬¼ì²´ë¥¼ ììœ ë‚™í•˜ ì‹œì¼°ì„ë•Œ ì§€ë©´ì— ë„ë‹¬í•˜ê¸°ê¹Œì§€ ê±¸ë¦¬ëŠ” ì‹œê°„ì„ ê°ê° ì˜ˆì¸¡í•˜ë¼.\n(í’€ì´)\nğŸ—£ï¸(\n\nh â€“&gt; sqrt â€“&gt; linr â€“&gt; t_hat\n\n\nclass FreeFallNet(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linr = torch.nn.Linear(1,1)\n    def forward(self,h):\n        netout = self.linr(torch.sqrt(h))\n        return netout\n\n\nnet = FreeFallNet()\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n#---#\nfor epoc in range(2000):\n    #1\n    netout = net(h)\n    #2\n    loss = loss_fn(netout,t)\n    #3\n    loss.backward()\n    #4\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(h,t,'o',alpha=0.5)\nplt.xlabel('Height (m)')\nplt.ylabel('Time to fall (sec)')\nplt.title('Free Fall Time vs Height')\nplt.plot(h,net(h).data,'--')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nnet = FreeFallNet()\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n#---#\nfor epoc in range(10000):\n    #1\n    netout = net(h)\n    #2\n    loss = loss_fn(netout,t)\n    #3\n    loss.backward()\n    #4\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(h,t,'o',alpha=0.5)\nplt.xlabel('Height (m)')\nplt.ylabel('Time to fall (sec)')\nplt.title('Free Fall Time vs Height')\nplt.plot(h,net(h).data,'--')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nì½”ë“œê°€ ê¹”ë”í•¨\n\n\nhh = torch.tensor([20,30,40,50,60,70]).reshape(6,1)\nnet(hh)\n\ntensor([[2.0253],\n        [2.4746],\n        [2.8534],\n        [3.1872],\n        [3.4889],\n        [3.7664]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\n# ì§€ë‚œ ì‹œê°„ ì½”ë“œ (xx ì¶”ê°€ë¨)\nhh = torch.tensor([40,60,80]).float().reshape(3,1)\nxx = torch.sqrt(hh)\nnet(xx)\n)ğŸ—£ï¸\n\nclass FreeFallNet(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linr = torch.nn.Linear(1,1)\n    def forward(self,h):\n        netout = self.linr(torch.sqrt(h))\n        return netout    \n\n\nnet = FreeFallNet()\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n#---#\nfor epoc in range(10000):\n    #1\n    netout = net(h)\n    #2\n    loss = loss_fn(netout,t)\n    #3\n    loss.backward()\n    #4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(h,t,'o',alpha=0.5)\nplt.xlabel('Height (m)')\nplt.ylabel('Time to fall (sec)')\nplt.title('Free Fall Time vs Height')\nplt.plot(h,net(h).data,'--')\n\n\n\n\n\n\n\n\n\nhh = torch.tensor([20,30,40,50,60,70]).reshape(6,1)\nnet(hh)\n\ntensor([[2.0253],\n        [2.4746],\n        [2.8534],\n        [3.1872],\n        [3.4889],\n        [3.7664]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\n#"
  },
  {
    "objectID": "posts/11wk-1.html#b.-mf-based-ì¶”ì²œì‹œìŠ¤í…œ-ì¬ì„¤ê³„-1",
    "href": "posts/11wk-1.html#b.-mf-based-ì¶”ì²œì‹œìŠ¤í…œ-ì¬ì„¤ê³„-1",
    "title": "11wk-1: (ì¶”ì²œì‹œìŠ¤í…œ) â€“ Embedding ë ˆì´ì–´, ì‚¬ìš©ìì •ì˜ ë„¤íŠ¸ì›Œí¬, MF-based ì¶”ì²œì‹œìŠ¤í…œì„ ë„˜ì–´ì„œ",
    "section": "B. MF-based ì¶”ì²œì‹œìŠ¤í…œ ì¬ì„¤ê³„",
    "text": "B. MF-based ì¶”ì²œì‹œìŠ¤í…œ ì¬ì„¤ê³„\nì•„ë˜ì˜ ìë£Œë¥¼ í™œìš©í•˜ì—¬ ì¶”ì²œì‹œìŠ¤í…œì„ ì„¤ê³„í•˜ê³ ìí•œë‹¤.\n\ndf_view = pd.read_csv('https://raw.githubusercontent.com/guebin/DL2024/main/posts/solo.csv',index_col=0)\ndf_view\n\n\n\n\n\n\n\n\nì˜ì‹(IN)\nì˜ì² (IN)\nì˜í˜¸(IS)\nê´‘ìˆ˜(IS)\nìƒì² (EN)\nì˜ìˆ˜(EN)\nê·œë¹ˆ(ES)\në‹¤í˜¸(ES)\n\n\n\n\nì˜¥ìˆœ(IN)\nNaN\n4.02\n3.45\n3.42\n0.84\n1.12\n0.43\n0.49\n\n\nì˜ì(IN)\n3.93\n3.99\n3.63\n3.43\n0.98\n0.96\n0.52\nNaN\n\n\nì •ìˆ™(IS)\n3.52\n3.42\n4.05\n4.06\n0.39\nNaN\n0.93\n0.99\n\n\nì˜ìˆ™(IS)\n3.43\n3.57\nNaN\n3.95\n0.56\n0.52\n0.89\n0.89\n\n\nìˆœì(EN)\n1.12\nNaN\n0.59\n0.43\n4.01\n4.16\n3.52\n3.38\n\n\ní˜„ìˆ™(EN)\n0.94\n1.05\n0.32\n0.45\n4.02\n3.78\nNaN\n3.54\n\n\nì„œì—°(ES)\n0.51\n0.56\n0.88\n0.89\n3.50\n3.64\n4.04\n4.10\n\n\në³´ëŒ(ES)\n0.48\n0.51\n1.03\nNaN\n3.52\n4.00\n3.82\nNaN\n\n\ní•˜ë‹ˆ(I)\n4.85\n4.82\nNaN\n4.98\n4.53\n4.39\n4.45\n4.52\n\n\n\n\n\n\n\nì‚¬ìš©ìì •ì˜ ë„¤íŠ¸ì›Œí¬ë¥¼ ì´ìš©í•˜ì—¬ MF-based ì¶”ì²œì‹œìŠ¤í…œì„ ì„¤ê³„í•˜ë¼.\n(í’€ì´1) â€“ net(x1,x2)\nğŸ—£ï¸(\n\nì´ì „ í’€ì´ê°€ ë„ˆë¬´ ë³µì¡í•˜ì—¬ yhatì„ ë‹¤ìŒê³¼ ê°™ì´ ë°”ê¾¸ê³  ì‹¶ìŒ\n\n#df_view\nloss_fn = torch.nn.MSELoss() \nebdd1 = torch.nn.Embedding(9,2)\nebdd2 = torch.nn.Embedding(8,2)\nb1 = torch.nn.Embedding(9,1)\nb2 = torch.nn.Embedding(8,1)\nparams = list(ebdd1.parameters()) + list(ebdd2.parameters())  + list(b1.parameters()) + list(b2.parameters())\noptimizr = torch.optim.Adam(params)\nsig = torch.nn.Sigmoid()\n#----#\nfor epoc in range(5000):\n    #step1\n    # W_features = ebdd1(x1) \n    # M_features = ebdd2(x2) \n    # W_bias = b1(x1)\n    # M_bais = b2(x2)\n    # yhat = sig((W_features * M_features).sum(axis=1).reshape(-1,1) + W_bias + M_bais)*5\n    yhat = net(x1,x2)\n    #step2\n    loss = loss_fn(yhat,y)\n    #step3\n    loss.backward()\n    #step4\n    optimizr.step()\n    optimizr.zero_grad()\n\nì´ì— ë§ì¶° networkë¥¼ ì„¤ê³„í•˜ë©´\n\n\n#df_view\nclass MFbased1(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.ebdd1 = torch.nn.Embedding(9,2)\n        self.ebdd2 = torch.nn.Embedding(8,2)\n        self.b1 = torch.nn.Embedding(9,1)\n        self.b2 = torch.nn.Embedding(8,1)\n        self.sig = torch.nn.Sigmoid()\n    def forward(self,x1,x2):\n        W_features = self.ebdd1(x1) \n        M_features = self.ebdd2(x2) \n        W_bias = self.b1(x1)\n        M_bais = self.b2(x2)\n        yhat = self.sig((W_features * M_features).sum(axis=1).reshape(-1,1) + W_bias + M_bais)*5\n        return yhat\n\nnet = MFbased1()\nloss_fn = torch.nn.MSELoss() \n# params = list(ebdd1.parameters()) + list(ebdd2.parameters())  + list(b1.parameters()) + list(b2.parameters())\n# ì´ë ‡ê²Œ netì„ ì„ ì–¸í•˜ë©´ net.parameters()ì™€ ë™ì¼\noptimizr = torch.optim.Adam(net.parameters())\n#----#\nfor epoc in range(10000):\n    #step1\n    yhat = net(x1,x2)\n    #step2\n    loss = loss_fn(yhat,y)\n    #step3\n    loss.backward()\n    #step4\n    optimizr.step()\n    optimizr.zero_grad()\n\n\ny[:5], yhat[:5] # ì˜ ì í•©ë¨\n\n(tensor([[4.0200],\n         [3.4500],\n         [3.4200],\n         [0.8400],\n         [1.1200]]),\n tensor([[4.0793],\n         [3.4667],\n         [3.3657],\n         [0.9102],\n         [0.9532]], grad_fn=&lt;SliceBackward0&gt;))\n\n\n\n# list(net.parameters()) # parameter == ì´ì „ params í™•ì¸ ì½”ë“œ\n\n)ğŸ—£ï¸\n\n#df_view\nclass MFbased1(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.ebdd1 = torch.nn.Embedding(9,2)\n        self.ebdd2 = torch.nn.Embedding(8,2)\n        self.b1 = torch.nn.Embedding(9,1)\n        self.b2 = torch.nn.Embedding(8,1)        \n        self.sig = torch.nn.Sigmoid()\n    def forward(self,x1,x2):\n        W_features = self.ebdd1(x1) \n        M_features = self.ebdd2(x2) \n        W_bias = self.b1(x1)\n        M_bais = self.b2(x2)\n        yhat = self.sig((W_features * M_features).sum(axis=1).reshape(-1,1) + W_bias + M_bais)*5        \n        return yhat\nnet = MFbased1()\nloss_fn = torch.nn.MSELoss() \noptimizr = torch.optim.Adam(net.parameters())\n#----#\nfor epoc in range(10000):\n    #step1\n    yhat = net(x1,x2)\n    #step2\n    loss = loss_fn(yhat,y)\n    #step3\n    loss.backward()\n    #step4\n    optimizr.step()\n    optimizr.zero_grad()\n\n\ny[:5], yhat[:5]\n\n(tensor([[4.0200],\n         [3.4500],\n         [3.4200],\n         [0.8400],\n         [1.1200]]),\n tensor([[4.0800],\n         [3.4664],\n         [3.3650],\n         [0.9111],\n         [0.9538]], grad_fn=&lt;SliceBackward0&gt;))\n\n\n(í’€ì´2) â€“ net(X)\nğŸ—£ï¸(\n\nx1, x2\n\n(tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3,\n         3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6,\n         6, 6, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8]),\n tensor([1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 6, 7, 0, 1, 3,\n         4, 5, 6, 7, 0, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 7, 0, 1, 2, 3, 4, 5,\n         6, 7, 0, 1, 2, 4, 5, 6, 0, 1, 3, 4, 5, 6, 7]))\n\n\n\nX = torch.stack([x1,x2],axis=1) \n\n\nclass MFbased2(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.ebdd1 = torch.nn.Embedding(9,2)\n        self.ebdd2 = torch.nn.Embedding(8,2)\n        self.b1 = torch.nn.Embedding(9,1)\n        self.b2 = torch.nn.Embedding(8,1)        \n        self.sig = torch.nn.Sigmoid()\n    def forward(self,X):\n        x1 = X[:,0] # ë¶„ë¦¬\n        x2 = X[:,1]\n        W_features = self.ebdd1(x1) \n        M_features = self.ebdd2(x2) \n        W_bias = self.b1(x1)\n        M_bais = self.b2(x2)\n        yhat = self.sig((W_features * M_features).sum(axis=1).reshape(-1,1) + W_bias + M_bais)*5        \n        return yhat\nnet = MFbased2()\nloss_fn = torch.nn.MSELoss() \noptimizr = torch.optim.Adam(net.parameters())\n#----#\nfor epoc in range(10000):\n    #step1\n    yhat = net(X)\n    #step2\n    loss = loss_fn(yhat,y)\n    #step3\n    loss.backward()\n    #step4\n    optimizr.step()\n    optimizr.zero_grad()\n\n\ny[:5], yhat[:5] # ì˜ ì í•©ë¨\n\n(tensor([[4.0200],\n         [3.4500],\n         [3.4200],\n         [0.8400],\n         [1.1200]]),\n tensor([[4.0800],\n         [3.4664],\n         [3.3651],\n         [0.9111],\n         [0.9538]], grad_fn=&lt;SliceBackward0&gt;))\n\n\n\nì‚¬ìš©ì ì •ì˜ ë„¤íŠ¸ì›Œí¬ë¥¼ ì´ìš©í•˜ë©´ ì§€ì €ë¶„í•œ ë¶€ë¶„ì„ ìœ„ì—(class) ëª°ì•„ë„£ê³  ì•„ë˜ëŠ” ê¹”ë”í•˜ê²Œ ì •ë¦¬í•  ìˆ˜ ìˆìŒ\n\n)ğŸ—£ï¸\n\nX = torch.stack([x1,x2],axis=1) \n\n\nclass MFbased2(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.ebdd1 = torch.nn.Embedding(9,2)\n        self.ebdd2 = torch.nn.Embedding(8,2)\n        self.b1 = torch.nn.Embedding(9,1)\n        self.b2 = torch.nn.Embedding(8,1)        \n        self.sig = torch.nn.Sigmoid()\n    def forward(self,X):\n        x1 = X[:,0]\n        x2 = X[:,1]\n        W_features = self.ebdd1(x1) \n        M_features = self.ebdd2(x2) \n        W_bias = self.b1(x1)\n        M_bais = self.b2(x2)\n        yhat = self.sig((W_features * M_features).sum(axis=1).reshape(-1,1) + W_bias + M_bais)*5        \n        return yhat\nnet = MFbased2()\nloss_fn = torch.nn.MSELoss() \noptimizr = torch.optim.Adam(net.parameters())\n#----#\nfor epoc in range(10000):\n    #step1\n    yhat = net(X)\n    #step2\n    loss = loss_fn(yhat,y)\n    #step3\n    loss.backward()\n    #step4\n    optimizr.step()\n    optimizr.zero_grad()\n\n\ny[:5], yhat[:5]\n\n(tensor([[4.0200],\n         [3.4500],\n         [3.4200],\n         [0.8400],\n         [1.1200]]),\n tensor([[4.0800],\n         [3.4664],\n         [3.3651],\n         [0.9111],\n         [0.9538]], grad_fn=&lt;SliceBackward0&gt;))"
  },
  {
    "objectID": "posts/11wk-1.html#a.-nn-based-ë°©ì‹",
    "href": "posts/11wk-1.html#a.-nn-based-ë°©ì‹",
    "title": "11wk-1: (ì¶”ì²œì‹œìŠ¤í…œ) â€“ Embedding ë ˆì´ì–´, ì‚¬ìš©ìì •ì˜ ë„¤íŠ¸ì›Œí¬, MF-based ì¶”ì²œì‹œìŠ¤í…œì„ ë„˜ì–´ì„œ",
    "section": "A. NN-based ë°©ì‹",
    "text": "A. NN-based ë°©ì‹\nì•„ë˜ì˜ ìë£Œë¥¼ í™œìš©í•˜ì—¬ ì¶”ì²œì‹œìŠ¤í…œì„ ì„¤ê³„í•˜ê³ ìí•œë‹¤.\n\ndf_view = pd.read_csv('https://raw.githubusercontent.com/guebin/DL2025/main/posts/iamsolo.csv',index_col=0)\ndf_view\n\n\n\n\n\n\n\n\nì˜ì‹(IN)\nì˜ì² (IN)\nì˜í˜¸(IS)\nê´‘ìˆ˜(IS)\nìƒì² (EN)\nì˜ìˆ˜(EN)\nê·œë¹ˆ(ES)\në‹¤í˜¸(ES)\n\n\n\n\nì˜¥ìˆœ(IN)\nNaN\n4.02\n3.45\n3.42\n0.84\n1.12\n0.43\n0.49\n\n\nì˜ì(IN)\n3.93\n3.99\n3.63\n3.43\n0.98\n0.96\n0.52\nNaN\n\n\nì •ìˆ™(IS)\n3.52\n3.42\n4.05\n4.06\n0.39\nNaN\n0.93\n0.99\n\n\nì˜ìˆ™(IS)\n3.43\n3.57\nNaN\n3.95\n0.56\n0.52\n0.89\n0.89\n\n\nìˆœì(EN)\n1.12\nNaN\n0.59\n0.43\n4.01\n4.16\n3.52\n3.38\n\n\ní˜„ìˆ™(EN)\n0.94\n1.05\n0.32\n0.45\n4.02\n3.78\nNaN\n3.54\n\n\nì„œì—°(ES)\n0.51\n0.56\n0.88\n0.89\n3.50\n3.64\n4.04\n4.10\n\n\në³´ëŒ(ES)\n0.48\n0.51\n1.03\nNaN\n3.52\n4.00\n3.82\nNaN\n\n\ní•˜ë‹ˆ(I)\n4.85\n4.82\nNaN\n4.98\n4.53\n4.39\n4.45\n4.52\n\n\n\n\n\n\n\n\n#df_view\ndf_train = df_view.stack().reset_index().set_axis(['ì—¬ì„±ì¶œì—°ì','ë‚¨ì„±ì¶œì—°ì','ê¶í•©ì ìˆ˜'],axis=1)\nì—¬ì„±ì¸ë±ìŠ¤ = {'ì˜¥ìˆœ(IN)':0, 'ì˜ì(IN)':1, 'ì •ìˆ™(IS)':2, 'ì˜ìˆ™(IS)':3, 'ìˆœì(EN)':4, 'í˜„ìˆ™(EN)':5, 'ì„œì—°(ES)':6, 'ë³´ëŒ(ES)':7, 'í•˜ë‹ˆ(I)':8}\në‚¨ì„±ì¸ë±ìŠ¤ = {'ì˜ì‹(IN)':0, 'ì˜ì² (IN)':1, 'ì˜í˜¸(IS)':2, 'ê´‘ìˆ˜(IS)':3, 'ìƒì² (EN)':4, 'ì˜ìˆ˜(EN)':5, 'ê·œë¹ˆ(ES)':6, 'ë‹¤í˜¸(ES)':7}\nx1 = torch.tensor(df_train.ì—¬ì„±ì¶œì—°ì.map(ì—¬ì„±ì¸ë±ìŠ¤))\nx2 = torch.tensor(df_train.ë‚¨ì„±ì¶œì—°ì.map(ë‚¨ì„±ì¸ë±ìŠ¤))\ny = torch.tensor(df_train.ê¶í•©ì ìˆ˜).reshape(-1,1).float()\n\nNN-based ì¶”ì²œì‹œìŠ¤í…œì„ ì„¤ê³„í•˜ë¼.\n(í’€ì´1) â€“ ì‹¤íŒ¨\nğŸ—£ï¸(\n\nclass NNbased1(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.ebdd1 = torch.nn.Embedding(9,2)\n        self.ebdd2 = torch.nn.Embedding(8,2)\n        self.b1 = torch.nn.Embedding(9,1)\n        self.b2 = torch.nn.Embedding(8,1)\n        self.sig = torch.nn.Sigmoid()\n        self.mlp = torch.nn.Sequential(\n            torch.nn.Linear(6,1),\n            torch.nn.Sigmoid()\n        )\n    def forward(self,x1,x2):\n        W_features = self.ebdd1(x1) \n        M_features = self.ebdd2(x2) \n        W_bias = self.b1(x1)\n        M_bias = self.b2(x2)\n        # yhat = self.sig((W_features * M_features).sum(axis=1).reshape(-1,1) + W_bias + M_bais)*5\n        Z = torch.concat([W_features, M_features, W_bias, M_bias], axis=1)\n        # Z ---&gt; yhat (n,6) -&gt; (n,1)\n        yhat = self.mlp(Z) * 5\n        return yhat\n\n\nnet = NNbased1()\nloss_fn = torch.nn.MSELoss() \noptimizr = torch.optim.Adam(net.parameters())\n#----#\nfor epoc in range(5000):\n    #step1\n    yhat = net(x1,x2)\n    #step2\n    loss = loss_fn(yhat,y)\n    #step3\n    loss.backward()\n    #step4\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nyhat[:5], y[:5] # ì „í˜€ ë§ì§€ ì•ŠìŒ\n\n(tensor([[2.1566],\n         [1.6486],\n         [2.0613],\n         [1.9107],\n         [2.3132]], grad_fn=&lt;SliceBackward0&gt;),\n tensor([[4.0200],\n         [3.4500],\n         [3.4200],\n         [0.8400],\n         [1.1200]]))\n\n\n\nëª¨ë¸ì´ ë„ˆë¬´ ë‹¨ìˆœí•´ì„œ?\n\n)ğŸ—£ï¸\n\nclass NNbased1(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        #--#\n        self.ebdd1 = torch.nn.Embedding(9,2)\n        self.ebdd2 = torch.nn.Embedding(8,2)\n        self.b1 = torch.nn.Embedding(9,1)\n        self.b2 = torch.nn.Embedding(8,1)\n        self.sig = torch.nn.Sigmoid()\n        self.mlp = torch.nn.Sequential(\n            torch.nn.Linear(6,1),\n            torch.nn.Sigmoid()\n        )\n    def forward(self,x1,x2):\n        W_feature = self.ebdd1(x1)\n        W_bias = self.b1(x1)\n        M_feature = self.ebdd2(x2)\n        M_bias = self.b2(x2)\n        #yhat = sig((W_feature * M_feature).sum(axis=1).reshape(-1,1) + W_bias + M_bias ) * 5 \n        Z = torch.concat([W_feature, M_feature, W_bias, M_bias],axis=1)\n        yhat = self.mlp(Z) * 5 \n        return yhat\n\n\nnet = NNbased1()\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters(),lr=0.1) # ì´ê²Œ í¸í•´ìš”!!\n#--# \nfor epoc in range(5000):\n    # 1\n    yhat = net(x1,x2) \n    # 2\n    loss = loss_fn(yhat,y)\n    # 3 \n    loss.backward()\n    # 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nyhat[:5], y[:5]\n\n(tensor([[2.1563],\n         [1.6488],\n         [2.0608],\n         [1.9119],\n         [2.3126]], grad_fn=&lt;SliceBackward0&gt;),\n tensor([[4.0200],\n         [3.4500],\n         [3.4200],\n         [0.8400],\n         [1.1200]]))\n\n\n(í’€ì´2) â€“ ì—ë¼ ëª¨ë¥´ê² ë‹¤ ê¹Šì€ì‹ ê²½ë§..\nğŸ—£ï¸(\n\nclass NNbased2(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.ebdd1 = torch.nn.Embedding(9,2)\n        self.ebdd2 = torch.nn.Embedding(8,2)\n        self.b1 = torch.nn.Embedding(9,1)\n        self.b2 = torch.nn.Embedding(8,1)\n        self.sig = torch.nn.Sigmoid()\n        self.mlp = torch.nn.Sequential(\n            torch.nn.Linear(6,15),\n            torch.nn.ReLU(),\n            torch.nn.Linear(15,15),\n            torch.nn.ReLU(),\n            torch.nn.Linear(15,1),\n            torch.nn.Sigmoid()\n        )\n    def forward(self,x1,x2):\n        W_features = self.ebdd1(x1) \n        M_features = self.ebdd2(x2) \n        W_bias = self.b1(x1)\n        M_bias = self.b2(x2)\n        # yhat = self.sig((W_features * M_features).sum(axis=1).reshape(-1,1) + W_bias + M_bais)*5\n        Z = torch.concat([W_features, M_features, W_bias, M_bias], axis=1)\n        # Z ---&gt; yhat (n,6) -&gt; (n,1)\n        yhat = self.mlp(Z) * 5\n        return yhat\n\n\nnet = NNbased2()\nloss_fn = torch.nn.MSELoss() \noptimizr = torch.optim.Adam(net.parameters())\n#----#\nfor epoc in range(5000):\n    #step1\n    yhat = net(x1,x2)\n    #step2\n    loss = loss_fn(yhat,y)\n    #step3\n    loss.backward()\n    #step4\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nyhat[:5], y[:5] # ë„ˆë¬´ ì˜ ë§ëŠ” ëŠë‚Œ (overfitting ì˜ì‹¬)\n\n(tensor([[4.0179],\n         [3.4466],\n         [3.4170],\n         [0.8365],\n         [1.1180]], grad_fn=&lt;SliceBackward0&gt;),\n tensor([[4.0200],\n         [3.4500],\n         [3.4200],\n         [0.8400],\n         [1.1200]]))\n\n\n(ì˜¥ìˆœ-ì˜ì‹), (ì˜ì-ë‹¤í˜¸), (í•˜ë‹ˆ-ì˜í˜¸) ë¥¼ ì˜ˆì¸¡í•´ë³´ì.\n\ndf_view\n\n\n\n\n\n\n\n\nì˜ì‹(IN)\nì˜ì² (IN)\nì˜í˜¸(IS)\nê´‘ìˆ˜(IS)\nìƒì² (EN)\nì˜ìˆ˜(EN)\nê·œë¹ˆ(ES)\në‹¤í˜¸(ES)\n\n\n\n\nì˜¥ìˆœ(IN)\nNaN\n4.02\n3.45\n3.42\n0.84\n1.12\n0.43\n0.49\n\n\nì˜ì(IN)\n3.93\n3.99\n3.63\n3.43\n0.98\n0.96\n0.52\nNaN\n\n\nì •ìˆ™(IS)\n3.52\n3.42\n4.05\n4.06\n0.39\nNaN\n0.93\n0.99\n\n\nì˜ìˆ™(IS)\n3.43\n3.57\nNaN\n3.95\n0.56\n0.52\n0.89\n0.89\n\n\nìˆœì(EN)\n1.12\nNaN\n0.59\n0.43\n4.01\n4.16\n3.52\n3.38\n\n\ní˜„ìˆ™(EN)\n0.94\n1.05\n0.32\n0.45\n4.02\n3.78\nNaN\n3.54\n\n\nì„œì—°(ES)\n0.51\n0.56\n0.88\n0.89\n3.50\n3.64\n4.04\n4.10\n\n\në³´ëŒ(ES)\n0.48\n0.51\n1.03\nNaN\n3.52\n4.00\n3.82\nNaN\n\n\ní•˜ë‹ˆ(I)\n4.85\n4.82\nNaN\n4.98\n4.53\n4.39\n4.45\n4.52\n\n\n\n\n\n\n\n{'ì˜¥ìˆœ(IN)': 0, 'ì˜ì(IN)': 1, 'ì •ìˆ™(IS)': 2, 'ì˜ìˆ™(IS)': 3, 'ìˆœì(EN)': 4,\n 'í˜„ìˆ™(EN)': 5, 'ì„œì—°(ES)': 6, 'ë³´ëŒ(ES)': 7, 'í•˜ë‹ˆ(I)': 8}\n\n{'ì˜ì‹(IN)': 0, 'ì˜ì² (IN)': 1, 'ì˜í˜¸(IS)': 2, 'ê´‘ìˆ˜(IS)': 3, 'ìƒì² (EN)': 4,\n 'ì˜ìˆ˜(EN)': 5, 'ê·œë¹ˆ(ES)': 6, 'ë‹¤í˜¸(ES)': 7}\n\nxx1 = torch.tensor([0,1,8])\nxx2 = torch.tensor([0,7,2])\n\n\nnet(xx1,xx2)\n\ntensor([[3.3455],\n        [1.3572],\n        [4.9131]], grad_fn=&lt;MulBackward0&gt;)\n\n\n\n4, 0.5, 5ê·¼ì²˜ ì •ë„ê°€ ë§ëŠ” ê²ƒ ê°™ìŒ (overfitting)\nepochì„ 2,000ìœ¼ë¡œ ì¤„ì´ë©´\n\n\nnet = NNbased2()\nloss_fn = torch.nn.MSELoss() \noptimizr = torch.optim.Adam(net.parameters())\n#----#\nfor epoc in range(2000):\n    #step1\n    yhat = net(x1,x2)\n    #step2\n    loss = loss_fn(yhat,y)\n    #step3\n    loss.backward()\n    #step4\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nyhat[:5], y[:5]\n\n(tensor([[4.0040],\n         [3.4680],\n         [3.4130],\n         [0.8093],\n         [1.1156]], grad_fn=&lt;SliceBackward0&gt;),\n tensor([[4.0200],\n         [3.4500],\n         [3.4200],\n         [0.8400],\n         [1.1200]]))\n\n\n\nnet(xx1,xx2) # ë¹„ìŠ·í•¨\n\ntensor([[3.8184],\n        [0.5261],\n        [4.9015]], grad_fn=&lt;MulBackward0&gt;)\n\n\n\nâœï¸ ê°•ì˜ ì˜ìƒ(overfitting)ê³¼ ë‹¤ë¥´ê²Œ ì˜ ë˜ê¸´ í•˜ì˜€ìŒ\nê°•ì˜ ì˜ìƒì²˜ëŸ¼ ëª¨í˜• ë‹¨ìˆœí™”\n\n\nclass NNbased2(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.ebdd1 = torch.nn.Embedding(9,2)\n        self.ebdd2 = torch.nn.Embedding(8,2)\n        self.b1 = torch.nn.Embedding(9,1)\n        self.b2 = torch.nn.Embedding(8,1)\n        self.sig = torch.nn.Sigmoid()\n        self.mlp = torch.nn.Sequential(\n            torch.nn.Linear(6,15),\n            torch.nn.ReLU(),\n            torch.nn.Linear(15,1),\n            torch.nn.Sigmoid()\n        )\n    def forward(self,x1,x2):\n        W_features = self.ebdd1(x1) \n        M_features = self.ebdd2(x2) \n        W_bias = self.b1(x1)\n        M_bias = self.b2(x2)\n        # yhat = self.sig((W_features * M_features).sum(axis=1).reshape(-1,1) + W_bias + M_bais)*5\n        Z = torch.concat([W_features, M_features, W_bias, M_bias], axis=1)\n        # Z ---&gt; yhat (n,6) -&gt; (n,1)\n        yhat = self.mlp(Z) * 5\n        return yhat\n\n\nnet = NNbased2()\nloss_fn = torch.nn.MSELoss() \noptimizr = torch.optim.Adam(net.parameters())\n#----#\nfor epoc in range(2000):\n    #step1\n    yhat = net(x1,x2)\n    #step2\n    loss = loss_fn(yhat,y)\n    #step3\n    loss.backward()\n    #step4\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nyhat[:5], y[:5]\n\n(tensor([[3.6244],\n         [3.5508],\n         [3.5255],\n         [0.7445],\n         [1.0485]], grad_fn=&lt;SliceBackward0&gt;),\n tensor([[4.0200],\n         [3.4500],\n         [3.4200],\n         [0.8400],\n         [1.1200]]))\n\n\n\nnet(xx1,xx2)\n\ntensor([[3.5422],\n        [0.9444],\n        [4.9080]], grad_fn=&lt;MulBackward0&gt;)\n\n\n\nâœï¸ ì´ë²ˆì—ëŠ” ê°•ì˜ ì˜ìƒê³¼ ë‹¤ë¥´ê²Œ ì˜ ì•ˆë¨\në°‘ì˜ ê°•ì˜ ë…¸íŠ¸ ì½”ë“œ: ëœë¤ìœ¼ë¡œ í•´ë„ ì•ˆì •ì ìœ¼ë¡œ ê²°ê³¼ê°€ ì˜ ë‚˜ì˜´\nepochì„ ì¤„ì´ê¸°ë§Œ í•˜ì—¬ë„ overfitting ë°©ì§€ íš¨ê³¼ê°€ ìˆìŒ (early stopping)\n\n)ğŸ—£ï¸\n\nclass NNbased2(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        #--#\n        self.ebdd1 = torch.nn.Embedding(9,2)\n        self.ebdd2 = torch.nn.Embedding(8,2)\n        self.b1 = torch.nn.Embedding(9,1)\n        self.b2 = torch.nn.Embedding(8,1)\n        self.sig = torch.nn.Sigmoid()\n        self.mlp = torch.nn.Sequential(\n            torch.nn.Linear(6,15),\n            torch.nn.ReLU(),\n            torch.nn.Linear(15,1),\n            torch.nn.Sigmoid()\n        )\n    def forward(self,x1,x2):\n        W_feature = self.ebdd1(x1)\n        W_bias = self.b1(x1)\n        M_feature = self.ebdd2(x2)\n        M_bias = self.b2(x2)\n        #yhat = sig((W_feature * M_feature).sum(axis=1).reshape(-1,1) + W_bias + M_bias ) * 5 \n        Z = torch.concat([W_feature, M_feature, W_bias, M_bias],axis=1)\n        yhat = self.mlp(Z) * 5 \n        return yhat\n\n\nnet = NNbased2()\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters(),lr=0.1)\n#--# \nfor epoc in range(3000):\n    # 1\n    yhat = net(x1,x2) \n    # 2\n    loss = loss_fn(yhat,y)\n    # 3 \n    loss.backward()\n    # 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nyhat[:10], y[:10]\n\n(tensor([[4.0395],\n         [3.4880],\n         [3.4527],\n         [0.8548],\n         [1.1290],\n         [0.4416],\n         [0.5041],\n         [3.9470],\n         [4.0075],\n         [3.6544]], grad_fn=&lt;SliceBackward0&gt;),\n tensor([[4.0200],\n         [3.4500],\n         [3.4200],\n         [0.8400],\n         [1.1200],\n         [0.4300],\n         [0.4900],\n         [3.9300],\n         [3.9900],\n         [3.6300]]))\n\n\n(ì˜¥ìˆœ-ì˜ì‹), (ì˜ì-ë‹¤í˜¸), (í•˜ë‹ˆ-ì˜í˜¸) ë¥¼ ì˜ˆì¸¡í•´ë³´ì.\n\ndf_view\n\n\n\n\n\n\n\n\nì˜ì‹(IN)\nì˜ì² (IN)\nì˜í˜¸(IS)\nê´‘ìˆ˜(IS)\nìƒì² (EN)\nì˜ìˆ˜(EN)\nê·œë¹ˆ(ES)\në‹¤í˜¸(ES)\n\n\n\n\nì˜¥ìˆœ(IN)\nNaN\n4.02\n3.45\n3.42\n0.84\n1.12\n0.43\n0.49\n\n\nì˜ì(IN)\n3.93\n3.99\n3.63\n3.43\n0.98\n0.96\n0.52\nNaN\n\n\nì •ìˆ™(IS)\n3.52\n3.42\n4.05\n4.06\n0.39\nNaN\n0.93\n0.99\n\n\nì˜ìˆ™(IS)\n3.43\n3.57\nNaN\n3.95\n0.56\n0.52\n0.89\n0.89\n\n\nìˆœì(EN)\n1.12\nNaN\n0.59\n0.43\n4.01\n4.16\n3.52\n3.38\n\n\ní˜„ìˆ™(EN)\n0.94\n1.05\n0.32\n0.45\n4.02\n3.78\nNaN\n3.54\n\n\nì„œì—°(ES)\n0.51\n0.56\n0.88\n0.89\n3.50\n3.64\n4.04\n4.10\n\n\në³´ëŒ(ES)\n0.48\n0.51\n1.03\nNaN\n3.52\n4.00\n3.82\nNaN\n\n\ní•˜ë‹ˆ(I)\n4.85\n4.82\nNaN\n4.98\n4.53\n4.39\n4.45\n4.52\n\n\n\n\n\n\n\n\nxx1 = torch.tensor([0,1,8])\nxx2 = torch.tensor([0,7,2])\n\n\nnet(xx1,xx2)\n\ntensor([[3.9317],\n        [0.6682],\n        [4.9322]], grad_fn=&lt;MulBackward0&gt;)"
  },
  {
    "objectID": "posts/11wk-1.html#b.-ncf-he2017neural",
    "href": "posts/11wk-1.html#b.-ncf-he2017neural",
    "title": "11wk-1: (ì¶”ì²œì‹œìŠ¤í…œ) â€“ Embedding ë ˆì´ì–´, ì‚¬ìš©ìì •ì˜ ë„¤íŠ¸ì›Œí¬, MF-based ì¶”ì²œì‹œìŠ¤í…œì„ ë„˜ì–´ì„œ",
    "section": "B. NCF [@he2017neural]",
    "text": "B. NCF [@he2017neural]\n- MF-basedì™€ NN-baseë¥¼ í•©ì¹œê²ƒ\nğŸ—£ï¸(\nMF-based    íŠ¹ì§•1*íŠ¹ì§•2\nNN-based    [íŠ¹ì§•1, íŠ¹ì§•2] --&gt; nn\n\n# ê·¸ë¦¼ ì„¤ëª…\nUser: ë‚¨ì„± ì¶œì—°ì, Item: ì—¬ì„± ì¶œì—°ìë¡œ ìƒê°\n\nMF User Vector: ë‚¨ì„± ì¶œì—°ìì˜ íŠ¹ì§•, MF Item Vector: ì—¬ì„± ì¶œì—°ìì˜ íŠ¹ì§•\n(ìœ„ì˜ ë‘ íŠ¹ì§• ë²¡í„° ë§Œë“œëŠ” ë°©ì‹ì€ embedding layer: one hot encoding + linear)\nMF User Vector MF Item Vector --Element-wise Product--&gt; GMF Layer\nGMF Layerì—ì„œ Scoreë¡œ ë°”ë¡œ ê°€ë©´ MF-based\n\nMLP User Vector: ë‚¨ì„± ì¶œì—°ìì˜ íŠ¹ì§•, MLP Item Vector: ì—¬ì„± ì¶œì—°ìì˜ íŠ¹ì§•\nMLP User Vector MLP Item Vector ---concat---&gt;\n    ---ReLU ... Linear Transform ... ReLu---&gt; MLP Layer X\nMLP Layer Xì—ì„œ ë°”ë¡œ Scoreë¡œ ê°€ë©´ NN-based\n\nìš”ì¦˜ ë§ì´ ì“°ì´ëŠ” í˜¼í•© ë°©ì‹:\nGMF Layerì™€ MLP Layer Xë¡œ ì ë‹¹íˆ ì¡°í•©í•˜ì—¬ í•˜ë‚˜ì˜ Layerë¥¼ ë§Œë“¤ê³ \nLinear, Sigmoid ë“±ì„ ì´ìš©í•˜ì—¬ ë§ˆë¬´ë¦¬\n)ğŸ—£ï¸"
  },
  {
    "objectID": "posts/05wk-1.html",
    "href": "posts/05wk-1.html",
    "title": "05wk-1: (ì‹ ê²½ë§) â€“ ì˜ˆì¸¡, ì‹œë²¤ì½”ì •ë¦¬ì˜ ì´ë©´, ë“œëì•„ì›ƒ",
    "section": "",
    "text": "ğŸ“˜ Note Format Guide\nThis format serves as a structured guide for organizing lecture content, personal interpretation, experiments, and study-related questions.\nğŸ“ ğŸ—£ï¸ âœï¸ ğŸ”¬ â“"
  },
  {
    "objectID": "posts/05wk-1.html#a.-ë°ì´í„°",
    "href": "posts/05wk-1.html#a.-ë°ì´í„°",
    "title": "05wk-1: (ì‹ ê²½ë§) â€“ ì˜ˆì¸¡, ì‹œë²¤ì½”ì •ë¦¬ì˜ ì´ë©´, ë“œëì•„ì›ƒ",
    "section": "A. ë°ì´í„°",
    "text": "A. ë°ì´í„°\n\ntorch.manual_seed(43052)\nx,_ = torch.randn(100).sort()\neps = torch.randn(100)*0.5\ny = x * 4 + 2.5 + eps\nx,y = x.reshape(-1,1), y.reshape(-1,1)\n\n\nplt.plot(x,y,'o')"
  },
  {
    "objectID": "posts/05wk-1.html#b.-í•™ìŠµ",
    "href": "posts/05wk-1.html#b.-í•™ìŠµ",
    "title": "05wk-1: (ì‹ ê²½ë§) â€“ ì˜ˆì¸¡, ì‹œë²¤ì½”ì •ë¦¬ì˜ ì´ë©´, ë“œëì•„ì›ƒ",
    "section": "B. í•™ìŠµ",
    "text": "B. í•™ìŠµ\n\nğŸ—£ï¸ í˜„ì¬ ìˆ˜ì¤€ì—ì„œëŠ” ë‹¤ìŒì²˜ëŸ¼ ìƒê°\n\nyê°€ ì—°ì†: MSELoss\nyê°€ 0 ë˜ëŠ” 1ë§Œ: BCELoss\n\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1)\n)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters(),lr=0.1)\n## \nfor epoc in range(200):\n    ## step1 \n    yhat = net(x) \n    ## step2 \n    loss = loss_fn(yhat,y)\n    ## step3 \n    loss.backward()\n    ## step4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'o')\nplt.plot(x,net(x).data,'--')\n\n\n\n\n\n\n\n\n\nnet[0].weight, net[0].bias\n\n(Parameter containing:\n tensor([[4.0042]], requires_grad=True),\n Parameter containing:\n tensor([2.4459], requires_grad=True))"
  },
  {
    "objectID": "posts/05wk-1.html#c.-ì˜ˆì¸¡",
    "href": "posts/05wk-1.html#c.-ì˜ˆì¸¡",
    "title": "05wk-1: (ì‹ ê²½ë§) â€“ ì˜ˆì¸¡, ì‹œë²¤ì½”ì •ë¦¬ì˜ ì´ë©´, ë“œëì•„ì›ƒ",
    "section": "C. ì˜ˆì¸¡",
    "text": "C. ì˜ˆì¸¡\nì˜¨ë„ê°€ 0.1 ë„ì¼ë•Œ, ì»¤í”¼ë¥¼ ì–¼ë§ˆë‚˜ íŒ”ê¹Œ?\n\n0.1 * 4.0042 + 2.4459 \n\n2.84632\n\n\n\nxx = torch.tensor([[0.1]])\nnet(xx)\n\ntensor([[2.8463]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\nì˜¨ë„ê°€ 0.2ë„ì¼ë•Œ ì»¤í”¼ë¥¼ ì–¼ë§ˆë‚˜ íŒ”ê¹Œ?\n\n0.2 * 4.0042 + 2.4459 \n\n3.24674\n\n\n\nxx = torch.tensor([[0.2]])\nnet(xx)\n\ntensor([[3.2467]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\nì˜¨ë„ê°€ [0.1, 0.2] ì¼ë•Œì˜ ì˜ˆì¸¡ê°’ì„ í•œë²ˆì— ë³´ê³  ì‹¶ë‹¤ë©´?\n\nxx = torch.tensor([[0.1],\n                   [0.2]])\nnet(xx)\n\ntensor([[2.8463],\n        [3.2467]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\n\n\n\n\n\n\nNote\n\n\n\nì´ê±° ì§ˆë¬¸ì´ ì™€ì„œ ì¢€ ë” ìì„¸íˆ ì„¤ëª…í•˜ê² ìŠµë‹ˆë‹¤. (ì•„ì§ net(x)ì˜ ê³„ì‚° ê³¼ì •ì„ ì„ í˜• ë³€í™˜ ê´€ì ì—ì„œ ìˆ˜ì‹ìœ¼ë¡œ ì •ë¦¬í•˜ëŠ” ë° ìµìˆ™í•˜ì§€ ì•Šìœ¼ì…”ì„œ ê·¸ëŸ´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ê±´ ë‹¨ìˆœ ì‚°ìˆ˜ë¼ì„œ í•˜ë‚˜ì”© ì°¨ê·¼ì°¨ê·¼ ë”°ë¼ê°€ë‹¤ ë³´ë©´ ì¶©ë¶„íˆ ì´í•´í•˜ì‹¤ ìˆ˜ ìˆì–´ìš”. ì²˜ìŒë¶€í„° ë°”ë¡œ ì´í•´ë˜ì§€ ì•Šë”ë¼ë„ ì „í˜€ ê±±ì •í•˜ì‹¤ í•„ìš” ì—†ìŠµë‹ˆë‹¤.)\ní•˜ë‚˜ì˜ ê°’ \\(x\\)ì— ëŒ€í•˜ì—¬ \\(net(x)\\)ëŠ” ì•„ë˜ë¥¼ ì˜ë¯¸í•˜ëŠ” ì—°ì‚°ì„ í•©ë‹ˆë‹¤.\nnet(x) = 4.0042 * x + 2.4459  = net[0].weight * x + net[0].bias\nì‚¬ì‹¤ ìœ„ì˜ ê³¼ì •ì„ ìˆ˜ì‹ìœ¼ë¡œ ì—„ë°€í•˜ê²Œ ì“°ë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.\n\\[net(\\begin{bmatrix} x \\end{bmatrix}) = 2.4459 + \\begin{bmatrix} x \\end{bmatrix} \\begin{bmatrix} 4.0042 \\end{bmatrix}\\]\nì—¬ê¸°ì—ì„œ \\(\\begin{bmatrix} x \\end{bmatrix}\\) ì™€ \\(\\begin{bmatrix} 4.0042  \\end{bmatrix}\\) ëŠ” ëª¨ë‘ \\(1\\times 1\\) matrixë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ë§Œì•½ì— \\(2 \\times 1\\) matrix \\({\\bf x} = \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}\\)ë¥¼ ë„¤íŠ¸ì›Œí¬ì˜ ì…ë ¥ìœ¼ë¡œ ê³ ë ¤í•œë‹¤ë©´ ì•„ë˜ì™€ ê°™ì´ ë©ë‹ˆë‹¤.\n\\[net({\\bf x})=net\\left(\\begin{bmatrix}x_1 \\\\ x_2 \\end{bmatrix}\\right) = 2.4459 + \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} \\begin{bmatrix} 4.0042 \\end{bmatrix} = \\begin{bmatrix} 2.4459 + 4.0042 x_1 \\\\ 2.4459 + 4.0042 x_2\\end{bmatrix} \\]\në”°ë¼ì„œ \\({\\bf xx} = \\begin{bmatrix} 0.1 \\\\ 0.2 \\end{bmatrix}\\) ë¥¼ ë„¤íŠ¸ì›Œí¬ì˜ ì…ë ¥ìœ¼ë¡œ ë„£ìœ¼ë©´\n\\[net({\\bf xx})= \\begin{bmatrix} 2.4459 + 4.0042 \\times 0.1 \\\\ 2.4459 + 4.0042 \\times 0.2\\end{bmatrix}= \\begin{bmatrix} 2.8463 \\\\ 3.2467 \\end{bmatrix}\\]\nì™€ ê°™ì´ ê³„ì‚°ë˜ê² ì£ ."
  },
  {
    "objectID": "posts/05wk-1.html#a.-ì˜¤ë²„í”¼íŒ…",
    "href": "posts/05wk-1.html#a.-ì˜¤ë²„í”¼íŒ…",
    "title": "05wk-1: (ì‹ ê²½ë§) â€“ ì˜ˆì¸¡, ì‹œë²¤ì½”ì •ë¦¬ì˜ ì´ë©´, ë“œëì•„ì›ƒ",
    "section": "A. ì˜¤ë²„í”¼íŒ…",
    "text": "A. ì˜¤ë²„í”¼íŒ…\n- ì˜¤ë²„í”¼íŒ…ì´ë€?\n\nìœ„í‚¤: In mathematical modeling, overfitting is â€œthe production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit to additional data or predict future observations reliablyâ€. (ìˆ˜í•™ì  ëª¨ë¸ë§ì—ì„œ ê³¼ì í•©ì´ë€ â€œì–´ë–¤ ëª¨ë¸ì´ ì£¼ì–´ì§„ ë°ì´í„°ì— ë„ˆë¬´ ê¼­ ë§ì¶°ì ¸ ìˆì–´ì„œ, ìƒˆë¡œìš´ ë°ì´í„°ë‚˜ ë¯¸ë˜ì˜ ê²°ê³¼ë¥¼ ì˜ ì˜ˆì¸¡í•˜ì§€ ëª»í•  ìˆ˜ ìˆëŠ” ìƒíƒœâ€ë¥¼ ì˜ë¯¸í•œë‹¤.)\nì œ ê°œë…: ë°ì´í„°ë¥¼ â€œë°ì´í„° = ì–¸ë”ë¼ì‰ + ì˜¤ì°¨â€ë¼ê³  ìƒê°í• ë•Œ ìš°ë¦¬ê°€ ë°ì´í„°ë¡œë¶€í„° ì í•©í•  ê²ƒì€ ì–¸ë”ë¼ì‰ì¸ë° ì˜¤ì°¨í•­ì„ ì í•©í•˜ê³  ìˆëŠ” í˜„ìƒ."
  },
  {
    "objectID": "posts/05wk-1.html#b.-ì˜¤ë²„í”¼íŒ…-ì˜ˆì‹œ",
    "href": "posts/05wk-1.html#b.-ì˜¤ë²„í”¼íŒ…-ì˜ˆì‹œ",
    "title": "05wk-1: (ì‹ ê²½ë§) â€“ ì˜ˆì¸¡, ì‹œë²¤ì½”ì •ë¦¬ì˜ ì´ë©´, ë“œëì•„ì›ƒ",
    "section": "B. ì˜¤ë²„í”¼íŒ… ì˜ˆì‹œ",
    "text": "B. ì˜¤ë²„í”¼íŒ… ì˜ˆì‹œ\nğŸ—£ï¸ ë„¤íŠ¸ì›Œí¬ì˜ í‘œí˜„ë ¥ì´ ë„ˆë¬´ ì¢‹ì„ ë•Œ\n- \\(m\\)ì´ ë§¤ìš° í´ë•Œ ì•„ë˜ì˜ ë„¤íŠ¸ì›Œí¬ ê±°ì˜ ë¬´ì—‡ì´ë“  ë§ì¶œ ìˆ˜ ìˆë‹¤ê³  ë³´ë©´ ëœë‹¤.\n\n\\(\\underset{(n,1)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,m)}{\\boldsymbol u^{(1)}} \\overset{h}{\\to} \\underset{(n,m)}{\\boldsymbol v^{(1)}} \\overset{l_2}{\\to} \\underset{(n,1)}{\\hat{\\boldsymbol y}}\\)\n\\(\\underset{(n,1)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,m)}{\\boldsymbol u^{(1)}} \\overset{sig}{\\to} \\underset{(n,m)}{\\boldsymbol v^{(1)}} \\overset{l_2}{\\to} \\underset{(n,1)}{\\hat{\\boldsymbol y}}\\)\n\\(\\underset{(n,1)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,m)}{\\boldsymbol u^{(1)}} \\overset{relu}{\\to} \\underset{(n,m)}{\\boldsymbol v^{(1)}} \\overset{l_2}{\\to} \\underset{(n,1)}{\\hat{\\boldsymbol y}}\\)\n\n- ê·¸ëŸ°ë° ì¢…ì¢… ë§ì¶”ì§€ ë§ì•„ì•¼ í•  ê²ƒë“¤ë„ ë§ì¶˜ë‹¤.\n\\[\\text{model:} \\quad y_i = (0\\times x_i) + \\epsilon_i,~~ \\text{where}~ \\epsilon_i \\sim N(0,0.01^2)\\]\nğŸ—£ï¸ yëŠ” xì— ëŒ€í•œ ì‹ X / underlying = 0 / structure: í—ˆêµ¬, ì˜¤ì°¨í•­ì´ ë§Œë“¤ì–´ë‚¸ ìš°ì—°\n\ntorch.manual_seed(5) \nx = torch.linspace(0,1,100).reshape(100,1)\ny = torch.randn(100).reshape(100,1)*0.01\nplt.plot(x,y,'--o',alpha=0.5)\n\n\n\n\n\n\n\n\n\ntorch.manual_seed(1)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,512),\n    torch.nn.ReLU(),\n    torch.nn.Linear(512,1)\n)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n#---#\nfor epoc in range(1000):\n    ## step1 \n    yhat = net(x) \n    ## step2 \n    loss = loss_fn(yhat,y)\n    ## step3 \n    loss.backward()\n    ## step4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'--o',alpha=0.5)\nplt.plot(x,net(x).data,'--')\n\n\n\n\n\n\n\n\nğŸ—£ï¸ 0ìœ¼ë¡œ ì í•©í•˜ì§€ ì•Šì€ ê²ƒì€ ë‹¤ í‹€ë¦¼"
  },
  {
    "objectID": "posts/05wk-1.html#c.-ì˜¤ë²„í”¼íŒ…ì´ë¼ëŠ”-ëšœë ·í•œ-ì¦ê±°-train-test",
    "href": "posts/05wk-1.html#c.-ì˜¤ë²„í”¼íŒ…ì´ë¼ëŠ”-ëšœë ·í•œ-ì¦ê±°-train-test",
    "title": "05wk-1: (ì‹ ê²½ë§) â€“ ì˜ˆì¸¡, ì‹œë²¤ì½”ì •ë¦¬ì˜ ì´ë©´, ë“œëì•„ì›ƒ",
    "section": "C. ì˜¤ë²„í”¼íŒ…ì´ë¼ëŠ” ëšœë ·í•œ ì¦ê±°! (train / test)",
    "text": "C. ì˜¤ë²„í”¼íŒ…ì´ë¼ëŠ” ëšœë ·í•œ ì¦ê±°! (train / test)\nğŸ—£ï¸ 0ë³´ë‹¤ ì£¼í™©ìƒ‰ ì„ ì´ ë” ì¢‹ì€ ê²ƒ ê°™ë‹¤ëŠ” ì£¼ì¥ì— ëŒ€í•œ ë°˜ë°• (ì˜ˆì¸¡)\n- ë°ì´í„°ì˜ ë¶„ë¦¬í•˜ì—¬ ë³´ì.\n\ntorch.manual_seed(5) \nx_all = torch.linspace(0,1,100).reshape(100,1)\ny_all = torch.randn(100).reshape(100,1)*0.01\nx,xx = x_all[:80], x_all[80:]\ny,yy = y_all[:80], y_all[80:]\nplt.plot(x,y,'--o',alpha=0.5,label=\"training\")\nplt.plot(xx,yy,'--o',alpha=0.5,label=\"test\")\nplt.legend()\n\n\n\n\n\n\n\n\n- trainë§Œ í•™ìŠµ\nğŸ—£ï¸ Bì™€ ë˜‘ê°™ì€ ì¡°ê±´\n\ntorch.manual_seed(1)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,512),\n    torch.nn.ReLU(),\n    torch.nn.Linear(512,1)\n)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n#---#\nfor epoc in range(1000):\n    ## step1 \n    yhat = net(x) \n    ## step2 \n    loss = loss_fn(yhat,y)\n    ## step3 \n    loss.backward()\n    ## step4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n- training dataë¡œ í•™ìŠµí•œ netë¥¼ training data ì— ì ìš©\n\nplt.plot(x_all,y_all,'--o',alpha=0.5,color=\"gray\")\nplt.plot(x,net(x).data,'--')\n\n\n\n\n\n\n\n\n\ntrainingì—ì„œëŠ” ê·¸ëŸ­ì €ëŸ­ ì˜ ë§ì¶¤\n\n- training dataë¡œ í•™ìŠµí•œ netë¥¼ test data ì— ì ìš©\n\nplt.plot(x_all,y_all,'--o',alpha=0.5,color=\"gray\")\nplt.plot(x,net(x).data,'--')\nplt.plot(xx,net(xx).data,'--')\n\n\n\n\n\n\n\n\n\ntrainì—ì„œëŠ” ê·¸ëŸ­ì €ëŸ­ ì˜ ë§ì¶”ëŠ”ë° testì—ì„œëŠ” ì—‰ë§ì´ë‹¤ = overfit\n\nğŸ—£ï¸ randomì´ê¸° ë•Œë¬¸ì— trendëŠ” ì—†ìŒ"
  },
  {
    "objectID": "posts/05wk-1.html#d.-ì‹œë²¤ì½”ì •ë¦¬ì˜-ì˜¬ë°”ë¥¸-ì´í•´",
    "href": "posts/05wk-1.html#d.-ì‹œë²¤ì½”ì •ë¦¬ì˜-ì˜¬ë°”ë¥¸-ì´í•´",
    "title": "05wk-1: (ì‹ ê²½ë§) â€“ ì˜ˆì¸¡, ì‹œë²¤ì½”ì •ë¦¬ì˜ ì´ë©´, ë“œëì•„ì›ƒ",
    "section": "D. ì‹œë²¤ì½”ì •ë¦¬ì˜ ì˜¬ë°”ë¥¸ ì´í•´",
    "text": "D. ì‹œë²¤ì½”ì •ë¦¬ì˜ ì˜¬ë°”ë¥¸ ì´í•´\n\n\n\n\n\n\nNote\n\n\n\nì‹œë²¤ì½”ì˜ í•­ë³€(?) [@cybenko1989approximation]\ní•˜ë‚˜ì˜ ì€ë‹‰ì¸µì„ ê°€ì§€ëŠ” ì•„ë˜ì™€ ê°™ì€ ê¼´ì˜ ë„¤íŠ¸ì›Œí¬ \\(net: {\\bf X}_{n \\times p} \\to {\\bf y}_{n\\times q}\\)ëŠ”\nnet = torch.nn.Sequential(\n    torch.nn.Linear(p,???),\n    torch.nn.Sigmoid(), ## &lt;-- ì—¬ê¸°ì— ë ë£¨ë¥¼ ì¨ë„ ëœë‹¤. \n    torch.nn.Linear(???,q)\n)\nëª¨ë“  ë³´ë ê°€ì¸¡í•¨ìˆ˜\n\\[f: {\\bf X}_{n \\times p} \\to {\\bf y}_{n\\times q}\\]\në¥¼ ì›í•˜ëŠ” ì •í™•ë„ë¡œ â€œê·¼ì‚¬â€ì‹œí‚¬ ìˆ˜ ìˆë‹¤. ì‰½ê²Œ ë§í•˜ë©´ \\({\\bf X} \\to {\\bf y}\\) ì¸ ì–´ë– í•œ ë³µì¡í•œ ê·œì¹™ë¼ë„ í•˜ë‚˜ì˜ ì€ë‹‰ì¸µì„ ê°€ì§„ ì‹ ê²½ë§ì´ ì›í•˜ëŠ” ì •í™•ë„ë¡œ ê·¼ì‚¬ì‹œí‚¨ë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤. ê·¸ë ‡ì§€ë§Œ ì´ëŸ¬í•œ ê·œì¹™ì´ ë„¤í¬ì›Œí¬ê°€ í•™ìŠµí•˜ì§€ ëª»í–ˆë˜ ìë£Œ (ì²˜ìŒ ë³´ëŠ” ìë£Œ, unseen data) \\({\\bf XX}_{m \\times p}\\), \\({\\bf yy}_{m \\times q}\\) ì— ëŒ€í•˜ì—¬ì„œë„ ì˜¬ë°”ë¥´ê²Œ ì ìš©ëœë‹¤ë¼ëŠ” ë³´ì¥ì€ ì—†ë‹¤. ì‹œë²¤ì½”ëŠ” ë‹¨ì§€ netê°€ ê°€ì§€ëŠ” í‘œí˜„ë ¥ì˜ í•œê³„ë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ ë°í˜”ì„ ë¿ì´ë‹¤."
  },
  {
    "objectID": "posts/05wk-1.html#a.-ì˜¤ë²„í”¼íŒ…ì˜-í•´ê²°",
    "href": "posts/05wk-1.html#a.-ì˜¤ë²„í”¼íŒ…ì˜-í•´ê²°",
    "title": "05wk-1: (ì‹ ê²½ë§) â€“ ì˜ˆì¸¡, ì‹œë²¤ì½”ì •ë¦¬ì˜ ì´ë©´, ë“œëì•„ì›ƒ",
    "section": "A. ì˜¤ë²„í”¼íŒ…ì˜ í•´ê²°",
    "text": "A. ì˜¤ë²„í”¼íŒ…ì˜ í•´ê²°\n- ì˜¤ë²„í”¼íŒ…ì˜ í•´ê²°ì±…: ë“œëì•„ì›ƒ\n- ë°ì´í„°\n\ntorch.manual_seed(5) \nx_all = torch.linspace(0,1,100).reshape(100,1)\ny_all = torch.randn(100).reshape(100,1)*0.01\n#plt.plot(x_all,y_all,'--o',alpha=0.5)\nx,y = x_all[:80], y_all[:80]\nxx,yy = x_all[80:], y_all[80:]\nplt.plot(x,y,'--o',color=\"C0\")\nplt.plot(xx,yy,'--o',color=\"C1\")\n\n\n\n\n\n\n\n\n- í•™ìŠµ\nğŸ—£ï¸ torch.nn.Dropout(0.8) ì¶”ê°€ (í•™ìŠµ ì‹œ ì¼ë¶€ë§Œ ì‚¬ìš©)\n\ntorch.manual_seed(1)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,512),\n    torch.nn.ReLU(),\n    torch.nn.Dropout(0.8),\n    torch.nn.Linear(512,1)\n)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n#---#\nfor epoc in range(1000):\n    ## step1 \n    yhat = net(x) \n    ## step2 \n    loss = loss_fn(yhat,y)\n    ## step3 \n    loss.backward()\n    ## step4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n- ê²°ê³¼ì‹œê°í™” (ì˜ëª»ëœ ì‚¬ìš©)\n\nplt.plot(x_all,y_all,'--o',alpha=0.5,color=\"gray\")\nplt.plot(x,net(x).data,'--')\nplt.plot(xx,net(xx).data,'--')\n\n\n\n\n\n\n\n\n- ê²°ê³¼ì‹œê°í™” (ì˜¬ë°”ë¥¸ ì‚¬ìš©)\n\nnet.training \n\nTrue\n\n\nğŸ—£ï¸ í‰ê°€ ëª¨ë“œë¡œ ë°”ê¾¸ê¸°\n\nnet.eval()\n\nSequential(\n  (0): Linear(in_features=1, out_features=512, bias=True)\n  (1): ReLU()\n  (2): Dropout(p=0.8, inplace=False)\n  (3): Linear(in_features=512, out_features=1, bias=True)\n)\n\n\n\nnet.training\n\nFalse\n\n\n\nplt.plot(x_all,y_all,'--o',alpha=0.5,color=\"gray\")\nplt.plot(x,net(x).data,'--')\nplt.plot(xx,net(xx).data,'--')\n\n\n\n\n\n\n\n\nğŸ—£ï¸ ì–´ëŠ ì •ë„ 0ìœ¼ë¡œ ë–¨ì–´ì§, ì˜¤ë²„í”¼íŒ… ë¬¸ì œê°€ ì™„ì „íˆ í•´ê²°ë˜ì§€ëŠ” ì•Šì•˜ì§€ë§Œ ì–´ëŠì •ë„ ì™„í™”"
  },
  {
    "objectID": "posts/05wk-1.html#b.-ë“œëì•„ì›ƒ-ë ˆì´ì–´",
    "href": "posts/05wk-1.html#b.-ë“œëì•„ì›ƒ-ë ˆì´ì–´",
    "title": "05wk-1: (ì‹ ê²½ë§) â€“ ì˜ˆì¸¡, ì‹œë²¤ì½”ì •ë¦¬ì˜ ì´ë©´, ë“œëì•„ì›ƒ",
    "section": "B. ë“œëì•„ì›ƒ ë ˆì´ì–´",
    "text": "B. ë“œëì•„ì›ƒ ë ˆì´ì–´\n- ë“œëì•„ì›ƒì˜ ì„±ì§ˆ1: ë“œëì•„ì›ƒì˜ ê³„ì‚°ë°©ì‹ì„ ì´í•´í•´ë³´ì.\nğŸ—£ï¸ default= 0.5\n\nu = torch.randn(10,2)\nd = torch.nn.Dropout(0.9)\nu\n\ntensor([[ 0.5951,  0.2245],\n        [ 0.8238,  0.5230],\n        [ 0.4772, -1.0465],\n        [-0.6826,  0.4257],\n        [ 0.5113,  0.4130],\n        [-0.3946,  0.0827],\n        [ 1.4149, -1.7569],\n        [ 0.3142, -0.9964],\n        [-0.4613,  0.3530],\n        [-0.2743, -0.5558]])\n\n\n\nd(u)\n\ntensor([[0.0000, 0.0000],\n        [0.0000, 0.0000],\n        [0.0000, -0.0000],\n        [-0.0000, 0.0000],\n        [5.1128, 4.1303],\n        [-0.0000, 0.0000],\n        [0.0000, -0.0000],\n        [0.0000, -0.0000],\n        [-0.0000, 3.5305],\n        [-0.0000, -0.0000]])\n\n\n\n90%ì˜ ë“œëì•„ì›ƒ: ë“œëì•„ì›ƒì¸µì˜ ì…ë ¥ ì¤‘ ì„ì˜ë¡œ 90%ë¥¼ ê³¨ë¼ì„œ ê²°ê³¼ë¥¼ 0ìœ¼ë¡œ ë§Œë“ ë‹¤. + ê·¸ë¦¬ê³  0ì´ ë˜ì§€ì•Šê³  ì‚´ì•„ë‚¨ì€ ê°’ë“¤ì€ 10ë°° ë§Œí¼ ê°’ì´ ì»¤ì§„ë‹¤.\në‚¨ì€ê°’ì„ 10ë°° í‚¤ìš°ëŠ” ì´ìœ ? ì¶œë ¥ì˜ í‰ê· ê°’ì„ ë³´ì •í•˜ê¸° ìœ„í•´ì„œ\n\n- ë“œëì•„ì›ƒì˜ ì„±ì§ˆ2: ë“œëì•„ì›ƒì„ on/off í•˜ëŠ” ë°©ë²•ì„ ì´í•´í•´ë³´ì.\n\nu = torch.randn(10,2)\nu\n\ntensor([[ 0.8395,  1.8825],\n        [-0.0415, -2.3987],\n        [-0.3658, -1.3403],\n        [-1.4066,  0.7178],\n        [-1.0465,  0.9663],\n        [-1.2350,  1.3424],\n        [-1.1903,  0.3955],\n        [ 0.4236, -0.7882],\n        [-0.4348,  0.2669],\n        [-0.9102, -0.3219]])\n\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Dropout(0.9)\n)\nnet\n\nSequential(\n  (0): Dropout(p=0.9, inplace=False)\n)\n\n\n\nu,net(u)\n\n(tensor([[ 0.8395,  1.8825],\n         [-0.0415, -2.3987],\n         [-0.3658, -1.3403],\n         [-1.4066,  0.7178],\n         [-1.0465,  0.9663],\n         [-1.2350,  1.3424],\n         [-1.1903,  0.3955],\n         [ 0.4236, -0.7882],\n         [-0.4348,  0.2669],\n         [-0.9102, -0.3219]]),\n tensor([[  0.0000,   0.0000],\n         [ -0.0000,  -0.0000],\n         [ -0.0000,  -0.0000],\n         [-14.0662,   0.0000],\n         [ -0.0000,   0.0000],\n         [-12.3497,   0.0000],\n         [ -0.0000,   0.0000],\n         [  4.2361,  -0.0000],\n         [ -0.0000,   0.0000],\n         [ -0.0000,  -3.2190]]))\n\n\nğŸ—£ï¸ ë§¤ë²ˆ ì–´ë–¤ ë…¸ë“œê°€ ì£½ì„ì§€ ëª¨ë¦„ (ë‹¤ 0ì´ ë˜ê¸°ë„ í•¨)\nâ“ ë¹„ìœ¨? í™•ë¥ ?\n\nnet.training\n\nTrue\n\n\n\nnet.eval() # ë“œëì•„ì›ƒì´ ë¬´ë ¥í™”\n\nSequential(\n  (0): Dropout(p=0.9, inplace=False)\n)\n\n\n\nu,net(u)\n\n(tensor([[ 0.8395,  1.8825],\n         [-0.0415, -2.3987],\n         [-0.3658, -1.3403],\n         [-1.4066,  0.7178],\n         [-1.0465,  0.9663],\n         [-1.2350,  1.3424],\n         [-1.1903,  0.3955],\n         [ 0.4236, -0.7882],\n         [-0.4348,  0.2669],\n         [-0.9102, -0.3219]]),\n tensor([[ 0.8395,  1.8825],\n         [-0.0415, -2.3987],\n         [-0.3658, -1.3403],\n         [-1.4066,  0.7178],\n         [-1.0465,  0.9663],\n         [-1.2350,  1.3424],\n         [-1.1903,  0.3955],\n         [ 0.4236, -0.7882],\n         [-0.4348,  0.2669],\n         [-0.9102, -0.3219]]))\n\n\n- ë“œëì•„ì›ƒë ˆì´ì–´ ì •ë¦¬\n\nê³„ì‚°: (1) ì…ë ¥ì˜ ì¼ë¶€ë¥¼ ì„ì˜ë¡œ 0ìœ¼ë¡œ ë§Œë“œëŠ” ì—­í•  (2) 0ì´ ì•ˆëœê²ƒë“¤ì€ ìŠ¤ì¹¼ë¼ë°°í•˜ì—¬ ë“œëì•„ì›ƒì„ í†µê³¼í•œ ëª¨ë“  ìˆ«ìë“¤ì˜ ì´í•©ì´ ëŒ€ì²´ë¡œ ì¼ì •í•˜ê²Œ ë˜ë„ë¡ ì¡°ì •\non/off: í•™ìŠµì‹œì—ëŠ” dropout on / í•™ìŠµì„ í•˜ì§€ ì•Šì„ ê²½ìš°ëŠ” dropout off\nëŠë‚Œ: ì¼ë¶€ëŸ¬ íŒ¨ë„í‹°ë¥¼ ì•ˆê³  í•™ìŠµí•˜ëŠ” ëŠë‚Œ..\níš¨ê³¼: ì˜¤ë²„í”¼íŒ…ì„ ì–µì œí•˜ëŠ” íš¨ê³¼ê°€ ìˆìŒ\n\nğŸ—£ï¸ ëœë¤ í¬ë ˆìŠ¤íŠ¸ì™€ ë™ì¼\n\nì°¸ê³ : ì˜¤ë²„í”¼íŒ…ì„ ì¡ëŠ” ë°©ë²•ì€ ë“œëì•„ì›ƒë§Œ ìˆëŠ”ê²Œ ì•„ë‹ˆë‹¤..\n\nğŸ—£ï¸ ê·¼ë³¸: ì‹œê°í™” í›„ ë°ì´í„°ì— ë§ì¶˜ ëª¨ë¸ì„ ì°¾ìŒ (ì–´ë ¤ì›€) / ì‹¤ì œ: ì‹œë²¤ì½” ì •ë¦¬ë¡œ ì í•©ì„ í•œ í›„ ë“œëì•„ì›ƒì„ ê±¸ì–´ ì˜¤ë²„í”¼íŒ… ë°©ì§€"
  },
  {
    "objectID": "posts/05wk-1.html#c.-ë“œëì•„ì›ƒ-ë ˆì´ì–´ì˜-ìœ„ì¹˜",
    "href": "posts/05wk-1.html#c.-ë“œëì•„ì›ƒ-ë ˆì´ì–´ì˜-ìœ„ì¹˜",
    "title": "05wk-1: (ì‹ ê²½ë§) â€“ ì˜ˆì¸¡, ì‹œë²¤ì½”ì •ë¦¬ì˜ ì´ë©´, ë“œëì•„ì›ƒ",
    "section": "C. ë“œëì•„ì›ƒ ë ˆì´ì–´ì˜ ìœ„ì¹˜",
    "text": "C. ë“œëì•„ì›ƒ ë ˆì´ì–´ì˜ ìœ„ì¹˜\n- ReLU,dropoutì˜ íŠ¹ì´í•œ ì„±ì§ˆ: \\(\\text{dropout}(\\text{relu}({\\bf x}))=\\text{relu}(\\text{dropout}({\\bf x}))\\)\nğŸ—£ï¸ ë‘˜ ë‹¤ xë¥¼ ê·¸ëŒ€ë¡œ ë‚´ë³´ë‚´ê±°ë‚˜ 0ìœ¼ë¡œ ë§Œë“¦ (ìˆœì„œ ìƒê´€ ì—†ì´ ê²°ê³¼ ë™ì¼)\n\nu = torch.randn(10,2)\nr = torch.nn.ReLU()\nd = torch.nn.Dropout()\n\n\ntorch.manual_seed(0)\nd(r(u))\n\ntensor([[0.0000, 0.0000],\n        [0.0000, 0.0000],\n        [0.0000, 0.0000],\n        [0.0000, 0.5372],\n        [2.6658, 2.1870],\n        [0.3798, 0.0000],\n        [0.0000, 1.6593],\n        [0.9300, 0.0000],\n        [0.0000, 0.0000],\n        [0.0000, 0.0000]])\n\n\n\ntorch.manual_seed(0)\nr(d(u))\n\ntensor([[0.0000, 0.0000],\n        [-0.0000, 0.0000],\n        [0.0000, 0.0000],\n        [0.0000, 0.5372],\n        [2.6658, 2.1870],\n        [0.3798, -0.0000],\n        [0.0000, 1.6593],\n        [0.9300, 0.0000],\n        [0.0000, 0.0000],\n        [-0.0000, 0.0000]])\n\n\n- ë‹¤ë¥¸ í™œì„±í™”í•¨ìˆ˜ëŠ” ì„±ë¦½ì•ˆí•¨\nğŸ—£ï¸ í™œì„±í™” í•¨ìˆ˜: ë¹„ì„ í˜• í•¨ìˆ˜, activation í•¨ìˆ˜\n\nu = torch.randn(10,2)\ns = torch.nn.Sigmoid()\nd = torch.nn.Dropout()\n\n\ntorch.manual_seed(0)\nd(s(u))\n\ntensor([[0.4801, 0.0000],\n        [0.0000, 1.4006],\n        [0.3487, 0.0000],\n        [0.0000, 1.2299],\n        [0.9213, 1.6180],\n        [1.1322, 0.0000],\n        [0.0000, 1.4407],\n        [0.6015, 1.4349],\n        [0.0000, 1.7626],\n        [0.0000, 0.0000]])\n\n\n\ntorch.manual_seed(0)\ns(d(u))\n\ntensor([[0.0907, 0.5000],\n        [0.5000, 0.8452],\n        [0.0427, 0.5000],\n        [0.5000, 0.7183],\n        [0.4218, 0.9472],\n        [0.6300, 0.5000],\n        [0.5000, 0.8691],\n        [0.1561, 0.8657],\n        [0.5000, 0.9822],\n        [0.5000, 0.5000]])\n\n\n- ê²°ë¡ : ë“œëì•„ì›ƒì€ í™œì„±í™” í•¨ìˆ˜ ë°”ë¡œ ë’¤ì— ì˜¤ëŠ”ê²Œ ë§ìŒ. (ê·¸ë ‡ì§€ ì•Šë‹¤ë©´ 0ì„ ë§Œë“¤ ìˆ˜ ì—†ëŠ”ê±¸?) ê·¸ë ‡ì§€ë§Œ ReLUì˜ ê²½ìš° í™œì„±í™” í•¨ìˆ˜ ì§ì „ì— ì·¨í•˜ê¸°ë„ í•¨.\nğŸ—£ï¸ ReLUëŠ” ìˆœì„œë¥¼ ë°”ê¾¸ëŠ” ê²ƒì´ ê³„ì‚° ìƒì— íš¨ìœ¨ì´ ìˆë‹¤ê³  í•¨"
  },
  {
    "objectID": "posts/05wk-1.html#d.-í‰ê· ë³´ì •ì˜-í•„ìš”ì„±-ì„ íƒí•™ìŠµ",
    "href": "posts/05wk-1.html#d.-í‰ê· ë³´ì •ì˜-í•„ìš”ì„±-ì„ íƒí•™ìŠµ",
    "title": "05wk-1: (ì‹ ê²½ë§) â€“ ì˜ˆì¸¡, ì‹œë²¤ì½”ì •ë¦¬ì˜ ì´ë©´, ë“œëì•„ì›ƒ",
    "section": "D. í‰ê· ë³´ì •ì˜ í•„ìš”ì„± (ì„ íƒí•™ìŠµ)",
    "text": "D. í‰ê· ë³´ì •ì˜ í•„ìš”ì„± (ì„ íƒí•™ìŠµ)\n\n\n\n\n\n\nNote\n\n\n\n90%ì˜ ë“œëì•„ì›ƒì—ì„œ ì¶œë ¥ê²°ê³¼ì— ì™œ x10í•˜ëŠ”ì§€ ì¢€ ë” ìì„¸íˆ ì„¤ëª…í•œ ì±•í„°ì…ë‹ˆë‹¤. ê¶ê¸ˆí•˜ì‹œë‹¤ë©´ ì½ì–´ë³´ì‹œê³  ì•„ë‹ˆë¼ë©´ ë„˜ì–´ê°€ì…”ë„ ë¬´ë°©í•©ë‹ˆë‹¤.\n\n\n- ì•„ë˜ì˜ ë°ì´í„°ë¥¼ ê´€ì°°í•˜ì.\n\nx,_ = torch.randn(300).sort()\ny = relu(20*x) + torch.randn(300)\nx,y = x.reshape(-1,1), y.reshape(-1,1)\n\n\nplt.plot(x,y,'o')\n\n\n\n\n\n\n\n\n- ì í•©í•´ë³´ì.\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1000),\n    torch.nn.ReLU(),\n    torch.nn.Dropout(0.1),\n    torch.nn.Linear(1000,1,bias=False),\n)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n#---#\nfor epoc in range(5000):\n    ## 1 \n    yhat = net(x)\n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nnet.eval()\n\nSequential(\n  (0): Linear(in_features=1, out_features=1000, bias=True)\n  (1): ReLU()\n  (2): Dropout(p=0.1, inplace=False)\n  (3): Linear(in_features=1000, out_features=1, bias=False)\n)\n\n\n\nnet.training\n\nFalse\n\n\n\nplt.plot(x,y,'o')\nplt.plot(x,net(x).data,'--')\n\n\n\n\n\n\n\n\n- ì£¼í™©ìƒ‰ì„ ì´ë‚˜ì˜¤ëŠ” ì´ìœ  ì„¤ëª…í•´ë³´ì.\n\nU = net[:-1](x).data \nW = net[-1].weight.T \n\nì•„ë˜3ê°œëŠ” ë™ì¼í•œì½”ë“œì„\n\nnet(x).reshape(-1)[:10] # ì½”ë“œ1\n\ntensor([-0.9858, -0.5127, -0.4687,  0.0514,  0.0558,  0.2089,  0.2213,  0.2619,\n         0.2691,  0.2823], grad_fn=&lt;SliceBackward0&gt;)\n\n\n\n(U@W).reshape(-1)[:10] # ì½”ë“œ2\n\ntensor([-0.9858, -0.5127, -0.4687,  0.0514,  0.0558,  0.2089,  0.2213,  0.2619,\n         0.2691,  0.2823], grad_fn=&lt;SliceBackward0&gt;)\n\n\n\n((U*W.reshape(-1)).sum(axis=1))[:10] # ì½”ë“œ3\n\ntensor([-0.9858, -0.5127, -0.4687,  0.0514,  0.0558,  0.2089,  0.2213,  0.2619,\n         0.2691,  0.2823], grad_fn=&lt;SliceBackward0&gt;)\n\n\në”°ë¼ì„œ ì•„ë˜ì˜ ì£¼í™©ìƒ‰ì„ ë“¤ì˜ .sum(axis=1) í•˜ê¸°ë§Œ í•˜ë©´ net(x)ì˜ ê²°ê³¼ê°€ ëœë‹¤.\n\nplt.plot(x,U*W.reshape(-1).data,color=\"C1\",alpha=0.02);\n\n\n\n\n\n\n\n\n- ì¦‰ ì™¼ìª½ì˜ ì£¼í™©ìƒ‰ì„ 1ì´ ëª¨ë‘ í•©ì³ì ¸ì„œ ì˜¤ë¥¸ìª½ì˜ ì ì„ ì´ëœë‹¤.\n\nfig,ax = plt.subplots(1,2,figsize=(9,3))\nax[0].plot(x,U*W.reshape(-1).data,color=\"C1\",alpha=0.02);\nax[0].set_title(\"1,000 ReLUs\")\nax[1].plot(x,net(x).data,'--',color=\"C1\")\nax[1].set_title(r\"$net({\\bf x})$=sum(1,000 ReLUs)\");\n\n\n\n\n\n\n\n\n\në§Œì•½ì— ì™¼ìª½ì˜ ì£¼í™©ìƒ‰ì„ ì´ 10%ë§Œ ì‚¬ìš©ë˜ì–´ì„œ 100ê°œì˜ ë ë£¨ë§Œ ì‚¬ìš©ë˜ì—ˆë‹¤ë©´? ëŒ€ì¶© x10ì„ í•´ì¤˜ì•¼ net(x) ê°€ ë‚˜ì˜¤ì§€ ì•Šê² ì–´ìš”?"
  },
  {
    "objectID": "posts/05wk-1.html#footnotes",
    "href": "posts/05wk-1.html#footnotes",
    "title": "05wk-1: (ì‹ ê²½ë§) â€“ ì˜ˆì¸¡, ì‹œë²¤ì½”ì •ë¦¬ì˜ ì´ë©´, ë“œëì•„ì›ƒ",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n1000ê°œê°€ ìˆìŒâ†©ï¸"
  },
  {
    "objectID": "posts/13wk-1.html",
    "href": "posts/13wk-1.html",
    "title": "13wk-1: (ê°•í™”í•™ìŠµ) â€“ ê°•í™”í•™ìŠµ Intro, Bandit ê²Œì„ ì„¤ëª…, Bandit í™˜ê²½ ì„¤ê³„ ë° í’€ì´",
    "section": "",
    "text": "ğŸ“˜ Note Format Guide\nThis format serves as a structured guide for organizing lecture content, personal interpretation, experiments, and study-related questions.\nğŸ“ ğŸ—£ï¸ âœï¸ ğŸ”¬ â“"
  },
  {
    "objectID": "posts/13wk-1.html#a.-ëŒ€ì¶©-ê°œë…ë§Œ-ì‹¤ìŠµ",
    "href": "posts/13wk-1.html#a.-ëŒ€ì¶©-ê°œë…ë§Œ-ì‹¤ìŠµ",
    "title": "13wk-1: (ê°•í™”í•™ìŠµ) â€“ ê°•í™”í•™ìŠµ Intro, Bandit ê²Œì„ ì„¤ëª…, Bandit í™˜ê²½ ì„¤ê³„ ë° í’€ì´",
    "section": "A. ëŒ€ì¶© ê°œë…ë§Œ ì‹¤ìŠµ",
    "text": "A. ëŒ€ì¶© ê°œë…ë§Œ ì‹¤ìŠµ\nğŸ—£ï¸(\n\naction_space = [0,1]\naction = np.random.choice(action_space)\naction\n\nnp.int64(1)\n\n\n\naction space: agentê°€ í•  ìˆ˜ ìˆëŠ” actionë“¤ì˜ ì§‘í•©\nì²˜ìŒ actionì€ randomìœ¼ë¡œ ë½‘ìŒ\n\n\nif action == 1:\n    reward = 10\nelif action == 0:\n    reward = 1\nelse:\n    pass\n\n\nreward\n\n10\n\n\nif action == 1:\n    reward = 10\nelse:\n    reward = 1\n\në™ì¼ ì½”ë“œ\n\n\naction_space = [0,1]\naction = np.random.choice(action_space)\nif action == 1:\n    reward = 10\nelse:\n    reward = 1\n(action, reward)\n\n(np.int64(0), 1)\n\n\n\nactionê³¼ rewardì˜ historyë¥¼ ì»´í“¨í„°ì— ì €ì¥í•  ê³µê°„ì´ í•„ìš”í•¨\n\nê²Œì„ì„ ì˜¤ë˜í•˜ë©´ í•„ìš”í•œ ë©”ëª¨ë¦¬ ê³µê°„ì´ ë§¤ìš° ì»¤ì§€ë¯€ë¡œ ë‹¤ìŒê³¼ ê°™ì€ ìë£Œí˜• í•„ìš”\n\n\n\nactions = collections.deque(maxlen=5) # list? numpy? ë¹„ìŠ·í•œ ê²ƒ\n\n\nactions\n\ndeque([], maxlen=5)\n\n\n\nactions.append(action)\n\n\nactions\n\ndeque([np.int64(0)], maxlen=5)\n\n\n\naction_space = [0,1]\nactions = collections.deque(maxlen=5)\n#---#\n\n\naction = np.random.choice(action_space)\nif action == 1:\n    reward = 10\nelse:\n    reward = 1\nactions.append(action)\n\n\nactions\n\ndeque([np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1)],\n      maxlen=5)\n\n\n\nìœ„ì˜ ë‘ ì½”ë“œë¥¼ 5ë²ˆ ì‹¤í–‰í•˜ë‹ˆ deque([np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1)], maxlen=5) ê³¼ ê°™ì€ ê²°ê³¼ë¥¼ ì–»ì—ˆìŒ\ní•œ ë²ˆ ë” ì‹¤í–‰í•œë‹¤ë©´ ì´ì „ ê²°ê³¼ê°€ ë°€ë¦¬ë©´ì„œ ì‚¬ë¼ì§\n\ndeque([np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1)], maxlen=5)\n\n\n\naction_space = [0,1]\nactions_deque = collections.deque(maxlen=500)\nrewards_deque = collections.deque(maxlen=500)\n#---#\n\n\nfor _ in range(10):\n    action = np.random.choice(action_space)\n    if action == 1:\n        reward = 10\n    else:\n        reward = 1\n    actions_deque.append(action)\n    rewards_deque.append(reward)\n\n\nactions_deque\n\ndeque([np.int64(0),\n       np.int64(0),\n       np.int64(1),\n       np.int64(1),\n       np.int64(0),\n       np.int64(0),\n       np.int64(0),\n       np.int64(1),\n       np.int64(0),\n       np.int64(1)],\n      maxlen=500)\n\n\n\nrewards_deque\n\ndeque([1, 1, 10, 10, 1, 1, 1, 10, 1, 10], maxlen=500)\n\n\n\nnp.array(actions_deque) == 1\n\narray([False, False,  True,  True, False, False, False,  True, False,\n        True])\n\n\n\në¸Œë¡œë“œìºìŠ¤íŒ…ì„ í•˜ê¸° ìœ„í•´ numpy í™œìš©\n\n\nactions_numpy = np.array(actions_deque)\nrewards_numpy = np.array(rewards_deque)\n\n\nactions_numpy\n\narray([0, 0, 1, 1, 0, 0, 0, 1, 0, 1])\n\n\n\nrewards_numpy\n\narray([ 1,  1, 10, 10,  1,  1,  1, 10,  1, 10])\n\n\n\nrewards_numpy[actions_numpy == 1]\n\narray([10, 10, 10, 10])\n\n\n\nrewards_numpy[actions_numpy == 1].mean()\n\nnp.float64(10.0)\n\n\n\nì—¬ê¸°ì„œëŠ” rewardê°€ ì¼ì •í•˜ì§€ë§Œ randomí•˜ê²Œ ì£¼ëŠ” ê²½ìš°ë„ ìˆìŒ\n\nq0 = 0ì„ ëˆŒë €ì„ë•Œ ë°›ëŠ” ë³´ìƒì˜ í‰ê· \nq1 = 1ì„ ëˆŒë €ì„ë•Œ ë°›ëŠ” ë³´ìƒì˜ í‰ê· \nq_table = [q0, q1]\n\nq0 = rewards_numpy[actions_numpy == 0].mean()\nq1 = rewards_numpy[actions_numpy == 1].mean()\nq_table = np.array([q0,q1])\nq_table\n\narray([ 1., 10.])\n\n\n\nq_tableì„ ë³´ê³  action í•˜ëŠ” ê²ƒì„ ì½”ë“œë¡œ êµ¬í˜„\n\n\nq_table.argmax()\n\nnp.int64(1)\n\n\n\naction = q_table.argmax()\n\n\nfor _ in range(10):\n    action = q_table.argmax() # ì´ì œëŠ” actionì„ randomìœ¼ë¡œ í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ q_tableì„ ë³´ê³  í•¨\n    if action == 1:\n        reward = 10\n    else:\n        reward = 1\n    actions_deque.append(action)\n    rewards_deque.append(reward)\n    actions_numpy = np.array(actions_deque)\n    rewards_numpy = np.array(rewards_deque)\n    q0 = rewards_numpy[actions_numpy == 0].mean()\n    q1 = rewards_numpy[actions_numpy == 1].mean()\n    q_table = np.array([q0,q1]) # q_table update\n\n\nrewards_numpy\n\narray([ 1,  1, 10, 10,  1,  1,  1, 10,  1, 10, 10, 10, 10, 10, 10, 10, 10,\n       10, 10, 10])\n\n\n\nì²˜ìŒì—ëŠ” randomì´ë‹¤ê°€ ëì—ëŠ” 10ì ì„ ë°›ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŒ\n\n\nactions_numpy\n\narray([0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n\n\n)ğŸ—£ï¸\n\naction_space = [0,1] \nactions_deque = collections.deque(maxlen=500)\nrewards_deque =  collections.deque(maxlen=500)\n#---#\n\n\nfor _ in range(10):\n    action = np.random.choice(action_space)\n    if action == 1:\n        reward = 10 \n    else:\n        reward = 1\n    actions_deque.append(action)\n    rewards_deque.append(reward)\n\n\nactions_deque\n\ndeque([0, 1, 0, 0, 0, 1, 0, 1, 1, 0], maxlen=500)\n\n\n\nrewards_deque\n\ndeque([1, 10, 1, 1, 1, 10, 1, 10, 10, 1], maxlen=500)\n\n\n\nactions_numpy = np.array(actions_deque)\nrewards_numpy = np.array(rewards_deque)\n\n\nq0 = rewards_numpy[actions_numpy == 0].mean()\nq1 = rewards_numpy[actions_numpy == 1].mean()\nq_table = np.array([q0,q1])\nq_table\n\narray([ 1., 10.])\n\n\n\naction = q_table.argmax()\n\n\nfor _ in range(5):\n    action = q_table.argmax()\n    if action == 1:\n        reward = 10 \n    else:\n        reward = 1\n    actions_deque.append(action)\n    rewards_deque.append(reward)\n    actions_numpy = np.array(actions_deque)\n    rewards_numpy = np.array(rewards_deque)    \n    q0 = rewards_numpy[actions_numpy == 0].mean()\n    q1 = rewards_numpy[actions_numpy == 1].mean()\n    q_table = np.array([q0,q1])\n\n\nactions_numpy\n\narray([0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1])\n\n\n\nrewards_numpy\n\narray([ 1, 10,  1,  1,  1, 10,  1, 10, 10,  1, 10, 10, 10, 10, 10])"
  },
  {
    "objectID": "posts/13wk-1.html#b.-í´ë˜ìŠ¤ë¥¼-ì´ìš©í•œ-êµ¬í˜„",
    "href": "posts/13wk-1.html#b.-í´ë˜ìŠ¤ë¥¼-ì´ìš©í•œ-êµ¬í˜„",
    "title": "13wk-1: (ê°•í™”í•™ìŠµ) â€“ ê°•í™”í•™ìŠµ Intro, Bandit ê²Œì„ ì„¤ëª…, Bandit í™˜ê²½ ì„¤ê³„ ë° í’€ì´",
    "section": "B. í´ë˜ìŠ¤ë¥¼ ì´ìš©í•œ êµ¬í˜„",
    "text": "B. í´ë˜ìŠ¤ë¥¼ ì´ìš©í•œ êµ¬í˜„\nğŸ—£ï¸(\n\nê°•í™”í•™ìŠµì—ì„œ ê°ì²´ë¼ê³  ë¶ˆë¦´ ìˆ˜ ìˆëŠ” ê²ƒ: Agent, Environment\n\nAgentê°€ í•˜ëŠ” í–‰ë™: action, ì €ì¥(Environmentì˜ reward)\nEnvironmentê°€ í•˜ëŠ” í–‰ë™: reward(actionì„ ë°›ì•„ì„œ)\n\n\n\nclass Bandit:\n    def __init__(self):\n        pass # ì´ˆê¸°ê°’ íŒ¨ìŠ¤\n    def step(self, action):\n        # action --&gt; reward\n        if action == 0:\n            reward = 1\n        else:\n            reward = 10\n        return reward\n\n\nenv = Bandit()\n\n\nenv.step(1)\n\n10\n\n\n\nenv.step(0)\n\n1\n\n\n\nenvì˜ rewardë„ ì €ì¥í•˜ê²Œ í•˜ê³  ì‹¶ë‹¤ë©´\n\n\nclass Bandit:\n    def __init__(self):\n        self.reward = None\n    def step(self, action):\n        # action --&gt; reward\n        if action == 0:\n            self.reward = 1\n        else:\n            self.reward = 10\n        return self.reward\n\n\nenv = Bandit()\n\n\nenv.step(1)\n\n10\n\n\n\nenv.reward\n\n10\n\n\n\nenv.step(0)\n\n1\n\n\n\nenv.reward\n\n1\n\n\n\nclass Agent:\n    def __init__(self):\n        pass\n    def act(self):\n        # ë§Œì•½ì— ê²½í—˜ì´ 20ë³´ë‹¤ ì‘ìŒ --&gt; ëœë¤ ì•¡ì…˜\n        # ê²½í—˜ì´ 20ë³´ë‹¤ í¬ë©´ --&gt; action = q_table.argmax()\n        pass\n    def save_experience(self):\n        # ë°ì´í„°\n        pass\n    def learn(self):\n        # q_tableì„ ì—…ë°ì´íŠ¸í•˜ëŠ” ê³¼ì •\n        pass\n\n\në‹¤ìŒ ì‹œê°„ì— ì´ì–´ì„œ\n\n)ğŸ—£ï¸\n\nclass Bandit:\n    def __init__(self):\n        self.reward = None \n    def step(self,action):\n        if action == 0:\n            self.reward = 1\n        else: \n            self.reward = 10 \n        return self.reward \n\n\nenv = Bandit()\n\n\nclass Agent:\n    def __init__(self):\n        pass \n    def act(self):\n        # ë§Œì•½ì— ê²½í—˜ì´ 20ë³´ë‹¤ ì‘ìŒ --&gt; ëœë¤ì•¡ì…˜ \n        # ê²½í—˜ì´ 20ë³´ë‹¤ í¬ë©´ --&gt; action = q_tabel.argmax()\n        pass \n    def save_experience(self):\n        # ë°ì´í„° ì €ì¥ \n        pass \n    def learn(self):\n        # q_table ì„ ì—…ë°ì´íŠ¸í•˜ëŠ” ê³¼ì • \n        pass\n\n\n\n\n\n\n\nImportant\n\n\n\nì•ìœ¼ë¡œì˜ ìˆ˜ì—…ì—ì„œëŠ” ì•„ë˜ì— í•´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤ì˜ ê¸°ë³¸ ê°œë…ì„ ìˆ™ì§€í•˜ì…”ì•¼ í•©ë‹ˆë‹¤. (13wk-2 ì£¼ì°¨ ê°•ì˜ë“£ê¸°ì „ê¹Œì§€ ê¼­!)\n\ní´ë˜ìŠ¤ì™€ ì¸ìŠ¤í„´ìŠ¤ì˜ ê°œë…, __init__, self, ë©”ì†Œë“œ\ní´ë˜ìŠ¤ì˜ ìƒì†\n\nê´€ë ¨í•˜ì—¬ ì œê°€ ì‘ë…„ì— ìˆ˜ì—…í•œ ìë£ŒëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤\n\nhttps://guebin.github.io/PP2024/posts/11wk-2.html ì—ì„œ 1-7ê¹Œì§€..\nhttps://guebin.github.io/PP2024/posts/14wk-2.html ì—ì„œ 8-A\n\në¬¼ë¡ , ê¼­ ì œ ê°•ì˜ë…¸íŠ¸ë¡œë§Œ ê³µë¶€í•˜ì…”ì•¼í•˜ëŠ”ê²ƒì€ ì•„ë‹™ë‹ˆë‹¤. ì œ ìˆ˜ì—… ì™¸ì—ë„ í´ë˜ìŠ¤ë¥¼ ì˜ ì„¤ëª…í•˜ëŠ” ë‹¤ì–‘í•œ ìë£Œë“¤ì´ ë§ì´ ìˆìœ¼ë‹ˆ ììœ ë¡­ê²Œ ì°¸ê³ í•˜ì—¬ í•™ìŠµí•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."
  },
  {
    "objectID": "posts/15wk-1.html",
    "href": "posts/15wk-1.html",
    "title": "15wk-1: (ê°•í™”í•™ìŠµ) â€“ LunarLander",
    "section": "",
    "text": "ğŸ“˜ Note Format Guide\nThis format serves as a structured guide for organizing lecture content, personal interpretation, experiments, and study-related questions.\n\n\n\n\n\n\n\n\nType\nWhat It Means\nWhen I Use It\n\n\n\n\nğŸ“ Lecture\nOriginal material from the professorâ€™s notes\nWhen Iâ€™m referencing core concepts or provided code\n\n\nğŸ—£ï¸ In-Class Note\nVerbal explanations shared during the lecture\nWhen I want to record something the professor said in class but didnâ€™t include in the official notes\n\n\nâœï¸ My Note\nMy thoughts, interpretations, or additional explanations\nWhen I reflect on or explain something in my own words\n\n\nğŸ”¬ Experiment\nCode I tried out or changed to explore further\nWhen I test variations or go beyond the original example\n\n\nâ“ Question\nQuestions I had while studying\nWhen I want to revisit or research something more deeply\n\n\n\nğŸ“ ğŸ—£ï¸ âœï¸ ğŸ”¬ â“\n\n1. ê°•ì˜ë…¸íŠ¸ ì›ë³¸ ë° ì˜ìƒ ë§í¬\nhttps://guebin.github.io/DL2025/posts/15wk-1.html\n\n\n2. Imports ğŸ“\n\n\n\n\n\n\nNote\n\n\n\nì½”ë©ì‚¬ìš©ìëŠ” ì•„ë˜ì½”ë“œ ì‹¤í–‰í›„ ì‹¤ìŠµí• ê²ƒ \n!pip install swig\n!pip install gymnasium[box2d]\ní•™ê³¼ì„œë²„ì‚¬ìš©ìëŠ” ê°€ìƒí™˜ê²½ì—ì„œ ì•„ë˜ë¥¼ ì„¤ì¹˜\nconda install conda-forge::gymnasium-box2d \n\n\n\nimport gymnasium as gym\n#--#\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\nimport IPython\n#--#\nimport collections\nimport random\n#--#\nimport torch\n\n\ndef show(imgs,jump=10):\n    imgs = imgs[::jump]\n    fig = plt.Figure()\n    ax = fig.subplots()\n    def update(i):\n        ax.imshow(imgs[i])\n    ani = FuncAnimation(fig,update,frames=len(imgs))\n    display(IPython.display.HTML(ani.to_jshtml()))\n\n\n\n3. ì˜ˆë¹„í•™ìŠµ ğŸ“\n- random.sample()ì˜ ìš©ë²•ì„ ì‚´í´ë³´ì.\nğŸ—£ï¸(\n\nì˜ˆì‹œ1\n\n[1,2,3,4,5] ì¤‘ 2ê°œë¥¼ ë½‘ìŒ\n\nì˜ˆì‹œ2\n\n3ê°œ ì¤‘ 2ê°œë¥¼ ë½‘ìŒ\n\n\n\ns = [[0,0], [0,2], [3,2]]\na = [0,1,2]\nmemory = list(zip(s,a))\nmemory\n\n[([0, 0], 0), ([0, 2], 1), ([3, 2], 2)]\n\n\n)ğŸ—£ï¸\n# ì˜ˆì‹œ1\n\nrandom.sample([1,2,3,4,5],2)\n\n[3, 2]\n\n\n# ì˜ˆì‹œ2\n\ns = [[0,0], [0,2], [3,2]]\na = [0,1,2]\nmemory = list(zip(s,a))\nrandom.sample(memory,2)\n\n[([0, 2], 1), ([0, 0], 0)]\n\n\n\n\n4. env: LunarLander ğŸ“\nğŸ—£ï¸(\n\ní™˜ê²½ì„ ì§ì ‘ ë§Œë“¤ì§€ ì•Šê³  ê¸°ì¡´ì— ìˆëŠ” í´ë˜ìŠ¤ í™œìš©\nê²Œì„ ì„¤ëª…\n\nì§€í˜•ì´ ë°”ë€œ\në°”ëŒì˜ ì˜í–¥ë„ ì¡°ê¸ˆ ìˆìŒ\nëª©ì : ì˜ ì¡°ì •ì„ í•´ì„œ ê¹ƒë°œ ì•ˆì— ë–¨ì–´ì§€ê²Œ í•¨\n\n\n)ğŸ—£ï¸\n- ref: https://gymnasium.farama.org/environments/box2d/lunar_lander/\n- Lunar Lander: ìš”ì•½\nObservation Space (State Space) â€“ 8ê°œì˜ ë³€ìˆ˜\n\nì°©ë¥™ì„ ì˜ x ì¢Œí‘œ\nì°©ë¥™ì„ ì˜ y ì¢Œí‘œ\nì°©ë¥™ì„ ì˜ x ì†ë„\nì°©ë¥™ì„ ì˜ y ì†ë„\nì°©ë¥™ì„ ì˜ ê°ë„\nì°©ë¥™ì„ ì˜ ê°ì†ë„\nì™¼ìª½ ë‹¤ë¦¬ê°€ ë•…ì— ë‹¿ì•„ìˆëŠ”ì§€ ì—¬ë¶€ (1 ë˜ëŠ” 0)\nì˜¤ë¥¸ìª½ ë‹¤ë¦¬ê°€ ë•…ì— ë‹¿ì•„ìˆëŠ”ì§€ ì—¬ë¶€ (1 ë˜ëŠ” 0)\n\nAction Space â€“ 4ê°œì˜ ë³€ìˆ˜\n\n{0 : ì•„ë¬´ í–‰ë™ë„ í•˜ì§€ ì•ŠìŒ}\n{1 : ì™¼ìª½ ì—”ì§„ ë°œì‚¬ (ì˜¤ë¥¸ìª½ìœ¼ë¡œ ê¸°ìš¸ì„)}\n{2 : ë©”ì¸ ì—”ì§„ ë°œì‚¬ (ìœ„ë¡œ ë°€ì–´ ì˜¬ë¦¼)}\n{3 : ì˜¤ë¥¸ìª½ ì—”ì§„ ë°œì‚¬ (ì™¼ìª½ìœ¼ë¡œ ê¸°ìš¸ì„)}\n\nReward\n\nê±°ë¦¬ ë³´ìƒ: ì°©ë¥™ íŒ¨ë“œì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë³´ìƒ ì¦ê°€\nì†ë„ ë³´ìƒ: ì†ë„ê°€ ë‚®ì„ìˆ˜ë¡ ë³´ìƒ ì¦ê°€\nê°ë„ ë³´ìƒ: ê°ë„ê°€ ìˆ˜ì§ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë³´ìƒ ì¦ê°€\nì°©ë¥™ ë‹¤ë¦¬ ë³´ìƒ: ë‹¤ë¦¬ê°€ ì°©ë¥™ íŒ¨ë“œì— ë‹¿ìœ¼ë©´ ë³´ìƒ\nì—°ë£Œ ì‚¬ìš© íŒ¨ë„í‹°: ì—”ì§„ ì‚¬ìš© ì‹œ íŒ¨ë„í‹°\nì•ˆì „í•œ ì°©ë¥™ ë³´ìƒ: ì•ˆì •ì ìœ¼ë¡œ ì°©ë¥™ ì‹œ í° ë³´ìƒ (+100~+140)\nì¶©ëŒ íŒ¨ë„í‹°: ì°©ë¥™ íŒ¨ë“œ ì´ì™¸ì˜ ì¥ì†Œì— ì¶©ëŒ ì‹œ íŒ¨ë„í‹° (-100)\n\n- í™˜ê²½ìƒì„±\n\nenv = gym.make(\"LunarLander-v3\", render_mode = \"rgb_array\")\nenv \n\n&lt;frozen importlib._bootstrap&gt;:228: RuntimeWarning: Your system is avx2 capable but pygame was not built with support for it. The performance of some of your blits could be adversely affected. Consider enabling compile time detection with environment variables like PYGAME_DETECT_AVX2=1 if you are compiling without cross compilation.\n\n\n&lt;TimeLimit&lt;OrderEnforcing&lt;PassiveEnvChecker&lt;LunarLander&lt;LunarLander-v3&gt;&gt;&gt;&gt;&gt;\n\n\n- state_space\nğŸ—£ï¸(\n\nenv.observation_space\n\nBox([ -2.5        -2.5       -10.        -10.         -6.2831855 -10.\n  -0.         -0.       ], [ 2.5        2.5       10.        10.         6.2831855 10.\n  1.         1.       ], (8,), float32)\n\n\n\në¬´ì—‡ì„ ì˜ë¯¸í•˜ëŠ”ì§€ ì˜ ëª¨ë¥´ê² ìŒ\nâœï¸ ë²”ìœ„?\nìƒ˜í”Œì„ ë½‘ì•„ë³´ë©´\n\n)ğŸ—£ï¸\n\nenv.observation_space.sample()\n\narray([-9.2788523e-01,  2.1088607e+00,  3.9729960e-03,  4.4444327e+00,\n        6.4698160e-01,  5.2979026e+00,  9.8654026e-01,  6.8356961e-01],\n      dtype=float32)\n\n\n\n8ê°œì˜ ìˆ«ìê°€ í¬í•¨ëœ arrayê°€ ë‚˜ì˜´\nì´ 8ê°œì˜ ìˆ«ìëŠ” ê°ê° ìƒíƒœë¥¼ ì˜ë¯¸í•¨\n\n- action_space\nğŸ—£ï¸(\n\nenv.action_space\n\nDiscrete(4)\n\n\n\nìƒ˜í”Œì„ ë½‘ì•„ë³´ë©´\n\n)ğŸ—£ï¸\n\nenv.action_space.sample()\n\nnp.int64(1)\n\n\n\n0,1,2,3 ì¤‘ í•˜ë‚˜ê°€ ëœë¤ìœ¼ë¡œ ë½‘í˜\n\n- env.reset()\n\nenv.reset()\n\n(array([ 0.00563831,  1.4064701 ,  0.5710932 , -0.19778925, -0.00652668,\n        -0.12936108,  0.        ,  0.        ], dtype=float32),\n {})\n\n\nğŸ—£ï¸(\n\nenv.start() ëŠë‚Œ\në‘ ê°œì˜ return ê°’\n\n\n{ }: ì¶”ê°€ì ì¸ information\n\n\n)ğŸ—£ï¸\n- env.render()\nğŸ—£ï¸(\n\nenv.render()\n\narray([[[  0,   0,   0],\n        [  0,   0,   0],\n        [  0,   0,   0],\n        ...,\n        [  0,   0,   0],\n        [  0,   0,   0],\n        [  0,   0,   0]],\n\n       [[  0,   0,   0],\n        [  0,   0,   0],\n        [  0,   0,   0],\n        ...,\n        [  0,   0,   0],\n        [  0,   0,   0],\n        [  0,   0,   0]],\n\n       [[  0,   0,   0],\n        [  0,   0,   0],\n        [  0,   0,   0],\n        ...,\n        [  0,   0,   0],\n        [  0,   0,   0],\n        [  0,   0,   0]],\n\n       ...,\n\n       [[255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255],\n        ...,\n        [255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255]],\n\n       [[255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255],\n        ...,\n        [255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255]],\n\n       [[255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255],\n        ...,\n        [255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255]]], dtype=uint8)\n\n\n\nenv.render().shape\n\n(400, 600, 3)\n\n\n\nì´ë¯¸ì§€ ê°™ìŒ\ní˜„ì¬ì˜ ê²Œì„ í™”ë©´ì„ ê·¸ë¦¼ìœ¼ë¡œ rendering í•˜ì—¬ ë³¼ ìˆ˜ ìˆê²Œ í•´ì¤Œ\n\nplt.imshow í•¨ìˆ˜ ì‚¬ìš©\n\n\n)ğŸ—£ï¸\n\nplt.imshow(env.render())\n\n\n\n\n\n\n\n\n- env.step\n\nenv.step??\n\n\nSignature: env.step(action: 'ActType') -&gt; 'tuple[ObsType, SupportsFloat, bool, bool, dict[str, Any]]'\nSource:   \n    def step(\n        self, action: ActType\n    ) -&gt; tuple[ObsType, SupportsFloat, bool, bool, dict[str, Any]]:\n        \"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\n        Args:\n            action: The environment step action\n        Returns:\n            The environment step ``(observation, reward, terminated, truncated, info)`` with `truncated=True`\n            if the number of steps elapsed &gt;= max episode steps\n        \"\"\"\n        observation, reward, terminated, truncated, info = self.env.step(action)\n        self._elapsed_steps += 1\n        if self._elapsed_steps &gt;= self._max_episode_steps:\n            truncated = True\n        return observation, reward, terminated, truncated, info\nFile:      ~/anaconda3/envs/dl2025/lib/python3.9/site-packages/gymnasium/wrappers/common.py\nType:      method\n\n\n\n\në¦¬í„´ë˜ëŠ” ê°’ì€ observation, reward, terminated, truncated, info\nìš°ë¦¬ê°€ ì“°ëŠ” ê°’ì€ observation, reward, terminated, truncated\n\nğŸ—£ï¸(\n\nactionì„ ì…ë ¥ìœ¼ë¡œ ë°›ìŒ\ntruncated\n\n1,000ë²ˆì˜ í”„ë ˆì„ ë™ì•ˆë§Œ ê²Œì„ í”Œë ˆì´ ê°€ëŠ¥\nê³„ì† ì˜¬ë¦¬ë©´ ê²Œì„ì´ ëë‚˜ì§€ ì•Šê¸° ë•Œë¬¸ì—\n\nì •ìƒì ìœ¼ë¡œ í”Œë ˆì´í•˜ë‹¤ê°€ clear\n\nterminated = True\ntruncated = False\n\nì •ìƒì ìœ¼ë¡œ í”Œë ˆì´í•˜ë‹¤ê°€ die\n\nterminated = True\ntruncated = False\n\nì •ìƒì ìœ¼ë¡œ í”Œë ˆì´í•˜ë‹¤ê°€ timeout\n\nterminated = False\ntruncated = True\n\n\n\nenv.step(0)\n\n(array([ 0.01127691,  1.4014435 ,  0.5703218 , -0.22344746, -0.01291902,\n        -0.12785819,  0.        ,  0.        ], dtype=float32),\n np.float64(-0.955798700878546),\n False,\n False,\n {})\n\n\n\nobservation, reward, terminated, truncated ìˆœ\ninfoëŠ” ë¹„ì–´ ìˆìŒ\n\n)ğŸ—£ï¸\n- play\nì²«ì‹œì‘í™”ë©´\n\nenv.reset()\nplt.imshow(env.render())\n\n\n\n\n\n\n\n\ní”Œë ˆì´í•´ë³´ì\nğŸ—£ï¸(\nAction Space -- 4ê°œì˜ ë³€ìˆ˜\n\n{0 : ì•„ë¬´ í–‰ë™ë„ í•˜ì§€ ì•ŠìŒ}\n{1 : ì™¼ìª½ ì—”ì§„ ë°œì‚¬ (ì˜¤ë¥¸ìª½ìœ¼ë¡œ ê¸°ìš¸ì„)}\n{2 : ë©”ì¸ ì—”ì§„ ë°œì‚¬ (ìœ„ë¡œ ë°€ì–´ ì˜¬ë¦¼)}\n{3 : ì˜¤ë¥¸ìª½ ì—”ì§„ ë°œì‚¬ (ì™¼ìª½ìœ¼ë¡œ ê¸°ìš¸ì„)}\n\nenv.step(0)\nplt.imshow(env.render())\n\n\n\n\n\n\n\n\n\nenv.step(0)\nplt.imshow(env.render())\n\n\n\n\n\n\n\n\n\në§¤ìš° ëŠë¦¼\n10ë²ˆ ì •ë„ ë°˜ë³µ ì‹¤í–‰ í•˜ë©´ (ì„¸ë¶€ actionì€ ê°•ì˜ ì˜ìƒê³¼ëŠ” ë‹¤ë¦„)\n\n\nfor _ in range(10):\n    env.step(0)\n    plt.imshow(env.render())\n\n\n\n\n\n\n\n\n\nfor _ in range(10):\n    env.step(0)\n    plt.imshow(env.render())\n\n\n\n\n\n\n\n\n\nfor _ in range(10):\n    env.step(0)\n    plt.imshow(env.render())\n\n\n\n\n\n\n\n\n\nfor _ in range(3):\n    env.step(3)\n    plt.imshow(env.render())\n\n\n\n\n\n\n\n\n\nfor _ in range(3):\n    env.step(2)\n    plt.imshow(env.render())\n\n\n\n\n\n\n\n\n\nfor _ in range(5):\n    env.step(0)\n    env.step(3)\n    plt.imshow(env.render())\n\n\n\n\n\n\n\n\n\nfor _ in range(5):\n    env.step(0)\n    env.step(0)\n    env.step(0)\n    plt.imshow(env.render())\n\n\n\n\n\n\n\n\n\nprint(env.step(0))\n\n(array([ 0.21715555, -0.04394788,  0.40055856, -0.02491728, -0.06356283,\n       -0.31115538,  0.        ,  0.        ], dtype=float32), -100, True, False, {})\n\n\n\nreward: -100, terminated: True =&gt; die\n\n)ğŸ—£ï¸\n\nfor _ in range(5):\n    env.step(0) \n    env.step(3) \nplt.imshow(env.render())\n\n\n\n\n\n\n\n\n\n\n5. ì‹œê°í™” ğŸ“\nğŸ—£ï¸(\n\nenv.reset()\n\n(array([-1.2360573e-03,  1.4078571e+00, -1.2521335e-01, -1.3613933e-01,\n         1.4390640e-03,  2.8362691e-02,  0.0000000e+00,  0.0000000e+00],\n       dtype=float32),\n {})\n\n\n\nstate, _ = env.reset()\nstate\n\narray([-0.00307007,  1.4083712 , -0.31098348, -0.1132924 ,  0.00356426,\n        0.07044236,  0.        ,  0.        ], dtype=float32)\n\n\n\nimgs: ì´ë¯¸ì§€ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\nshow: ì²˜ìŒì— ì •ì˜í•œ í•¨ìˆ˜\n\n)ğŸ—£ï¸\n\nstate, _ = env.reset()\nimgs = []\nfor t in range(500):\n    action = env.action_space.sample()\n    next_state, reward, terminated, truncated, _ = env.step(action)\n    imgs.append(env.render())\n    state = next_state \n    if terminated or truncated:\n        break\n\n\nshow(imgs)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n6. RandomAgent ğŸ“\nğŸ—£ï¸(\nclass RandomAgent:\n    def __init__(self):\n        pass\n    def act(self):\n        pass\n    def learn(self):\n        pass\n    def save_experience(self):\n        self.states.append(self.state)\n        self.actions.append(self.action)\n        self.rewards.append(self.reward)\n        self.next_states.append(self.next_state)\n        self.terminations.append(self.terminated)\nclass RandomAgent:\n    def __init__(self):\n        self.state = None\n        self.action = None\n        self.reward = None\n        self.next_state = None\n        self.terminated = None\n        #---#\n        self.states = collections.deque(maxlen = 5000)\n        self.actions = collections.deque(maxlen = 5000)\n        self.rewards = collections.deque(maxlen = 5000)\n        self.next_states = collections.deque(maxlen = 5000)\n        self.terminations = collections.deque(maxlen = 5000)\n    def act(self):\n        pass\n    def learn(self):\n        pass\n    def save_experience(self):\n        self.states.append(self.state)\n        self.actions.append(self.action)\n        self.rewards.append(self.reward)\n        self.next_states.append(self.next_state)\n        self.terminations.append(self.terminated)\n\ntruncatedëŠ” êµ³ì´ ì“¸ í•„ìš”ê°€ ì—†ì„ ê²ƒ ê°™ì•„ ì €ì¥ X\n\n\nclass RandomAgent:\n    def __init__(self):\n        self.action_space = gym.spaces.Discrete(4)\n        #---#\n        self.state = None\n        self.action = None\n        self.reward = None\n        self.next_state = None\n        self.terminated = None\n        #---#\n        self.states = collections.deque(maxlen = 5000)\n        self.actions = collections.deque(maxlen = 5000)\n        self.rewards = collections.deque(maxlen = 5000)\n        self.next_states = collections.deque(maxlen = 5000)\n        self.terminations = collections.deque(maxlen = 5000)\n    def act(self):\n        self.action = self.action_space.sample()\n    def learn(self):\n        pass\n    def save_experience(self):\n        self.states.append(self.state)\n        self.actions.append(self.action)\n        self.rewards.append(self.reward)\n        self.next_states.append(self.next_state)\n        self.terminations.append(self.terminated)\n\n\nstep 1: action\nstep 2: ì „ë‹¬\nstep 3: save & learn\nstep 4: ë‹¤ìŒ iteration ì¤€ë¹„\n\n\nenv = gym.make(\"LunarLander-v3\", render_mode = \"rgb_array\")\nplayer = RandomAgent()\nplayer.state,_ = env.reset()\nfor e in range(1,101):\n    while True:\n        # step1\n        player.act()\n        # step2\n        player.next_state, player.reward, player.terminated, player.truncated, _ = env.step(player.action)\n        # step3\n        player.save_experience()\n        player.learn()\n        # step4 \n        if player.terminated or player.truncated: \n            player.state, _ = env.reset()\n            break \n        else: \n            player.state = player.next_state\n\n\nì˜ ì €ì¥ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸\n\n\n# player.states # numpy array\n\n\n# player.actions\n\n\nplayer.truncated\n\nFalse\n\n\n\n# player.terminations\n\n\nì°¸ê³ ) ê°•ì˜ ë…¸íŠ¸ ì›ë³¸ (ì•„ë˜) RandomAgentëŠ” save_experienceì—ì„œ torch.tensorê°€ ë“¤ì–´ê°€ ìˆìŒ\n7.4 ì „ê¹Œì§€ëŠ” ê·¸ëƒ¥ ì—†ëŠ” ë²„ì „ìœ¼ë¡œ ì‹¤í–‰í•˜ì˜€ê³  (ê°•ì˜ ë…¸íŠ¸ ì›ë³¸ ì½”ë“œ ì œì™¸, ê°•ì˜ ë…¸íŠ¸ ì›ë³¸ ì½”ë“œëŠ” ì‹¤í–‰ ì•ˆ í•¨)\n7.4ë¶€í„°ëŠ” ì›ë³¸ ë²„ì „ìœ¼ë¡œ ì‹¤í–‰\n\n)ğŸ—£ï¸\n\nclass RandomAgent:\n    def __init__(self):\n        self.action_spcae = gym.spaces.Discrete(4)\n        self.n_experieces = 0 \n        #---#\n        self.state = None \n        self.action = None \n        self.reward = None \n        self.next_state = None \n        self.terminated = None \n        #---#\n        self.states = collections.deque(maxlen = 5000)\n        self.actions = collections.deque(maxlen = 5000)\n        self.rewards = collections.deque(maxlen = 5000)\n        self.next_states = collections.deque(maxlen = 5000)\n        self.terminations = collections.deque(maxlen = 5000)\n    def act(self):\n        self.action = self.action_spcae.sample()\n    def learn(self):\n        pass \n    def save_experience(self):\n        self.states.append(torch.tensor(self.state))\n        self.actions.append(self.action)\n        self.rewards.append(self.reward)\n        self.next_states.append(torch.tensor(self.next_state))\n        self.terminations.append(self.terminated)\n        self.n_experieces = self.n_experieces+1\n\n\nenv = gym.make(\"LunarLander-v3\", render_mode = \"rgb_array\")\nplayer = RandomAgent()\nplayer.state,_ = env.reset()\nfor e in range(1,101):\n    while True:\n        # step1\n        player.act()\n        # step2\n        player.next_state, player.reward, player.terminated, player.truncated, _ = env.step(player.action)\n        # step3\n        player.save_experience()\n        player.learn()\n        # step4 \n        if player.terminated or player.truncated: \n            player.state, _ = env.reset()\n            break \n        else: \n            player.state = player.next_state\n\n\n\n7. q_net ğŸ“\n\n# ì´ì œ ìš°ë¦¬ê°€ í• ê²ƒ: q_table --&gt; action ì„ ê²°ì •í•´ì•¼í•¨. \n\n# 4x4 ê·¸ë¦¬ë“œ -- ë³µìŠµ \n# q_table[ìƒíƒœ] = [í–‰ë™0ì„í–ˆì„ë•Œ í’ˆì§ˆ, í–‰ë™1ì„í–ˆì„ë•Œí’ˆì§ˆ, í–‰ë™2ë¥¼í–ˆì„ë•Œí’ˆì§ˆ, í–‰ë™3ì„í–ˆì„ë•Œí’ˆì§ˆ]  \n# í–‰ë™ = argmax(q_table[ìƒíƒœ])\n\n# ë£¨ë‚˜ëœë” -- ì˜¤ëŠ˜ í• ê²ƒ \n# q_net[8ê°œì˜ìˆ«ì] = [í–‰ë™0ì„í–ˆì„ë•Œ í’ˆì§ˆ, í–‰ë™1ì„í–ˆì„ë•Œí’ˆì§ˆ, í–‰ë™2ë¥¼í–ˆì„ë•Œí’ˆì§ˆ, í–‰ë™3ì„í–ˆì„ë•Œí’ˆì§ˆ] # ê²°êµ­ ìˆ«ì8ê°œë¥¼ ìˆ«ì4ê°œë¡œ ë§Œë“¤ì–´ì£¼ëŠ” ì ë‹¹í•œ q_netì„ êµ¬ì„±\n# í–‰ë™ = argmax(q_net[8ê°œì˜ìˆ«ì])\n\nğŸ—£ï¸(\n\nê³¼ê±°\n\nq_table[0,0, 0] = ??\nq_table[0,0, 1] = ??\nq_table[0,0, 2] = ??\nq_table[0,0, 3] = ??\n#---#\nq_table[0,1, 0] = ??\nq_table[0,1, 1] = ??\nq_table[0,1, 2] = ??\nq_table[0,1, 3] = ??\n\ní˜„ì¬\n\nìˆ«ìê°€ ì—°ì†í˜•ì´ë¼ ê¹Œë‹¤ë¡œì›€\n\n\n\nenv.observation_space.sample()\n\narray([ 0.38724837,  0.19059889, -6.182767  ,  3.6453786 , -3.1740656 ,\n        8.74314   ,  0.31359115,  0.6998068 ], dtype=float32)\n\n\n\nenv.observation_space.sample()\n\narray([-0.54199886,  0.37739053, -6.8368096 ,  8.243433  , -1.4476558 ,\n       -4.5691466 ,  0.67033297,  0.26339018], dtype=float32)\n\n\nq_table[0.38724837,  0.19059889, -6.182767  ,  3.6453786 , -3.1740656 , 8.74314   ,  0.31359115,  0.6998068, 0] = ??\nq_table[0.38724837,  0.19059889, -6.182767  ,  3.6453786 , -3.1740656 , 8.74314   ,  0.31359115,  0.6998068, 1] = ??\nq_table[0.38724837,  0.19059889, -6.182767  ,  3.6453786 , -3.1740656 , 8.74314   ,  0.31359115,  0.6998068, 2] = ??\nq_table[0.38724837,  0.19059889, -6.182767  ,  3.6453786 , -3.1740656 , 8.74314   ,  0.31359115,  0.6998068, 3] = ??\n#---#\nq_table[-0.54199886,  0.37739053, -6.8368096 ,  8.243433  , -1.4476558 , -4.5691466 ,  0.67033297,  0.26339018, 0] = ??\nq_table[-0.54199886,  0.37739053, -6.8368096 ,  8.243433  , -1.4476558 , -4.5691466 ,  0.67033297,  0.26339018, 1] = ??\nq_table[-0.54199886,  0.37739053, -6.8368096 ,  8.243433  , -1.4476558 , -4.5691466 ,  0.67033297,  0.26339018, 2] = ??\nq_table[-0.54199886,  0.37739053, -6.8368096 ,  8.243433  , -1.4476558 , -4.5691466 ,  0.67033297,  0.26339018, 3] = ??\n)ğŸ—£ï¸\n- ì „ëµ: 4x4ì—ì„œ q_tableì— ëŒ€ì‘í•˜ëŠ” ë­”ê°€ê°€ ìˆìœ¼ë©´ ëœë‹¤. ê·¸ëŸ°ë° q_tableì™€ ê°™ì´ í…Œì´ë¸” í˜•ì‹ìœ¼ë¡œëŠ” í˜ë“¤ê²ƒ ê°™ë‹¤. \\(\\to\\) q_netë¥¼ ë§Œë“¤ì.\n\n4x4 grid: ìƒíƒœê³µê°„ì˜ ì°¨ì›ì€ 2ì°¨ì›ì´ë©° ê°€ì§ˆìˆ˜ ìˆëŠ” ê°’ì€ 16ê°œ, ê° ìƒíƒœê³µê°„ì—ì„œ í• ìˆ˜ ìˆëŠ” í–‰ë™ì´ 4ê°œ -&gt; ì´ 16*4ì˜ ê²½ìš°ì˜ ìˆ˜ì— ëŒ€í•œ rewardë§Œ ì¡°ì‚¬í•˜ë©´ ë˜ì—ˆìŒ\nLunarLander: ìƒíƒœê³µê°„ì˜ ì°¨ì›ì€ 8ì°¨ì›ì´ì§€ë§Œ ê°€ì§ˆìˆ˜ ìˆëŠ” ê°’ì˜ ë²”ìœ„ëŠ” ë¬´í•œëŒ€ -&gt; ë¬´ìˆ˜íˆ ë§ì€ ê²½ìš°ì— ëŒ€í•œ reward ê°’ì„ ì¡°ì‚¬í•˜ëŠ”ê±´ í˜„ì‹¤ì ìœ¼ë¡œ ë¶ˆê°€ëŠ¥\n\nìƒí™©\n\nstate = player.states[100]\naction = player.actions[100]\nreward = player.rewards[100]\nnext_state = player.next_states[100]\nterminated = player.terminations[100]\n\n1. q_net\nğŸ—£ï¸(\n\n# player.q_net # í˜„ì¬ ì—†ì–´ì„œ error\n\n\nplayer.q_net = torch.nn.Sequential(\n    torch.nn.Linear(8,256),\n    torch.nn.ReLU(),\n    torch.nn.Linear(256,128),\n    torch.nn.ReLU(),\n    torch.nn.Linear(128,64),\n    torch.nn.ReLU(),\n    torch.nn.Linear(64,4)\n)\n\n\n4ê°œì˜ ì¸µ\n\n\nplayer.q_net\n\nSequential(\n  (0): Linear(in_features=8, out_features=256, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=256, out_features=128, bias=True)\n  (3): ReLU()\n  (4): Linear(in_features=128, out_features=64, bias=True)\n  (5): ReLU()\n  (6): Linear(in_features=64, out_features=4, bias=True)\n)\n\n\n\ntorch.tensor(env.observation_space.sample())\n\ntensor([-1.3189,  0.9664,  2.3762, -7.5418,  5.8477,  0.6544,  0.3398,  0.7094])\n\n\n\nplayer.q_net(torch.tensor(env.observation_space.sample()))\n\ntensor([-0.1090,  0.2078, -0.2280,  0.0221], grad_fn=&lt;ViewBackward0&gt;)\n\n\n\n8ê°œì˜ ìˆ«ìë¥¼ ë„£ìœ¼ë©´ 4ê°œì˜ ìˆ«ìê°€ ë‚˜ì˜´\n\n\nstate = torch.tensor(env.observation_space.sample())\nplayer.q_net(state)\n\ntensor([-0.0012,  0.0180, -0.3318, -0.1024], grad_fn=&lt;ViewBackward0&gt;)\n\n\n\nnotation ì •ë¦¬ (ê°•ì˜ ë…¸íŠ¸ ìƒí™© ì„¤ì • ì „)\n\n\nstate = env.observation_space.sample()\ns = torch.tensor(state)\nplayer.q_net(s)\n\ntensor([-0.1517, -0.0282, -0.1357, -0.0040], grad_fn=&lt;ViewBackward0&gt;)\n\n\n\nìƒí™© ì„¤ì • í›„\n\n\nstate = player.states[100]\naction = player.actions[100]\nreward = player.rewards[100]\nnext_state = player.next_states[100]\nterminated = player.terminations[100]\n\n\nplayer.q_net(torch.tensor(state))\n\ntensor([ 0.0353,  0.0744, -0.0374, -0.0312], grad_fn=&lt;ViewBackward0&gt;)\n\n\n)ğŸ—£ï¸\n\nplayer.q_net = torch.nn.Sequential(\n    torch.nn.Linear(8,256),\n    torch.nn.ReLU(),\n    torch.nn.Linear(256,128),\n    torch.nn.ReLU(),\n    torch.nn.Linear(128,64),\n    torch.nn.ReLU(),\n    torch.nn.Linear(64,4)\n)\n\n\nplayer.q_net(state)\n\ntensor([-0.1211,  0.1420,  0.0979, -0.1142], grad_fn=&lt;ViewBackward0&gt;)\n\n\n\n8ê°œì˜ ìˆ«ìê°€ ë“¤ì–´ê°€ì„œ 4ê°œì˜ ìˆ«ìê°€ ë‚˜ì˜´\n\n2. q_hat\nğŸ—£ï¸(\n\nstate = torch.tensor(env.observation_space.sample())\nplayer.q_net(state)\n\ntensor([-0.0392, -0.0271, -0.1871, -0.0522], grad_fn=&lt;ViewBackward0&gt;)\n\n\n\naction 0,1,2,3ì„ í–ˆì„ ë•Œì˜ í’ˆì§ˆ\n\n\nstate = torch.tensor(env.observation_space.sample())\naction = 0\nplayer.q_net(state)[action]\n\ntensor(-0.0842, grad_fn=&lt;SelectBackward0&gt;)\n\n\n\nnotation ì •ë¦¬ (ê°•ì˜ ë…¸íŠ¸ ìƒí™© ì„¤ì • ì „)\n\n\nstate = env.observation_space.sample()\ns = torch.tensor(state)\na = action = 0\nplayer.q_net(s)[a]\n\ntensor(-0.1071, grad_fn=&lt;SelectBackward0&gt;)\n\n\n\nìƒí™© ì„¤ì • í›„\n\n\nstate = player.states[100]\naction = player.actions[100]\nreward = player.rewards[100]\nnext_state = player.next_states[100]\nterminated = player.terminations[100]\n\n\nq_hat = player.q_net(torch.tensor(state))[action]\n\n\nq_hat\n\ntensor(0.0744, grad_fn=&lt;SelectBackward0&gt;)\n\n\n)ğŸ—£ï¸\n\nq_hat = player.q_net(state)[action]\n\n3. q (\\(q = r + 0.99 \\times {\\tt future}\\))\nğŸ—£ï¸(\n\nenv.step(action)\n\n(array([-0.01383076,  1.4314287 , -0.69949836,  0.442844  ,  0.01585995,\n         0.15681908,  0.        ,  0.        ], dtype=float32),\n np.float64(-0.30288280327448547),\n False,\n False,\n {})\n\n\n\n# next_state, reward, terminated, truncated, _ = env.step(action)\n# future = player.qnet(torch.tensor(next_state)).max()\n# reward + 0.99 * future\n\n\nìƒí™© ì„¤ì • í›„\n\n\nstate = player.states[100]\naction = player.actions[100]\nreward = player.rewards[100]\nnext_state = player.next_states[100]\nterminated = player.terminations[100]\n\n\nfuture = player.q_net(torch.tensor(next_state)).max()\nq = reward + 0.99 * future\n\n\nq\n\ntensor(0.6048, grad_fn=&lt;AddBackward0&gt;)\n\n\n\nq_hatì€ í˜„ì¬ ì—‰í„°ë¦¬ ê°’\nëª©í‘œ: q_hat \\(\\approx\\) q\nê·¸ëŸ¬ë‚˜ yê°€ ì£¼ì–´ì ¸ ìˆëŠ” ì´ì „ ìƒí™© y_hat \\(\\approx\\) y ì™€ ë‹¬ë¦¬ í˜„ì¬ëŠ” q ë„ ë„¤íŠ¸ì›Œí¬ì—ì„œ ë‚˜ì˜´\n\nrewardë§Œ ì£¼ì–´ì ¸ ìˆê³  ë‚˜ë¨¸ì§€ëŠ” ì•„ë‹ˆë¼ ì• ë§¤í•¨\n\nqë¥¼ ì£¼ì–´ì§„ ê°’ìœ¼ë¡œ ìƒê°í•˜ë©´\n\n\nfuture\n\ntensor(0.0752, grad_fn=&lt;MaxBackward1&gt;)\n\n\n\nfuture = player.q_net(torch.tensor(next_state)).max().data\nq = reward + 0.99 * future\n\n\nq_hat, q\n\n(tensor(0.0744, grad_fn=&lt;SelectBackward0&gt;), tensor(0.6048))\n\n\n\nq_hatê³¼ qê°€ ë¹„ìŠ·í•´ì§ˆ ë•Œê¹Œì§€ í•™ìŠµì´ ë˜ê² ë‹¤ê³  ìƒê° ê°€ëŠ¥\n\n\nif terminated: \n    q = reward # qëŠ” ê¼¬ë¦¬í‘œê°€ ì—†ëŠ” ìˆ«ì \nelse: \n    future = player.q_net(torch.tensor(next_state)).max().data # futureì— ê¼¬ë¦¬í‘œê°€ ìˆìœ¼ë©´ qì—ë„ ê¼¬ë¦¬í‘œê°€ ìƒê¸°ë¯€ë¡œ ê¼¬ë¦¬í‘œ ì œê±° \n    q = reward + 0.99 * future # qëŠ” ê¼¬ë¦¬í‘œê°€ ì—†ëŠ” ìˆ«ì \n\n)ğŸ—£ï¸\n\nif terminated: \n    q = reward # qëŠ” ê¼¬ë¦¬í‘œê°€ ì—†ëŠ” ìˆ«ì \nelse: \n    future = player.q_net(next_state).max().data # futureì— ê¼¬ë¦¬í‘œê°€ ìˆìœ¼ë©´ qì—ë„ ê¼¬ë¦¬í‘œê°€ ìƒê¸°ë¯€ë¡œ ê¼¬ë¦¬í‘œ ì œê±° \n    q = reward + 0.99 * future # qëŠ” ê¼¬ë¦¬í‘œê°€ ì—†ëŠ” ìˆ«ì \n\n4. q_hat ì„ ì ì  q ì™€ ë¹„ìŠ·í•˜ê²Œ ë§Œë“œëŠ” ê³¼ì • = player.q_netë¥¼ í•™ìŠµí•˜ëŠ” ê³¼ì •\nğŸ—£ï¸(\n\nì—¬ê¸°ì„œë¶€í„°ëŠ” ê°•ì˜ ë…¸íŠ¸ ì›ë³¸ ë²„ì „ìœ¼ë¡œ Agent ì‹¤í–‰\n\n\nclass RandomAgent:\n    def __init__(self):\n        self.action_spcae = gym.spaces.Discrete(4)\n        self.n_experieces = 0 \n        #---#\n        self.state = None \n        self.action = None \n        self.reward = None \n        self.next_state = None \n        self.terminated = None \n        #---#\n        self.states = collections.deque(maxlen = 5000)\n        self.actions = collections.deque(maxlen = 5000)\n        self.rewards = collections.deque(maxlen = 5000)\n        self.next_states = collections.deque(maxlen = 5000)\n        self.terminations = collections.deque(maxlen = 5000)\n    def act(self):\n        self.action = self.action_spcae.sample()\n    def learn(self):\n        pass \n    def save_experience(self):\n        self.states.append(torch.tensor(self.state))\n        self.actions.append(self.action)\n        self.rewards.append(self.reward)\n        self.next_states.append(torch.tensor(self.next_state))\n        self.terminations.append(self.terminated)\n        self.n_experieces = self.n_experieces+1\n\n\nenv = gym.make(\"LunarLander-v3\", render_mode = \"rgb_array\")\nplayer = RandomAgent()\nplayer.state,_ = env.reset()\nfor e in range(1,101):\n    while True:\n        # step1\n        player.act()\n        # step2\n        player.next_state, player.reward, player.terminated, player.truncated, _ = env.step(player.action)\n        # step3\n        player.save_experience()\n        player.learn()\n        # step4 \n        if player.terminated or player.truncated: \n            player.state, _ = env.reset()\n            break \n        else: \n            player.state = player.next_state\n\n\nstate = player.states[100]\naction = player.actions[100]\nreward = player.rewards[100]\nnext_state = player.next_states[100]\nterminated = player.terminations[100]\n\n\nplayer.q_net = torch.nn.Sequential(\n    torch.nn.Linear(8,256),\n    torch.nn.ReLU(),\n    torch.nn.Linear(256,128),\n    torch.nn.ReLU(),\n    torch.nn.Linear(128,64),\n    torch.nn.ReLU(),\n    torch.nn.Linear(64,4)\n)\n\n\nplayer.q_net(state)\n\ntensor([ 0.0076, -0.0058, -0.0310,  0.1106], grad_fn=&lt;ViewBackward0&gt;)\n\n\n\nq_hat = player.q_net(state)[action]\n\n\nif terminated: \n    q = reward # qëŠ” ê¼¬ë¦¬í‘œê°€ ì—†ëŠ” ìˆ«ì \nelse: \n    future = player.q_net(next_state).max().data # futureì— ê¼¬ë¦¬í‘œê°€ ìˆìœ¼ë©´ qì—ë„ ê¼¬ë¦¬í‘œê°€ ìƒê¸°ë¯€ë¡œ ê¼¬ë¦¬í‘œ ì œê±° \n    q = reward + 0.99 * future # qëŠ” ê¼¬ë¦¬í‘œê°€ ì—†ëŠ” ìˆ«ì \n\n\nì´ì „ ì½”ë“œ ì‹¤í–‰ ë\n\n\n# loss = (q_hat - q)**2\n# lossë¥¼ ì ì°¨ ì¤„ì´ë©´ë¨\n\n\nplayer.q_net.parameters()ë¥¼ í•™ìŠµí•´ì•¼í•˜ë¯€ë¡œ optimizerê°€ í•„ìš”í•¨\n\n\nplayer.optimizr = torch.optim.Adam(player.q_net.parameters())\n\n\nq_hat\n\ntensor(0.0076, grad_fn=&lt;SelectBackward0&gt;)\n\n\n\nq\n\ntensor(-0.3956)\n\n\n\nq_hatê³¼ qê°€ ìŠ¤ì¹¼ë¼ì´ë¯€ë¡œ lossë¥¼ ìŠ¤ì¹¼ë¼ë¡œ ì½”ë“œë¥¼ ì§œì„œ ê³„ì‚°í•˜ë ¤ê³  í•¨\në²¡í„°ë¡œ ê³„ì‚°í•˜ëŠ” ê²ƒì´ ì¢‹ì§€ë§Œ ë‹¹ì¥ ì“°ê¸° ê¹Œë‹¤ë¡œì›Œì„œ (ì§€ì €ë¶„ í•¨)\n\n\nplayer.optimizr = torch.optim.Adam(player.q_net.parameters())\nmemory = list(zip(player.states, player.actions, player.rewards, player.next_states, player.terminations))\n\n\nlen(memory)\n\n5000\n\n\n\nmemory[0]\n\n(tensor([-0.0021,  1.4146, -0.2150,  0.1644,  0.0025,  0.0487,  0.0000,  0.0000]),\n np.int64(1),\n np.float64(0.03960169070262623),\n tensor([-0.0043,  1.4177, -0.2233,  0.1385,  0.0066,  0.0826,  0.0000,  0.0000]),\n False)\n\n\n\nmemory[1]\n\n(tensor([-0.0043,  1.4177, -0.2233,  0.1385,  0.0066,  0.0826,  0.0000,  0.0000]),\n np.int64(1),\n np.float64(-0.4859194165555596),\n tensor([-0.0066,  1.4203, -0.2332,  0.1118,  0.0127,  0.1223,  0.0000,  0.0000]),\n False)\n\n\n\nmemory[2]\n\n(tensor([-0.0066,  1.4203, -0.2332,  0.1118,  0.0127,  0.1223,  0.0000,  0.0000]),\n np.int64(0),\n np.float64(0.2302028958607707),\n tensor([-0.0089,  1.4222, -0.2333,  0.0851,  0.0188,  0.1223,  0.0000,  0.0000]),\n False)\n\n\n\nloss ê³„ì‚°\n\n\nloss = 0\nfor s, a, r, ss, tmd in memory:\n    qhat = player.q_net(s)[a]\n    if tmd:\n        q = r\n    else:\n        future = player.q_net(ss).max().data\n        q = r + 0.99 * future\n    loss = loss + (q_hat - q)**2\nloss = loss / 5000\n\n\nloss\n\ntensor(126.6637, grad_fn=&lt;DivBackward0&gt;)\n\n\nì—¬ëŸ¬ ê²½í—˜ì´ ìˆëŠ”ë°,\nì €ì¥ëœ ê²ƒì„ ì—…ë°ì´íŠ¸í•  ë•Œ\n1 2 3 ... 5000 -- ì—…ë°ì´íŠ¸ --&gt;\n2~5001 -- ì—…ë°ì´íŠ¸ --&gt;\n3~5002 -- ì—…ë°ì´íŠ¸ --&gt;\nì´ëŸ° ì‹ìœ¼ë¡œ ì „ë¶€ í•˜ì§€ë§ê³ \nëª‡ê°œ ì”©ë§Œ ë½‘ì•„ì„œ ì—…ë°ì´íŠ¸ë¥¼ í•˜ë©´\ní•™ìŠµì´ ì¡°ê¸ˆ ë” ì•ˆì •ì ìœ¼ë¡œ ì˜ ë¨\n\nloss = 0\nmini_batch = random.sample(memory, 64)\nfor s, a, r, ss, tmd in mini_batch:\n    qhat = player.q_net(s)[a]\n    if tmd:\n        q = r\n    else:\n        future = player.q_net(ss).max().data\n        q = r + 0.99 * future\n    loss = loss + (q_hat - q)**2\nloss = loss / 64\n\n\nloss\n\ntensor(6.0927, grad_fn=&lt;DivBackward0&gt;)\n\n\n\nì •ë¦¬\n\n\nplayer.optimizr = torch.optim.Adam(player.q_net.parameters())\nfor epoc in range(5):\n    memory = list(zip(player.states, player.actions, player.rewards, player.next_states, player.terminations))\n    mini_batch = random.sample(memory,64)\n    # step1-2 \n    loss = 0\n    for s,a,r,ss,tmd in mini_batch:\n        q_hat = player.q_net(s)[a]\n        if tmd: \n            q = r \n        else: \n            future = player.q_net(ss).max().data \n            q = r + 0.99 * future\n        loss = loss + (q_hat-q)**2\n    loss = loss / 64\n    # step3\n    loss.backward()\n    # step4 \n    player.optimizr.step()\n    player.optimizr.zero_grad()\n\n)ğŸ—£ï¸\n\n# loss = (q_hat - q)**2\n# lossë¥¼ ì ì°¨ ì¤„ì´ë©´ë¨\n\n\nplayer.optimizr = torch.optim.Adam(player.q_net.parameters())\nfor epoc in range(5):\n    memory = list(zip(player.states, player.actions, player.rewards, player.next_states, player.terminations))\n    mini_batch = random.sample(memory,64)\n    # step1-2 \n    loss = 0\n    for s,a,r,ss,tmd in mini_batch:\n        q_hat = player.q_net(s)[a]\n        if tmd: \n            q = r \n        else: \n            future = player.q_net(ss).max().data \n            q = r + 0.99 * future\n        loss = loss + (q_hat-q)**2\n    loss = loss / 64\n    # step3\n    loss.backward()\n    # step4 \n    player.optimizr.step()\n    player.optimizr.zero_grad()\n\n5. í–‰ë™..?\nğŸ—£ï¸(\n\nplayer.q_net(s) # í˜„ì¬ ìƒíƒœ ë„£ê¸°\n\ntensor([-0.1928, -0.0458, -0.0677,  0.0760], grad_fn=&lt;ViewBackward0&gt;)\n\n\n\nplayer.q_net(s).argmax()\n\ntensor(3)\n\n\n\nplayer.q_net(s).argmax().item() # action\n\n3\n\n\n\n# ì´ì „ì—ëŠ” ì•„ë˜ì™€ ê°™ì€ ë°©ì‹\n## 1. íŠ¹ì • ì‹œì  ì´ì „ì—ëŠ” ê³„ì† ëœë¤ì•¡ì…˜ë§Œ\n## 2. íŠ¹ì • ì‹œì  ì´í›„ì—ëŠ” ê³„ì† q_tableì—ì„œ ë„ì¶œë˜ëŠ” í–‰ë™ë§Œ \n# ì´ë²ˆì—ëŠ” ì•„ë˜ì™€ ê°™ì´ í•´ë³´ì. \n## 1. ì²˜ìŒì—ëŠ” ëœë¤ì•¡ì…˜\n## 2. ì ì°¨ ì—í”¼ì†Œë“œê°€ ì§€ë‚ ìˆ˜ë¡, q_netì—ì„œ ê·¼ê±°í•œ í–‰ë™ë§Œ\n\n\nrandom.random() # 0ê³¼ 1 ì‚¬ì´\n\n0.36176671095762414\n\n\n\nplayer.state\n\narray([ 1.2628555e-03,  1.3995621e+00,  1.2789896e-01, -5.0480115e-01,\n       -1.4565670e-03, -2.8971046e-02,  0.0000000e+00,  0.0000000e+00],\n      dtype=float32)\n\n\n\nplayer.eps = 0.5\nif random.random() &lt; player.eps:\n    player.action = player.action_space.sample()\nelse:\n    state = torch.tensor(player.state)\n    player.action = player.q_net(state).argmax().item()\n\n\n# ë‹¤ìŒ ì—í”¼ì†Œë“œì—ì„œëŠ” ì•„ë˜ì™€ ê°™ì´ í™•ë¥ ì„ ì¡°ê¸ˆ ë‚®ê²Œ\nplayer.eps = player.eps*0.99\n\n)ğŸ—£ï¸\n\n# ì´ì „ì—ëŠ” ì•„ë˜ì™€ ê°™ì€ ë°©ì‹\n## 1. íŠ¹ì • ì‹œì  ì´ì „ì—ëŠ” ê³„ì† ëœë¤ì•¡ì…˜ë§Œ\n## 2. íŠ¹ì • ì‹œì  ì´í›„ì—ëŠ” ê³„ì† q_tableì—ì„œ ë„ì¶œë˜ëŠ” í–‰ë™ë§Œ \n# ì´ë²ˆì—ëŠ” ì•„ë˜ì™€ ê°™ì´ í•´ë³´ì. \n## 1. ì²˜ìŒì—ëŠ” ëœë¤ì•¡ì…˜\n## 2. ì ì°¨ ì—í”¼ì†Œë“œê°€ ì§€ë‚ ìˆ˜ë¡, q_netì—ì„œ ê·¼ê±°í•œ í–‰ë™ë§Œ\n\n\nplayer.eps = 0.5 \nif random.random() &lt; player.eps: \n    player.action = player.action_spcae.sample()\nelse:\n    state = torch.tensor(player.state)\n    player.action = player.q_net(state).argmax().item()\n\n\n# ë‹¤ìŒì—í”¼ì†Œë“œì—ì„œëŠ” ì•„ë˜ì™€ ê°™ì´ í™•ë¥ ì„ ì¡°ê¸ˆ ë‚®ê²Œ \nplayer.eps = player.eps* 0.99 \n\n\n\n8. Agent ğŸ“\nğŸ—£ï¸(\n\nìœ„ì˜ ë‚´ìš©ë“¤ì„ ì¢…í•©í•˜ì—¬ ì‹¤ì œë¡œ ë™ì‘í•˜ëŠ” Agentë¥¼ ë§Œë“¤ë ¤ê³  í•¨\n\n\nclass Agent(RandomAgent):\n    def __init__(self):\n        super().__init__()\n        self.eps = 1.0\n        self.q_net = torch.nn.Sequential(\n            torch.nn.Linear(8,256),\n            torch.nn.ReLU(),\n            torch.nn.Linear(256,128),\n            torch.nn.ReLU(),\n            torch.nn.Linear(128,64),\n            torch.nn.ReLU(),\n            torch.nn.Linear(64,4)\n        )\n        self.optimizr = torch.optim.Adam(self.q_net.parameters())\n    def act(self):\n        if random.random() &lt; self.eps:\n            self.action = self.action_space.sample()\n        else:\n            state = torch.tensor(self.state)\n            self.action = self.q_net(state).argmax().item()\n    def learn():\n        self.optimizr = torch.optim.Adam(self.q_net.parameters())\n        for epoc in range(1): # 5ë²ˆë„ ë„ˆë¬´ ë§ìœ¼ë¯€ë¡œ 1ë²ˆìœ¼ë¡œ ì¤„ì„\n            memory = list(zip(self.states, self.actions, self.rewards, self.next_states, self.terminations))\n            mini_batch = random.sample(memory,64)\n            # step1-2 \n            loss = 0\n            for s,a,r,ss,tmd in mini_batch:\n                q_hat = self.q_net(s)[a]\n                if tmd: \n                    q = r \n                else: \n                    future = self.q_net(ss).max().data \n                    q = r + 0.99 * future\n                loss = loss + (q_hat-q)**2\n            loss = loss / 64\n            # step3\n            loss.backward()\n            # step4 \n            self.optimizr.step()\n            self.optimizr.zero_grad()\n\n)ğŸ—£ï¸\n\nclass Agent(RandomAgent):\n    def __init__(self):\n        super().__init__()\n        self.eps = 1.0 \n        self.q_net = torch.nn.Sequential(\n            torch.nn.Linear(8,256),\n            torch.nn.ReLU(),\n            torch.nn.Linear(256,128),\n            torch.nn.ReLU(),\n            torch.nn.Linear(128,64),\n            torch.nn.ReLU(),\n            torch.nn.Linear(64,4)\n        )\n        self.optimizr = torch.optim.Adam(self.q_net.parameters())\n    def act(self):\n        if random.random() &lt; self.eps: \n            self.action = self.action_spcae.sample()\n        else:\n            state = torch.tensor(self.state)\n            self.action = self.q_net(state).argmax().item()\n    def learn(self):\n        if self.n_experieces &gt; 64:\n            for epoc in range(1):\n                memory = list(zip(self.states, self.actions, self.rewards, self.next_states, self.terminations))\n                mini_batch = random.sample(memory,64)\n                # step1-2 \n                loss = 0\n                for s,a,r,ss,tmd in mini_batch:\n                    q_hat = self.q_net(s)[a]\n                    if tmd: \n                        q = r \n                    else: \n                        future = self.q_net(ss).max().data \n                        q = r + 0.99 * future\n                    loss = loss + (q_hat-q)**2\n                loss = loss / 64\n                # step3\n                loss.backward()\n                # step4 \n                self.optimizr.step()\n                self.optimizr.zero_grad()        \n\n\n\n9. Solve ğŸ“\nğŸ—£ï¸(\nenv = gym.make(\"LunarLander-v3\", render_mode = \"rgb_array\")\nplayer = Agent()\nplayer.state, _ = env.reset()\n#---#\nfor e in range(1,2001):\n    #---ì—í”¼ì†Œë“œì‹œì‘---#\n    while True:\n        # step1\n        player.act()\n        # step2\n        player.next_state, player.reward, player.terminated, player.truncated, _ = env.step(player.action)\n        # step3\n        player.save_experience()\n        player.learn()\n        # step4\n        if player.terminated or player.truncated:\n            player.state, _ = env.reset()\n        else:\n            player.state = player.next_state\n    #---ì—í”¼ì†Œë“œë---#\n    player.eps = player.eps * 0.995\n\nscoreë¥¼ ê³„ì‚°í•  í•„ìš”ê°€ ìˆìŒ\nê³µì‹ ë¬¸ì„œë¥¼ ë³´ë©´ í•œ ì—í”¼ì†Œë“œì—ì„œ 200ì ì„ ë°›ìœ¼ë©´ solutionìœ¼ë¡œ ê°„ì£¼í•´ë„ ì¢‹ë‹¤ê³  í•¨\n\nenv = gym.make(\"LunarLander-v3\", render_mode = \"rgb_array\")\nplayer = Agent()\nplayer.state, _ = env.reset()\nscore = 0\nscores = []\n#---#\nfor e in range(1,2001):\n    #---ì—í”¼ì†Œë“œì‹œì‘---#\n    while True:\n        # step1\n        player.act()\n        # step2\n        player.next_state, player.reward, player.terminated, player.truncated, _ = env.step(player.action)\n        # step3\n        player.save_experience()\n        player.learn()\n        # step4\n        if player.terminated or player.truncated:\n            score = score + player.reward\n            scores.append(score)\n            score = 0\n            player.state, _ = env.reset()\n        else:\n            score = score + player.reward\n            player.state = player.next_state\n    #---ì—í”¼ì†Œë“œë---#\n    player.eps = player.eps * 0.995\n\nì—í”¼ì†Œë“œ ë³„ í”Œë ˆì´ íƒ€ì„ë„ ê¶ê¸ˆí•¨\n\nenv = gym.make(\"LunarLander-v3\", render_mode = \"rgb_array\")\nplayer = Agent()\nplayer.state, _ = env.reset()\nscore = 0\nscores = []\nplaytime = 0\nplaytimes = []\n#---#\nfor e in range(1,2001):\n    #---ì—í”¼ì†Œë“œì‹œì‘---#\n    while True:\n        # step1\n        player.act()\n        # step2\n        player.next_state, player.reward, player.terminated, player.truncated, _ = env.step(player.action)\n        # step3\n        player.save_experience()\n        player.learn()\n        # step4\n        if player.terminated or player.truncated:\n            score = score + player.reward\n            scores.append(score)\n            score = 0\n            playtimes.append(playtime)\n            player.state, _ = env.reset()\n        else:\n            score = score + player.reward\n            playtime = playtime + 1\n            player.state = player.next_state\n    #---ì—í”¼ì†Œë“œë---#\n    player.eps = player.eps * 0.995\n\nn_experencesê°€ ì—†ìœ¼ë©´ batchë¥¼ ë½‘ì„ ë•Œ errorê°€ ë‚  ìˆ˜ ìˆìœ¼ë¯€ë¡œ RandomAgentì— ì¶”ê°€ í›„ ì½”ë“œ ìˆ˜ì •\n\nì°¸ê³ : n_experieces (n ì—†ìŒ)\n7.4ì—ì„œ ì´ë¯¸ ìˆ˜ì •ëœ ê°•ì˜ë…¸íŠ¸ ë²„ì „(RandomAgent)ì„ ì‹¤í–‰í•˜ê¸´ í•˜ì—¬ Agent ìˆ˜ì • ê³¼ì •ë¶€í„° ìˆìŒ\n\n\n\nclass Agent(RandomAgent):\n    def __init__(self):\n        super().__init__()\n        self.eps = 1.0 \n        self.q_net = torch.nn.Sequential(\n            torch.nn.Linear(8,256),\n            torch.nn.ReLU(),\n            torch.nn.Linear(256,128),\n            torch.nn.ReLU(),\n            torch.nn.Linear(128,64),\n            torch.nn.ReLU(),\n            torch.nn.Linear(64,4)\n        )\n        self.optimizr = torch.optim.Adam(self.q_net.parameters())\n    def act(self):\n        if random.random() &lt; self.eps: \n            self.action = self.action_spcae.sample()\n        else:\n            state = torch.tensor(self.state)\n            self.action = self.q_net(state).argmax().item()\n    def learn(self):\n        if self.n_experieces &gt; 64:\n            for epoc in range(1):\n                memory = list(zip(self.states, self.actions, self.rewards, self.next_states, self.terminations))\n                mini_batch = random.sample(memory,64)\n                # step1-2 \n                loss = 0\n                for s,a,r,ss,tmd in mini_batch:\n                    q_hat = self.q_net(s)[a]\n                    if tmd: \n                        q = r \n                    else: \n                        future = self.q_net(ss).max().data \n                        q = r + 0.99 * future\n                    loss = loss + (q_hat-q)**2\n                loss = loss / 64\n                # step3\n                loss.backward()\n                # step4 \n                self.optimizr.step()\n                self.optimizr.zero_grad()        \n\n\nì§ìˆ˜ë²ˆì§¸ë§ˆë‹¤ ì¶œë ¥ì„ í•˜ê³  ì‹¶ìŒ\nì˜¤ë˜ ê±¸ë¦¬ë¯€ë¡œ ì¼ë‹¨ 20ë²ˆë§Œ\nì´ì™¸ ì¶”ê°€ëœ ë‚´ìš©\n\nstep4 playtime = 0, break\n\n\n\nenv = gym.make(\"LunarLander-v3\", render_mode = \"rgb_array\")\nplayer = Agent()\nplayer.state, _ = env.reset()\nscore = 0\nscores = []\nplaytime = 0\nplaytimes = []\n#---#\nfor e in range(1,21):\n    #---ì—í”¼ì†Œë“œì‹œì‘---#\n    while True:\n        # step1\n        player.act()\n        # step2\n        player.next_state, player.reward, player.terminated, player.truncated, _ = env.step(player.action)\n        # step3\n        player.save_experience()\n        player.learn()\n        # step4\n        if player.terminated or player.truncated:\n            score = score + player.reward\n            scores.append(score)\n            score = 0\n            playtimes.append(playtime)\n            playtime = 0\n            player.state, _ = env.reset()\n            break\n        else:\n            score = score + player.reward\n            playtime = playtime + 1\n            player.state = player.next_state\n    #---ì—í”¼ì†Œë“œë---#\n    player.eps = player.eps * 0.995\n    if (e % 2) == 0:\n        print(\n            f\"ì—í”¼ì†Œë“œ{e}\\t\",\n            f\"ê²½í—˜: {player.n_experieces}\\t\",\n            f\"ì ìˆ˜(í‰ê· ): {np.mean(scores[-100:]):.2f}\\t\", # ìµœê·¼ 100ë²ˆë§Œ\n            f\"ê²Œì„ì‹œê°„(í‰ê· ): {np.mean(playtimes[-100:]):.2f}\\t\",\n            f\"ëŒë°œí–‰ë™: {player.eps:.2f}\\t\",\n        )\n\nì—í”¼ì†Œë“œ2    ê²½í—˜: 212     ì ìˆ˜(í‰ê· ): -386.56     ê²Œì„ì‹œê°„(í‰ê· ): 105.00    ëŒë°œí–‰ë™: 0.99 \nì—í”¼ì†Œë“œ4    ê²½í—˜: 396     ì ìˆ˜(í‰ê· ): -261.93     ê²Œì„ì‹œê°„(í‰ê· ): 98.00     ëŒë°œí–‰ë™: 0.98 \nì—í”¼ì†Œë“œ6    ê²½í—˜: 564     ì ìˆ˜(í‰ê· ): -226.24     ê²Œì„ì‹œê°„(í‰ê· ): 93.00     ëŒë°œí–‰ë™: 0.97 \nì—í”¼ì†Œë“œ8    ê²½í—˜: 801     ì ìˆ˜(í‰ê· ): -222.69     ê²Œì„ì‹œê°„(í‰ê· ): 99.12     ëŒë°œí–‰ë™: 0.96 \nì—í”¼ì†Œë“œ10   ê²½í—˜: 945     ì ìˆ˜(í‰ê· ): -212.39     ê²Œì„ì‹œê°„(í‰ê· ): 93.50     ëŒë°œí–‰ë™: 0.95 \nì—í”¼ì†Œë“œ12   ê²½í—˜: 1138    ì ìˆ˜(í‰ê· ): -193.89     ê²Œì„ì‹œê°„(í‰ê· ): 93.83     ëŒë°œí–‰ë™: 0.94 \nì—í”¼ì†Œë“œ14   ê²½í—˜: 1358    ì ìˆ˜(í‰ê· ): -184.52     ê²Œì„ì‹œê°„(í‰ê· ): 96.00     ëŒë°œí–‰ë™: 0.93 \nì—í”¼ì†Œë“œ16   ê²½í—˜: 1570    ì ìˆ˜(í‰ê· ): -180.59     ê²Œì„ì‹œê°„(í‰ê· ): 97.12     ëŒë°œí–‰ë™: 0.92 \nì—í”¼ì†Œë“œ18   ê²½í—˜: 1732    ì ìˆ˜(í‰ê· ): -174.40     ê²Œì„ì‹œê°„(í‰ê· ): 95.22     ëŒë°œí–‰ë™: 0.91 \nì—í”¼ì†Œë“œ20   ê²½í—˜: 1936    ì ìˆ˜(í‰ê· ): -170.77     ê²Œì„ì‹œê°„(í‰ê· ): 95.80     ëŒë°œí–‰ë™: 0.90 \n\n\n\n# env = gym.make(\"LunarLander-v3\", render_mode = \"rgb_array\")\n# player = Agent()\n# player.state, _ = env.reset()\n# score = 0\n# scores = []\n# playtime = 0\n# playtimes = []\n# #---#\n# for e in range(1,2001):\n#     #---ì—í”¼ì†Œë“œì‹œì‘---#\n#     while True:\n#         # step1\n#         player.act()\n#         # step2\n#         player.next_state, player.reward, player.terminated, player.truncated, _ = env.step(player.action)\n#         # step3\n#         player.save_experience()\n#         player.learn()\n#         # step4\n#         if player.terminated or player.truncated:\n#             score = score + player.reward\n#             scores.append(score)\n#             score = 0\n#             playtimes.append(playtime)\n#             playtime = 0\n#             player.state, _ = env.reset()\n#             break\n#         else:\n#             score = score + player.reward\n#             playtime = playtime + 1\n#             player.state = player.next_state\n#     #---ì—í”¼ì†Œë“œë---#\n#     player.eps = player.eps * 0.995\n#     if (e % 2) == 0:\n#         print(\n#             f\"ì—í”¼ì†Œë“œ{e}\\t\",\n#             f\"ê²½í—˜: {player.n_experieces}\\t\",\n#             f\"ì ìˆ˜(í‰ê· ): {np.mean(scores[-100:]):.2f}\\t\", # ìµœê·¼ 100ë²ˆë§Œ\n#             f\"ê²Œì„ì‹œê°„(í‰ê· ): {np.mean(playtimes[-100:]):.2f}\\t\",\n#             f\"ëŒë°œí–‰ë™: {player.eps:.2f}\\t\",\n#         )\n#     if np.mean(scores[-100:]) &gt; 200:\n#         print(\"--ë£¨ë‚˜ëœë” í´ë¦¬ì–´(2025.06.17.)--\")\n#         break\n\n\nğŸ”¬\n\nì‹¤í–‰í•˜ì˜€ìœ¼ë‚˜ ì‹œê°„ì´ ì§€ë‚˜ì ê²°ê³¼ê°€ ì›ìƒ ë³µê·€ ë˜ì–´ ì¼ë‹¨ ë³´ë¥˜\n\nì—í”¼ì†Œë“œ2 ê²½í—˜: 173 ì ìˆ˜(í‰ê· ): -213.37 ê²Œì„ì‹œê°„(í‰ê· ): 85.50 ëŒë°œí–‰ë™: 0.99\nì—í”¼ì†Œë“œ168 ê²½í—˜: 22060 ì ìˆ˜(í‰ê· ): -69.89 ê²Œì„ì‹œê°„(í‰ê· ): 150.31 ëŒë°œí–‰ë™: 0.43\nì—í”¼ì†Œë“œ226 ê²½í—˜: 44217 ì ìˆ˜(í‰ê· ): -45.61 ê²Œì„ì‹œê°„(í‰ê· ): 288.84 ëŒë°œí–‰ë™: 0.32\nì—í”¼ì†Œë“œ306 ê²½í—˜: 79354 ì ìˆ˜(í‰ê· ): -63.40 ê²Œì„ì‹œê°„(í‰ê· ): 433.97 ëŒë°œí–‰ë™: 0.22\nì—í”¼ì†Œë“œ322 ê²½í—˜: 86479 ì ìˆ˜(í‰ê· ): -80.99 ê²Œì„ì‹œê°„(í‰ê· ): 452.58 ëŒë°œí–‰ë™: 0.20\nì—í”¼ì†Œë“œ382 ê²½í—˜: 106714 ì ìˆ˜(í‰ê· ): -200.86 ê²Œì„ì‹œê°„(í‰ê· ): 402.77 ëŒë°œí–‰ë™: 0.15\nì—í”¼ì†Œë“œ416 ê²½í—˜: 123862 ì ìˆ˜(í‰ê· ): -264.81 ê²Œì„ì‹œê°„(í‰ê· ): 406.12 ëŒë°œí–‰ë™: 0.12\n\ní•œ ë²ˆ ë” í•´ë´¤ëŠ”ë° ë¹„ìŠ·í•œ ê²°ê³¼\n\nì—í”¼ì†Œë“œ10 ê²½í—˜: 821 ì ìˆ˜(í‰ê· ): -188.66 ê²Œì„ì‹œê°„(í‰ê· ): 81.10 ëŒë°œí–‰ë™: 0.95\nì—í”¼ì†Œë“œ110 ê²½í—˜: 11691 ì ìˆ˜(í‰ê· ): -108.81 ê²Œì„ì‹œê°„(í‰ê· ): 107.70 ëŒë°œí–‰ë™: 0.58\nì—í”¼ì†Œë“œ270 ê²½í—˜: 59490 ì ìˆ˜(í‰ê· ): -4.08 ê²Œì„ì‹œê°„(í‰ê· ): 387.21 ëŒë°œí–‰ë™: 0.26\nì—í”¼ì†Œë“œ330 ê²½í—˜: 82537 ì ìˆ˜(í‰ê· ): -59.13 ê²Œì„ì‹œê°„(í‰ê· ): 410.44 ëŒë°œí–‰ë™: 0.19\nì—í”¼ì†Œë“œ400 ê²½í—˜: 103908 ì ìˆ˜(í‰ê· ): -197.65 ê²Œì„ì‹œê°„(í‰ê· ): 337.59 ëŒë°œí–‰ë™: 0.13\n\n\n\n)ğŸ—£ï¸\n\nenv = gym.make(\"LunarLander-v3\", render_mode = \"rgb_array\")\nplayer = Agent()\nplayer.state, _ = env.reset()\nscore = 0\nplaytime = 0\nscores = [] \nplaytimes = []\n#---#\nfor e in range(1,2001):\n    #---ì—í”¼ì†Œë“œì‹œì‘---#\n    while True:\n        #step1\n        player.act()\n        #step2\n        player.next_state, player.reward, player.terminated, player.truncated, _ = env.step(player.action)\n        #step3\n        player.save_experience()\n        player.learn()\n        #step4\n        if player.terminated or player.truncated:\n            score = score + player.reward\n            scores.append(score)\n            score = 0\n            playtimes.append(playtime)\n            playtime = 0\n            player.state, _ = env.reset()\n            break\n        else: \n            score = score + player.reward\n            playtime = playtime + 1 \n            player.state = player.next_state\n    #---ì—í”¼ì†Œë“œë---#\n    player.eps = player.eps * 0.995\n    if (e % 50) ==0:\n        print(\n            f\"ì—í”¼ì†Œë“œ: {e}\\t\",\n            f\"ê²½í—˜: {player.n_experieces}\\t\",\n            f\"ì ìˆ˜(í‰ê· ): {np.mean(scores[-100:]):.2f}\\t\",\n            f\"ê²Œì„ì‹œê°„(í‰ê· ): {np.mean(playtimes[-100:]):.2f}\\t\",\n            f\"ëŒë°©í–‰ë™: {player.eps:.2f}\\t\",\n        )\n    if np.mean(scores[-100:]) &gt; 200:\n        print(\"--ë£¨ë‚˜ëœë” í´ë¦¬ì–´(2025.06.14.)--\")\n        break\n\nì—í”¼ì†Œë“œ: 50     ê²½í—˜: 5568    ì ìˆ˜(í‰ê· ): -126.60     ê²Œì„ì‹œê°„(í‰ê· ): 110.36    ëŒë°©í–‰ë™: 0.78 \n\n\n\nì•„ë˜ì½”ë“œ ì‹¤í–‰í•˜ë©´ ì œê°€ ì‹¤ìŠµì— ì‚¬ìš©í•œ íŒŒì¼ ë°›ì•„ì˜¬ìˆ˜ìˆì–´ìš”\n!wget https://github.com/guebin/DL2025/raw/main/posts/q_net.pth\n\n\nplayer_dummy = Agent()\nplayer_dummy.q_net.load_state_dict(\n    torch.load(\"q_net.pth\")\n)\nplayer_dummy.state, _ = env.reset()\nimgs = []\n\n\nplayer_dummy.eps = 0 \nwhile True:\n    player_dummy.act()\n    player_dummy.next_state, player_dummy.reward, player_dummy.terminated, player_dummy.truncated, _ = env.step(player_dummy.action)\n    imgs.append(env.render())\n    if player_dummy.terminated or player_dummy.truncated:\n        break\n    else:\n        player_dummy.state = player_dummy.next_state\n\n\nshow(imgs)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nğŸ—£ï¸(\n\në§Œì•½ í•™ìŠµì´ ì˜ ë˜ì—ˆë‹¤ë©´\n\n\n# list(player.q_net.parameters()) # ì—¬ê¸°ì— ê°’ë“¤ì´ ì˜ ì €ì¥ë˜ì–´ ìˆìŒ\n\n\nì €ì¥ ë°©ë²•\n\n\n# player.q_net.state_dict() # ì´ëŸ¬í•œ í•¨ìˆ˜ê°€ ìˆìŒ\n\n\n# torch.save(player.q_net.state_dict(), \"test.pth\")\n\nplayer_dummy = Agent()\nplayer_dummy.q_net.load_state_dict(\n    torch.load(\"test.pth\")\n)\nplayer_dummy.state, _ = env.reset()\nimgs = []\n\ndummyì—ë„ ê°™ì€ ê°’ì´ ìˆìŒ\nq_netìœ¼ë¡œ í•´ë³´ë©´\n\n\nplayer_dummy = Agent()\nplayer_dummy.q_net.load_state_dict(\n    torch.load(\"q_net.pth\")\n)\nplayer_dummy.state, _ = env.reset()\nimgs = []\n\n\nplayer_dummy.eps = 0 \nwhile True:\n    player_dummy.act()\n    player_dummy.next_state, player_dummy.reward, player_dummy.terminated, player_dummy.truncated, _ = env.step(player_dummy.action)\n    imgs.append(env.render())\n    if player_dummy.terminated or player_dummy.truncated:\n        break\n    else:\n        player_dummy.state = player_dummy.next_state\n\n\nshow(imgs)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\nenv.reset()ì´ ì‹¤í–‰ë˜ì–´ í™˜ê²½ì´ ë°”ë€Œì–´ë„ ì˜ ë¨\n\n)ğŸ—£ï¸"
  },
  {
    "objectID": "posts/03wk-2.html",
    "href": "posts/03wk-2.html",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "",
    "text": "ğŸ“˜ Note Format Guide\nThis format serves as a structured guide for organizing lecture content, personal interpretation, experiments, and study-related questions.\nğŸ“ ğŸ—£ï¸ âœï¸ ğŸ”¬ â“"
  },
  {
    "objectID": "posts/03wk-2.html#a.-ë¡œì§€ìŠ¤í‹±-ëª¨í˜•",
    "href": "posts/03wk-2.html#a.-ë¡œì§€ìŠ¤í‹±-ëª¨í˜•",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "A. ë¡œì§€ìŠ¤í‹± ëª¨í˜•",
    "text": "A. ë¡œì§€ìŠ¤í‹± ëª¨í˜•\n- \\(x\\)ê°€ ì»¤ì§ˆìˆ˜ë¡ (í˜¹ì€ ì‘ì•„ì§ˆìˆ˜ë¡) \\(y=1\\)ì´ ì˜ë‚˜ì˜¤ëŠ” ëª¨í˜•ì€ ì•„ë˜ì™€ ê°™ì´ ì„¤ê³„í•  ìˆ˜ ìˆìŒ &lt;â€” ì™¸ìš°ì„¸ìš”!!!\n\n\\(y_i \\sim {\\cal B}(\\pi_i),\\quad\\) where \\(\\pi_i = \\frac{\\exp(w_0+w_1x_i)}{1+\\exp(w_0+w_1x_i)} = \\frac{1}{1+\\exp(-w_0-w_1x_i)}\\)\n\\(\\hat{y}_i= \\frac{\\exp(\\hat{w}_0+\\hat{w}_1x_i)}{1+\\exp(\\hat{w}_0+\\hat{w}_1x_i)}=\\frac{1}{1+\\exp(-\\hat{w}_0-\\hat{w}_1x_i)}\\)\n\n- íšŒê·€ëª¨í˜•ê³¼ ë¡œì§€ìŠ¤í‹± ëª¨í˜•ì˜ ë¹„êµ\n\níšŒê·€ëª¨í˜•: \\(y_i \\sim {\\cal N}(w_0+w_1x_i, \\sigma^2)\\)1\në¡œì§€ìŠ¤í‹±: \\(y_i \\sim {\\cal B}\\big(\\frac{\\exp(w_0+w_1x_i)}{1+\\exp(w_0+w_1x_i)}\\big)\\)\n\n- ìš°ë¦¬ê°€ ì˜ˆì¸¡í•˜ê³  ì‹¶ì€ê²ƒ\n\níšŒê·€ëª¨í˜•: ì •ê·œë¶„í¬ì˜ í‰ê· ì„ ì˜ˆì¸¡í•˜ê³  ì‹¶ìŒ. ì¦‰ \\(w_0+w_1x_i\\)ë¥¼ ì˜ˆì¸¡í•˜ê³  ì‹¶ìŒ. ì˜ˆì¸¡ê°’ìœ¼ë¡œëŠ” \\(\\hat{w}_0 + \\hat{w}_1x_i\\)ë¥¼ ì‚¬ìš©!\në¡œì§€ìŠ¤í‹±: ë² ë¥´ëˆ„ì´ì˜ í‰ê· ì„ ì˜ˆì¸¡í•˜ê³  ì‹¶ìŒ. ì¦‰ \\(\\frac{\\exp(w_0+w_1x_i)}{1+\\exp(w_0+w_1x_i)}\\)ë¥¼ ì˜ˆì¸¡í•˜ê³  ì‹¶ìŒ. ì˜ˆì¸¡ê°’ìœ¼ë¡œëŠ” \\(\\frac{\\exp(\\hat{w}_0+\\hat{w}_1x_i)}{1+\\exp(\\hat{w}_0+\\hat{w}_1x_i)}\\)ë¥¼ ì‚¬ìš©!"
  },
  {
    "objectID": "posts/03wk-2.html#b.-ë°ì´í„°-ìŠ¤í™ê³¼-ì·¨ì—…",
    "href": "posts/03wk-2.html#b.-ë°ì´í„°-ìŠ¤í™ê³¼-ì·¨ì—…",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "B. ë°ì´í„° â€“ ìŠ¤í™ê³¼ ì·¨ì—…",
    "text": "B. ë°ì´í„° â€“ ìŠ¤í™ê³¼ ì·¨ì—…\nğŸ—£ï¸(\n\në°ì´í„° ë§Œë“¤ê¸°\n\n\ntorch.linspace(-1,1,2000)\n\ntensor([-1.0000, -0.9990, -0.9980,  ...,  0.9980,  0.9990,  1.0000])\n\n\n\nlen(torch.linspace(-1,1,2000))\n\n2000\n\n\n\nx = torch.linspace(-1,1,2000).reshape(2000,1)\nx\n\ntensor([[-1.0000],\n        [-0.9990],\n        [-0.9980],\n        ...,\n        [ 0.9980],\n        [ 0.9990],\n        [ 1.0000]])\n\n\n\nì´ ìƒíƒœì—ì„œ ì„ í˜• ë³€í™˜ì„ í•œë‹¤ë©´\n\n-1 + x*5 : ì„ í˜• ëª¨ë¸\në¡œì§€ìŠ¤í‹± ëª¨í˜•ì€\n\n\n\nx = torch.linspace(-1,1,2000).reshape(2000,1)\nprob = torch.exp(-1 + x*5) / (1+ torch.exp(-1 + x*5))\nplt.plot(x,prob)\n\n\n\n\n\n\n\n\n\në‹¤ë¥¸ ë°©ë²• (ë³´ê¸° ì¢‹ê²Œ)\n\n\nx = torch.linspace(-1,1,2000).reshape(2000,1)\nw0, w1 = -1, 5\nprob = torch.exp(w0 + x*w1) / (1+ torch.exp(w0 + x*w1))\nplt.plot(x,prob)\n\n\n\n\n\n\n\n\n\nprob\n\ntensor([[0.0025],\n        [0.0025],\n        [0.0025],\n        ...,\n        [0.9818],\n        [0.9819],\n        [0.9820]])\n\n\n\nprob.shape\n\ntorch.Size([2000, 1])\n\n\n\ny ë§Œë“¤ê¸° (probë¡œ ë² ë¥´ëˆ„ì´ ì‹œí–‰)\n\n\ntorch.bernoulli(prob)\n\ntensor([[0.],\n        [0.],\n        [0.],\n        ...,\n        [1.],\n        [1.],\n        [1.]])\n\n\n\ntorch.bernoulli(prob).shape\n\ntorch.Size([2000, 1])\n\n\n\nseed ê³ ì • í›„ ì‹œê°í™”\n\n\ntorch.manual_seed(43052)\nx = torch.linspace(-1,1,2000).reshape(2000,1)\nw0, w1 = -1, 5\nprob = torch.exp(w0 + x*w1) / (1+ torch.exp(w0 + x*w1))\ny = torch.bernoulli(prob)\n\n\nplt.plot(x,y) # ë³´ê¸° ì‰½ì§€ëŠ” ì•ŠìŒ \n\n\n\n\n\n\n\n\n\nplt.plot(x,y, 'o') # ì ë“¤ì´ ë„ˆë¬´ ë§ì´ ê²¹ì¹¨\n\n\n\n\n\n\n\n\n\nplt.plot(x,y,'.',alpha=0.03) # íˆ¬ëª…ë„ ì¡°ì ˆ\n\n\n\n\n\n\n\n\n\nxê°€ ì¦ê°€í• ìˆ˜ë¡ yëŠ” 1ì´ ë‚˜ì˜¬ ê°€ëŠ¥ì„±ì´ ë†’ì•„ì§€ê³ \nxê°€ ê°ì†Œí• ìˆ˜ë¡ yëŠ” 0ì´ ë‚˜ì˜¬ ê°€ëŠ¥ì„±ì´ ë†’ì•„ì§\n\n\nplt.plot(x,y,'.',alpha=0.03) # ê´€ì¸¡(error í¬í•¨)\nplt.plot(x,prob,'--') # ì‹¤ì²´ ë°ì´í„°ì—ì„œëŠ” ê´€ì¸¡ ë¶ˆê°€ëŠ¥ (error-free structure)\n\n\n\n\n\n\n\n\n)ğŸ—£ï¸\n\ntorch.manual_seed(43052)\nx = torch.linspace(-1,1,2000).reshape(2000,1)\nw0,w1 = -1, 5\nprob = torch.exp(w0+w1*x) / (1+torch.exp(w0+w1*x)) \ny = torch.bernoulli(prob)\n\n\nplt.plot(x,y,'.',alpha=0.03)\nplt.plot(x[0],y[0],'.',label=r\"$(x_i,y_i)$\",color=\"C0\")\nplt.plot(x,prob,'--r',label=r\"prob (true, unknown) = $\\frac{exp(-1+5x)}{1+exp(-1+5x)}$\")\nplt.legend()\n\n\n\n\n\n\n\n\n\nğŸ—£ï¸\n\nprob: í™•ë¥ \níŒŒë€ìƒ‰ ì : ê´€ì¸¡ê°’\nëª©í‘œ: ë¹¨ê°„ìƒ‰ ì„  ì˜ ë§ì¶”ê¸°\n\në°©ë²•: ìµœì´ˆì˜ ê³¡ì„ ì„ ê·¸ë¦¬ê³  update"
  },
  {
    "objectID": "posts/03wk-2.html#c.-step1-net-ì„¤ê³„-ëª¨ë¸ë§",
    "href": "posts/03wk-2.html#c.-step1-net-ì„¤ê³„-ëª¨ë¸ë§",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "C. Step1: net ì„¤ê³„ (ëª¨ë¸ë§)",
    "text": "C. Step1: net ì„¤ê³„ (ëª¨ë¸ë§)\n- ìµœì´ˆì˜ ê³¡ì„ ì„ ê·¸ë ¤ë³´ì.\n\nìµœì´ˆì˜ì§ì„ : \\(\\hat{y}_i= \\hat{w}_0+\\hat{w}_1x_i\\) ì—ì„œ ì•„ë¬´ \\(\\hat{w}_0\\), \\(\\hat{w}_1\\) ì„ ì„¤ì •í•˜ë©´ ëœë‹¤.\nìµœì´ˆì˜ê³¡ì„ : \\(\\hat{y}_i= \\frac{\\exp(\\hat{w}_0+\\hat{w}_1x_i)}{1+\\exp(\\hat{w}_0+\\hat{w}_1x_i)}=\\frac{1}{1+\\exp(-\\hat{w}_0-\\hat{w}_1x_i)}\\) ì—ì„œ ì•„ë¬´ \\(\\hat{w}_0\\), \\(\\hat{w}_1\\) ì„ ì„¤ì •í•˜ë©´ ëœë‹¤.\n\n\n\n\n\n\n\nNote\n\n\n\nì¼ë‹¨ì€ ì´ˆê¸° ì„¤ì •ê°’ì„ \\(\\hat{w}_0 = -0.8\\), \\(\\hat{w}_1 = -0.3\\) ìœ¼ë¡œ í•˜ì. (ì‹¤ì œê°’ì€ \\(w_0=-1\\), \\(w_1=5\\) ì´ë‹¤)\n\n\n# ë°©ë²•1 â€“ l1, sigmoid\nğŸ—£ï¸(\n\nw0hat = -4\nw1hat = 10\nyhat = torch.exp(w0hat + w1hat*x) / (1+ torch.exp(w0hat + w1hat*x))\n\n\nyhat\n\ntensor([[8.3153e-07],\n        [8.3989e-07],\n        [8.4833e-07],\n        ...,\n        [9.9748e-01],\n        [9.9750e-01],\n        [9.9753e-01]])\n\n\n\nplt.plot(x,y,'.',alpha=0.03)\nplt.plot(x, yhat, '--')\n\n\n\n\n\n\n\n\n\nyhatì„ ë‹¤ìŒê³¼ ê°™ì´ í•  ìˆ˜ë„ ìˆìŒ\n\n\nlinr = torch.nn.Linear(1,1)\n# linr(x)\n\n\ndef sigmoid(x):\n    return torch.exp(x) / (1+ torch.exp(x)) #  í¸ì˜ìƒ linr(x) ëŒ€ì‹  xë¡œ ì‘ì„±\n\n\nlinr(x)\n\ntensor([[ 0.6311],\n        [ 0.6304],\n        [ 0.6297],\n        ...,\n        [-0.6902],\n        [-0.6909],\n        [-0.6916]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\n\nyhat = sigmoid(linr(x))\nplt.plot(x, yhat.data) # ê³¡ì„  ì¤‘ ì¼ë¶€ë§Œ ê·¸ë ¤ì ¸ ì§ì„ ì²˜ëŸ¼ ë³´ì„\n\n\n\n\n\n\n\n\n\nlinr(x)ê°€ ê³„ì‚°ë˜ëŠ” ê³¼ì •\n\n\nlinr.weight, linr.bias\n\n(Parameter containing:\n tensor([[-0.6613]], requires_grad=True),\n Parameter containing:\n tensor([-0.0303], requires_grad=True))\n\n\n\n-0.6613*x + -0.0303\n\ntensor([[ 0.6310],\n        [ 0.6303],\n        [ 0.6297],\n        ...,\n        [-0.6903],\n        [-0.6909],\n        [-0.6916]])\n\n\n\nê°’ì„ ì•„ê¹Œì²˜ëŸ¼ ì§€ì •í•´ì£¼ë©´\n\n\nlinr.weight.data = torch.tensor([[10.0]])\nlinr.bias.data = torch.tensor([-4.0])\n\nâ“ biasëŠ” [[-4.0]]ì´ ì•„ë‹ˆë¼ [-4.0]\nğŸ”¬(\n\nlinr.weight.data = torch.tensor([[10.0]])\nlinr.bias.data = torch.tensor([[-4.0]])\n\n\nlinr(x)\n\ntensor([[-14.0000],\n        [-13.9900],\n        [-13.9800],\n        ...,\n        [  5.9800],\n        [  5.9900],\n        [  6.0000]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\nâ“ ìƒê´€ì—†ëŠ”ë“¯?\n\nlinr.weight.data = torch.tensor([10.0])\nlinr.bias.data = torch.tensor([[-4.0]])\n\n\n# linr(x) # error: RuntimeError: mat2 must be a matrix, got 1-D tensor\n\n)ğŸ”¬\nğŸ”¬ ì°¸ê³ ) -4.0ì´ ì•„ë‹ˆë¼ -4ë¥¼ ì“°ë©´ error\n\nlinr.weight.data = torch.tensor([[10.0]])\nlinr.bias.data = torch.tensor([-4.0])\n\n\nlinr(x)\n\ntensor([[-14.0000],\n        [-13.9900],\n        [-13.9800],\n        ...,\n        [  5.9800],\n        [  5.9900],\n        [  6.0000]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\n\nw0hat + w1hat*x # ìœ„ì™€ ë™ì¼\n\ntensor([[-14.0000],\n        [-13.9900],\n        [-13.9800],\n        ...,\n        [  5.9800],\n        [  5.9900],\n        [  6.0000]])\n\n\n\n-4*x + 10 # ì´ê²ƒë„ ë™ì¼\n\ntensor([[14.0000],\n        [13.9960],\n        [13.9920],\n        ...,\n        [ 6.0080],\n        [ 6.0040],\n        [ 6.0000]])\n\n\n\në‹¤ì‹œ ì •ë¦¬í•˜ë©´\n\n\ndef sigmoid(x):\n    return torch.exp(x) / (1+ torch.exp(x)) #  í¸ì˜ìƒ linr(x) ëŒ€ì‹  xë¡œ ì‘ì„±\n\n\nl1 = torch.nn.Linear(1,1)\nl1.weight.data = torch.tensor([[10.0]])\nl1.bias.data = torch.tensor([-4.0])\n#yhat = torch.exp(l1(x)) / (1+ torch.exp(l1(x)))\nyhat = sigmoid(l1(x))\n\n\nplt.plot(x,yhat.data)\n\n\n\n\n\n\n\n\n\nê°’ì„ ë°”ê¾¸ê³  ì‹¶ìœ¼ë©´\n\n\nl1 = torch.nn.Linear(1,1)\nl1.weight.data = torch.tensor([[-0.3]])\nl1.bias.data = torch.tensor([-0.8])\n#yhat = torch.exp(l1(x)) / (1+ torch.exp(l1(x)))\nyhat = sigmoid(l1(x))\n\n\nplt.plot(x,yhat.data)\n\n\n\n\n\n\n\n\n\nplt.plot(x,y,'.',alpha=0.03)\nplt.plot(x, yhat.data, '--')\n\n\n\n\n\n\n\n\n)ğŸ—£ï¸\n\nl1 = torch.nn.Linear(1,1)\nl1(x) # w0hat + w1hat*x \n\ntensor([[ 0.4735],\n        [ 0.4728],\n        [ 0.4721],\n        ...,\n        [-0.9890],\n        [-0.9897],\n        [-0.9905]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\nğŸ—£ï¸ ì‹¤í–‰í•  ë•Œë§ˆë‹¤ ë‹¬ë¼ì§€ë¯€ë¡œ ì•„ë˜ì™€ ê°™ì´ ê³ ì •\n\nl1.weight.data = torch.tensor([[-0.3]])\nl1.bias.data = torch.tensor([-0.8])\n\n\ndef sigmoid(x):\n    return torch.exp(x)/(1+torch.exp(x))\n\n\nplt.plot(x,y,'.',alpha=0.03)\nplt.plot(x[0],y[0],'o',label=r\"$(x_i,y_i)$\",color=\"C0\")\nplt.plot(x,prob,'--r',label=r\"prob (true, unknown) = $\\frac{exp(-1+5x)}{1+exp(-1+5x)}$\")\nplt.plot(x,sigmoid(l1(x)).data,'--b', label=r\"prob (estimated) = $(x_i,\\hat{y}_i)$ -- first curve\")\nplt.legend()\n\n\n\n\n\n\n\n\n#\n# ë°©ë²•2 â€“ l1, a1\nğŸ—£ï¸(\nx -&gt; w0hat + w1hat*x  # ìµœì´ˆì˜ ê³¡ì„ ì„ ê·¸ë¦¬ê¸° ìœ„í•œ ì„ í˜• ë³€í™˜\nu = w0hat + w1hat*x  # ê²°ê³¼ë¥¼ uë¡œ ì €ì¥\nfirst_curve = yhat = prob_hat = sigmoid(u)\nu = w0hat + w1hat*x = l1(x) # l1ì„ ë§Œë“ ë‹¤ë©´ ì´ë ‡ê²Œë„ ì“¸ ìˆ˜ ìˆìŒ\nl1 = torch.nn.Linear(1,1)\nl1.weight.data = torch.tensor([[-0.3]])\nl1.bias.data = torch.tensor([-0.8])\nu = l1(x)\nyhat = sigmoid(u)\n\nsigmoidëŠ” ì§ì ‘ ë§Œë“¤ì—ˆìŒ\n\n\nsigmoid?\n\n\nSignature: sigmoid(x)\nDocstring: &lt;no docstring&gt;\nFile:      /tmp/ipykernel_30452/3273882758.py\nType:      function\n\n\n\n\nsigmoid??\n\n\nSignature: sigmoid(x)\nDocstring: &lt;no docstring&gt;\nSource:   \ndef sigmoid(x):\n    return torch.exp(x)/(1+torch.exp(x))\nFile:      /tmp/ipykernel_30452/3273882758.py\nType:      function\n\n\n\n\në‹¤ìŒê³¼ ê°™ì´ë„ í•  ìˆ˜ ìˆìŒ (torch.nnì˜ í´ë˜ìŠ¤ ì´ìš©)\n\n\nsig = torch.nn.Sigmoid()\nsig\n\nSigmoid()\n\n\n\nl1 = torch.nn.Linear(1,1)\nl1.weight.data = torch.tensor([[-0.3]])\nl1.bias.data = torch.tensor([-0.8])\nu = l1(x)\nyhat = sig(u)\n\n\nyhat\n\ntensor([[0.3775],\n        [0.3775],\n        [0.3774],\n        ...,\n        [0.2499],\n        [0.2498],\n        [0.2497]], grad_fn=&lt;SigmoidBackward0&gt;)\n\n\n\nl1 = torch.nn.Linear(1,1)\nl1.weight.data = torch.tensor([[-0.3]])\nl1.bias.data = torch.tensor([-0.8])\nyhat = sig(l1(x)) # x --&gt; l1 --&gt; sig ë¡œ ì´í•´\n\n\nplt.plot(x,y,'.',alpha=0.03)\nplt.plot(x[0],y[0],'o',label=r\"$(x_i,y_i)$\",color=\"C0\")\nplt.plot(x,prob,'--r',label=r\"prob (true, unknown) = $\\frac{exp(-1+5x)}{1+exp(-1+5x)}$\")\nplt.plot(x,sigmoid(l1(x)).data,'--b', label=r\"prob (estimated) = $(x_i,\\hat{y}_i)$ -- first curve\")\nplt.legend()\n\n\n\n\n\n\n\n\n\në°©ë²•1ê³¼ ë™ì¼í•œ ê²°ê³¼\n\n)ğŸ—£ï¸\n\nl1 = torch.nn.Linear(1,1)\nl1.weight.data = torch.tensor([[-0.3]])\nl1.bias.data = torch.tensor([-0.8])\n\n\na1 = torch.nn.Sigmoid()\n\n\nsigmoid(l1(x)), a1(l1(x)) # ë˜‘ê°™ì•„ìš”\n\n(tensor([[0.3775],\n         [0.3775],\n         [0.3774],\n         ...,\n         [0.2499],\n         [0.2498],\n         [0.2497]], grad_fn=&lt;DivBackward0&gt;),\n tensor([[0.3775],\n         [0.3775],\n         [0.3774],\n         ...,\n         [0.2499],\n         [0.2498],\n         [0.2497]], grad_fn=&lt;SigmoidBackward0&gt;))\n\n\n- ì§€ê¸ˆê¹Œì§€ì˜ êµ¬í˜„ í™•ì¸\n\nplt.plot(x,y,'.',alpha=0.03)\nplt.plot(x[0],y[0],'o',label=r\"$(x_i,y_i)$\",color=\"C0\")\nplt.plot(x,prob,'--r',label=r\"prob (true, unknown) = $\\frac{exp(-1+5x)}{1+exp(-1+5x)}$\")\nplt.plot(x,a1(l1(x)).data,'--b', label=r\"prob (estimated) = $(x_i,\\hat{y}_i)$ -- first curve with $(a_1 \\circ l_1)(x)$\")\nplt.legend()\n\n\n\n\n\n\n\n\n#\n# ë°©ë²•3 - l1, a1 ë§Œë“¤ê³  \\(\\to\\) net\nğŸ—£ï¸(\n\nyhat\n\ntensor([[0.3775],\n        [0.3775],\n        [0.3774],\n        ...,\n        [0.2499],\n        [0.2498],\n        [0.2497]], grad_fn=&lt;SigmoidBackward0&gt;)\n\n\n\na1(l1(x))\n\ntensor([[0.3775],\n        [0.3775],\n        [0.3774],\n        ...,\n        [0.2499],\n        [0.2498],\n        [0.2497]], grad_fn=&lt;SigmoidBackward0&gt;)\n\n\n\nnet = al \\(\\circ\\) l1 ì„ ì •ì˜í•˜ì—¬ net(x)ë„ ê°™ì€ ê²°ê³¼ë¥¼ ë‚˜ì˜¤ê²Œ í•˜ê³  ì‹¶ìŒ\n\n\ntorch.nn.Sequential(l1,a1)\n\nSequential(\n  (0): Linear(in_features=1, out_features=1, bias=True)\n  (1): Sigmoid()\n)\n\n\n\nnet = torch.nn.Sequential(l1,a1)\nnet(x) # a1(l1(x))\n\ntensor([[0.3775],\n        [0.3775],\n        [0.3774],\n        ...,\n        [0.2499],\n        [0.2498],\n        [0.2497]], grad_fn=&lt;SigmoidBackward0&gt;)\n\n\n\nì´ë ‡ê²Œ í•œ ì´ìœ : parameters()ë¥¼ ì´ìš©í•˜ì—¬ optimizerë¥¼ ë§Œë“¤ ìˆ˜ ìˆìŒ\n\n\nnet.parameters()\n\n&lt;generator object Module.parameters at 0x7f34682a17b0&gt;\n\n\n)ğŸ—£ï¸\n- ê´€ì°°: ì§€ê¸ˆ ì•„ë˜ì˜ êµ¬ì¡°ì´ë‹¤.\n\\[{\\bf x} \\overset{l_1}{\\to} {\\bf u} \\overset{a_1}{\\to} {\\bf v} = \\hat{\\bf y}\\]\n- ì†Œë§: í•¨ìˆ˜ \\(l_1, a_1\\) ì˜ í•©ì„±ì„ í•˜ë‚˜ë¡œ ë¬¶ì–´ì„œ\n\\[(a_1\\circ l_1)({\\bf x}) := net({\\bf x})\\]\nì´ëŸ¬í•œ ê¸°ëŠ¥ì„ í•˜ëŠ” í•˜ë‚˜ì˜ í•¨ìˆ˜ \\(net\\)ì„ ë§Œë“¤ ìˆ˜ ì—†ì„ê¹Œ?\n\nl1 = torch.nn.Linear(1,1)\nl1.weight.data = torch.tensor([[-0.3]])\nl1.bias.data = torch.tensor([-0.8])\na1 = torch.nn.Sigmoid()\n\n\nnet = torch.nn.Sequential(l1,a1) #l1ì„ ì·¨í•˜ê³  ê·¸ë‹¤ìŒì— a1ì„ ì·¨í•˜ë¼ëŠ” ì˜ë¯¸\n\n\nnet(x), a1(l1(x)), sigmoid(l1(x))\n\n(tensor([[0.3775],\n         [0.3775],\n         [0.3774],\n         ...,\n         [0.2499],\n         [0.2498],\n         [0.2497]], grad_fn=&lt;SigmoidBackward0&gt;),\n tensor([[0.3775],\n         [0.3775],\n         [0.3774],\n         ...,\n         [0.2499],\n         [0.2498],\n         [0.2497]], grad_fn=&lt;SigmoidBackward0&gt;),\n tensor([[0.3775],\n         [0.3775],\n         [0.3774],\n         ...,\n         [0.2499],\n         [0.2498],\n         [0.2497]], grad_fn=&lt;DivBackward0&gt;))\n\n\n* net êµ¬ì¡° ì ê¹ ì‚´í´ë³´ê¸°\nğŸ—£ï¸(\n\nnet\n\nSequential(\n  (0): Linear(in_features=1, out_features=1, bias=True)\n  (1): Sigmoid()\n)\n\n\n\nnet[0]\n\nLinear(in_features=1, out_features=1, bias=True)\n\n\n\nl1\n\nLinear(in_features=1, out_features=1, bias=True)\n\n\n\nnet[1]\n\nSigmoid()\n\n\n\naaa = torch.nn.Sigmoid()\naaa\n\nSigmoid()\n\n\n\nnetê°€ ë¦¬ìŠ¤íŠ¸ì²˜ëŸ¼ ë˜ì–´ ìˆì–´ ì²«ë²ˆì§¸ ì›ì†Œ net[0]ì€ l1 ì´ê³  ë‘ë²ˆì§¸ ì›ì†Œ net[1]ì€ aaaì¸ë“¯\ní™•ì¸ ë°©ë²•: ì•„ë˜\n\n\nl1 is net[0]\n\nTrue\n\n\n\na1 is net[1]\n\nTrue\n\n\n\në‹¤ë¥¸ í™•ì¸ ë°©ë²•\n\nì˜¤ë¸Œì íŠ¸: ë©”ëª¨ë¦¬ì— ì €ì¥\nì €ì¥ë˜ì–´ ìˆëŠ” ì£¼ì†Œê°€ ë™ì¼í•˜ë©´ ê°™ì€ ì˜¤ë¸Œì íŠ¸\n\n\n\nid(net[0]), id(l1)\n\n(139863062109248, 139863062109248)\n\n\n\nid(net[1]), id(a1)\n\n(139863062108960, 139863062108960)\n\n\n\nnet(x), a1(l1(x))\n\n(tensor([[0.3775],\n         [0.3775],\n         [0.3774],\n         ...,\n         [0.2499],\n         [0.2498],\n         [0.2497]], grad_fn=&lt;SigmoidBackward0&gt;),\n tensor([[0.3775],\n         [0.3775],\n         [0.3774],\n         ...,\n         [0.2499],\n         [0.2498],\n         [0.2497]], grad_fn=&lt;SigmoidBackward0&gt;))\n\n\n\nnet(x), net[1](net[0](x)) # ì´ê²ƒë„ ë™ì¼\n\n(tensor([[0.3775],\n         [0.3775],\n         [0.3774],\n         ...,\n         [0.2499],\n         [0.2498],\n         [0.2497]], grad_fn=&lt;SigmoidBackward0&gt;),\n tensor([[0.3775],\n         [0.3775],\n         [0.3774],\n         ...,\n         [0.2499],\n         [0.2498],\n         [0.2497]], grad_fn=&lt;SigmoidBackward0&gt;))\n\n\n)ğŸ—£ï¸\n\nnet[0], net[1]\n\n(Linear(in_features=1, out_features=1, bias=True), Sigmoid())\n\n\n\nl1 is net[0]\n\nTrue\n\n\n\na1 is net[1]\n\nTrue\n\n\n#\n# ë°©ë²•4 â€“ netì„ ë°”ë¡œ ë§Œë“¤ê¸°\nğŸ—£ï¸(\n# x --&gt; yhat: íšŒê·€ë¶„ì„ì—ì„œ ìµœì´ˆì˜ ì§ì„  ë°”ë¡œ ë§Œë“œëŠ” ë°©ë²•\nnet = torch.nn.Linear(1,1)\nyhat - net(x)\n# x --&gt; yhat: ë¡œì§€ìŠ¤í‹±ì—ì„œ ìµœì´ˆì˜ ê³¡ì„  ë°”ë¡œ ë§Œë“œëŠ” ë°©ë²•\nnet = torch.nn.Sequential(\n    l1,\n    a1\n)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n)\nyhat = net(x)\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n)\nyhat = net(x)\n\n\nnet[0].weight # ì•„ë¬´ parameterê°€ ë“¤ì–´ê°€ ìˆìŒ\n\nParameter containing:\ntensor([[0.4945]], requires_grad=True)\n\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n)\nnet[0].weight.data = torch.tensor([[-0.3]])\nnet[0].bias.data = torch.tensor([-0.8])\nyhat = net(x)\n\n\nnet(x)\n\ntensor([[0.3775],\n        [0.3775],\n        [0.3774],\n        ...,\n        [0.2499],\n        [0.2498],\n        [0.2497]], grad_fn=&lt;SigmoidBackward0&gt;)\n\n\n)ğŸ—£ï¸\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n)\nnet[0].weight.data = torch.tensor([[-0.3]])\nnet[0].bias.data = torch.tensor([-0.8])\nyhat = net(x)\n\n\nnet(x)\n\ntensor([[0.3775],\n        [0.3775],\n        [0.3774],\n        ...,\n        [0.2499],\n        [0.2498],\n        [0.2497]], grad_fn=&lt;SigmoidBackward0&gt;)\n\n\nğŸ—£ï¸ ê²°ë¡ : ìœ„ì˜ ë°©ë²•ìœ¼ë¡œ ì‚¬ìš©í•˜ë©´ ë¨\n#"
  },
  {
    "objectID": "posts/03wk-2.html#d.-step14",
    "href": "posts/03wk-2.html#d.-step14",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "D. Step1~4",
    "text": "D. Step1~4\nğŸ—£ï¸(\n\ní•™ìŠµ ì‹œì‘\n\n\nplt.plot(x,y,'.',alpha=0.03) # given data\n\n\n\n\n\n\n\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n)\nnet[0].weight.data = torch.tensor([[-0.3]])\nnet[0].bias.data = torch.tensor([-0.8])\nyhat = net(x)\n\n\nplt.plot(x,y,'.',alpha=0.03) # given data\nplt.plot(x,net(x).data, '--') # ìµœì´ˆì˜ ê³¡ì„  ê·¸ë¦¬ê¸°\n\n\n\n\n\n\n\n\n\nìµœì´ˆì˜ ê³¡ì„ ë³´ë‹¤ ë‚˜ì€ ê³¡ì„ ì„ ì°¾ìœ¼ë©° updateí•˜ë©´ ë¨\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n)\nnet[0].weight.data = torch.tensor([[-0.3]])\nnet[0].bias.data = torch.tensor([-0.8])\nyhat = net(x)\nloss = torch.mean((y-yhat)**2) # loss í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ ì¤Œ\nloss\n\ntensor(0.2747, grad_fn=&lt;MeanBackward0&gt;)\n\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n)\nnet[0].weight.data = torch.tensor([[-0.3]])\nnet[0].bias.data = torch.tensor([-0.8])\noptimizr = torch.optim.SGD(net.parameters(), lr=0.25)\nfor epoc in range(200):\n    yhat = net(x)\n    loss = torch.mean((y-yhat)**2)\n    loss.backward() # ë¯¸ë¶„\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'.',alpha=0.03)\nplt.plot(x,net(x).data, '--')\n\n\n\n\n\n\n\n\n\nìµœì´ˆì˜ ê³¡ì„ ë³´ë‹¤ëŠ” ê·¸ëŸ´ë“¯í•´ì§\nì•Œê³  ìˆëŠ” True ê°’ê³¼ ë¹„êµí•´ë³´ë©´\n\nì£¼í™©ìƒ‰ ì„ : True\n200ë²ˆ ì •ë„ ë°˜ë³µí•˜ë‹ˆ ì–´ëŠ ì •ë„ ì˜¨ ê²ƒ ê°™ì§€ë§Œ ë”± ë§ë‹¤ê³  ë³´ê¸°ëŠ” ì–´ë ¤ì›€\n\n\n\nplt.plot(x,y,'.',alpha=0.03)\nplt.plot(x,prob,'--')\nplt.plot(x,net(x).data, '--')\n\n\n\n\n\n\n\n\n\n200ë²ˆ ë” (ì´ 400ë²ˆ)\n\n\nfor epoc in range(200):\n    yhat = net(x)\n    loss = torch.mean((y-yhat)**2)\n    loss.backward() # ë¯¸ë¶„\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'.',alpha=0.03)\nplt.plot(x,prob,'--')\nplt.plot(x,net(x).data, '--')\n\n\n\n\n\n\n\n\n\n200ë²ˆ ë” (ì´ 600ë²ˆ)\n\n\nfor epoc in range(200):\n    yhat = net(x)\n    loss = torch.mean((y-yhat)**2)\n    loss.backward() # ë¯¸ë¶„\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'.',alpha=0.03)\nplt.plot(x,prob,'--')\nplt.plot(x,net(x).data, '--')\n\n\n\n\n\n\n\n\n\n200ë²ˆ ë” (ì´ 800ë²ˆ)\n\n\nfor epoc in range(200):\n    yhat = net(x)\n    loss = torch.mean((y-yhat)**2)\n    loss.backward() # ë¯¸ë¶„\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'.',alpha=0.03)\nplt.plot(x,prob,'--')\nplt.plot(x,net(x).data, '--')\n\n\n\n\n\n\n\n\n\nëŒë¦´ìˆ˜ë¡ ê°€ê¹Œì›Œì§ˆ ê²ƒ ê°™ìŒ\n\n)ğŸ—£ï¸\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1, out_features=1),\n    torch.nn.Sigmoid()\n)\nl1, a1 = net \nl1.weight.data = torch.tensor([[-0.3]])\nl1.bias.data = torch.tensor([-0.8])\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25)\n#---#\nfor epoc in range(100):\n    ## 1\n    yhat = net(x) \n    ## 2 \n    loss = torch.mean((y-yhat)**2)\n    ## 3\n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'.',alpha=0.05)\nplt.plot(x,prob,'--r')\nplt.plot(x,yhat.data,'--b')\nplt.title('after 100 epochs')\n\nText(0.5, 1.0, 'after 100 epochs')\n\n\n\n\n\n\n\n\n\n\nfor epoc in range(4900):\n    ## 1\n    yhat = net(x) \n    ## 2 \n    loss = torch.mean((y-yhat)**2)\n    ## 3\n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'.',alpha=0.05)\nplt.plot(x,prob,'--r')\nplt.plot(x,yhat.data,'--b')\nplt.title('after 5000 epochs')\n\nText(0.5, 1.0, 'after 5000 epochs')\n\n\n\n\n\n\n\n\n\nğŸ—£ï¸ ë¡œì§€ìŠ¤í‹±ì´ í•´ê²°ëœ ê²ƒì²˜ëŸ¼ ë³´ì„\nğŸ—£ï¸(\n\në‹¤ìŒê³¼ ê°™ì´ í•´ë„ ë§ˆì°¬ê°€ì§€ (ì´ˆê¸°ê°’ì€ í¬ê²Œ ì¤‘ìš”í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ)\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1, out_features=1),\n    torch.nn.Sigmoid()\n)\n# l1, a1 = net \n# l1.weight.data = torch.tensor([[-0.3]])\n# l1.bias.data = torch.tensor([-0.8])\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25)\n#---#\nfor epoc in range(100):\n    ## 1\n    yhat = net(x) \n    ## 2 \n    loss = torch.mean((y-yhat)**2)\n    ## 3\n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'.',alpha=0.05)\nplt.plot(x,prob,'--r')\nplt.plot(x,yhat.data,'--b')\nplt.title('after 100 epochs')\n\nText(0.5, 1.0, 'after 100 epochs')\n\n\n\n\n\n\n\n\n\n\nfor epoc in range(4900):\n    ## 1\n    yhat = net(x) \n    ## 2 \n    loss = torch.mean((y-yhat)**2)\n    ## 3\n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'.',alpha=0.05)\nplt.plot(x,prob,'--r')\nplt.plot(x,yhat.data,'--b')\nplt.title('after 5000 epochs')\n\nText(0.5, 1.0, 'after 5000 epochs')\n\n\n\n\n\n\n\n\n\n\nì„±ê³µí•œ ê²ƒ ê°™ì§€ë§Œ ì‹¤ìƒì€ ê·¸ë ‡ì§€ ì•ŠìŒ\n\nfor epoc in range(4900):\n    ## 1\n    yhat = net(x) \n    ## 2 \n    loss = torch.mean((y-yhat)**2) # ì´ ë¶€ë¶„ì— ë¬¸ì œê°€ ìˆì–´ ì„¤ëª…í•  ì˜ˆì •\n    ## 3\n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n)ğŸ—£ï¸"
  },
  {
    "objectID": "posts/03wk-2.html#a.-ì‹œê°í™”ë¥¼-ìœ„í•œ-ì¤€ë¹„",
    "href": "posts/03wk-2.html#a.-ì‹œê°í™”ë¥¼-ìœ„í•œ-ì¤€ë¹„",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "A. ì‹œê°í™”ë¥¼ ìœ„í•œ ì¤€ë¹„",
    "text": "A. ì‹œê°í™”ë¥¼ ìœ„í•œ ì¤€ë¹„\n\ndef plot_loss(loss_fn, ax=None, Wstar=[-1,5]):\n    w0hat,w1hat =torch.meshgrid(torch.arange(-10,3,0.1),torch.arange(-1,10,0.1),indexing='ij')\n    w0hat = w0hat.reshape(-1)\n    w1hat = w1hat.reshape(-1)\n    def l(w0hat,w1hat):\n        yhat = torch.exp(w0hat+w1hat*x)/(1+torch.exp(w0hat+w1hat*x))\n        return loss_fn(yhat,y) \n    loss = list(map(l,w0hat,w1hat))\n    #---#\n    if ax is None: \n        fig = plt.figure()\n        ax = fig.add_subplot(1,1,1,projection='3d')\n    ax.scatter(w0hat,w1hat,loss,s=0.001) \n    ax.scatter(w0hat[::20],w1hat[::20],loss[::20],s=0.1,color='C0') \n    w0star,w1star = np.array(Wstar).reshape(-1)\n    ax.scatter(w0star,w1star,l(w0star,w1star),s=200,marker='*',color='red',label=f\"W=[{w0star:.1f},{w1star:.1f}]\")\n    #---#\n    ax.elev = 15\n    ax.dist = -20\n    ax.azim = 75    \n    ax.legend()\n    ax.set_xlabel(r'$w_0$')  # xì¶• ë ˆì´ë¸” ì„¤ì •\n    ax.set_ylabel(r'$w_1$')  # yì¶• ë ˆì´ë¸” ì„¤ì •\n    ax.set_xticks([-10,-5,0])  # xì¶• í‹± ê°„ê²© ì„¤ì •\n    ax.set_yticks([-10,0,10])  # yì¶• í‹± ê°„ê²© ì„¤ì •\n\n\ndef _learn_and_record(net, loss_fn, optimizr):\n    yhat_history = [] \n    loss_history = []\n    What_history = []\n    Whatgrad_history = []\n    What_history.append([net[0].bias.data.item(), net[0].weight.data.item()])\n    for epoc in range(100): \n        ## step1 \n        yhat = net(x)\n        ## step2 \n        loss = loss_fn(yhat,y)\n        ## step3\n        loss.backward() \n        ## step4 \n        optimizr.step()\n        ## record \n        if epoc % 5 ==0: \n            yhat_history.append(yhat.reshape(-1).data.tolist())\n            loss_history.append(loss.item())\n            What_history.append([net[0].bias.data.item(), net[0].weight.data.item()])\n            Whatgrad_history.append([net[0].bias.grad.item(), net[0].weight.grad.item()])\n        optimizr.zero_grad() \n        \n    return yhat_history, loss_history, What_history, Whatgrad_history\n    \ndef show_animation(net, loss_fn, optimizr):\n    yhat_history,loss_history,What_history,Whatgrad_history = _learn_and_record(net,loss_fn,optimizr)\n    \n    fig = plt.figure(figsize=(7.5,3.5))\n    ax1 = fig.add_subplot(1, 2, 1)\n    ax2 = fig.add_subplot(1, 2, 2, projection='3d')\n    ## ax1: ì™¼ìª½ê·¸ë¦¼ \n    ax1.scatter(x,y,alpha=0.01)\n    ax1.scatter(x[0],y[0],color='C0',label=r\"observed data = $(x_i,y_i)$\")\n    ax1.plot(x,prob,'--',label=r\"prob (true) = $(x_i,\\frac{exp(-1+5x_i)}{1+exp(-1+5x_i)})$\")    \n    line, = ax1.plot(x,yhat_history[0],'--',label=r\"prob (estimated) = $(x_i,\\hat{y}_i)$\") \n    ax1.legend()\n    ## ax2: ì˜¤ë¥¸ìª½ê·¸ë¦¼ \n    plot_loss(loss_fn,ax2)\n    ax2.scatter(np.array(What_history)[0,0],np.array(What_history)[0,1],loss_history[0],color='blue',s=200,marker='*')    \n    def animate(epoc):\n        line.set_ydata(yhat_history[epoc])\n        w0hat = np.array(What_history)[epoc,0]\n        w1hat = np.array(What_history)[epoc,1]\n        w0hatgrad = np.array(Whatgrad_history)[epoc,0]\n        w1hatgrad = np.array(Whatgrad_history)[epoc,1]\n        ax2.scatter(w0hat,w1hat,loss_history[epoc],color='grey')\n        ax2.set_title(f\"What.grad=[{w0hatgrad:.4f},{w1hatgrad:.4f}]\",y=0.8)\n        fig.suptitle(f\"epoch={epoc*5} // What=[{w0hat:.2f},{w1hat:.2f}] // Loss={loss_fn.__class__.__name__} // Opt={optimizr.__class__.__name__}\")\n        return line\n    ani = animation.FuncAnimation(fig, animate, frames=20)    \n    plt.close()\n    return ani\n\n\nfrom matplotlib import animation\nplt.rcParams[\"animation.html\"] = \"jshtml\"\n\ní•¨ìˆ˜ì‚¬ìš©ë²•\n\nloss_fn = torch.nn.MSELoss()\nplot_loss(loss_fn)\n\n\n\n\n\n\n\n\nğŸ—£ï¸(\n\ndef loss_fn2(yhat,y):\n    return loss_fn(yhat,y)*2\n\n\nplot_loss(loss_fn2)\n\n\n\n\n\n\n\n\n\nzì¶•ë§Œ 2ë°° ì¦ê°€ (í•¨ìˆ˜: ê³¡ë©´ì„ ê·¸ë ¤ì£¼ëŠ” ì—­í• )\n\n\n# show_animation??\n\n\nSignature: show_animation(net, loss_fn, optimizr)\n\nnet: ì´ˆê¸° ì„¤ì • ê°’ (w0, w1)\nloss_fn: ê·¸ë¦¼\noptimizr: í•™ìŠµ ê³¼ì •\n\në°‘ ì½”ë“œ: ì–´ë– í•œ ì´ˆê¸°ê°’ì„ ë°›ì•„ í•™ìŠµí•˜ëŠ” ê³¼ì •ì„ ê·¸ë ¤ì¤Œ\n\nì‹¤í–‰í•  ë•Œë§ˆë‹¤ ì´ˆê¸°ê°’ ë‹¬ë¼ì§\n\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25)\nshow_animation(net,loss_fn,optimizr)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\nì´ˆê¸°ê°’ ê³ ì •\n\në§Œì•½ í•™ìŠµë¥ ì´ 2.5ë©´ ë” ë¹¨ë¦¬ ë–¨ì–´ì§\n\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n)\nnet[0].weight.data = torch.tensor([[-0.8]])\nnet[0].bias.data = torch.tensor([-0.3])\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25)\nshow_animation(net,loss_fn,optimizr)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n)ğŸ—£ï¸\n\ntorch.manual_seed(42)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25) \nshow_animation(net,loss_fn,optimizr)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\nğŸ—£ï¸\n\nì´ˆê¸°ê°’ì— ë”°ë¼ í•™ìŠµì´ ë‹¬ë¼ì§\në§Œì•½ ì´ˆê¸° ê°’ì´ ìš°ì¸¡ ìƒë‹¨ì´ë¼ë©´ í‰í‰í•˜ê¸° ë•Œë¬¸ì— updateê°€ ì•„ì£¼ ì¡°ê¸ˆì”© ì¼ì–´ë‚¨"
  },
  {
    "objectID": "posts/03wk-2.html#b.-ì¢‹ì€-ì´ˆê¸°ê°’",
    "href": "posts/03wk-2.html#b.-ì¢‹ì€-ì´ˆê¸°ê°’",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "B. ì¢‹ì€ ì´ˆê¸°ê°’",
    "text": "B. ì¢‹ì€ ì´ˆê¸°ê°’\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nnet[0].bias.data = torch.tensor([-0.8])\nnet[0].weight.data = torch.tensor([[-0.3]])\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25) \n#---#\nshow_animation(net,loss_fn,optimizr)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\nğŸ—£ï¸\n\nì´ ê²½ìš°ëŠ” ê¸°ë‹¤ë¦¬ë©´ í•™ìŠµì´ ì˜ ë  ê±° ê°™ìŒ"
  },
  {
    "objectID": "posts/03wk-2.html#c.-ê°€ëŠ¥ì„±-ìˆëŠ”-ì´ˆê¸°ê°’",
    "href": "posts/03wk-2.html#c.-ê°€ëŠ¥ì„±-ìˆëŠ”-ì´ˆê¸°ê°’",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "C. ê°€ëŠ¥ì„± ìˆëŠ” ì´ˆê¸°ê°’",
    "text": "C. ê°€ëŠ¥ì„± ìˆëŠ” ì´ˆê¸°ê°’\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nnet[0].bias.data = torch.tensor([-3.0])\nnet[0].weight.data = torch.tensor([[-1.0]])\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25) \n#---#\nshow_animation(net,loss_fn,optimizr)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\nğŸ—£ï¸\n\në§ˆì§€ë§‰ì— ì•½ê°„ í¬ë§ì´ ë³´ì„\në§ˆìŒ ë¨¹ê³  20,000ë²ˆ ì •ë„ ëŒë¦¬ë©´ ë  ê±° ê°™ìŒ"
  },
  {
    "objectID": "posts/03wk-2.html#d.-ìµœì•…ì˜-ì´ˆê¸°ê°’",
    "href": "posts/03wk-2.html#d.-ìµœì•…ì˜-ì´ˆê¸°ê°’",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "D. ìµœì•…ì˜ ì´ˆê¸°ê°’",
    "text": "D. ìµœì•…ì˜ ì´ˆê¸°ê°’\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nnet[0].bias.data = torch.tensor([-10.0])\nnet[0].weight.data = torch.tensor([[-1.0]])\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25) \n#---#\nshow_animation(net,loss_fn,optimizr)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\nğŸ—£ï¸\n\nì´ ê²½ìš°ëŠ” í¬ë§ì´ ì—†ìŒ\nê³¡ì„ ì´ ì•„ë˜ë¡œ ë³¼ë¡í•œ 2ì°¨ í•¨ìˆ˜ê°€ ì•„ë‹ˆê³  4ì°¨ í•¨ìˆ˜ë¼ë©´ í•™ìŠµë¥ ê³¼ ì´ˆê¸° ê°’ì— ë”°ë¼ ê°–í˜€ë²„ë¦´ ìˆ˜ë„ ìˆê³  ìš´ì— ë”°ë¼ ë‹¬ë¼ì§\n\n\ní•´ê²°í•˜ëŠ” ì ‘ê·¼ë²•:\n\nì»´ê³µìŠ¤íƒ€ì¼: ì—í­ì„ ëŠ˜ë ¤ë³¼ê¹Œ?\nì‚°ê³µìŠ¤íƒ€ì¼: ì˜µí‹°ë§ˆì´ì €ë¥¼ ë°”ê¿”ë³¼ê¹Œ?\ní†µê³„ìŠ¤íƒ€ì¼: Lossë¥¼ ë°”ê¿”ë³¼ê¹Œ?\nğŸ—£ï¸\n\nì´ˆê¸° ê°’ì„ ë°”ê¿”ê°€ë©° ë¬´ìˆ˜íˆ ì‹¤í–‰í•˜ë©° ì°¾ìŒ\nì´ ì–´ë ¤ìš´ ê³¡ë©´ì— ëŒ€í•´ ì˜µí‹°ë§ˆì´ì €ë¥¼ ìˆ˜ì •\nê³¡ë©´ ìì²´ë¥¼ ìµœì í™”ê°€ ì˜ ë˜ê²Œ ë°”ê¿ˆ (loss í•¨ìˆ˜ë¥¼ ë°”ê¿ˆ: MSE Loss ë§ê³  ë‹¤ë¥¸ Loss?)"
  },
  {
    "objectID": "posts/03wk-2.html#a.-bce-lossë¥¼-ì‚¬ìš©í•˜ì—¬-í•™ìŠµ",
    "href": "posts/03wk-2.html#a.-bce-lossë¥¼-ì‚¬ìš©í•˜ì—¬-í•™ìŠµ",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "A. BCE Lossë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ",
    "text": "A. BCE Lossë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ\n- BCE lossë¼ëŠ”ê²Œ ìˆìŒ.\n\n\\(loss= - \\sum_{i=1}^{n} \\big(y_i\\log(\\hat{y}_i)+(1-y_i)\\log(1-\\hat{y}_i)\\big)\\)\nhttps://en.wikipedia.org/wiki/Cross-entropy\n\nğŸ—£ï¸(\nyi = 0\nyi_hat = 0.001\nlog(1) = 0\nloss = 0\nyi = 0\nyi_hat = 0.9999\nlog(1-0.999) = log(0) = -ë¬´í•œëŒ€\nloss = ë¬´í•œëŒ€\nyi = 1\nyi_hat = 1\nloss = 0\nyi = 1\nyi_hat = 0.0001\nloss = ë¬´í•œëŒ€\n\në¹„ìŠ·í• ìˆ˜ë¡ 0, ë‹¤ë¥¼ìˆ˜ë¡ ë¬´í•œëŒ€ê¹Œì§€ ê° -&gt; lossì˜ ì—­í• ì€ í•¨\nì›ë¦¬: - log likelihoood\n\n)ğŸ—£ï¸\n\nğŸ—£ï¸\n\nnet[0] = torch.nn.Linear(in_features=1, out_features=1)\nnet[1] = torch.nn.Sigmoid()\nnet = [net[0], net[1]] ëŠë‚Œ\nl1, a1 = [net[0], net[1]] ëŠë‚Œ\n\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1, out_features=1),\n    torch.nn.Sigmoid()\n)\nl1, a1 = net \nl1.weight.data = torch.tensor([[-0.3]])\nl1.bias.data = torch.tensor([-0.8])\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25)\n#---#\nfor epoc in range(100):\n    ## 1\n    yhat = net(x) \n    ## 2 \n    #loss = torch.mean((y-yhat)**2) # loss_fn(yhat,y)\n    loss = -torch.mean(y*torch.log(yhat) + (1-y)*torch.log(1-yhat))\n    ## 3\n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'.',alpha=0.05)\nplt.plot(x,prob,'--r')\nplt.plot(x,yhat.data,'--b')\nplt.title('after 100 epochs')\n\nText(0.5, 1.0, 'after 100 epochs')\n\n\n\n\n\n\n\n\n\nê°™ì€ 100 ì—í­ì¸ë° í›¨ì”¬ ì˜ë§ì¶¤..\nğŸ—£ï¸ ë™ì¼í•œ ì´ˆê¸° ê°’\n- lossìˆ˜ì‹ì„ ëª»ì™¸ìš°ê² ë‹¤ë©´?\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1, out_features=1),\n    torch.nn.Sigmoid()\n)\nl1, a1 = net \nl1.weight.data = torch.tensor([[-0.3]])\nl1.bias.data = torch.tensor([-0.8])\nloss_fn = torch.nn.BCELoss()\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25)\n#---#\nfor epoc in range(100):\n    ## 1\n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y) # yhatë¶€í„° ì¨ì•¼í•¨\n    ## 3\n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'.',alpha=0.05)\nplt.plot(x,prob,'--r')\nplt.plot(x,yhat.data,'--b')\nplt.title('after 100 epochs')\n\nText(0.5, 1.0, 'after 100 epochs')"
  },
  {
    "objectID": "posts/03wk-2.html#b.-loss-function-ì‹œê°í™”",
    "href": "posts/03wk-2.html#b.-loss-function-ì‹œê°í™”",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "B. Loss Function ì‹œê°í™”",
    "text": "B. Loss Function ì‹œê°í™”\n\nplot_loss(torch.nn.MSELoss())\n\n\n\n\n\n\n\n\nğŸ—£ï¸ MSELossëŠ” ìš°ì¸¡ ìƒë‹¨ì— ìˆìœ¼ë©´ ì•ˆ ë  ê²ƒ ê°™ìŒ\n\nplot_loss(torch.nn.BCELoss())\n\n\n\n\n\n\n\n\n- ë¹„êµí•´ë³´ì.\n\nfig = plt.figure()\nax1 = fig.add_subplot(1,2,1,projection='3d')\nax2 = fig.add_subplot(1,2,2,projection='3d')\nplot_loss(torch.nn.MSELoss(),ax1)\nplot_loss(torch.nn.BCELoss(),ax2)\n\n\n\n\n\n\n\n\n\nğŸ—£ï¸\n\nì˜¤ë¥¸ìª½ê³¼ ê°™ì€ ê²½ìš°ë¥¼ ì–´ë ¤ìš´ ë§ë¡œ convex functionì´ë¼ê³  í•¨\nloss í•¨ìˆ˜ê°€ convex functionì´ë©´ ìˆ˜ë ´ì‹œí‚¤ê¸° ì‰¬ì›€"
  },
  {
    "objectID": "posts/03wk-2.html#c.-í•™ìŠµê³¼ì •-ì‹œê°í™”-ì¢‹ì€-ì´ˆê¸°ê°’",
    "href": "posts/03wk-2.html#c.-í•™ìŠµê³¼ì •-ì‹œê°í™”-ì¢‹ì€-ì´ˆê¸°ê°’",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "C. í•™ìŠµê³¼ì • ì‹œê°í™” â€“ ì¢‹ì€ ì´ˆê¸°ê°’",
    "text": "C. í•™ìŠµê³¼ì • ì‹œê°í™” â€“ ì¢‹ì€ ì´ˆê¸°ê°’\n- MSELoss\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nnet[0].bias.data = torch.tensor([-0.8])\nnet[0].weight.data = torch.tensor([[-0.3]])\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25) \n#---#\nshow_animation(net,loss_fn,optimizr)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n- BCELoss\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nnet[0].bias.data = torch.tensor([-0.8])\nnet[0].weight.data = torch.tensor([[-0.3]])\nloss_fn = torch.nn.BCELoss()\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25) \n#---#\nshow_animation(net,loss_fn,optimizr)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nğŸ—£ï¸ ê°™ì€ ì´ˆê¸°ê°’ì¸ë° BCELossê°€ ë” ìˆ˜ë ´ì„ ì˜ í•  ê²ƒ ê°™ìŒ"
  },
  {
    "objectID": "posts/03wk-2.html#d.-í•™ìŠµê³¼ì •-ì‹œê°í™”-ê°€ëŠ¥ì„±-ìˆëŠ”-ì´ˆê¸°ê°’",
    "href": "posts/03wk-2.html#d.-í•™ìŠµê³¼ì •-ì‹œê°í™”-ê°€ëŠ¥ì„±-ìˆëŠ”-ì´ˆê¸°ê°’",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "D. í•™ìŠµê³¼ì • ì‹œê°í™” â€“ ê°€ëŠ¥ì„± ìˆëŠ” ì´ˆê¸°ê°’",
    "text": "D. í•™ìŠµê³¼ì • ì‹œê°í™” â€“ ê°€ëŠ¥ì„± ìˆëŠ” ì´ˆê¸°ê°’\n- MSELoss\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nnet[0].bias.data = torch.tensor([-3.0])\nnet[0].weight.data = torch.tensor([[-1.0]])\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25) \n#---#\nshow_animation(net,loss_fn,optimizr)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n- BCELoss\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nnet[0].bias.data = torch.tensor([-3.0])\nnet[0].weight.data = torch.tensor([[-1.0]])\nloss_fn = torch.nn.BCELoss()\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25) \n#---#\nshow_animation(net,loss_fn,optimizr)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nğŸ—£ï¸ BCELossëŠ” ì²˜ìŒë¶€í„° ì˜ ë–¨ì–´ì§"
  },
  {
    "objectID": "posts/03wk-2.html#e.-í•™ìŠµê³¼ì •-ì‹œê°í™”-ìµœì•…ì˜-ì´ˆê¸°ê°’",
    "href": "posts/03wk-2.html#e.-í•™ìŠµê³¼ì •-ì‹œê°í™”-ìµœì•…ì˜-ì´ˆê¸°ê°’",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "E. í•™ìŠµê³¼ì • ì‹œê°í™” â€“ ìµœì•…ì˜ ì´ˆê¸°ê°’",
    "text": "E. í•™ìŠµê³¼ì • ì‹œê°í™” â€“ ìµœì•…ì˜ ì´ˆê¸°ê°’\n- MSELoss\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nnet[0].bias.data = torch.tensor([-10.0])\nnet[0].weight.data = torch.tensor([[-1.0]])\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25) \n#---#\nshow_animation(net,loss_fn,optimizr)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n- BCELoss\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nnet[0].bias.data = torch.tensor([-10.0])\nnet[0].weight.data = torch.tensor([[-1.0]])\nloss_fn = torch.nn.BCELoss()\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25) \n#---#\nshow_animation(net,loss_fn,optimizr)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nğŸ—£ï¸ BCELossëŠ” ì²˜ìŒë¶€í„° ì˜ ë–¨ì–´ì§"
  },
  {
    "objectID": "posts/03wk-2.html#a.-í•™ìŠµê³¼ì •-ì‹œê°í™”-ì¢‹ì€-ì´ˆê¸°ê°’",
    "href": "posts/03wk-2.html#a.-í•™ìŠµê³¼ì •-ì‹œê°í™”-ì¢‹ì€-ì´ˆê¸°ê°’",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "A. í•™ìŠµê³¼ì • ì‹œê°í™” â€“ ì¢‹ì€ ì´ˆê¸°ê°’",
    "text": "A. í•™ìŠµê³¼ì • ì‹œê°í™” â€“ ì¢‹ì€ ì´ˆê¸°ê°’\n- MSELoss + SGD\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nnet[0].bias.data = torch.tensor([-0.8470])\nnet[0].weight.data = torch.tensor([[-0.3467]])\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25) \n#---#\nshow_animation(net,loss_fn,optimizr)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n- MSELoss + Adam\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nnet[0].bias.data = torch.tensor([-0.8])\nnet[0].weight.data = torch.tensor([[-0.3]])\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters(),lr=0.25) \n#---#\nshow_animation(net,loss_fn,optimizr)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nğŸ—£ï¸ Adamì„ ì‚¬ìš©í•˜ë‹ˆ ë¹¨ë¦¬ ë–¨ì–´ì§€ë©´ì„œ ì˜ ìˆ˜ë ´í•¨ (í˜ìœ¼ë¡œ ë¯¸ëŠ” ëŠë‚Œ)"
  },
  {
    "objectID": "posts/03wk-2.html#b.-í•™ìŠµê³¼ì •-ì‹œê°í™”-ê°€ëŠ¥ì„±-ìˆëŠ”-ì´ˆê¸°ê°’",
    "href": "posts/03wk-2.html#b.-í•™ìŠµê³¼ì •-ì‹œê°í™”-ê°€ëŠ¥ì„±-ìˆëŠ”-ì´ˆê¸°ê°’",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "B. í•™ìŠµê³¼ì • ì‹œê°í™” â€“ ê°€ëŠ¥ì„± ìˆëŠ” ì´ˆê¸°ê°’",
    "text": "B. í•™ìŠµê³¼ì • ì‹œê°í™” â€“ ê°€ëŠ¥ì„± ìˆëŠ” ì´ˆê¸°ê°’\n- MSELoss + SGD\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nnet[0].bias.data = torch.tensor([-3.0])\nnet[0].weight.data = torch.tensor([[-1.0]])\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25) \n#---#\nshow_animation(net,loss_fn,optimizr)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n- MSELoss + Adam\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nnet[0].bias.data = torch.tensor([-3.0])\nnet[0].weight.data = torch.tensor([[-1.0]])\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters(),lr=0.25) \n#---#\nshow_animation(net,loss_fn,optimizr)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nğŸ—£ï¸ Adamì„ ì‚¬ìš©í•˜ë‹ˆ ë¹¨ë¦¬ ë–¨ì–´ì§€ë©´ì„œ ì˜ ìˆ˜ë ´í•¨ (ë§ˆì§€ë§‰ì€ ì‚´ì§ ëŒì•„ê°€ëŠ” ëŠë‚Œ)"
  },
  {
    "objectID": "posts/03wk-2.html#c.-í•™ìŠµê³¼ì •-ì‹œê°í™”-ìµœì•…ì˜-ì´ˆê¸°ê°’",
    "href": "posts/03wk-2.html#c.-í•™ìŠµê³¼ì •-ì‹œê°í™”-ìµœì•…ì˜-ì´ˆê¸°ê°’",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "C. í•™ìŠµê³¼ì • ì‹œê°í™” â€“ ìµœì•…ì˜ ì´ˆê¸°ê°’",
    "text": "C. í•™ìŠµê³¼ì • ì‹œê°í™” â€“ ìµœì•…ì˜ ì´ˆê¸°ê°’\n- MSELoss + SGD\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nnet[0].bias.data = torch.tensor([-10.0])\nnet[0].weight.data = torch.tensor([[-1.0]])\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD(net.parameters(),lr=0.05) \n#---#\nshow_animation(net,loss_fn,optimizr)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n- MSELoss + Adam\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nnet[0].bias.data = torch.tensor([-10.0])\nnet[0].weight.data = torch.tensor([[-1.0]])\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters(),lr=0.25) \n#---#\nshow_animation(net,loss_fn,optimizr)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nğŸ—£ï¸ Adamì„ ì‚¬ìš©í•˜ë‹ˆ ë¹¨ë¦¬ ë–¨ì–´ì§€ë©´ì„œ ì˜ ìˆ˜ë ´í•¨ (ë‚´ë ¤ì˜¤ëŠ” í˜ì´ ê°•í•´ì„œ ê·¸ëŸ°ì§€ ë§ˆì§€ë§‰ì€ ì‚´ì§ ëŒë‹¤ê°€ ê°€ëŠ” ëŠë‚Œ)\nğŸ—£ï¸ í˜„ì¬ ìµœì í™”ë¥¼ ì˜í•˜ê³  ì‹¶ìœ¼ë©´ Adamì„ ì‚¬ìš©í•˜ë©´ ë¨"
  },
  {
    "objectID": "posts/03wk-2.html#d.-ì°¸ê³ ìë£Œ",
    "href": "posts/03wk-2.html#d.-ì°¸ê³ ìë£Œ",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "D. ì°¸ê³ ìë£Œ",
    "text": "D. ì°¸ê³ ìë£Œ\nhttps://www.youtube.com/watch?v=MD2fYip6QsQ\n\n11:50 â€“ Momentum\n12:30 â€“ RMSprop\n15:55 â€“ Adam\nğŸ—£ï¸ local minê³¼ global minì´ ë”°ë¡œ ìˆì„ ë•Œ\n\nì¼ë°˜ì ì¸ ê²½ì‚¬í•˜ê°•ë²•ì€ ë³´í†µ local minì— ë¹ ì§\nAdamì€ local minì„ ì˜ íƒˆì¶œí•¨ (í•­ìƒì€ X)"
  },
  {
    "objectID": "posts/03wk-2.html#a.-ì‹ ë¬¸ê¸°ì‚¬-ë°ì´í„°ì˜-ëª¨í‹°ë¸Œ",
    "href": "posts/03wk-2.html#a.-ì‹ ë¬¸ê¸°ì‚¬-ë°ì´í„°ì˜-ëª¨í‹°ë¸Œ",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "A. ì‹ ë¬¸ê¸°ì‚¬ (ë°ì´í„°ì˜ ëª¨í‹°ë¸Œ)",
    "text": "A. ì‹ ë¬¸ê¸°ì‚¬ (ë°ì´í„°ì˜ ëª¨í‹°ë¸Œ)\n- ìŠ¤í™ì´ ë†’ì•„ë„ ì·¨ì—…ì´ ì•ˆëœë‹¤ê³  í•©ë‹ˆë‹¤..\nì¤‘ì†ŒÂ·ì§€ë°© ê¸°ì—… â€œë½‘ì•„ë´¤ì ê·¸ë§Œë‘ë‹ˆê¹Œâ€\nì¤‘ì†Œê¸°ì—… ê´€ê³„ìë“¤ì€ ê³ ìŠ¤í™ ì§€ì›ìë¥¼ êº¼ë¦¬ëŠ” ì´ìœ ë¡œ ë†’ì€ í‡´ì§ë¥ ì„ ê¼½ëŠ”ë‹¤. ì—¬ê±´ì´ ì¢‹ì€ ëŒ€ê¸°ì—…ìœ¼ë¡œ ì´ì§í•˜ê±°ë‚˜ íšŒì‚¬ë¥¼ ê´€ë‘ëŠ” ê²½ìš°ê°€ ë§ë‹¤ëŠ” í•˜ì†Œì—°ì´ë‹¤. ê³ ìš©ì •ë³´ì›ì´ ì§€ë‚œ 3ì¼ ê³µê°œí•œ ìë£Œì— ë”°ë¥´ë©´ ì¤‘ì†Œê¸°ì—… ì²­ë…„ì·¨ì—…ì ê°€ìš´ë° 49.5%ê°€ 2ë…„ ë‚´ì— íšŒì‚¬ë¥¼ ê·¸ë§Œë‘ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ë‹¤.\nì¤‘ì†Œ ITì—…ì²´ ê´€ê³„ìëŠ” â€œê¸°ì—… ì…ì¥ì—ì„œ ê°€ì¥ ë¼ˆì•„í”ˆ ê²Œ ì‹ ì…ì‚¬ì›ì´ ê·¸ë§Œë‘¬ì„œ ìƒˆë¡œ ë½‘ëŠ” ì¼â€ì´ë¼ë©° â€œëª…ë¬¸ëŒ€ ë‚˜ì˜¨ ìŠ¤í™ ì¢‹ì€ ì§€ì›ìë¥¼ ë½‘ì•„ë†”ë„ 1ë…„ì„ ì±„ìš°ì§€ ì•Šê³  ê·¸ë§Œë‘ëŠ” ì‚¬ì›ì´ ëŒ€ë¶€ë¶„ì´ë¼ ìš°ë¦¬ë„ ëˆˆì„ ë‚®ì¶° ì‚¬ëŒì„ ë½‘ëŠ”ë‹¤â€ê³  ë§í–ˆë‹¤."
  },
  {
    "objectID": "posts/03wk-2.html#b.-ê°€ì§œë°ì´í„°-ìŠ¤í™ì˜-ì—­ì„¤",
    "href": "posts/03wk-2.html#b.-ê°€ì§œë°ì´í„°-ìŠ¤í™ì˜-ì—­ì„¤",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "B. ê°€ì§œë°ì´í„° â€“ ìŠ¤í™ì˜ ì—­ì„¤",
    "text": "B. ê°€ì§œë°ì´í„° â€“ ìŠ¤í™ì˜ ì—­ì„¤\nğŸ—£ï¸ x: ìŠ¤í™, prob: í•©ê²©í•  í™•ë¥ \n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DL2025/main/posts/ironyofspec.csv\")\ndf\n\n\n\n\n\n\n\n\nx\nprob\ny\n\n\n\n\n0\n-1.000000\n0.000045\n0.0\n\n\n1\n-0.998999\n0.000046\n0.0\n\n\n2\n-0.997999\n0.000047\n0.0\n\n\n3\n-0.996998\n0.000047\n0.0\n\n\n4\n-0.995998\n0.000048\n0.0\n\n\n...\n...\n...\n...\n\n\n1995\n0.995998\n0.505002\n0.0\n\n\n1996\n0.996998\n0.503752\n0.0\n\n\n1997\n0.997999\n0.502501\n0.0\n\n\n1998\n0.998999\n0.501251\n1.0\n\n\n1999\n1.000000\n0.500000\n1.0\n\n\n\n\n2000 rows Ã— 3 columns\n\n\n\n\nx = torch.tensor(df.x).float().reshape(-1,1)\ny = torch.tensor(df.y).float().reshape(-1,1)\nprob = torch.tensor(df.prob).float().reshape(-1,1)\n\n\nplt.plot(x,y,'o',alpha=0.02)\nplt.plot(x[0],y[0],'o',label= r\"observed data = $(x_i,y_i)$\",color=\"C0\")\nplt.plot(x,prob,'--b',label= r\"prob (true, unknown)\")\nplt.legend()\n\n\n\n\n\n\n\n\nğŸ—£ï¸ ìŠ¤í™ì´ ë„ˆë¬´ ë†’ìœ¼ë©´ ì˜¤íˆë ¤ ë–¨ì–´ì§"
  },
  {
    "objectID": "posts/03wk-2.html#c.-ë¡œì§€ìŠ¤í‹±ìœ¼ë¡œ-ì í•©",
    "href": "posts/03wk-2.html#c.-ë¡œì§€ìŠ¤í‹±ìœ¼ë¡œ-ì í•©",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "C. ë¡œì§€ìŠ¤í‹±ìœ¼ë¡œ ì í•©",
    "text": "C. ë¡œì§€ìŠ¤í‹±ìœ¼ë¡œ ì í•©\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n)\nloss_fn = torch.nn.BCELoss()\noptimizr = torch.optim.Adam(net.parameters())\n#---# \nfor epoc in range(5000):\n    ## 1 \n    yhat = net(x)\n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'o',alpha=0.02)\nplt.plot(x[0],y[0],'o',label= r\"observed data = $(x_i,y_i)$\",color=\"C0\")\nplt.plot(x,prob,'--b',label= r\"prob (true, unknown)\")\nplt.plot(x,net(x).data, '--', label= r\"prob (estimated) = $(x_i,\\hat{y}_i)$\")\nplt.legend()\n\n\n\n\n\n\n\n\n- Epochì„ 10ì–µë²ˆìœ¼ë¡œ ì„¤ì •í•´ë„ ì´ê±´ ëª» ë§ì¶œê²ƒ ê°™ìŒ.\n\nğŸ—£ï¸\n\nì£¼í™©ìƒ‰ ì„ (model)ì´ ì˜¬ë¼ê°€ë‹¤ê°€ ë‚´ë ¤ì˜¤ëŠ” ê²ƒì€ ìµœì´ˆì˜ ê³¡ì„ ì´ ë°”ë€” ìˆ˜ ìˆëŠ” ë²”ìœ„ë¥¼ ë²—ì–´ë‚¨ (ìˆ˜ì‹ì ìœ¼ë¡œ)\nì´ëŸ° ê²½ìš° ëª¨í˜•ì˜ í‘œí˜„ë ¥ì´ ë‚®ë‹¤ê³  í‘œí˜„í•¨"
  },
  {
    "objectID": "posts/03wk-2.html#d.-ë¡œì§€ìŠ¤í‹±-í•œê³„ê·¹ë³µ-ì•„ì´ë””ì–´ë§Œ",
    "href": "posts/03wk-2.html#d.-ë¡œì§€ìŠ¤í‹±-í•œê³„ê·¹ë³µ-ì•„ì´ë””ì–´ë§Œ",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "D. ë¡œì§€ìŠ¤í‹± í•œê³„ê·¹ë³µ â€“ ì•„ì´ë””ì–´ë§Œ",
    "text": "D. ë¡œì§€ìŠ¤í‹± í•œê³„ê·¹ë³µ â€“ ì•„ì´ë””ì–´ë§Œ\nğŸ—£ï¸ ë°˜ë°˜ ì˜ë¼ì„œ í•˜ë©´ ë  ê²ƒ ê°™ìŒ\n- sigmoidë¥¼ ë„£ê¸° ì „ì˜ ìƒíƒœê°€ ì§ì„ ì´ ì•„ë‹ˆë¼ êº½ì´ëŠ” ì§ì„ ì´ì•¼ í•œë‹¤.\n\na = torch.nn.Sigmoid()\n\n\nfig,ax = plt.subplots(4,2,figsize=(8,8))\nu1 = torch.tensor([-6,-4,-2,0,2,4,6])\nu2 = torch.tensor([6,4,2,0,-2,-4,-6])\nu3 = torch.tensor([-6,-2,2,6,2,-2,-6])\nu4 = torch.tensor([-6,-2,2,6,4,2,0])\nax[0,0].plot(u1,'--o',color='C0',label = r\"$u_1$\")\nax[0,0].legend()\nax[0,1].plot(a(u1),'--o',color='C0',label = r\"$a(u_1)=\\frac{exp(u_1)}{exp(u_1)+1}$\")\nax[0,1].legend()\nax[1,0].plot(u2,'--o',color='C1',label = r\"$u_2$\")\nax[1,0].legend()\nax[1,1].plot(a(u2),'--o',color='C1',label = r\"$a(u_2)=\\frac{exp(u_2)}{exp(u_2)+1}$\")\nax[1,1].legend()\nax[2,0].plot(u3,'--o',color='C2', label = r\"$u_3$\")\nax[2,0].legend()\nax[2,1].plot(a(u3),'--o',color='C2', label = r\"$a(u_3)=\\frac{exp(u_3)}{exp(u_3)+1}$\")\nax[2,1].legend()\nax[3,0].plot(u4,'--o',color='C3', label = r\"$u_4$\")\nax[3,0].legend()\nax[3,1].plot(a(u4),'--o',color='C3', label = r\"$a(u_4)=\\frac{exp(u_4)}{exp(u_4)+1}$\")\nax[3,1].legend()"
  },
  {
    "objectID": "posts/03wk-2.html#footnotes",
    "href": "posts/03wk-2.html#footnotes",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nì›ë˜ëŠ” ì´ë ‡ê²Œ ì¼ì—ˆì§€.. \\(y_i = w_0 + w_1x_i + \\epsilon_i \\quad \\epsilon_i \\sim {\\cal N}(0,\\sigma^2)\\)â†©ï¸"
  },
  {
    "objectID": "posts/05wk-2.html",
    "href": "posts/05wk-2.html",
    "title": "05wk-2: (ì‹ ê²½ë§) â€“ ì‹ ê²½ë§ì˜ í‘œí˜„, GPUì‚¬ìš©ë²•, í™•ë¥ ì ê²½ì‚¬í•˜ê°•ë²•",
    "section": "",
    "text": "ğŸ“˜ Note Format Guide\nThis format serves as a structured guide for organizing lecture content, personal interpretation, experiments, and study-related questions.\nğŸ“ ğŸ—£ï¸ âœï¸ ğŸ”¬ â“"
  },
  {
    "objectID": "posts/05wk-2.html#a.-ë¡œì§€ìŠ¤í‹±",
    "href": "posts/05wk-2.html#a.-ë¡œì§€ìŠ¤í‹±",
    "title": "05wk-2: (ì‹ ê²½ë§) â€“ ì‹ ê²½ë§ì˜ í‘œí˜„, GPUì‚¬ìš©ë²•, í™•ë¥ ì ê²½ì‚¬í•˜ê°•ë²•",
    "section": "A. ë¡œì§€ìŠ¤í‹±",
    "text": "A. ë¡œì§€ìŠ¤í‹±\n\\[\\underset{(n,1)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,1)}{\\boldsymbol u^{(1)}} \\overset{sig}{\\to} \\underset{(n,1)}{\\boldsymbol v^{(1)}} =\\underset{(n,1)}{\\hat{\\bf y}}\\]\nğŸ—£ï¸(\n\nìœ„ë¥¼ ë³´ê³  ë‹¤ìŒê³¼ ê°™ì€ í‘œí˜„ì´ ë– ì˜¬ë¼ì•¼ í•¨\n\në‹¤ìŒì€ ê´€ìŠµì  í‘œí˜„\nu: ë³´í†µ Xì— ì„ í˜•ë³€í™˜\nv: ë³´í†µ uì— ë¹„ì„ í˜•ë³€í™˜(Sigmoid, ReLU)\nv ê²°ê³¼ì— ì„ í˜•ë³€í™˜ì„ í•˜ë©´ ë˜ uë¼ê³  í•˜ê¸° ë•Œë¬¸ì— ìš°ì¸¡ìƒë‹¨ì— ìˆ«ìë¡œ êµ¬ë¶„\n\n\nnet torch.nn.Sequential(\n    torch.nn.Linear(1,1), # l1(X) = u\n    torch.nn.Sigmoid() # sig(u) = v\n)\nyhat = net(X)\n)ğŸ—£ï¸\n- ëª¨ë“  observationê³¼ ê°€ì¤‘ì¹˜ë¥¼ ëª…ì‹œí•œ ë²„ì „\n(í‘œí˜„1)\n\n\në‹¨ì : ë˜‘ê°™ì€ ê·¸ë¦¼ì˜ ë°˜ë³µì´ ë„ˆë¬´ ë§ìŒ\n\n- observation ë°˜ë³µì„ ìƒëµí•œ ë²„ì „ë“¤\n(í‘œí˜„2) ëª¨ë“  \\(i\\)ì— ëŒ€í•˜ì—¬ ì•„ë˜ì˜ ê·¸ë¦¼ì„ ë°˜ë³µí•œë‹¤ê³  í•˜ë©´ (í‘œí˜„1)ê³¼ ê°™ë‹¤.\n\n(í‘œí˜„3) ê·¸ëŸ°ë° (í‘œí˜„2)ì—ì„œ ì•„ë˜ì™€ ê°™ì´ \\(x_i\\), \\(y_i\\) ëŒ€ì‹ ì— ê°„ë‹¨íˆ \\(x\\), \\(y\\)ë¡œ ì“°ëŠ” ê²½ìš°ë„ ë§ìŒ\n\nğŸ—£ï¸ x: vector\n- 1ì„ ìƒëµí•œ ë²„ì „ë“¤\n(í‘œí˜„4) bais=False ëŒ€ì‹ ì— bias=Trueë¥¼ ì£¼ë©´ 1ì„ ìƒëµí•  ìˆ˜ ìˆìŒ\n\n(í‘œí˜„4ì˜ ìˆ˜ì •) \\(\\hat{w}_1\\)ëŒ€ì‹ ì— \\(\\hat{w}\\)ë¥¼ ì“°ëŠ” ê²ƒì´ ë” ìì—°ìŠ¤ëŸ¬ì›€\n\n(í‘œí˜„5) ì„ í˜•ë³€í™˜ì˜ ê²°ê³¼ëŠ” ì•„ë˜ì™€ ê°™ì´ \\(u\\)ë¡œ í‘œí˜„í•˜ê¸°ë„ í•œë‹¤.\n\n\në‹¤ì´ì–´ê·¸ë¨ì€ ê·¸ë¦¬ëŠ” ì‚¬ëŒì˜ ì·¨í–¥ì— ë”°ë¼ ê·¸ë¦¬ëŠ” ë°©ë²•ì´ ì¡°ê¸ˆì”© ë‹¤ë¦…ë‹ˆë‹¤. ì¦‰ êµì¬ë§ˆë‹¤ ë‹¬ë¼ìš”."
  },
  {
    "objectID": "posts/05wk-2.html#b.-ìŠ¤í™ì˜ì—­ì„¤",
    "href": "posts/05wk-2.html#b.-ìŠ¤í™ì˜ì—­ì„¤",
    "title": "05wk-2: (ì‹ ê²½ë§) â€“ ì‹ ê²½ë§ì˜ í‘œí˜„, GPUì‚¬ìš©ë²•, í™•ë¥ ì ê²½ì‚¬í•˜ê°•ë²•",
    "section": "B. ìŠ¤í™ì˜ì—­ì„¤",
    "text": "B. ìŠ¤í™ì˜ì—­ì„¤\n\\[\\underset{(n,1)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,2)}{\\boldsymbol u^{(1)}} \\overset{relu}{\\to} \\underset{(n,2)}{\\boldsymbol v^{(1)}} \\overset{l_2}{\\to} \\underset{(n,1)}{\\boldsymbol u^{(2)}} \\overset{sig}{\\to} \\underset{(n,1)}{\\boldsymbol v^{(2)}} =\\underset{(n,1)}{\\hat{\\bf y}}\\]\nì°¸ê³ : ì½”ë“œë¡œ í‘œí˜„\ntorch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=2),\n    torch.nn.ReLU(),\n    torch.nn.Linear(in_features=2,out_features=1),\n    torch.nn.Sigmoid()\n)\n- ì´í•´ë¥¼ ìœ„í•´ì„œ ì˜ˆì  ì— ë‹¤ë£¨ì—ˆë˜ ì•„ë˜ì˜ ìƒí™©ì„ ê³ ë ¤í•˜ì.\n\n(ê°•ì˜ë…¸íŠ¸ì˜ í‘œí˜„)\n\n(ì¢€ ë” ì¼ë°˜í™”ëœ í‘œí˜„) ìƒí™©ì„ ì¼ë°˜í™”í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.\n\n* Layerì˜ ê°œë…: \\({\\bf X}\\)ì—ì„œ \\(\\hat{\\boldsymbol y}\\)ë¡œ ê°€ëŠ” ê³¼ì •ì€ â€œì„ í˜•ë³€í™˜+ë¹„ì„ í˜•ë³€í™˜â€ì´ ë°˜ë³µë˜ëŠ” êµ¬ì¡°ì´ë‹¤. â€œì„ í˜•ë³€í™˜+ë¹„ì„ í˜•ë³€í™˜â€ì„ í•˜ë‚˜ì˜ ì„¸íŠ¸ë¡œ ë³´ë©´ ì•„ë˜ì™€ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.\n\n\\(\\underset{(n,1)}{\\bf X}  \\overset{l_1}{\\to} \\left( \\underset{(n,2)}{\\boldsymbol u^{(1)}} \\overset{relu}{\\to} \\underset{(n,2)}{\\boldsymbol v^{(1)}} \\right) \\overset{l_2}{\\to} \\left(\\underset{(n,1)}{\\boldsymbol u^{(2)}} \\overset{sig}{\\to} \\underset{(n,1)}{\\boldsymbol v^{(2)}}\\right), \\quad  \\underset{(n,1)}{\\boldsymbol v^{(2)}}=\\underset{(n,1)}{net({\\bf X})}=\\underset{(n,1)}{\\hat{\\bf y}}\\)\n\nğŸ—£ï¸ ì„ í˜•ë³€í™˜ -&gt; ì„ í˜•ë³€í™˜: ì•„ë¬´ëŸ° ì´ì ì´ ì—†ìŒ, ì˜ëª» ì„¤ê³„ (ë°”ë¡œ ê°€ëŠ” ì„ í˜•ë³€í™˜ì´ ìˆê¸° ë•Œë¬¸)\nì´ê²ƒì„ ë‹¤ì´ì–´ê·¸ë¨ìœ¼ë¡œ í‘œí˜„í•œë‹¤ë©´ ì•„ë˜ì™€ ê°™ë‹¤.\n(ì„ í˜•+ë¹„ì„ í˜•ì„ í•˜ë‚˜ì˜ Layerë¡œ ë¬¶ì€ í‘œí˜„)\n\nğŸ—£ï¸ Layer 0(Input Layerë¼ê³ ë„ í•¨)ì€ ì„¸ì§€ X\nLayerë¥¼ ì„¸ëŠ” ë°©ë²•\n\nì œ ë°©ì‹: í•™ìŠµê°€ëŠ¥í•œ íŒŒë¼ë©”í„°ê°€ ëª‡ì¸µìœ¼ë¡œ ìˆëŠ”ì§€â€¦ &lt;â€“ ì´ê²ƒë§Œ ê¸°ì–µí•˜ì„¸ì—¬\nì¼ë¶€ êµì¬ ì„¤ëª…: ì…ë ¥ì¸µì€ ê³„ì‚°í•˜ì§€ ì•ŠìŒ, activation layerëŠ” ê³„ì‚°í•˜ì§€ ì•ŠìŒ. &lt;â€“ ë¬´ì‹œí•˜ì„¸ìš”.. ì´ëŸ¬ë©´ í—·ê°ˆë¦½ë‹ˆë‹¤..\nìœ„ì˜ ì˜ˆì œì˜ ê²½ìš° number of layer = 2 ì´ë‹¤.\n\nHidden Layerì˜ ìˆ˜ë¥¼ ì„¸ëŠ” ë°©ë²•\n\nì œ ë°©ì‹: Hidden Layerì˜ ìˆ˜ = Layerì˜ ìˆ˜ -1 &lt;â€“ ì´ê±¸ ê¸°ì–µí•˜ì„¸ì—¬..\n\nì¼ë¶€ êµì¬ ì„¤ëª…: Layerì˜ ìˆ˜ = Hidden Layerì˜ ìˆ˜ + ì¶œë ¥ì¸µì˜ ìˆ˜ = Hidden Layerì˜ ìˆ˜ + 1 &lt;â€“ ê¸°ì–µí•˜ì§€ ë§ˆì„¸ì—¬\nìœ„ì˜ ì˜ˆì œì˜ ê²½ìš° number of hidden layer = 1 ì´ë‹¤.\n\nğŸ—£ï¸ ì´ ê²½ìš° Hiden Layer: Layer 1 / ì¶œë ¥ì¸µ: Layer 2 (yhat)\n\n\n\n\n\n\nImportant\n\n\n\në¬´ì¡°ê±´ í•™ìŠµê°€ëŠ¥í•œ íŒŒë¼ë©”í„°ê°€ ëª‡ê²¹ìœ¼ë¡œ ìˆëŠ”ì§€ë§Œ íŒë‹¨í•˜ì„¸ìš”. ë”´ê±° ì•„ë¬´ê²ƒë„ ìƒê°í•˜ì§€ë§ˆì„¸ì—¬\n## ì˜ˆì‹œ1 -- 2ì¸µ (íˆë“ ë ˆì´ì–´ëŠ” 1ì¸µ)\ntorch.nn.Sequential(\n    torch.nn.Linear(??,??), ## &lt;-- í•™ìŠµí•´ì•¼í•  ê°€ì¤‘ì¹˜ê°€ ìˆëŠ” ì¸µ\n    torch.nn.ReLU(),\n    torch.nn.Linear(??,??), ## &lt;-- í•™ìŠµí•´ì•¼í•  ê°€ì¤‘ì¹˜ê°€ ìˆëŠ” ì¸µ\n)\n## ì˜ˆì‹œ2 -- 2ì¸µ (íˆë“ ë ˆì´ì–´ëŠ” 1ì¸µ)\ntorch.nn.Sequential(\n    torch.nn.Linear(??,??), ## &lt;-- í•™ìŠµí•´ì•¼í•  ê°€ì¤‘ì¹˜ê°€ ìˆëŠ” ì¸µ\n    torch.nn.ReLU(),\n    torch.nn.Linear(??,??), ## &lt;-- í•™ìŠµí•´ì•¼í•  ê°€ì¤‘ì¹˜ê°€ ìˆëŠ” ì¸µ\n    torch.nn.Sigmoid(),\n)\n## ì˜ˆì‹œ3 -- 1ì¸µ (íˆë“ ë ˆì´ì–´ëŠ” ì—†ìŒ!!)\ntorch.nn.Sequential(\n    torch.nn.Linear(??,??), ## &lt;-- í•™ìŠµí•´ì•¼í•  ê°€ì¤‘ì¹˜ê°€ ìˆëŠ” ì¸µ\n) \n## ì˜ˆì‹œ4 -- 1ì¸µ (íˆë“ ë ˆì´ì–´ëŠ” ì—†ìŒ!!)\ntorch.nn.Sequential(\n    torch.nn.Linear(??,??), ## &lt;-- í•™ìŠµí•´ì•¼í•  ê°€ì¤‘ì¹˜ê°€ ìˆëŠ” ì¸µ\n    torch.nn.Sigmoid()\n) \n## ì˜ˆì‹œ5 -- 3ì¸µ (íˆë“ ë ˆì´ì–´ëŠ” 2ì¸µ)\ntorch.nn.Sequential(\n    torch.nn.Linear(??,??), ## &lt;-- í•™ìŠµí•´ì•¼í•  ê°€ì¤‘ì¹˜ê°€ ìˆëŠ” ì¸µ\n    torch.nn.Sigmoid()\n    torch.nn.Linear(??,??), ## &lt;-- í•™ìŠµí•´ì•¼í•  ê°€ì¤‘ì¹˜ê°€ ìˆëŠ” ì¸µ\n    torch.nn.Sigmoid()\n    torch.nn.Linear(??,??), ## &lt;-- í•™ìŠµí•´ì•¼í•  ê°€ì¤‘ì¹˜ê°€ ìˆëŠ” ì¸µ    \n) \n## ì˜ˆì‹œ6 -- 3ì¸µ (íˆë“ ë ˆì´ì–´ëŠ” 2ì¸µ)\ntorch.nn.Sequential(\n    torch.nn.Linear(??,??), ## &lt;-- í•™ìŠµí•´ì•¼í•  ê°€ì¤‘ì¹˜ê°€ ìˆëŠ” ì¸µ\n    torch.nn.ReLU()\n    torch.nn.Dropout(??)\n    torch.nn.Linear(??,??), ## &lt;-- í•™ìŠµí•´ì•¼í•  ê°€ì¤‘ì¹˜ê°€ ìˆëŠ” ì¸µ\n    torch.nn.ReLU()\n    torch.nn.Dropout(??)\n    torch.nn.Linear(??,??), ## &lt;-- í•™ìŠµí•´ì•¼í•  ê°€ì¤‘ì¹˜ê°€ ìˆëŠ” ì¸µ  \n    torch.nn.Sigmoid()\n) \n\n\n\n\n\n\n\n\nImportant\n\n\n\në¬¸í—Œì— ë”°ë¼ì„œ ë ˆì´ì–´ë¥¼ ì„¸ëŠ” ê°œë…ì´ ì œê°€ ì„¤ëª…í•œ ë°©ì‹ê³¼ ë‹¤ë¥¸ê²½ìš°ê°€ ìˆìŠµë‹ˆë‹¤. ì œê°€ ì„¤ëª…í•œ ë°©ì‹ë³´ë‹¤ 1ì”© ë”í•´ì„œ ì…‰ë‹ˆë‹¤. ì¦‰ ì•„ë˜ì˜ ê²½ìš° ë ˆì´ì–´ë¥¼ 3ê°œë¡œ ì¹´ìš´íŠ¸í•©ë‹ˆë‹¤.\n## ì˜ˆì‹œ1 -- ë¬¸í—Œì— ë”°ë¼ 3ì¸µìœ¼ë¡œ ì„¸ëŠ” ê²½ìš°ê°€ ìˆìŒ (íˆë“ ë ˆì´ì–´ëŠ” 1ì¸µ)\ntorch.nn.Sequential(\n    torch.nn.Linear(??,??), ## &lt;-- í•™ìŠµí•´ì•¼í•  ê°€ì¤‘ì¹˜ê°€ ìˆëŠ” ì¸µ\n    torch.nn.ReLU(),\n    torch.nn.Linear(??,??), ## &lt;-- í•™ìŠµí•´ì•¼í•  ê°€ì¤‘ì¹˜ê°€ ìˆëŠ” ì¸µ\n    torch.nn.Sigmoid()\n)\nì˜ˆë¥¼ ë“¤ì–´ ì—¬ê¸°ì—ì„œëŠ” ìœ„ì˜ ê²½ìš° ë ˆì´ì–´ëŠ” 3ê°œë¼ê³  ì„¤ëª…í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì¹´ìš´íŒ…ì€ â€œë¬´ì‹œâ€í•˜ì„¸ìš”. ì œê°€ ì„¤ëª…í•œ ë°©ì‹ì´ ë§ì•„ìš”. ì´ ë§í¬ ì˜ëª»(?) ë‚˜ì™€ìˆëŠ” ì´ìœ ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.\n- ì§„ì§œ ì˜ˆì „ì— MLPë¥¼ ì†Œê°œí•  ì´ˆì°½ê¸°ì—ì„œëŠ” ìœ„ì˜ ê²½ìš° Layerë¥¼ 3ê°œë¡œ ì…ŒìŒ. [@rosenblatt1962principles]\n- ê·¸ëŸ°ë° ìš”ì¦˜ì€ ê·¸ë ‡ê²Œ ì•ˆì…ˆ.. (ê·¸ë¦¬ê³  ì• ì´ˆì— MLPë¼ëŠ” ìš©ì–´ë„ ì˜ ì•ˆì“°ì£ ..)\nì°¸ê³ ë¡œ íˆë“ ë ˆì´ì–´ì˜ ìˆ˜ëŠ” ì˜ˆì „ë°©ì‹ì´ë‚˜ ì§€ê¸ˆë°©ì‹ì´ë‚˜ ë™ì¼í•˜ê²Œ ì¹´ìš´íŠ¸í•˜ë¯€ë¡œ íˆë“ ë ˆì´ì–´ë§Œ ì„¸ë©´ í˜¼ëˆì´ ì—†ìŠµë‹ˆë‹¤.\n\n\n\nğŸ—£ï¸\n\nìš”ì¦˜ì€ MLPë³´ë‹¤ëŠ” DNN ìš©ì–´ ì‚¬ìš©\nìš”ì¦˜ì€ Dropoutë³´ë‹¤ Batch Normalizationì„ ì‚¬ìš©\n\nBatch Normalizationì€ í•™ìŠµ parameterê°€ ìˆìŒ\nê·¸ëŸ¬ë‚˜ layerë¡œ ì„¸ì§€ëŠ” X\n\nìš”ì¦˜ì€ layerê°€ ëª‡ ì¸µì¸ì§€ êµ³ì´ ë”°ì§€ì§€ëŠ” X\n\n\n* nodeì˜ ê°œë…: \\(u\\to v\\)ë¡œ ê°€ëŠ” ìŒì„ ê°„ë‹¨íˆ ë…¸ë“œë¼ëŠ” ê°œë…ì„ ì´ìš©í•˜ì—¬ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŒ.\n(ë…¸ë“œì˜ ê°œë…ì´ í¬í•¨ëœ ê·¸ë¦¼)\n\nì—¬ê¸°ì—ì„œ nodeì˜ ìˆ«ì = featureì˜ ìˆ«ìì™€ ê°™ì´ ì´í•´í•  ìˆ˜ ìˆë‹¤. ì¦‰ ì•„ë˜ì™€ ê°™ì´ ì´í•´í•  ìˆ˜ ìˆë‹¤.\n(â€œnumber of nodes = number of featuresâ€ë¡œ ì´í•´í•œ ê·¸ë¦¼)\n\n\në‹¤ì´ì–´ê·¸ë¨ì˜ í‘œí˜„ë°©ì‹ì€ êµì¬ë§ˆë‹¤ ë‹¬ë¼ì„œ ëª¨ë“  ì˜ˆì‹œë¥¼ ë‹¬ë‹¬ ì™¸ìš¸ í•„ìš”ëŠ” ì—†ìŠµë‹ˆë‹¤. ë‹¤ë§Œ ì„ì˜ì˜ ë‹¤ì´ì–´ê·¸ë¨ì„ ë³´ê³  ëŒ€ì‘í•˜ëŠ” ë„¤íŠ¸ì›Œí¬ë¥¼ pytorchë¡œ êµ¬í˜„í•˜ëŠ” ëŠ¥ë ¥ì€ ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/05wk-2.html#c.-mnist",
    "href": "posts/05wk-2.html#c.-mnist",
    "title": "05wk-2: (ì‹ ê²½ë§) â€“ ì‹ ê²½ë§ì˜ í‘œí˜„, GPUì‚¬ìš©ë²•, í™•ë¥ ì ê²½ì‚¬í•˜ê°•ë²•",
    "section": "C. MNIST",
    "text": "C. MNIST\n\\[\\underset{(n,784)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,32)}{\\boldsymbol u^{(1)}} \\overset{relu}{\\to} \\underset{(n,32)}{\\boldsymbol v^{(1)}} \\overset{l_2}{\\to} \\underset{(n,1)}{\\boldsymbol u^{(2)}} \\overset{sig}{\\to} \\underset{(n,1)}{\\boldsymbol v^{(2)}}=\\underset{(n,1)}{\\hat{\\boldsymbol y}}\\]\n(ë‹¤ì´ì–´ê·¸ë¨í‘œí˜„)\n\nğŸ—£ï¸ í•™ìŠµ ê°€ëŠ¥í•œ parameter ìˆ˜: 784*32 (\\(X_{(n,784)}@w_{?} = (n,32)\\))\n\nLayer0,1,2 ëŒ€ì‹ ì— Input Layer, Hidden Layer, Output Layerë¡œ í‘œí˜„í•¨\n\n- ìœ„ì˜ ë‹¤ì´ì–´ê·¸ë¨ì— ëŒ€ì‘í•˜ëŠ” ì½”ë“œ\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=28*28*1,out_features=32),\n    torch.nn.ReLU(),\n    torch.nn.Linear(in_features=32,out_features=1),\n    torch.nn.Sigmoid() \n)"
  },
  {
    "objectID": "posts/05wk-2.html#a.-gpu-ì‚¬ìš©ë°©ë²•",
    "href": "posts/05wk-2.html#a.-gpu-ì‚¬ìš©ë°©ë²•",
    "title": "05wk-2: (ì‹ ê²½ë§) â€“ ì‹ ê²½ë§ì˜ í‘œí˜„, GPUì‚¬ìš©ë²•, í™•ë¥ ì ê²½ì‚¬í•˜ê°•ë²•",
    "section": "A. GPU ì‚¬ìš©ë°©ë²•",
    "text": "A. GPU ì‚¬ìš©ë°©ë²•\n- cpu ì—°ì‚°ì´ ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ì— ë°ì´í„° ì €ì¥\n\ntorch.manual_seed(43052)\nx_cpu = torch.tensor([0.0,0.1,0.2]).reshape(-1,1) \ny_cpu = torch.tensor([0.0,0.2,0.4]).reshape(-1,1) \nnet_cpu = torch.nn.Linear(1,1) \n\n\nnet_cpu(x_cpu)\n\ntensor([[-0.8470],\n        [-0.8817],\n        [-0.9164]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\n\nx_cpu\n\ntensor([[0.0000],\n        [0.1000],\n        [0.2000]])\n\n\n- gpu ì—°ì‚°ì´ ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ì— ë°ì´í„° ì €ì¥\n\n!nvidia-smi # before\n\nMon Apr  7 09:48:42 2025       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.230.02             Driver Version: 535.230.02   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  NVIDIA GeForce RTX 3090        Off | 00000000:09:00.0 Off |                  N/A |\n|  0%   29C    P8              27W / 420W |     26MiB / 24576MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|    0   N/A  N/A      1152      G   /usr/lib/xorg/Xorg                            9MiB |\n|    0   N/A  N/A      1471      G   /usr/bin/gnome-shell                          8MiB |\n+---------------------------------------------------------------------------------------+\n\n\nğŸ”¬ ?\n\n!nvidia-smi # before\n\nTue May  6 15:07:19 2025       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  NVIDIA A100-SXM4-80GB          On  | 00000000:47:00.0 Off |                   On |\n| N/A   67C    P0             197W / 275W |                  N/A |     N/A      Default |\n|                                         |                      |              Enabled |\n+-----------------------------------------+----------------------+----------------------+\n\n+---------------------------------------------------------------------------------------+\n| MIG devices:                                                                          |\n+------------------+--------------------------------+-----------+-----------------------+\n| GPU  GI  CI  MIG |                   Memory-Usage |        Vol|      Shared           |\n|      ID  ID  Dev |                     BAR1-Usage | SM     Unc| CE ENC DEC OFA JPG    |\n|                  |                                |        ECC|                       |\n|==================+================================+===========+=======================|\n|  0    0   0   0  |           49841MiB / 81050MiB  | 98      0 |  7   0    5    1    1 |\n|                  |              15MiB / 131072MiB |           |                       |\n+------------------+--------------------------------+-----------+-----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n+---------------------------------------------------------------------------------------+\n\n\nğŸ—£ï¸(\n\nx_cpu\n\ntensor([[0.0000],\n        [0.1000],\n        [0.2000]])\n\n\n\nx_cpu.to(\"cuda:0\")\n\ntensor([[0.0000],\n        [0.1000],\n        [0.2000]], device='cuda:0')\n\n\n\ndevice=â€˜cuda:0â€™\n\n)ğŸ—£ï¸\nğŸ—£ï¸ :0 =&gt; GPU ID (ì—¬ëŸ¬ê°œì¸ ê²½ìš°)\n\ntorch.manual_seed(43052)\nx_gpu = x_cpu.to(\"cuda:0\")\ny_gpu = y_cpu.to(\"cuda:0\")\nnet_gpu = torch.nn.Linear(1,1).to(\"cuda:0\") \n\nğŸ—£ï¸ ì¼ë°˜ì ìœ¼ë¡œëŠ” ë©”ëª¨ë¦¬ì— ì €ì¥ë˜ì§€ë§Œ, to(â€œcuda:0â€)ë¥¼ í•˜ë©´ GPU ë©”ëª¨ë¦¬ì— ì €ì¥ë¨\n\n!nvidia-smi\n\nMon Apr  7 09:48:43 2025       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.230.02             Driver Version: 535.230.02   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  NVIDIA GeForce RTX 3090        Off | 00000000:09:00.0 Off |                  N/A |\n|  0%   34C    P2              65W / 420W |    287MiB / 24576MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|    0   N/A  N/A      1152      G   /usr/lib/xorg/Xorg                            9MiB |\n|    0   N/A  N/A      1471      G   /usr/bin/gnome-shell                          8MiB |\n|    0   N/A  N/A    140211      C   ...b3/anaconda3/envs/dl2025/bin/python      256MiB |\n+---------------------------------------------------------------------------------------+\n\n\nğŸ”¬ ?\n\n!nvidia-smi\n\nTue May  6 15:11:18 2025       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  NVIDIA A100-SXM4-80GB          On  | 00000000:47:00.0 Off |                   On |\n| N/A   68C    P0             186W / 275W |                  N/A |     N/A      Default |\n|                                         |                      |              Enabled |\n+-----------------------------------------+----------------------+----------------------+\n\n+---------------------------------------------------------------------------------------+\n| MIG devices:                                                                          |\n+------------------+--------------------------------+-----------+-----------------------+\n| GPU  GI  CI  MIG |                   Memory-Usage |        Vol|      Shared           |\n|      ID  ID  Dev |                     BAR1-Usage | SM     Unc| CE ENC DEC OFA JPG    |\n|                  |                                |        ECC|                       |\n|==================+================================+===========+=======================|\n|  0    0   0   0  |           50298MiB / 81050MiB  | 98      0 |  7   0    5    1    1 |\n|                  |              17MiB / 131072MiB |           |                       |\n+------------------+--------------------------------+-----------+-----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n+---------------------------------------------------------------------------------------+\n\n\n\nGPUì— ë©”ëª¨ë¦¬ë¥¼ ì˜¬ë¦¬ë©´ GPUë©”ëª¨ë¦¬ê°€ ì ìœ ëœë‹¤! (26MiB -&gt; 287MiB)\n\n- cpu í˜¹ì€ gpu ì—°ì‚°ì´ ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ì— ì €ì¥ëœ ê°’ë“¤ì„ í™•ì¸\n\nx_cpu, y_cpu, net_cpu.weight, net_cpu.bias\n\n(tensor([[0.0000],\n         [0.1000],\n         [0.2000]]),\n tensor([[0.0000],\n         [0.2000],\n         [0.4000]]),\n Parameter containing:\n tensor([[-0.3467]], requires_grad=True),\n Parameter containing:\n tensor([-0.8470], requires_grad=True))\n\n\n\nx_gpu, y_gpu, net_gpu.weight, net_gpu.bias\n\n(tensor([[0.0000],\n         [0.1000],\n         [0.2000]], device='cuda:0'),\n tensor([[0.0000],\n         [0.2000],\n         [0.4000]], device='cuda:0'),\n Parameter containing:\n tensor([[-0.3467]], device='cuda:0', requires_grad=True),\n Parameter containing:\n tensor([-0.8470], device='cuda:0', requires_grad=True))\n\n\n- gpuëŠ” gpuë¼ë¦¬ ì—°ì‚°ê°€ëŠ¥í•˜ê³  cpuëŠ” cpuë¼ë¦¬ ì—°ì‚°ê°€ëŠ¥í•¨\n(ì˜ˆì‹œ1)\n\nnet_cpu(x_cpu) \n\ntensor([[-0.8470],\n        [-0.8817],\n        [-0.9164]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\n(ì˜ˆì‹œ2)\n\nnet_gpu(x_gpu) \n\ntensor([[-0.8470],\n        [-0.8817],\n        [-0.9164]], device='cuda:0', grad_fn=&lt;AddmmBackward0&gt;)\n\n\n(ì˜ˆì‹œ3)\n\nnet_cpu(x_gpu) \n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[15], line 1\n----&gt; 1 net_cpu(x_gpu) \n\nFile ~/anaconda3/envs/dl2025/lib/python3.9/site-packages/torch/nn/modules/module.py:1739, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1737     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1738 else:\n-&gt; 1739     return self._call_impl(*args, **kwargs)\n\nFile ~/anaconda3/envs/dl2025/lib/python3.9/site-packages/torch/nn/modules/module.py:1750, in Module._call_impl(self, *args, **kwargs)\n   1745 # If we don't have any hooks, we want to skip the rest of the logic in\n   1746 # this function, and just call forward.\n   1747 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1748         or _global_backward_pre_hooks or _global_backward_hooks\n   1749         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1750     return forward_call(*args, **kwargs)\n   1752 result = None\n   1753 called_always_called_hooks = set()\n\nFile ~/anaconda3/envs/dl2025/lib/python3.9/site-packages/torch/nn/modules/linear.py:125, in Linear.forward(self, input)\n    124 def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 125     return F.linear(input, self.weight, self.bias)\n\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n\n\n\n(ì˜ˆì‹œ4)\n\nnet_gpu(x_cpu)\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[16], line 1\n----&gt; 1 net_gpu(x_cpu)\n\nFile ~/anaconda3/envs/dl2025/lib/python3.9/site-packages/torch/nn/modules/module.py:1739, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1737     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1738 else:\n-&gt; 1739     return self._call_impl(*args, **kwargs)\n\nFile ~/anaconda3/envs/dl2025/lib/python3.9/site-packages/torch/nn/modules/module.py:1750, in Module._call_impl(self, *args, **kwargs)\n   1745 # If we don't have any hooks, we want to skip the rest of the logic in\n   1746 # this function, and just call forward.\n   1747 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1748         or _global_backward_pre_hooks or _global_backward_hooks\n   1749         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1750     return forward_call(*args, **kwargs)\n   1752 result = None\n   1753 called_always_called_hooks = set()\n\nFile ~/anaconda3/envs/dl2025/lib/python3.9/site-packages/torch/nn/modules/linear.py:125, in Linear.forward(self, input)\n    124 def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 125     return F.linear(input, self.weight, self.bias)\n\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n\n\n\n(ì˜ˆì‹œ5)\n\ntorch.mean((y_cpu-net_cpu(x_cpu))**2)\n\ntensor(1.2068, grad_fn=&lt;MeanBackward0&gt;)\n\n\n(ì˜ˆì‹œ6)\n\ntorch.mean((y_gpu-net_gpu(x_gpu))**2)\n\ntensor(1.2068, device='cuda:0', grad_fn=&lt;MeanBackward0&gt;)\n\n\n(ì˜ˆì‹œ7)\n\ntorch.mean((y_gpu-net_cpu(x_cpu))**2)\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[19], line 1\n----&gt; 1 torch.mean((y_gpu-net_cpu(x_cpu))**2)\n\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\n\n\n\n(ì˜ˆì‹œ8)\n\ntorch.mean((y_cpu-net_gpu(x_gpu))**2)\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[20], line 1\n----&gt; 1 torch.mean((y_cpu-net_gpu(x_gpu))**2)\n\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
  },
  {
    "objectID": "posts/05wk-2.html#b.-ì‹œê°„ì¸¡ì •-ì˜ˆë¹„í•™ìŠµ",
    "href": "posts/05wk-2.html#b.-ì‹œê°„ì¸¡ì •-ì˜ˆë¹„í•™ìŠµ",
    "title": "05wk-2: (ì‹ ê²½ë§) â€“ ì‹ ê²½ë§ì˜ í‘œí˜„, GPUì‚¬ìš©ë²•, í™•ë¥ ì ê²½ì‚¬í•˜ê°•ë²•",
    "section": "B. ì‹œê°„ì¸¡ì • (ì˜ˆë¹„í•™ìŠµ)",
    "text": "B. ì‹œê°„ì¸¡ì • (ì˜ˆë¹„í•™ìŠµ)\n\nimport time \n\n\ntime.time()\n\n1746512391.3237932\n\n\nğŸ—£ï¸ ë­”ì§€ëŠ” ëª¨ë¥´ê² ì§€ë§Œ ì°¨ì´ëŠ” ì•Œ ìˆ˜ ìˆìŒ\n\nt1 = time.time()\n\n\nt2 = time.time()\n\n\nt2-t1\n\n4.3513031005859375"
  },
  {
    "objectID": "posts/05wk-2.html#c.-cpu-vs-gpu-500-nodes",
    "href": "posts/05wk-2.html#c.-cpu-vs-gpu-500-nodes",
    "title": "05wk-2: (ì‹ ê²½ë§) â€“ ì‹ ê²½ë§ì˜ í‘œí˜„, GPUì‚¬ìš©ë²•, í™•ë¥ ì ê²½ì‚¬í•˜ê°•ë²•",
    "section": "C. CPU vs GPU (500 nodes)",
    "text": "C. CPU vs GPU (500 nodes)\n- CPU (500 nodes)\n\ntorch.manual_seed(5) \nx=torch.linspace(0,1,100).reshape(-1,1)\ny=torch.randn(100).reshape(-1,1)*0.01\n#---#\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,500),\n    torch.nn.ReLU(),\n    torch.nn.Linear(500,1)\n)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n#---#\nt1 = time.time()\nfor epoc in range(1000):\n    # 1 \n    yhat = net(x)\n    # 2 \n    loss = loss_fn(yhat,y)\n    # 3 \n    loss.backward()\n    # 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2-t1\n\n0.36923766136169434\n\n\nğŸ”¬\n\ntorch.manual_seed(5) \nx=torch.linspace(0,1,100).reshape(-1,1)\ny=torch.randn(100).reshape(-1,1)*0.01\n#---#\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,500),\n    torch.nn.ReLU(),\n    torch.nn.Linear(500,1)\n)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n#---#\nt1 = time.time()\nfor epoc in range(1000):\n    # 1 \n    yhat = net(x)\n    # 2 \n    loss = loss_fn(yhat,y)\n    # 3 \n    loss.backward()\n    # 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2-t1\n\n0.8697936534881592\n\n\n- GPU (500 nodes)\n\ntorch.manual_seed(5) \nx=torch.linspace(0,1,100).reshape(-1,1).to(\"cuda:0\")\ny=(torch.randn(100).reshape(-1,1)*0.01).to(\"cuda:0\")\n#---#\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,500),\n    torch.nn.ReLU(),\n    torch.nn.Linear(500,1)\n).to(\"cuda:0\")\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n#---#\nt1 = time.time()\nfor epoc in range(1000):\n    # 1 \n    yhat = net(x)\n    # 2 \n    loss = loss_fn(yhat,y)\n    # 3 \n    loss.backward()\n    # 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2-t1\n\n0.5803208351135254\n\n\nğŸ”¬\n\ntorch.manual_seed(5) \nx=torch.linspace(0,1,100).reshape(-1,1).to(\"cuda:0\")\ny=(torch.randn(100).reshape(-1,1)*0.01).to(\"cuda:0\")\n#---#\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,500),\n    torch.nn.ReLU(),\n    torch.nn.Linear(500,1)\n).to(\"cuda:0\")\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n#---#\nt1 = time.time()\nfor epoc in range(1000):\n    # 1 \n    yhat = net(x)\n    # 2 \n    loss = loss_fn(yhat,y)\n    # 3 \n    loss.backward()\n    # 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2-t1\n\n0.8882694244384766\n\n\n\nCPUê°€ ë” ë¹ ë¥´ë‹¤??"
  },
  {
    "objectID": "posts/05wk-2.html#d.-cpu-vs-gpu-200000-nodes",
    "href": "posts/05wk-2.html#d.-cpu-vs-gpu-200000-nodes",
    "title": "05wk-2: (ì‹ ê²½ë§) â€“ ì‹ ê²½ë§ì˜ í‘œí˜„, GPUì‚¬ìš©ë²•, í™•ë¥ ì ê²½ì‚¬í•˜ê°•ë²•",
    "section": "D. CPU vs GPU (200,000 nodes)",
    "text": "D. CPU vs GPU (200,000 nodes)\n- CPU (200,000)\n\ntorch.manual_seed(5) \nx=torch.linspace(0,1,100).reshape(-1,1)\ny=torch.randn(100).reshape(-1,1)*0.01\n#---#\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,200000),\n    torch.nn.ReLU(),\n    torch.nn.Linear(200000,1)\n)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n#---#\nt1 = time.time()\nfor epoc in range(1000):\n    # 1 \n    yhat = net(x)\n    # 2 \n    loss = loss_fn(yhat,y)\n    # 3 \n    loss.backward()\n    # 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2-t1\n\n84.05620455741882\n\n\nğŸ”¬\n\ntorch.manual_seed(5) \nx=torch.linspace(0,1,100).reshape(-1,1)\ny=torch.randn(100).reshape(-1,1)*0.01\n#---#\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,200000),\n    torch.nn.ReLU(),\n    torch.nn.Linear(200000,1)\n)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n#---#\nt1 = time.time()\nfor epoc in range(1000):\n    # 1 \n    yhat = net(x)\n    # 2 \n    loss = loss_fn(yhat,y)\n    # 3 \n    loss.backward()\n    # 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2-t1\n\n83.98482537269592\n\n\n- GPU (204,800)\n\ntorch.manual_seed(5) \nx=torch.linspace(0,1,100).reshape(-1,1).to(\"cuda:0\")\ny=(torch.randn(100).reshape(-1,1)*0.01).to(\"cuda:0\")\n#---#\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,200000),\n    torch.nn.ReLU(),\n    torch.nn.Linear(200000,1)\n).to(\"cuda:0\")\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n#---#\nt1 = time.time()\nfor epoc in range(1000):\n    # 1 \n    yhat = net(x)\n    # 2 \n    loss = loss_fn(yhat,y)\n    # 3 \n    loss.backward()\n    # 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2-t1\n\n1.373826026916504\n\n\nğŸ”¬\n\ntorch.manual_seed(5) \nx=torch.linspace(0,1,100).reshape(-1,1).to(\"cuda:0\")\ny=(torch.randn(100).reshape(-1,1)*0.01).to(\"cuda:0\")\n#---#\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,200000),\n    torch.nn.ReLU(),\n    torch.nn.Linear(200000,1)\n).to(\"cuda:0\")\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n#---#\nt1 = time.time()\nfor epoc in range(1000):\n    # 1 \n    yhat = net(x)\n    # 2 \n    loss = loss_fn(yhat,y)\n    # 3 \n    loss.backward()\n    # 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2-t1\n\n2.6831233501434326\n\n\n\nğŸ—£ï¸\n\ní•­ìƒ GPUê°€ ë¹ ë¥¸ ê²ƒì€ ì•„ë‹˜ (nodeê°€ ì»¤ì§€ë©´ GPUê°€ ìœ ë¦¬)\nCPUëŠ” ì½”ì–´ê°€ ë§ì•„ì•¼ 60ì—¬ê°œ, GPUëŠ” ì½”ì–´ê°€ ë§Œ ê°œ ë‹¨ìœ„\n\nì™œ ì´ëŸ° ì°¨ì´ê°€ ë‚˜ëŠ”ê°€?\nì—°ì‚°ì„ í•˜ëŠ” ì£¼ì²´ëŠ” ì½”ì–´ì¸ë° CPUëŠ” ìˆ˜ëŠ” ì ì§€ë§Œ ì¼ì„ ì˜í•˜ëŠ” ì½”ì–´ë“¤ì„ ê°€ì§€ê³  ìˆê³  GPUëŠ” ì¼ì€ ëª»í•˜ì§€ë§Œ ë‹¤ìˆ˜ì˜ ì½”ì–´ë¥¼ ê°€ì§€ê³  ìˆê¸° ë•Œë¬¸"
  },
  {
    "objectID": "posts/05wk-2.html#e.-ì£¼ì˜ì ",
    "href": "posts/05wk-2.html#e.-ì£¼ì˜ì ",
    "title": "05wk-2: (ì‹ ê²½ë§) â€“ ì‹ ê²½ë§ì˜ í‘œí˜„, GPUì‚¬ìš©ë²•, í™•ë¥ ì ê²½ì‚¬í•˜ê°•ë²•",
    "section": "E. ì£¼ì˜ì ",
    "text": "E. ì£¼ì˜ì \n- tensor ì¼ ê²½ìš°\n\nx = torch.tensor([1,2,3])\nx.to(\"cuda:0\"), x\n\n(tensor([1, 2, 3], device='cuda:0'), tensor([1, 2, 3]))\n\n\n- netì¼ ê²½ìš°\n\nnet = torch.nn.Linear(1,1).to(\"cuda:0\")\nnet.weight, net.bias\n\n(Parameter containing:\n tensor([[-0.0084]], device='cuda:0', requires_grad=True),\n Parameter containing:\n tensor([-0.6216], device='cuda:0', requires_grad=True))\n\n\nğŸ—£ï¸(\n\nnetì€ ê°’ ìì²´ê°€ í†µì§¸ë¡œ cudaë¡œ ê°\n\n\nnet_cpu = torch.nn.Linear(1,1)\nnet_cpu\n\nLinear(in_features=1, out_features=1, bias=True)\n\n\n\nnet_gpu = net_cpu.to(\"cuda:0\") # ì´ê±°ë¥¼ ì‹¤í–‰í•˜ëŠ” ìˆœê°„ cudaë¡œ ê°€ê¸° ë•Œë¬¸ì—\n\n\nnet_cpu.weight # ë” ì´ìƒ cpuë¡œ ë¶€ë¥¼ ìˆ˜ ì—†ìŒ\n\nParameter containing:\ntensor([[0.1766]], device='cuda:0', requires_grad=True)\n\n\n\në¹„êµ\n\n\nx_cpu = torch.tensor([1,2,3])\nx_cpu\n\ntensor([1, 2, 3])\n\n\n\nx_gpu = x_cpu.to(\"cuda:0\")\n\n\nx_cpu\n\ntensor([1, 2, 3])\n\n\n\nx_gpu\n\ntensor([1, 2, 3], device='cuda:0')\n\n\n)ğŸ—£ï¸"
  },
  {
    "objectID": "posts/05wk-2.html#a.-ì˜ë¬¸-ì¢€-ì´ìƒí•˜ì§€-ì•Šì•„ìš”",
    "href": "posts/05wk-2.html#a.-ì˜ë¬¸-ì¢€-ì´ìƒí•˜ì§€-ì•Šì•„ìš”",
    "title": "05wk-2: (ì‹ ê²½ë§) â€“ ì‹ ê²½ë§ì˜ í‘œí˜„, GPUì‚¬ìš©ë²•, í™•ë¥ ì ê²½ì‚¬í•˜ê°•ë²•",
    "section": "A. ì˜ë¬¸: ì¢€ ì´ìƒí•˜ì§€ ì•Šì•„ìš”?",
    "text": "A. ì˜ë¬¸: ì¢€ ì´ìƒí•˜ì§€ ì•Šì•„ìš”?\n- êµ­ë¯¼ìƒì‹: GPU ë¹„ì‹¸ìš”.. https://bbs.ruliweb.com/community/board/300143/read/61066881\n\nGPU ë©”ëª¨ë¦¬ ë§ì•„ë´ì•¼ 24GB, ê·¸ë˜ë„ ë¹„ì‹¸ìš”.. http://shop.danawa.com/virtualestimate/?controller=estimateMain&methods=index&marketPlaceSeq=16\nGPU ë©”ëª¨ë¦¬ê°€ 80GBì¼ ê²½ìš° ê°€ê²©: https://prod.danawa.com/info/?pcode=21458333\n\n- ìš°ë¦¬ê°€ ë¶„ì„í•˜ëŠ” ë°ì´í„°\n\nx = torch.linspace(-10,10,100000).reshape(-1,1)\neps = torch.randn(100000).reshape(-1,1)\ny = x*2 + eps \n\n\nplt.plot(x,y,'.',alpha=0.05)\nplt.plot(x,2*x,'--')\n\n\n\n\n\n\n\n\n\nlen(x)\n\n100000\n\n\n- ë°ì´í„°ì˜ í¬ê¸°ê°€ ì»¤ì§€ëŠ” ìˆœê°„ x.to(\"cuda:0\"), y.to(\"cuda:0\") ì“°ë©´ ë‚œë¦¬ë‚˜ê² ëŠ”ê±¸? \\(\\to\\) ì´ëŸ°ì‹ì´ë©´ GPUë¥¼ ì´ìš©í•˜ì—¬ ì•„ë¬´ëŸ° ë¶„ì„ë„ ëª»í• ê²ƒ ê°™ì€ë°?? ë­”ê°€ ì¢€ ì´ìƒí•œë°??\n- ì•„ì´ë””ì–´: ë°ì´í„°ë¥¼ 100ê°œì¤‘ì— 1ê°œ ê¼´ë¡œë§Œ ì“°ë©´ ì–´ë–¨ê¹Œ?\n\nx[::2].shape\n\ntorch.Size([50000, 1])\n\n\n\nx[::100].shape\n\ntorch.Size([1000, 1])\n\n\n\nplt.plot(x[::100],y[::100],'o',alpha=0.05)\nplt.plot(x,2*x,'--')\n\n\n\n\n\n\n\n\n\nëŒ€ì¶© ì´ê±°ë§Œ ê°€ì§€ê³  ì í•©í•´ë„ ì¶©ë¶„íˆ ì •í™•í• ê²ƒ ê°™ì€ë°?"
  },
  {
    "objectID": "posts/05wk-2.html#b.-xy-ë°ì´í„°ë¥¼-êµ³ì´-ëª¨ë‘-gpuì—-ë„˜ê²¨ì•¼-í•˜ëŠ”ê°€",
    "href": "posts/05wk-2.html#b.-xy-ë°ì´í„°ë¥¼-êµ³ì´-ëª¨ë‘-gpuì—-ë„˜ê²¨ì•¼-í•˜ëŠ”ê°€",
    "title": "05wk-2: (ì‹ ê²½ë§) â€“ ì‹ ê²½ë§ì˜ í‘œí˜„, GPUì‚¬ìš©ë²•, í™•ë¥ ì ê²½ì‚¬í•˜ê°•ë²•",
    "section": "B. X,y ë°ì´í„°ë¥¼ êµ³ì´ ëª¨ë‘ GPUì— ë„˜ê²¨ì•¼ í•˜ëŠ”ê°€?",
    "text": "B. X,y ë°ì´í„°ë¥¼ êµ³ì´ ëª¨ë‘ GPUì— ë„˜ê²¨ì•¼ í•˜ëŠ”ê°€?\n- ë°ì´í„°ì…‹ì„ ì§í™€ë¡œ ë‚˜ëˆ„ì–´ì„œ ë²ˆê°ˆì•„ê°€ë©´ì„œ GPUì— ì˜¬ë ¸ë‹¤ ë‚´ë ¸ë‹¤í•˜ë©´ ì•ˆë˜ë‚˜?\n- ì•„ë˜ì˜ ì•Œê³ ë¦¬ì¦˜ì„ ìƒê°í•´ë³´ì.\n\në°ì´í„°ë¥¼ ë°˜ìœ¼ë¡œ ë‚˜ëˆˆë‹¤.\nì§ìˆ˜obsì˜ x,y ê·¸ë¦¬ê³  netì˜ ëª¨ë“  íŒŒë¼ë©”í„°ë¥¼ GPUì— ì˜¬ë¦°ë‹¤.\nyhat, loss, grad, update ìˆ˜í–‰\nì§ìˆ˜obsì˜ x,yë¥¼ GPUë©”ëª¨ë¦¬ì—ì„œ ë‚´ë¦°ë‹¤. ê·¸ë¦¬ê³  í™€ìˆ˜obsì˜ x,yë¥¼ GPUë©”ëª¨ë¦¬ì— ì˜¬ë¦°ë‹¤.\nyhat, loss, grad, update ìˆ˜í–‰\ní™€ìˆ˜obsì˜ x,yë¥¼ GPUë©”ëª¨ë¦¬ì—ì„œ ë‚´ë¦°ë‹¤. ê·¸ë¦¬ê³  ì§ìˆ˜obsì˜ x,yë¥¼ GPUë©”ëª¨ë¦¬ì— ì˜¬ë¦°ë‹¤.\në°˜ë³µ\n\n\nì´ëŸ¬ë©´ ë˜ëŠ”ê±°ì•„ë‹ˆì•¼???? â€”&gt; ë§ì•„ìš”\n\nğŸ—£ï¸ =&gt; í™•ë¥ ì ê²½ì‚¬í•˜ê°•ë²•"
  },
  {
    "objectID": "posts/05wk-2.html#c.-ê²½ì‚¬í•˜ê°•ë²•-í™•ë¥ ì ê²½ì‚¬í•˜ê°•ë²•-ë¯¸ë‹ˆë°°ì¹˜-ê²½ì‚¬í•˜ê°•ë²•",
    "href": "posts/05wk-2.html#c.-ê²½ì‚¬í•˜ê°•ë²•-í™•ë¥ ì ê²½ì‚¬í•˜ê°•ë²•-ë¯¸ë‹ˆë°°ì¹˜-ê²½ì‚¬í•˜ê°•ë²•",
    "title": "05wk-2: (ì‹ ê²½ë§) â€“ ì‹ ê²½ë§ì˜ í‘œí˜„, GPUì‚¬ìš©ë²•, í™•ë¥ ì ê²½ì‚¬í•˜ê°•ë²•",
    "section": "C. ê²½ì‚¬í•˜ê°•ë²•, í™•ë¥ ì ê²½ì‚¬í•˜ê°•ë²•, ë¯¸ë‹ˆë°°ì¹˜ ê²½ì‚¬í•˜ê°•ë²•",
    "text": "C. ê²½ì‚¬í•˜ê°•ë²•, í™•ë¥ ì ê²½ì‚¬í•˜ê°•ë²•, ë¯¸ë‹ˆë°°ì¹˜ ê²½ì‚¬í•˜ê°•ë²•\nğŸ—£ï¸ ì„ì˜ì˜ ë¬¶ìŒ: ë¯¸ë‹ˆë°°ì¹˜\n10ê°œì˜ ìƒ˜í”Œì´ ìˆë‹¤ê³  ê°€ì •. \\(\\{(x_i,y_i)\\}_{i=1}^{10}\\)\n# ver1 â€“ ëª¨ë“  ìƒ˜í”Œì„ ì´ìš©í•˜ì—¬ slope ê³„ì‚°\n(epoch 1) \\(loss=\\sum_{i=1}^{10}(y_i-\\hat{w}_0-\\hat{w}_1x_i)^2 \\to slope  \\to update\\)\n(epoch 2) \\(loss=\\sum_{i=1}^{10}(y_i-\\hat{w}_0-\\hat{w}_1x_i)^2 \\to slope  \\to update\\)\nğŸ—£ï¸ lossê°€ SSE\nâ€¦\n\nìš°ë¦¬ê°€ í•­ìƒ ì´ë ‡ê²Œ í–ˆì£ !\n\n# ver2 â€“ í•˜ë‚˜ì˜ ìƒ˜í”Œë§Œì„ ì´ìš©í•˜ì—¬ slope ê³„ì‚°\n(epoch 1)\n\n\\(loss=(y_1-\\hat{w}_0-\\hat{w}_1x_1)^2 \\to slope \\to update\\)\n\\(loss=(y_2-\\hat{w}_0-\\hat{w}_1x_2)^2 \\to slope \\to update\\)\nâ€¦\n\\(loss=(y_{10}-\\hat{w}_0-\\hat{w}_1x_{10})^2  \\to  slope  \\to  update\\)\n\n(epoch 2)\n\n\\(loss=(y_1-\\hat{w}_0-\\hat{w}_1x_1)^2  \\to slope  \\to  update\\)\n\\(loss=(y_2-\\hat{w}_0-\\hat{w}_1x_2)^2  \\to slope  \\to  update\\)\nâ€¦\n\\(loss=(y_{10}-\\hat{w}_0-\\hat{w}_1x_{10})^2  \\to  slope  \\to  update\\)\n\nğŸ—£ï¸ epoch 2 í›„ 20ë²ˆ update\nâ€¦\n# ver3 â€“ \\(m (\\leq n)\\) ê°œì˜ ìƒ˜í”Œì„ ì´ìš©í•˜ì—¬ slope ê³„ì‚°\nğŸ—£ï¸ ë¯¸ë‹ˆë°°ì¹˜ ì‚¬ì´ì¦ˆ = 3\n\\(m=3\\)ì´ë¼ê³  í•˜ì.\n(epoch 1)\n\n\\(loss=\\sum_{i=1}^{3}(y_i-\\hat{w}_0-\\hat{w}_1x_i)^2  \\to  slope  \\to  update\\)\n\\(loss=\\sum_{i=4}^{6}(y_i-\\hat{w}_0-\\hat{w}_1x_i)^2  \\to  slope  \\to  update\\)\n\\(loss=\\sum_{i=7}^{9}(y_i-\\hat{w}_0-\\hat{w}_1x_i)^2  \\to  slope  \\to  update\\)\n\\(loss=(y_{10}-\\hat{w}_0-\\hat{w}_1x_{10})^2  \\to  slope  \\to  update\\)\n\n(epoch 2)\n\n\\(loss=\\sum_{i=1}^{3}(y_i-\\hat{w}_0-\\hat{w}_1x_i)^2  \\to  slope  \\to  update\\)\n\\(loss=\\sum_{i=4}^{6}(y_i-\\hat{w}_0-\\hat{w}_1x_i)^2  \\to  slope  \\to  update\\)\n\\(loss=\\sum_{i=7}^{9}(y_i-\\hat{w}_0-\\hat{w}_1x_i)^2  \\to  slope  \\to  update\\)\n\\(loss=(y_{10}-\\hat{w}_0-\\hat{w}_1x_{10})^2  \\to  slope  \\to  update\\)\n\nğŸ—£ï¸ 10ì€ ë‹¨ë…ìœ¼ë¡œ update\nâ€¦"
  },
  {
    "objectID": "posts/05wk-2.html#d.-ìš©ì–´ì˜-ì •ë¦¬",
    "href": "posts/05wk-2.html#d.-ìš©ì–´ì˜-ì •ë¦¬",
    "title": "05wk-2: (ì‹ ê²½ë§) â€“ ì‹ ê²½ë§ì˜ í‘œí˜„, GPUì‚¬ìš©ë²•, í™•ë¥ ì ê²½ì‚¬í•˜ê°•ë²•",
    "section": "D. ìš©ì–´ì˜ ì •ë¦¬",
    "text": "D. ìš©ì–´ì˜ ì •ë¦¬\nì˜›ë‚ \n- ver1(ëª¨ë“ ): gradient descent, batch gradient descent\n- ver2(í•˜ë‚˜ë§Œ): stochastic gradient descent\n- ver3(ëª‡ê°œë§Œ): mini-batch gradient descent, mini-batch stochastic gradient descent\nğŸ—£ï¸ stochastic: ì°¨ë¡€ëŒ€ë¡œ í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ëœë¤ìœ¼ë¡œ ê³„ì† ë½‘ëŠ” ë°©ì‹ë„ ìˆì–´ì„œ ì´ê²ƒì— ê¸°ì›ì„ ë‘  / batch: ì „ì²´ data\nğŸ—£ï¸ ver3: ìœ„ ê·¸ë¦¼ì—ì„œ y10ì˜ ê²½ìš° ê°€ì¤‘ì¹˜ê°€ ê³„ì† ìˆëŠ” ê²ƒì´ ì‹«ì–´ì„œ ëœë¤ìœ¼ë¡œ 3ê°œì”© ë½‘ì„ ìˆ˜ë„ ìˆìŒ -&gt; stochastic\nìš”ì¦˜\n- ver1(ëª¨ë“ ): gradient descent\n- ver2(í•˜ë‚˜ë§Œ): stochastic gradient descent with batch size = 1\n- ver3(ëª‡ê°œë§Œ): stochastic gradient descent - https://www.deeplearningbook.org/contents/optimization.html, ì•Œê³ ë¦¬ì¦˜ 8-1 ì°¸ê³ .\nğŸ—£ï¸ ver3ì„ ì œì¼ ë§ì´ ì”€"
  },
  {
    "objectID": "posts/05wk-2.html#e.-datasetds-dataloaderdl",
    "href": "posts/05wk-2.html#e.-datasetds-dataloaderdl",
    "title": "05wk-2: (ì‹ ê²½ë§) â€“ ì‹ ê²½ë§ì˜ í‘œí˜„, GPUì‚¬ìš©ë²•, í™•ë¥ ì ê²½ì‚¬í•˜ê°•ë²•",
    "section": "E. Dataset(ds), DataLoader(dl)",
    "text": "E. Dataset(ds), DataLoader(dl)\n\nì·¨ì§€ëŠ” ì•Œê² ìœ¼ë‚˜, Cì˜ ê³¼ì •ì„ ì‹¤ì œ êµ¬í˜„í•˜ë ¤ë©´ ì§„ì§œ ì–´ë ¤ì›€.. (ì…ì½”ë”©ê³¼ ì†ì½”ë”©ì˜ ì°¨ì´) â€“&gt; ì´ê±¸ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ íŒŒì´í† ì¹˜ì—ì„œëŠ” DataLoaderë¼ëŠ” ì˜¤ë¸Œì íŠ¸ë¥¼ ì¤€ë¹„í–ˆìŒ!\n\n- ë°ì´í„°\n\nx=torch.tensor(range(10))\ny=torch.tensor([1.0]*5+[0.0]*5).reshape(-1,1)\n\n\nx, y\n\n(tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n tensor([[1.],\n         [1.],\n         [1.],\n         [1.],\n         [1.],\n         [0.],\n         [0.],\n         [0.],\n         [0.],\n         [0.]]))\n\n\n\nx.shape, y.shape # ëª‡ê°œì”© ìˆëŠ”ì§€ê°€ ì¤‘ìš”\n\n(torch.Size([10]), torch.Size([10]))\n\n\n\nx=torch.tensor(range(10)).float().reshape(-1,1)\ny=torch.tensor([1.0]*5+[0.0]*5).reshape(-1,1)\ntorch.concat([x,y],axis=1)\n\ntensor([[0., 1.],\n        [1., 1.],\n        [2., 1.],\n        [3., 1.],\n        [4., 1.],\n        [5., 0.],\n        [6., 0.],\n        [7., 0.],\n        [8., 0.],\n        [9., 0.]])\n\n\n- dsì˜¤ë¸Œì íŠ¸\n\nds = torch.utils.data.TensorDataset(x,y)\nds\n\n&lt;torch.utils.data.dataset.TensorDataset at 0x7f5c8eed5fa0&gt;\n\n\nğŸ—£ï¸ ë­”ê°€ ë§Œë“¤ì–´ì§ (ds: dataset)\n\nds.tensors \n# ìƒê¸´ê±´ ds.tensors = (x,y) ì„\n\n(tensor([[0.],\n         [1.],\n         [2.],\n         [3.],\n         [4.],\n         [5.],\n         [6.],\n         [7.],\n         [8.],\n         [9.]]),\n tensor([[1.],\n         [1.],\n         [1.],\n         [1.],\n         [1.],\n         [0.],\n         [0.],\n         [0.],\n         [0.],\n         [0.]]))\n\n\n\nds[0],(x,y)[0] # (x,y) íŠœí”Œìì²´ëŠ” ì•„ë‹˜.. ì¸ë±ì‹±ì´ ë‹¤ë¥´ê²Œ ë™ì‘\n\n((tensor([0.]), tensor([1.])),\n tensor([[0.],\n         [1.],\n         [2.],\n         [3.],\n         [4.],\n         [5.],\n         [6.],\n         [7.],\n         [8.],\n         [9.]]))\n\n\nğŸ—£ï¸(\n\n(x,y)[1] # y\n\ntensor([[1.],\n        [1.],\n        [1.],\n        [1.],\n        [1.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.]])\n\n\n\n(x,y)[2] # error\n\n\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nCell In[60], line 1\n----&gt; 1 (x,y)[2] # error\n\nIndexError: tuple index out of range\n\n\n\n\nds[:3] # ì¸ë±ì‹±ì´ í¸í•¨\n\n(tensor([[0.],\n         [1.],\n         [2.]]),\n tensor([[1.],\n         [1.],\n         [1.]]))\n\n\n)ğŸ—£ï¸\n- dl ì˜¤ë¸Œì íŠ¸\n\ndl = torch.utils.data.DataLoader(ds, batch_size=3)\n\nğŸ—£ï¸(\n\nbatch_size: ì›í•˜ëŠ” ê°’ìœ¼ë¡œ ì…ë ¥\n\n\ndl = torch.utils.data.DataLoader(ds, batch_size=5)\n\n\nfor _ in dl:\n    print(_)\n\n[tensor([[0.],\n        [1.],\n        [2.],\n        [3.],\n        [4.]]), tensor([[1.],\n        [1.],\n        [1.],\n        [1.],\n        [1.]])]\n[tensor([[5.],\n        [6.],\n        [7.],\n        [8.],\n        [9.]]), tensor([[0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.]])]\n\n\n\nfor x,y in dl:\n    print(x)\n\ntensor([[0.],\n        [1.],\n        [2.],\n        [3.],\n        [4.]])\ntensor([[5.],\n        [6.],\n        [7.],\n        [8.],\n        [9.]])\n\n\n\ndl = torch.utils.data.DataLoader(ds, batch_size=3)\n\n\nfor x,y in dl:\n    print(x)\n\ntensor([[0.],\n        [1.],\n        [2.]])\ntensor([[3.],\n        [4.],\n        [5.]])\ntensor([[6.],\n        [7.],\n        [8.]])\ntensor([[9.]])\n\n\n)ğŸ—£ï¸\n\nfor x_mbatch,y_mbatch in dl:\n    print(f\"x_mini_batch:{x_mbatch.tolist()} \\t y_mini_batch:{y_mbatch.tolist()}\")\n\nx_mini_batch:[[0.0], [1.0], [2.0]]   y_mini_batch:[[1.0], [1.0], [1.0]]\nx_mini_batch:[[3.0], [4.0], [5.0]]   y_mini_batch:[[1.0], [1.0], [0.0]]\nx_mini_batch:[[6.0], [7.0], [8.0]]   y_mini_batch:[[0.0], [0.0], [0.0]]\nx_mini_batch:[[9.0]]     y_mini_batch:[[0.0]]\n\n\n- ë§ˆì§€ë§‰ê´€ì¸¡ì¹˜ëŠ” ë­”ë° ë‹¨ë…ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•˜ëƒ?? â€“&gt; shuffle True ê°™ì´ ìì˜í•œ ì˜µì…˜ë„ ìˆìŒ..\n\ndl = torch.utils.data.DataLoader(ds,batch_size=3,shuffle=True)\nfor x_mbatch,y_mbatch in dl:\n    print(f\"x_mini_batch:{x_mbatch.tolist()} \\t y_mini_batch:{y_mbatch.tolist()}\")\n\nx_mini_batch:[[1.0], [5.0], [6.0]]   y_mini_batch:[[1.0], [0.0], [0.0]]\nx_mini_batch:[[7.0], [0.0], [9.0]]   y_mini_batch:[[0.0], [1.0], [0.0]]\nx_mini_batch:[[2.0], [8.0], [3.0]]   y_mini_batch:[[1.0], [0.0], [1.0]]\nx_mini_batch:[[4.0]]     y_mini_batch:[[1.0]]\n\n\nğŸ—£ï¸ ëŒë¦´ ë•Œë§ˆë‹¤ ë‹¬ë¼ì§"
  },
  {
    "objectID": "posts/05wk-2.html#f.-ì„±ëŠ¥ì²´í¬",
    "href": "posts/05wk-2.html#f.-ì„±ëŠ¥ì²´í¬",
    "title": "05wk-2: (ì‹ ê²½ë§) â€“ ì‹ ê²½ë§ì˜ í‘œí˜„, GPUì‚¬ìš©ë²•, í™•ë¥ ì ê²½ì‚¬í•˜ê°•ë²•",
    "section": "F. ì„±ëŠ¥ì²´í¬",
    "text": "F. ì„±ëŠ¥ì²´í¬\n- ëª©í‘œ: í™•ë¥ ì ê²½ì‚¬í•˜ê°•ë²•ê³¼ ê·¸ëƒ¥ ê²½ì‚¬í•˜ê°•ë²•ì˜ ì„±ëŠ¥ì„ â€œë™ì¼ ë°˜ë³µíšŸìˆ˜â€ë¡œ ë¹„êµí•´ë³´ì.\nğŸ—£ï¸(\n10 4 -&gt; 10, 10, 10, 10\n10 4 -&gt; 3, 3, 3, 1\n\nìœ„ê°€ ì¢‹ì„ ê²ƒ ê°™ì§€ë§Œ ë³„ ì°¨ì´ ì—†ìŒ\nbatch sizeë¥¼ ì˜ ì •í•˜ë©´ ë°‘ì´ ì˜¤íˆë ¤ ì¢‹ì„ ìˆ˜ë„ ìˆìŒ\n\n)ğŸ—£ï¸\n- MNISTìë£Œë¥¼ ê·¸ëƒ¥ ê²½ì‚¬í•˜ê°•ë²•ìœ¼ë¡œ ì í•©í•´ë³´ì.\n\nimport torchvision\n\n\ntrain_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True)\nto_tensor = torchvision.transforms.ToTensor()\nX0 = torch.stack([to_tensor(Xi) for Xi, yi in train_dataset if yi==0])\nX1 = torch.stack([to_tensor(Xi) for Xi, yi in train_dataset if yi==1])\nX = torch.concat([X0,X1],axis=0).reshape(-1,784)\ny = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1)\n\n\nX.shape, y.shape\n\n(torch.Size([12665, 784]), torch.Size([12665, 1]))\n\n\n\ntorch.manual_seed(1)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(784,32),\n    torch.nn.ReLU(),\n    torch.nn.Linear(32,1),\n    torch.nn.Sigmoid()\n)\nloss_fn = torch.nn.BCELoss()\noptimizr = torch.optim.SGD(net.parameters())\n\n\nfor epoc in range(700):\n    # step1 \n    yhat = net(X)\n    # step2 \n    loss = loss_fn(yhat,y)\n    # step3     \n    loss.backward()\n    # step4 \n    optimizr.step()\n    optimizr.zero_grad()    \n\n\n((yhat &gt; 0.5) ==  y).float().mean()\n\ntensor(0.9953)\n\n\n- MNISTìë£Œë¥¼ í™•ë¥ ì  ê²½ì‚¬í•˜ê°•ë²•ìœ¼ë¡œ ì í•©í•´ë³´ì. â€“ ë¯¸ë‹ˆë°°ì¹˜ ì“°ëŠ” í•™ìŠµ\n\n# train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True)\n# to_tensor = torchvision.transforms.ToTensor()\n# X0 = torch.stack([to_tensor(Xi) for Xi, yi in train_dataset if yi==0])\n# X1 = torch.stack([to_tensor(Xi) for Xi, yi in train_dataset if yi==1])\n# X = torch.concat([X0,X1],axis=0).reshape(-1,784)\n# y = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1)\nds = torch.utils.data.TensorDataset(X,y)\ndl = torch.utils.data.DataLoader(ds,batch_size=2048)\n\n\nlen(X)/2048\n\n6.18408203125\n\n\nğŸ—£ï¸ ë§ˆì§€ë§‰ ë©ì–´ë¦¬ëŠ” ì‘ìŒ\n\në”°ë¼ì„œ (mini) batchsize ê°€ 2048 ì´ë¼ë©´ í•œ epochë‹¹ 7íšŒ update\n\n\ntorch.manual_seed(1)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(784,32),\n    torch.nn.ReLU(),\n    torch.nn.Linear(32,1),\n    torch.nn.Sigmoid()\n)\nloss_fn = torch.nn.BCELoss()\noptimizr = torch.optim.SGD(net.parameters())\n\n\nfor epoc in range(100): \n    for xm,ym in dl:        \n        # step1 \n        ym_hat = net(xm)\n        # step2 \n        loss = loss_fn(ym_hat,ym)\n        # step3     \n        loss.backward()\n        # step4 \n        optimizr.step()\n        optimizr.zero_grad()\n\n\nğŸ—£ï¸\n\nì´ update ë°˜ë³µ ìˆ˜ëŠ” ë™ì¼ (7ë²ˆ * 100ë²ˆ)\nxm = (2048,784), ym = (2048,1)\n\n\n\n((net(X) &gt; 0.5) ==  y).float().mean()\n\ntensor(0.9931)"
  }
]