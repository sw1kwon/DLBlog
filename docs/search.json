[
  {
    "objectID": "posts/01wk-1.html",
    "href": "posts/01wk-1.html",
    "title": "01wk-1: (토치) – 강의소개, 파이토치 기본",
    "section": "",
    "text": "📘 Note Format Guide\nThis format serves as a structured guide for organizing lecture content, personal interpretation, experiments, and study-related questions.\n📝 🗣️ ✍️ 🔬 ❓"
  },
  {
    "objectID": "posts/01wk-1.html#a.-torch",
    "href": "posts/01wk-1.html#a.-torch",
    "title": "01wk-1: (토치) – 강의소개, 파이토치 기본",
    "section": "A. torch",
    "text": "A. torch\n🗣️ torch는 numpy와 비슷 (벡터 만들기 등)\n- 벡터\n\ntorch.tensor([1,2,3])\n\ntensor([1, 2, 3])\n\n\n- 벡터의 덧셈\n\ntorch.tensor([1,2,3]) + torch.tensor([2,2,2])\n\ntensor([3, 4, 5])\n\n\n- 브로드캐스팅\n\ntorch.tensor([1,2,3]) + 2\n\ntensor([3, 4, 5])"
  },
  {
    "objectID": "posts/01wk-1.html#b.-벡터와-매트릭스",
    "href": "posts/01wk-1.html#b.-벡터와-매트릭스",
    "title": "01wk-1: (토치) – 강의소개, 파이토치 기본",
    "section": "B. 벡터와 매트릭스",
    "text": "B. 벡터와 매트릭스\n🗣️ torch.tensor는 np.array와 비슷\n- \\(3 \\times 2\\) matrix\n\ntorch.tensor([[1,2],[3,4],[5,6]]) \n\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])\n\n\n- \\(3 \\times 1\\) matrix = \\(3 \\times 1\\) column vector\n\ntorch.tensor([[1],[3],[5]]) \n\ntensor([[1],\n        [3],\n        [5]])\n\n\n- \\(1 \\times 2\\) matrix = \\(1 \\times 2\\) row vector\n\ntorch.tensor([[1,2]]) \n\ntensor([[1, 2]])\n\n\n🗣️ torch.tensor([[1,2],[3,4],[5,6]])에서 [3,4],[5,6] 삭제라고 생각\n🗣️ column vector와 row vector는 구분되고 선언 방법이 다름\n- 더하기\n브로드캐스팅(편한거)\n\ntorch.tensor([[1,2],[3,4],[5,6]]) - 1\n\ntensor([[0, 1],\n        [2, 3],\n        [4, 5]])\n\n\n🗣️ “matrix - scalar”는 불가능하지만 알아서 원소별로 전부 뺌\n\ntorch.tensor([[1,2],[3,4],[5,6]]) + torch.tensor([[-1],[-3],[-5]])\n\ntensor([[0, 1],\n        [0, 1],\n        [0, 1]])\n\n\n🗣️ (3, 2) - (3, 1)을 알아서 뺌\n✍️ torch.tensor([[-1,-1],[-3, 3],[-5,-5]])\n\ntorch.tensor([[1,2],[3,4],[5,6]]) + torch.tensor([[-1,-2]])\n\ntensor([[0, 0],\n        [2, 2],\n        [4, 4]])\n\n\n🗣️ (3, 2) - (1, 2)을 알아서 뺌\n✍️ torch.tensor([[-1,-2],[-1,-2],[-1,-2]])\n잘못된 브로드캐스팅\n\ntorch.tensor([[1,2],[3,4],[5,6]]) + torch.tensor([[-1,-3,-5]])\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[11], line 1\n----&gt; 1 torch.tensor([[1,2],[3,4],[5,6]]) + torch.tensor([[-1,-3,-5]])\n\nRuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1\n\n\n\n🗣️ 세로로 쓰거나 가로로 두 개의 원소만 썼으면 가능\n✍️ torch.tensor([[-1],[-3],[-5]]) 또는 torch.tensor([[-1,-3],[-1,-3],[-1,-3]]) 등\n\ntorch.tensor([[1,2],[3,4],[5,6]]) + torch.tensor([[-1],[-2]])\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[12], line 1\n----&gt; 1 torch.tensor([[1,2],[3,4],[5,6]]) + torch.tensor([[-1],[-2]])\n\nRuntimeError: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 0\n\n\n\n🗣️ (3, 2) - (2, 1) 는 알아서 채우기 어려우므로 에러\n이상한 것\n\ntorch.tensor([[1,2],[3,4],[5,6]]) + torch.tensor([-1,-2])\n\ntensor([[0, 0],\n        [2, 2],\n        [4, 4]])\n\n\n🗣️ (3, 2) matrix - 길이가 2인 vector(2x1, 1x2 둘 다 아님)\n🗣️ matrix - vector를 row vector로 해석하고 늘려서 계산한 듯\n✍️ torch.tensor([[-1,-2],[-1,-2],[-1,-2]])\n🔬(\n\n차원 수만 알고 싶을 때 → tensor.dim() 또는 tensor.ndim\n각 차원의 크기까지 알고 싶을 때 → tensor.shape 또는 tensor.size()\n\n\nprint(torch.tensor([[1,2],[3,4],[5,6]]).dim())\nprint(torch.tensor([[1,2],[3,4],[5,6]]).shape)\nprint(torch.tensor([-1,-2]).dim())\nprint(torch.tensor([-1,-2]).shape)\nprint(torch.tensor([[1,2],[3,4],[5,6]]).ndim)\nprint(torch.tensor([[1,2],[3,4],[5,6]]).size())\nprint(torch.tensor([-1,-2]).ndim)\nprint(torch.tensor([-1,-2]).size())\n\n2\ntorch.Size([3, 2])\n1\ntorch.Size([2])\n2\ntorch.Size([3, 2])\n1\ntorch.Size([2])\n\n\n\n참고 (Chat GPT4o)\n\n\nNumPy와 PyTorch 차이 정리\n\n\n\n\n기능\nPyTorch\nNumPy\n\n\n\n\n차원 수\n.dim() 또는 .ndim\n.ndim\n\n\nshape 확인\n.shape 또는 .size()\n.shape\n\n\n크기 변경\n.view(), .reshape()\n.reshape()\n\n\n타입\ntorch.Tensor\nnp.ndarray\n\n\n\n\n실전 팁:\n\nPyTorch의 .dim()만 NumPy에서 안 먹힌다는 것만 기억하면 둘 다 거의 비슷하게 다룰 수 있음\n다차원 배열을 다룰 때 .ndim, .shape는 양쪽 모두 안전하게 쓸 수 있는 핵심 도구\ndim()은 PyTorch 고유 메서드\n\n\n)🔬\n\ntorch.tensor([[1,2],[3,4],[5,6]]) + torch.tensor([-1,-3,-5])\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[15], line 1\n----&gt; 1 torch.tensor([[1,2],[3,4],[5,6]]) + torch.tensor([-1,-3,-5])\n\nRuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1\n\n\n\n🗣️ 길이가 3인 vector를 column vector로 해석하고 (3,2)로 채워서 계산할 것 같지만 X (이번에 발견)\n- 행렬곱\n정상적인 행렬곱\n\ntorch.tensor([[1,2],[3,4],[5,6]]) @ torch.tensor([[1],[2]])\n\ntensor([[ 5],\n        [11],\n        [17]])\n\n\n🗣️ (3,2) matirx @ (2,1) vector = (3,1) matrix\n\ntorch.tensor([[1,2,3]]) @ torch.tensor([[1,2],[3,4],[5,6]]) \n\ntensor([[22, 28]])\n\n\n🗣️ (1,3) @ (3,2) = (1,2)\n잘못된 행렬곱\n\ntorch.tensor([[1,2],[3,4],[5,6]]) @ torch.tensor([[1,2]])\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[18], line 1\n----&gt; 1 torch.tensor([[1,2],[3,4],[5,6]]) @ torch.tensor([[1,2]])\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (3x2 and 1x2)\n\n\n\n🗣️ (3,2) @ (1,2) 불가\n\ntorch.tensor([[1],[2],[3]]) @ torch.tensor([[1,2],[3,4],[5,6]]) \n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[19], line 1\n----&gt; 1 torch.tensor([[1],[2],[3]]) @ torch.tensor([[1,2],[3,4],[5,6]]) \n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (3x1 and 3x2)\n\n\n\n🗣️ (3,1) @ (3,2) 불가\n이상한 것\n\ntorch.tensor([[1,2],[3,4],[5,6]]) @ torch.tensor([1,2]) # 이게 왜 가능..\n\ntensor([ 5, 11, 17])\n\n\n🗣️ (3,2) @ (2) 길이가 2인 vector / 사람마다 해석 애매 (2,1)? (1,2)? / 곱하기를 위해 (2,1) column vector로 해석\n🗣️ (3,2) @ (2,1)로 해석 후 계산하여 (3) 길이가 3인 vector가 나옴\n\ntorch.tensor([1,2,3]) @ torch.tensor([[1,2],[3,4],[5,6]]) # 이건 왜 가능?\n\ntensor([22, 28])\n\n\n🗣️ (3) @ (3,2)에서 (3)을 (1,3) row vector로 해석\n🗣️( 엄밀하게 하려면\n\ntorch.tensor([[1,2,3]]) @ torch.tensor([[1,2],[3,4],[5,6]])\n\ntensor([[22, 28]])\n\n\n✍️ 당연히 결과의 차원도 다름\n)🗣️"
  },
  {
    "objectID": "posts/01wk-1.html#c.-transpose-reshape",
    "href": "posts/01wk-1.html#c.-transpose-reshape",
    "title": "01wk-1: (토치) – 강의소개, 파이토치 기본",
    "section": "C. transpose, reshape",
    "text": "C. transpose, reshape\n- transpose\n\ntorch.tensor([[1,2],[3,4]]).T \n\ntensor([[1, 3],\n        [2, 4]])\n\n\n\ntorch.tensor([[1],[3]]).T \n\ntensor([[1, 3]])\n\n\n🗣️ column vector -&gt; row vector\n\ntorch.tensor([[1,2]]).T \n\ntensor([[1],\n        [2]])\n\n\n🗣️ row vector -&gt; column vector\n🗣️ 차원을 바꾸는 효과 (1,2) -&gt; (2,1)\n- reshape\n🗣️( 차원 보기\n\ntorch.tensor([[1,2]]).shape\n\ntorch.Size([1, 2])\n\n\n을 column vector로 바꾸고 싶으면\n\ntorch.tensor([[1,2]]).reshape(2,1)\n\ntensor([[1],\n        [2]])\n\n\ntranspose와 동일\n)🗣️\n일반적인 사용\n\ntorch.tensor([[1,2],[3,4],[5,6]]).reshape(2,3)\n\ntensor([[1, 2, 3],\n        [4, 5, 6]])\n\n\n\ntorch.tensor([[1,2],[3,4],[5,6]])\n\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])\n\n\n\ntorch.tensor([[1,2],[3,4],[5,6]]).reshape(1,6)\n\ntensor([[1, 2, 3, 4, 5, 6]])\n\n\n🗣️ (3,2) -&gt; (1,6)\n\ntorch.tensor([[1,2],[3,4],[5,6]]).reshape(6)\n\ntensor([1, 2, 3, 4, 5, 6])\n\n\n🗣️ (3,2)를 그냥 6으로 : 길이가 6인 vector로 바꿈\n편한 것\n\ntorch.tensor([[1,2],[3,4],[5,6]]).reshape(2,-1)\n\ntensor([[1, 2, 3],\n        [4, 5, 6]])\n\n\n🗣️ torch.tensor([[1,2],[3,4],[5,6]]).reshape(2,??)를 원할 때 ??를 알아서 맞춤 (불가능하면 error)\n\ntorch.tensor([[1,2],[3,4],[5,6]]).reshape(6,-1)\n\ntensor([[1],\n        [2],\n        [3],\n        [4],\n        [5],\n        [6]])\n\n\n\ntorch.tensor([[1,2],[3,4],[5,6]]).reshape(-1,6)\n\ntensor([[1, 2, 3, 4, 5, 6]])\n\n\n\ntorch.tensor([[1,2],[3,4],[5,6]]).reshape(-1)\n\ntensor([1, 2, 3, 4, 5, 6])\n\n\n🗣️ 전체를 vector로 바꾸고 싶을 때 (1차원)"
  },
  {
    "objectID": "posts/01wk-1.html#d.-concat-stack-starstarstar",
    "href": "posts/01wk-1.html#d.-concat-stack-starstarstar",
    "title": "01wk-1: (토치) – 강의소개, 파이토치 기본",
    "section": "D. concat, stack \\((\\star\\star\\star)\\)",
    "text": "D. concat, stack \\((\\star\\star\\star)\\)\n- concat\n\na = torch.tensor([[1],[3],[5]])\nb = torch.tensor([[2],[4],[6]])\ntorch.concat([a,b],axis=1)\n\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])\n\n\n🗣️(\n\na\n\ntensor([[1],\n        [3],\n        [5]])\n\n\n\nb\n\ntensor([[2],\n        [4],\n        [6]])\n\n\na와 b를 모두 vector로 갖고 있는데 [a b]처럼 놓고 싶을 때 사용\n\na, b\n\n(tensor([[1],\n         [3],\n         [5]]),\n tensor([[2],\n         [4],\n         [6]]))\n\n\n\ntorch.concat([a,b]) # 이렇게 하면 좌우가 아니라 위 아래로 합쳐짐\n\ntensor([[1],\n        [3],\n        [5],\n        [2],\n        [4],\n        [6]])\n\n\n(3,1)과 (3,1)을 (3,2)로 만들고 싶었는데 (6,1)이 됨 -&gt; axis=1 옵션 사용하면 (3,2) 가능 (모르겠으면 밑의 링크 참조)\n)🗣️\n\ntorch.concat([a,b],axis=1)\n\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])\n\n\n- stack\n\na = torch.tensor([1,3,5])\nb = torch.tensor([2,4,6])\ntorch.stack([a,b],axis=1)\n\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])\n\n\n🗣️(\n\na\n\ntensor([1, 3, 5])\n\n\n\nb\n\ntensor([2, 4, 6])\n\n\n\na.reshape(3,1) # 참고) concat 설명 예시와 동일\n\ntensor([[1],\n        [3],\n        [5]])\n\n\n\ntorch.concat([a.reshape(3,1), b.reshape(3,1)], axis=1) # 리스트로 만든 후 이렇게 하면 되긴하나 너무 힘듦\n\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])\n\n\n\ntorch.stack([a,b], axis=1) # 같은 결과\n\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])\n\n\n차이: concat은 바꾸려는 대상의 dimension을 바꾸지는 X (matrix는 matrix로, vector는 vector로) / stack은 dimension을 하나 늘려서 바꿔줌\nconcat과 stack 둘 다 알면 좋음\n)🗣️\n\ntorch.concat([a.reshape(3,1),b.reshape(3,1)],axis=1)\n\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])\n\n\n\n\n\n\n\n\nWarning\n\n\n\nconcat과 stack을 지금 처음본다면 아래를 복습하시는게 좋습니다.\nhttps://guebin.github.io/PP2024/posts/06wk-2.html#numpy와-축axis"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deep Learning",
    "section": "",
    "text": "Based on: https://guebin.github.io/DL2025/\n\n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nMar 5, 2025\n\n\n01wk-1: (토치) – 강의소개, 파이토치 기본\n\n\nsw1kwon \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]