[
  {
    "objectID": "posts/01wk-1.html",
    "href": "posts/01wk-1.html",
    "title": "01wk-1: (í† ì¹˜) â€“ ê°•ì˜ì†Œê°œ, íŒŒì´í† ì¹˜ ê¸°ë³¸",
    "section": "",
    "text": "ğŸ“˜ Note Format Guide\nThis format serves as a structured guide for organizing lecture content, personal interpretation, experiments, and study-related questions.\nğŸ“ ğŸ—£ï¸ âœï¸ ğŸ”¬ â“"
  },
  {
    "objectID": "posts/01wk-1.html#a.-torch",
    "href": "posts/01wk-1.html#a.-torch",
    "title": "01wk-1: (í† ì¹˜) â€“ ê°•ì˜ì†Œê°œ, íŒŒì´í† ì¹˜ ê¸°ë³¸",
    "section": "A. torch",
    "text": "A. torch\nğŸ—£ï¸ torchëŠ” numpyì™€ ë¹„ìŠ· (ë²¡í„° ë§Œë“¤ê¸° ë“±)\n- ë²¡í„°\n\ntorch.tensor([1,2,3])\n\ntensor([1, 2, 3])\n\n\n- ë²¡í„°ì˜ ë§ì…ˆ\n\ntorch.tensor([1,2,3]) + torch.tensor([2,2,2])\n\ntensor([3, 4, 5])\n\n\n- ë¸Œë¡œë“œìºìŠ¤íŒ…\n\ntorch.tensor([1,2,3]) + 2\n\ntensor([3, 4, 5])"
  },
  {
    "objectID": "posts/01wk-1.html#b.-ë²¡í„°ì™€-ë§¤íŠ¸ë¦­ìŠ¤",
    "href": "posts/01wk-1.html#b.-ë²¡í„°ì™€-ë§¤íŠ¸ë¦­ìŠ¤",
    "title": "01wk-1: (í† ì¹˜) â€“ ê°•ì˜ì†Œê°œ, íŒŒì´í† ì¹˜ ê¸°ë³¸",
    "section": "B. ë²¡í„°ì™€ ë§¤íŠ¸ë¦­ìŠ¤",
    "text": "B. ë²¡í„°ì™€ ë§¤íŠ¸ë¦­ìŠ¤\nğŸ—£ï¸ torch.tensorëŠ” np.arrayì™€ ë¹„ìŠ·\n- \\(3 \\times 2\\) matrix\n\ntorch.tensor([[1,2],[3,4],[5,6]]) \n\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])\n\n\n- \\(3 \\times 1\\) matrix = \\(3 \\times 1\\) column vector\n\ntorch.tensor([[1],[3],[5]]) \n\ntensor([[1],\n        [3],\n        [5]])\n\n\n- \\(1 \\times 2\\) matrix = \\(1 \\times 2\\) row vector\n\ntorch.tensor([[1,2]]) \n\ntensor([[1, 2]])\n\n\nğŸ—£ï¸ torch.tensor([[1,2],[3,4],[5,6]])ì—ì„œ [3,4],[5,6] ì‚­ì œë¼ê³  ìƒê°\nğŸ—£ï¸ column vectorì™€ row vectorëŠ” êµ¬ë¶„ë˜ê³  ì„ ì–¸ ë°©ë²•ì´ ë‹¤ë¦„\n- ë”í•˜ê¸°\në¸Œë¡œë“œìºìŠ¤íŒ…(í¸í•œê±°)\n\ntorch.tensor([[1,2],[3,4],[5,6]]) - 1\n\ntensor([[0, 1],\n        [2, 3],\n        [4, 5]])\n\n\nğŸ—£ï¸ â€œmatrix - scalarâ€ëŠ” ë¶ˆê°€ëŠ¥í•˜ì§€ë§Œ ì•Œì•„ì„œ ì›ì†Œë³„ë¡œ ì „ë¶€ ëºŒ\n\ntorch.tensor([[1,2],[3,4],[5,6]]) + torch.tensor([[-1],[-3],[-5]])\n\ntensor([[0, 1],\n        [0, 1],\n        [0, 1]])\n\n\nğŸ—£ï¸ (3, 2) - (3, 1)ì„ ì•Œì•„ì„œ ëºŒ\nâœï¸ torch.tensor([[-1,-1],[-3, 3],[-5,-5]])\n\ntorch.tensor([[1,2],[3,4],[5,6]]) + torch.tensor([[-1,-2]])\n\ntensor([[0, 0],\n        [2, 2],\n        [4, 4]])\n\n\nğŸ—£ï¸ (3, 2) - (1, 2)ì„ ì•Œì•„ì„œ ëºŒ\nâœï¸ torch.tensor([[-1,-2],[-1,-2],[-1,-2]])\nì˜ëª»ëœ ë¸Œë¡œë“œìºìŠ¤íŒ…\n\ntorch.tensor([[1,2],[3,4],[5,6]]) + torch.tensor([[-1,-3,-5]])\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[11], line 1\n----&gt; 1 torch.tensor([[1,2],[3,4],[5,6]]) + torch.tensor([[-1,-3,-5]])\n\nRuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1\n\n\n\nğŸ—£ï¸ ì„¸ë¡œë¡œ ì“°ê±°ë‚˜ ê°€ë¡œë¡œ ë‘ ê°œì˜ ì›ì†Œë§Œ ì¼ìœ¼ë©´ ê°€ëŠ¥\nâœï¸ torch.tensor([[-1],[-3],[-5]]) ë˜ëŠ” torch.tensor([[-1,-3],[-1,-3],[-1,-3]]) ë“±\n\ntorch.tensor([[1,2],[3,4],[5,6]]) + torch.tensor([[-1],[-2]])\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[12], line 1\n----&gt; 1 torch.tensor([[1,2],[3,4],[5,6]]) + torch.tensor([[-1],[-2]])\n\nRuntimeError: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 0\n\n\n\nğŸ—£ï¸ (3, 2) - (2, 1) ëŠ” ì•Œì•„ì„œ ì±„ìš°ê¸° ì–´ë ¤ìš°ë¯€ë¡œ error\nì´ìƒí•œ ê²ƒ\n\ntorch.tensor([[1,2],[3,4],[5,6]]) + torch.tensor([-1,-2])\n\ntensor([[0, 0],\n        [2, 2],\n        [4, 4]])\n\n\nğŸ—£ï¸ (3, 2) matrix - ê¸¸ì´ê°€ 2ì¸ vector(2x1, 1x2 ë‘˜ ë‹¤ ì•„ë‹˜)\nğŸ—£ï¸ â€œmatrix - vectorâ€ë¥¼ row vectorë¡œ í•´ì„í•˜ê³  ëŠ˜ë ¤ì„œ ê³„ì‚°í•œ ë“¯\nâœï¸ torch.tensor([[-1,-2],[-1,-2],[-1,-2]])\nğŸ”¬(\n\nì°¨ì› ìˆ˜ë§Œ ì•Œê³  ì‹¶ì„ ë•Œ â†’ tensor.dim() ë˜ëŠ” tensor.ndim\nê° ì°¨ì›ì˜ í¬ê¸°ê¹Œì§€ ì•Œê³  ì‹¶ì„ ë•Œ â†’ tensor.shape ë˜ëŠ” tensor.size()\n\n\nprint(torch.tensor([[1,2],[3,4],[5,6]]).dim())\nprint(torch.tensor([[1,2],[3,4],[5,6]]).shape)\nprint(torch.tensor([-1,-2]).dim())\nprint(torch.tensor([-1,-2]).shape)\nprint(torch.tensor([[1,2],[3,4],[5,6]]).ndim)\nprint(torch.tensor([[1,2],[3,4],[5,6]]).size())\nprint(torch.tensor([-1,-2]).ndim)\nprint(torch.tensor([-1,-2]).size())\n\n2\ntorch.Size([3, 2])\n1\ntorch.Size([2])\n2\ntorch.Size([3, 2])\n1\ntorch.Size([2])\n\n\n\nì°¸ê³  (Chat GPT4o)\n\n\nNumPyì™€ PyTorch ì°¨ì´ ì •ë¦¬\n\n\n\n\nê¸°ëŠ¥\nPyTorch\nNumPy\n\n\n\n\nì°¨ì› ìˆ˜\n.dim() ë˜ëŠ” .ndim\n.ndim\n\n\nshape í™•ì¸\n.shape ë˜ëŠ” .size()\n.shape\n\n\ní¬ê¸° ë³€ê²½\n.view(), .reshape()\n.reshape()\n\n\níƒ€ì…\ntorch.Tensor\nnp.ndarray\n\n\n\n\nì‹¤ì „ íŒ:\n\nPyTorchì˜ .dim()ë§Œ NumPyì—ì„œ ì•ˆ ë¨¹íŒë‹¤ëŠ” ê²ƒë§Œ ê¸°ì–µí•˜ë©´ ë‘˜ ë‹¤ ê±°ì˜ ë¹„ìŠ·í•˜ê²Œ ë‹¤ë£° ìˆ˜ ìˆìŒ\në‹¤ì°¨ì› ë°°ì—´ì„ ë‹¤ë£° ë•Œ .ndim, .shapeëŠ” ì–‘ìª½ ëª¨ë‘ ì•ˆì „í•˜ê²Œ ì“¸ ìˆ˜ ìˆëŠ” í•µì‹¬ ë„êµ¬\ndim()ì€ PyTorch ê³ ìœ  ë©”ì„œë“œ\n\n\n)ğŸ”¬\n\ntorch.tensor([[1,2],[3,4],[5,6]]) + torch.tensor([-1,-3,-5])\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[15], line 1\n----&gt; 1 torch.tensor([[1,2],[3,4],[5,6]]) + torch.tensor([-1,-3,-5])\n\nRuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1\n\n\n\nğŸ—£ï¸ ê¸¸ì´ê°€ 3ì¸ vectorë¥¼ column vectorë¡œ í•´ì„í•˜ê³  (3,2)ë¡œ ì±„ì›Œì„œ ê³„ì‚°í•  ê²ƒ ê°™ì§€ë§Œ X (ì´ë²ˆì— ë°œê²¬)\n- í–‰ë ¬ê³±\nì •ìƒì ì¸ í–‰ë ¬ê³±\n\ntorch.tensor([[1,2],[3,4],[5,6]]) @ torch.tensor([[1],[2]])\n\ntensor([[ 5],\n        [11],\n        [17]])\n\n\nğŸ—£ï¸ (3,2) matirx @ (2,1) vector = (3,1) matrix\n\ntorch.tensor([[1,2,3]]) @ torch.tensor([[1,2],[3,4],[5,6]]) \n\ntensor([[22, 28]])\n\n\nğŸ—£ï¸ (1,3) @ (3,2) = (1,2)\nì˜ëª»ëœ í–‰ë ¬ê³±\n\ntorch.tensor([[1,2],[3,4],[5,6]]) @ torch.tensor([[1,2]])\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[18], line 1\n----&gt; 1 torch.tensor([[1,2],[3,4],[5,6]]) @ torch.tensor([[1,2]])\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (3x2 and 1x2)\n\n\n\nğŸ—£ï¸ (3,2) @ (1,2) ë¶ˆê°€\n\ntorch.tensor([[1],[2],[3]]) @ torch.tensor([[1,2],[3,4],[5,6]]) \n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[19], line 1\n----&gt; 1 torch.tensor([[1],[2],[3]]) @ torch.tensor([[1,2],[3,4],[5,6]]) \n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (3x1 and 3x2)\n\n\n\nğŸ—£ï¸ (3,1) @ (3,2) ë¶ˆê°€\nì´ìƒí•œ ê²ƒ\n\ntorch.tensor([[1,2],[3,4],[5,6]]) @ torch.tensor([1,2]) # ì´ê²Œ ì™œ ê°€ëŠ¥..\n\ntensor([ 5, 11, 17])\n\n\nğŸ—£ï¸ (3,2) @ (2) ê¸¸ì´ê°€ 2ì¸ vector / ì‚¬ëŒë§ˆë‹¤ í•´ì„ ì• ë§¤ (2,1)? (1,2)? / ê³±í•˜ê¸°ë¥¼ ìœ„í•´ (2,1) column vectorë¡œ í•´ì„\nğŸ—£ï¸ (3,2) @ (2,1)ë¡œ í•´ì„ í›„ ê³„ì‚°í•˜ì—¬ (3) ê¸¸ì´ê°€ 3ì¸ vectorê°€ ë‚˜ì˜´\n\ntorch.tensor([1,2,3]) @ torch.tensor([[1,2],[3,4],[5,6]]) # ì´ê±´ ì™œ ê°€ëŠ¥?\n\ntensor([22, 28])\n\n\nğŸ—£ï¸ (3) @ (3,2)ì—ì„œ (3)ì„ (1,3) row vectorë¡œ í•´ì„\nğŸ—£ï¸( ì—„ë°€í•˜ê²Œ í•˜ë ¤ë©´\n\ntorch.tensor([[1,2,3]]) @ torch.tensor([[1,2],[3,4],[5,6]])\n\ntensor([[22, 28]])\n\n\nâœï¸ ë‹¹ì—°íˆ ê²°ê³¼ì˜ ì°¨ì›ë„ ë‹¤ë¦„\n)ğŸ—£ï¸"
  },
  {
    "objectID": "posts/01wk-1.html#c.-transpose-reshape",
    "href": "posts/01wk-1.html#c.-transpose-reshape",
    "title": "01wk-1: (í† ì¹˜) â€“ ê°•ì˜ì†Œê°œ, íŒŒì´í† ì¹˜ ê¸°ë³¸",
    "section": "C. transpose, reshape",
    "text": "C. transpose, reshape\n- transpose\n\ntorch.tensor([[1,2],[3,4]]).T \n\ntensor([[1, 3],\n        [2, 4]])\n\n\n\ntorch.tensor([[1],[3]]).T \n\ntensor([[1, 3]])\n\n\nğŸ—£ï¸ column vector -&gt; row vector\n\ntorch.tensor([[1,2]]).T \n\ntensor([[1],\n        [2]])\n\n\nğŸ—£ï¸ row vector -&gt; column vector\nğŸ—£ï¸ ì°¨ì›ì„ ë°”ê¾¸ëŠ” íš¨ê³¼ (1,2) -&gt; (2,1)\n- reshape\nğŸ—£ï¸( ì°¨ì› ë³´ê¸°\n\ntorch.tensor([[1,2]]).shape\n\ntorch.Size([1, 2])\n\n\nì„ column vectorë¡œ ë°”ê¾¸ê³  ì‹¶ìœ¼ë©´\n\ntorch.tensor([[1,2]]).reshape(2,1)\n\ntensor([[1],\n        [2]])\n\n\ntransposeì™€ ë™ì¼\n)ğŸ—£ï¸\nì¼ë°˜ì ì¸ ì‚¬ìš©\n\ntorch.tensor([[1,2],[3,4],[5,6]]).reshape(2,3)\n\ntensor([[1, 2, 3],\n        [4, 5, 6]])\n\n\n\ntorch.tensor([[1,2],[3,4],[5,6]])\n\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])\n\n\n\ntorch.tensor([[1,2],[3,4],[5,6]]).reshape(1,6)\n\ntensor([[1, 2, 3, 4, 5, 6]])\n\n\nğŸ—£ï¸ (3,2) -&gt; (1,6)\n\ntorch.tensor([[1,2],[3,4],[5,6]]).reshape(6)\n\ntensor([1, 2, 3, 4, 5, 6])\n\n\nğŸ—£ï¸ (3,2)ë¥¼ ê·¸ëƒ¥ 6ìœ¼ë¡œ : ê¸¸ì´ê°€ 6ì¸ vectorë¡œ ë°”ê¿ˆ\ní¸í•œ ê²ƒ\n\ntorch.tensor([[1,2],[3,4],[5,6]]).reshape(2,-1)\n\ntensor([[1, 2, 3],\n        [4, 5, 6]])\n\n\nğŸ—£ï¸ torch.tensor([[1,2],[3,4],[5,6]]).reshape(2,??)ë¥¼ ì›í•  ë•Œ ??ë¥¼ ì•Œì•„ì„œ ë§ì¶¤ (ë¶ˆê°€ëŠ¥í•˜ë©´ error)\n\ntorch.tensor([[1,2],[3,4],[5,6]]).reshape(6,-1)\n\ntensor([[1],\n        [2],\n        [3],\n        [4],\n        [5],\n        [6]])\n\n\n\ntorch.tensor([[1,2],[3,4],[5,6]]).reshape(-1,6)\n\ntensor([[1, 2, 3, 4, 5, 6]])\n\n\n\ntorch.tensor([[1,2],[3,4],[5,6]]).reshape(-1)\n\ntensor([1, 2, 3, 4, 5, 6])\n\n\nğŸ—£ï¸ ì „ì²´ë¥¼ vectorë¡œ ë°”ê¾¸ê³  ì‹¶ì„ ë•Œ (1ì°¨ì›)"
  },
  {
    "objectID": "posts/01wk-1.html#d.-concat-stack-starstarstar",
    "href": "posts/01wk-1.html#d.-concat-stack-starstarstar",
    "title": "01wk-1: (í† ì¹˜) â€“ ê°•ì˜ì†Œê°œ, íŒŒì´í† ì¹˜ ê¸°ë³¸",
    "section": "D. concat, stack \\((\\star\\star\\star)\\)",
    "text": "D. concat, stack \\((\\star\\star\\star)\\)\n- concat\n\na = torch.tensor([[1],[3],[5]])\nb = torch.tensor([[2],[4],[6]])\ntorch.concat([a,b],axis=1)\n\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])\n\n\nğŸ—£ï¸(\n\na\n\ntensor([[1],\n        [3],\n        [5]])\n\n\n\nb\n\ntensor([[2],\n        [4],\n        [6]])\n\n\naì™€ bë¥¼ ëª¨ë‘ vectorë¡œ ê°–ê³  ìˆëŠ”ë° [a b]ì²˜ëŸ¼ ë†“ê³  ì‹¶ì„ ë•Œ ì‚¬ìš©\n\na, b\n\n(tensor([[1],\n         [3],\n         [5]]),\n tensor([[2],\n         [4],\n         [6]]))\n\n\n\ntorch.concat([a,b]) # ì´ë ‡ê²Œ í•˜ë©´ ì¢Œìš°ê°€ ì•„ë‹ˆë¼ ìœ„ ì•„ë˜ë¡œ í•©ì³ì§\n\ntensor([[1],\n        [3],\n        [5],\n        [2],\n        [4],\n        [6]])\n\n\n(3,1)ê³¼ (3,1)ì„ (3,2)ë¡œ ë§Œë“¤ê³  ì‹¶ì—ˆëŠ”ë° (6,1)ì´ ë¨ -&gt; axis=1 ì˜µì…˜ ì‚¬ìš©í•˜ë©´ (3,2) ê°€ëŠ¥ (ëª¨ë¥´ê² ìœ¼ë©´ ë°‘ì˜ ë§í¬ ì°¸ì¡°)\n)ğŸ—£ï¸\n\ntorch.concat([a,b],axis=1)\n\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])\n\n\n- stack\n\na = torch.tensor([1,3,5])\nb = torch.tensor([2,4,6])\ntorch.stack([a,b],axis=1)\n\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])\n\n\nğŸ—£ï¸(\n\na\n\ntensor([1, 3, 5])\n\n\n\nb\n\ntensor([2, 4, 6])\n\n\n\na.reshape(3,1) # ì°¸ê³ ) concat ì„¤ëª… ì˜ˆì‹œì™€ ë™ì¼\n\ntensor([[1],\n        [3],\n        [5]])\n\n\n\ntorch.concat([a.reshape(3,1), b.reshape(3,1)], axis=1) # ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“  í›„ ì´ë ‡ê²Œ í•˜ë©´ ë˜ê¸´í•˜ë‚˜ ë„ˆë¬´ í˜ë“¦\n\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])\n\n\n\ntorch.stack([a,b], axis=1) # ê°™ì€ ê²°ê³¼\n\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])\n\n\nì°¨ì´: concatì€ ë°”ê¾¸ë ¤ëŠ” ëŒ€ìƒì˜ dimensionì„ ë°”ê¾¸ì§€ëŠ” X (matrixëŠ” matrixë¡œ, vectorëŠ” vectorë¡œ) / stackì€ dimensionì„ í•˜ë‚˜ ëŠ˜ë ¤ì„œ ë°”ê¿”ì¤Œ\nconcatê³¼ stack ë‘˜ ë‹¤ ì•Œë©´ ì¢‹ìŒ\n)ğŸ—£ï¸\n\ntorch.concat([a.reshape(3,1),b.reshape(3,1)],axis=1)\n\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])\n\n\n\n\n\n\n\n\nWarning\n\n\n\nconcatê³¼ stackì„ ì§€ê¸ˆ ì²˜ìŒë³¸ë‹¤ë©´ ì•„ë˜ë¥¼ ë³µìŠµí•˜ì‹œëŠ”ê²Œ ì¢‹ìŠµë‹ˆë‹¤.\nhttps://guebin.github.io/PP2024/posts/06wk-2.html#numpyì™€-ì¶•axis"
  },
  {
    "objectID": "posts/02wk-2.html",
    "href": "posts/02wk-2.html",
    "title": "02wk-2: (íšŒê·€) â€“ íŒŒë¼ë©”í„°ì˜ í•™ìŠµê³¼ì • ìŒë¯¸, MSE, íŒŒì´í† ì¹˜ì‹ ì½”ë”©íŒ¨í„´ (1)",
    "section": "",
    "text": "ğŸ“˜ Note Format Guide\nThis format serves as a structured guide for organizing lecture content, personal interpretation, experiments, and study-related questions.\nğŸ“ ğŸ—£ï¸ âœï¸ ğŸ”¬ â“"
  },
  {
    "objectID": "posts/02wk-2.html#a.-print",
    "href": "posts/02wk-2.html#a.-print",
    "title": "02wk-2: (íšŒê·€) â€“ íŒŒë¼ë©”í„°ì˜ í•™ìŠµê³¼ì • ìŒë¯¸, MSE, íŒŒì´í† ì¹˜ì‹ ì½”ë”©íŒ¨í„´ (1)",
    "section": "A. print",
    "text": "A. print\n\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad=True)\nalpha = 0.001\nprint(f\"ì‹œì‘ê°’ = {What.data.reshape(-1)}\")\nfor epoc in range(30):\n    yhat = X @ What\n    loss = torch.sum((y-yhat)**2)\n    loss.backward()\n    What.data = What.data - alpha * What.grad\n    print(f'loss = {loss:.2f} \\t ì—…ë°ì´íŠ¸í­ = {-alpha * What.grad.reshape(-1)} \\t ì—…ë°ì´íŠ¸ê²°ê³¼: {What.data.reshape(-1)}')\n    What.grad = None\n\nì‹œì‘ê°’ = tensor([-5., 10.])\nloss = 8587.69   ì—…ë°ì´íŠ¸í­ = tensor([ 1.3423, -1.1889])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([-3.6577,  8.8111])\nloss = 5675.21   ì—…ë°ì´íŠ¸í­ = tensor([ 1.1029, -0.9499])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([-2.5548,  7.8612])\nloss = 3755.64   ì—…ë°ì´íŠ¸í­ = tensor([ 0.9056, -0.7596])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([-1.6492,  7.1016])\nloss = 2489.58   ì—…ë°ì´íŠ¸í­ = tensor([ 0.7431, -0.6081])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([-0.9061,  6.4935])\nloss = 1654.04   ì—…ë°ì´íŠ¸í­ = tensor([ 0.6094, -0.4872])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([-0.2967,  6.0063])\nloss = 1102.32   ì—…ë°ì´íŠ¸í­ = tensor([ 0.4995, -0.3907])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([0.2028, 5.6156])\nloss = 737.84    ì—…ë°ì´íŠ¸í­ = tensor([ 0.4091, -0.3136])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([0.6119, 5.3020])\nloss = 496.97    ì—…ë°ì´íŠ¸í­ = tensor([ 0.3350, -0.2519])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([0.9469, 5.0501])\nloss = 337.71    ì—…ë°ì´íŠ¸í­ = tensor([ 0.2742, -0.2025])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([1.2211, 4.8477])\nloss = 232.40    ì—…ë°ì´íŠ¸í­ = tensor([ 0.2243, -0.1629])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([1.4454, 4.6848])\nloss = 162.73    ì—…ë°ì´íŠ¸í­ = tensor([ 0.1834, -0.1311])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([1.6288, 4.5537])\nloss = 116.63    ì—…ë°ì´íŠ¸í­ = tensor([ 0.1500, -0.1056])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([1.7787, 4.4480])\nloss = 86.13     ì—…ë°ì´íŠ¸í­ = tensor([ 0.1226, -0.0851])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([1.9013, 4.3629])\nloss = 65.93     ì—…ë°ì´íŠ¸í­ = tensor([ 0.1001, -0.0687])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([2.0014, 4.2942])\nloss = 52.57     ì—…ë°ì´íŠ¸í­ = tensor([ 0.0818, -0.0554])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([2.0832, 4.2388])\nloss = 43.72     ì—…ë°ì´íŠ¸í­ = tensor([ 0.0668, -0.0447])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([2.1500, 4.1941])\nloss = 37.86     ì—…ë°ì´íŠ¸í­ = tensor([ 0.0545, -0.0361])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([2.2045, 4.1579])\nloss = 33.97     ì—…ë°ì´íŠ¸í­ = tensor([ 0.0445, -0.0292])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([2.2490, 4.1287])\nloss = 31.40     ì—…ë°ì´íŠ¸í­ = tensor([ 0.0363, -0.0236])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([2.2853, 4.1051])\nloss = 29.70     ì—…ë°ì´íŠ¸í­ = tensor([ 0.0296, -0.0191])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([2.3150, 4.0860])\nloss = 28.57     ì—…ë°ì´íŠ¸í­ = tensor([ 0.0242, -0.0155])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([2.3392, 4.0705])\nloss = 27.83     ì—…ë°ì´íŠ¸í­ = tensor([ 0.0197, -0.0125])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([2.3589, 4.0580])\nloss = 27.33     ì—…ë°ì´íŠ¸í­ = tensor([ 0.0161, -0.0101])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([2.3750, 4.0479])\nloss = 27.00     ì—…ë°ì´íŠ¸í­ = tensor([ 0.0131, -0.0082])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([2.3881, 4.0396])\nloss = 26.79     ì—…ë°ì´íŠ¸í­ = tensor([ 0.0107, -0.0067])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([2.3988, 4.0330])\nloss = 26.64     ì—…ë°ì´íŠ¸í­ = tensor([ 0.0087, -0.0054])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([2.4075, 4.0276])\nloss = 26.55     ì—…ë°ì´íŠ¸í­ = tensor([ 0.0071, -0.0044])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([2.4146, 4.0232])\nloss = 26.48     ì—…ë°ì´íŠ¸í­ = tensor([ 0.0058, -0.0035])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([2.4204, 4.0197])\nloss = 26.44     ì—…ë°ì´íŠ¸í­ = tensor([ 0.0047, -0.0029])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([2.4251, 4.0168])\nloss = 26.41     ì—…ë°ì´íŠ¸í­ = tensor([ 0.0038, -0.0023])      ì—…ë°ì´íŠ¸ê²°ê³¼: tensor([2.4290, 4.0144])\n\n\n\nğŸ—£ï¸\n\nlossë§Œ ë³´ë©´ ì ì  ê°ì†Œí•¨, ê°ˆìˆ˜ë¡ ê°ì†Œí•˜ëŠ” í­ë„ ì‘ì•„ì§€ë©° 26 ê·¼ì²˜ë¡œ ìˆ˜ë ´\nì—…ë°ì´íŠ¸ í­ë„ ì²˜ìŒì—ëŠ” ì»¸ë‹¤ê°€ ê°ì†Œ\nì´ì— ë”°ë¼ ì—…ë°ì´íŠ¸ ê²°ê³¼ë„ ê°ˆìˆ˜ë¡ ì˜ ì•ˆ ë°”ë€œ"
  },
  {
    "objectID": "posts/02wk-2.html#b.-ì‹œê°í™”-yhatì˜-ê´€ì ì—ì„œ",
    "href": "posts/02wk-2.html#b.-ì‹œê°í™”-yhatì˜-ê´€ì ì—ì„œ",
    "title": "02wk-2: (íšŒê·€) â€“ íŒŒë¼ë©”í„°ì˜ í•™ìŠµê³¼ì • ìŒë¯¸, MSE, íŒŒì´í† ì¹˜ì‹ ì½”ë”©íŒ¨í„´ (1)",
    "section": "B. ì‹œê°í™” â€“ yhatì˜ ê´€ì ì—ì„œ!",
    "text": "B. ì‹œê°í™” â€“ yhatì˜ ê´€ì ì—ì„œ!\nğŸ—£ï¸(\n\nWhat = torch.tensor([[-5.0], [10.0]], requires_grad=True)\n\n\nyhat = X@What\nloss = torch.sum((yhat-y)**2)\nloss.backward()\nWhat.data = What.data - 0.001*What.grad\nWhat.grad = None\n\n\nplt.plot(x,y,'o')\nplt.plot(x, (X@What).data, '--', color=\"C1\") # ì„  ìƒ‰ê¹” ì£¼í™©ìƒ‰ ê³ ì •\n\n\n\n\n\n\n\n\n\nì•„ë˜ ì½”ë“œë¥¼ ë°˜ë³µí•˜ë©° ì§€ì¼œë³´ë©´ ì„ ì´ ë³€í™”í•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŒ (ë°‘ì˜ ê·¸ë˜í”„ëŠ” ì—¬ëŸ¬ë²ˆ ë°˜ë³µí•œ ìµœì¢… ê²°ê³¼)\n\n\nyhat = X@What\nloss = torch.sum((yhat-y)**2)\nloss.backward()\nWhat.data = What.data - 0.001*What.grad\nWhat.grad = None\nplt.plot(x,y,'o')\nplt.plot(x, (X@What).data, '--', color=\"C1\")\n\n\n\n\n\n\n\n\n\ní•œ ê°€ì§€ ì•„ì‰¬ìš´ ì : ì¤‘ê°„ ê³¼ì •ì˜ ê·¸ë˜í”„ê°€ ì‚¬ë¼ì§\n\n\nWhat = torch.tensor([[-5.0], [10.0]], requires_grad=True)\n\n\nplt.plot(x,y,'o')\nfig = plt.gcf() # ì¤‘ê°„ ê·¸ë¦¼ì„ ì €ì¥ (í˜¸ì¶œ ê°€ëŠ¥) get current figure\n\n\n\n\n\n\n\n\n\nax = fig.gca() # get current axes (axes: axisì˜ ë³µìˆ˜í˜•, ì—¬ê¸°ì„œëŠ” xì¶•,yì¶• ëª¨ë‘ë¥¼ ì§€ì¹­)\nyhat = X@What\nloss = torch.sum((yhat-y)**2)\nloss.backward()\nWhat.data = What.data - 0.001*What.grad\nWhat.grad = None\nax.plot(x, (X@What).data, '--', color=\"C1\") # pltë¥¼ axë¡œ ìˆ˜ì •\n\n\nfig # 1ë²ˆ ì‹¤í–‰\n\n\n\n\n\n\n\n\n\n1ë²ˆ ë” ì‹¤í–‰í•˜ë©´ ê²¹ì³ì§\n\n\nax = fig.gca() # get current axes (axes: axisì˜ ë³µìˆ˜í˜•, ì—¬ê¸°ì„œëŠ” xì¶•,yì¶• ëª¨ë‘ë¥¼ ì§€ì¹­)\nyhat = X@What\nloss = torch.sum((yhat-y)**2)\nloss.backward()\nWhat.data = What.data - 0.001*What.grad\nWhat.grad = None\nax.plot(x, (X@What).data, '--', color=\"C1\") # pltë¥¼ axë¡œ ìˆ˜ì •\n\n\nfig # 1ë²ˆ ë” ì‹¤í–‰ (ê²¹ì³ì§)\n\n\n\n\n\n\n\n\n\nì´ˆê¸°í™” í›„ ë°˜ë³µí•˜ë©´ ì—…ë°ì´íŠ¸ëœ í­ì„ ë³¼ ìˆ˜ ìˆìŒ (ì ì  ì¤„ì–´ë“œëŠ” ê²ƒ ê°™ìŒ)\n\n\nWhat = torch.tensor([[-5.0], [10.0]], requires_grad=True)\n\n\nplt.plot(x,y,'o')\nfig = plt.gcf()\n\n\n\n\n\n\n\n\n\nax = fig.gca()\nyhat = X@What\nloss = torch.sum((yhat-y)**2)\nloss.backward()\nWhat.data = What.data - 0.001*What.grad\nWhat.grad = None\nax.plot(x, (X@What).data, '--', color=\"C1\")\nfig\n\n\n\n\n\n\n\n\n\nì œëª©ì„ ë„£ì„ ìˆ˜ë„ ìˆìŒ (set_title, ë‹¨ìˆœ ë¬¸ìì—´ ì•„ë‹ˆì—¬ë„ ê°€ëŠ¥)\n\n\nWhat = torch.tensor([[-5.0], [10.0]], requires_grad=True)\n\nplt.plot(x,y,'o')\nfig = plt.gcf()\n\n\n\n\n\n\n\n\n\nfor epoc in range(20):\n    ax = fig.gca()\n    yhat = X@What\n    loss = torch.sum((yhat-y)**2)\n    loss.backward()\n    What.data = What.data - 0.001*What.grad\n    What.grad = None\n    ax.plot(x, (X@What).data, '--', color=\"C1\")\n    ax.set_title(What.data.reshape(-1))\n    fig\n\n\nfig\n\n\n\n\n\n\n\n\n)ğŸ—£ï¸\n\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad=True)\nalpha = 0.001\nplt.plot(x,y,'o',label = \"observed\")\nfig = plt.gcf()\nax = fig.gca()\nax.plot(x,X@What.data,'--',color=\"C1\")\nfor epoc in range(30):\n    yhat = X @ What\n    loss = torch.sum((y-yhat)**2)\n    loss.backward()\n    What.data = What.data - alpha * What.grad\n    ax.plot(x,X@What.data,'--',color=\"C1\",alpha=0.1)\n    What.grad = None\n\n\n\n\n\n\n\n\nğŸ—£ï¸ alpha: ê²¹ì³ì§€ë©´ ì§„í•´ì§"
  },
  {
    "objectID": "posts/02wk-2.html#c.-ì‹œê°í™”-lossì˜-ê´€ì ì—ì„œ",
    "href": "posts/02wk-2.html#c.-ì‹œê°í™”-lossì˜-ê´€ì ì—ì„œ",
    "title": "02wk-2: (íšŒê·€) â€“ íŒŒë¼ë©”í„°ì˜ í•™ìŠµê³¼ì • ìŒë¯¸, MSE, íŒŒì´í† ì¹˜ì‹ ì½”ë”©íŒ¨í„´ (1)",
    "section": "C. ì‹œê°í™” â€“ lossì˜ ê´€ì ì—ì„œ!!",
    "text": "C. ì‹œê°í™” â€“ lossì˜ ê´€ì ì—ì„œ!!\nğŸ—£ï¸(\n\ndef plot_loss():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    w0 = np.arange(-6, 11, 0.5) \n    w1 = np.arange(-6, 11, 0.5)\n    W1,W0 = np.meshgrid(w1,w0)\n    LOSS=W0*0\n    for i in range(len(w0)):\n        for j in range(len(w1)):\n            LOSS[i,j]=torch.sum((y-w0[i]-w1[j]*x)**2)\n    ax.plot_surface(W0, W1, LOSS, rstride=1, cstride=1, color='b',alpha=0.1)\n    ax.azim = 30  ## 3d plotì˜ view ì¡°ì ˆ \n    ax.dist = 8   ## 3d plotì˜ view ì¡°ì ˆ \n    ax.elev = 5   ## 3d plotì˜ view ì¡°ì ˆ \n    ax.set_xlabel(r'$w_0$')  # xì¶• ë ˆì´ë¸” ì„¤ì •\n    ax.set_ylabel(r'$w_1$')  # yì¶• ë ˆì´ë¸” ì„¤ì •\n    ax.set_xticks([-5,0,5,10])  # xì¶• í‹± ê°„ê²© ì„¤ì •\n    ax.set_yticks([-5,0,5,10])  # yì¶• í‹± ê°„ê²© ì„¤ì •\n    plt.close(fig)  # ìë™ ì¶œë ¥ ë°©ì§€\n    return fig\n\n\nfig = plot_loss()\nfig # loss_fn(w0hat, w1hat)ì„ zì— ì°ìŒ\n\n\n\n\n\n\n\n\n\n# ì†ì‹¤ 8587.6875 ë¥¼ ê³„ì‚°í•˜ëŠ” ë˜ ë‹¤ë¥¸ ë°©ì‹\ndef l(w0hat,w1hat):\n    yhat = w0hat + w1hat*x\n    return torch.sum((y-yhat)**2)\n\n\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad=True)\nWhat\n\ntensor([[-5.],\n        [10.]], requires_grad=True)\n\n\n\nl(-5,10) # ì†ì‹¤ ê³„ì‚°\n\ntensor(8587.6875)\n\n\n\ntorch.sum((y-X@What)**2) # ë‹¤ë¥¸ ë°©ë²•\n\ntensor(8587.6875, grad_fn=&lt;SumBackward0&gt;)\n\n\n\nyhat = -5 + 10*x\ntorch.sum((y-yhat)**2) # ë‹¤ë¥¸ ë°©ë²• 2\n\ntensor(8587.6875)\n\n\n\nfig\nax = fig.gca()\nax.scatter(-5, 10, l(-5,10)) # ì  ì°ê¸°\n\n&lt;mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7fc9261c2bb0&gt;\n\n\n\nfig\n\n\n\n\n\n\n\n\n\nfig\nax = fig.gca()\nax.scatter(-1, 3, l(-1,3)) # ë‹¤ë¥¸ ì  ì°ê¸°\n\n&lt;mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7fc925df1eb0&gt;\n\n\n\nfig\n\n\n\n\n\n\n\n\n\nìœ„ ê³¼ì •ì„ ë°˜ë³µí•˜ë©´ ê³¡ë©´ì„ ê·¸ë¦´ ìˆ˜ ìˆìŒ\në°‘ì€ True ê°’ ì°ê¸°\n\n\nfig\nax = fig.gca()\nax.scatter(2.5, 4.0, l(2.5,4.0)) # True ê°’\n\n&lt;mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7fc925f14670&gt;\n\n\n\nfig\n\n\n\n\n\n\n\n\n\në°‘ì— ì •ë¦¬ëœ ì½”ë“œ ê³¼ì •\n\n\nfig = plot_loss()\nfig # loss_fn(w0hat,w1hat)\nax = fig.gca()\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad=True)\nw0hat, w1hat = What.data.reshape(-1) # ì–¸íŒ¨í‚¹\nax.scatter(w0hat, w1hat, l(w0hat, w1hat)) # x, y, z\nfig # ìµœì´ˆì˜ ì§ì„ ì— ëŒ€ì‘í•˜ëŠ” What ê°’\n\n\n\n\n\n\n\n\n\nyhat = X@What\nloss = torch.sum((y-yhat)**2)\nloss.backward()\nWhat.data = What.data - 0.001*What.grad\nWhat.grad = None\nw0hat, w1hat = What.data.reshape(-1)\nax.scatter(w0hat, w1hat, l(w0hat, w1hat))\nfig # ë°˜ë³µ ì‹¤í–‰í• ìˆ˜ë¡ updateë¨\n\n\n\n\n\n\n\n\n\nyhat = X@What\nloss = torch.sum((y-yhat)**2)\nloss.backward()\nWhat.data = What.data - 0.001*What.grad\nWhat.grad = None\nw0hat, w1hat = What.data.reshape(-1)\nax.scatter(w0hat, w1hat, l(w0hat, w1hat), color=\"C1\") # ì•ìœ¼ë¡œëŠ” ì£¼í™©ìƒ‰ìœ¼ë¡œ ìƒ‰ê¹” ê³ ì •\nfig # ë°˜ë³µ ì‹¤í–‰í• ìˆ˜ë¡ ì ì  ìµœì†Œê°€ ë˜ëŠ” ìª½ìœ¼ë¡œ ì§„í–‰ë¨\n\n\n\n\n\n\n\n\n)ğŸ—£ï¸\n\ndef plot_loss():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    w0 = np.arange(-6, 11, 0.5) \n    w1 = np.arange(-6, 11, 0.5)\n    W1,W0 = np.meshgrid(w1,w0)\n    LOSS=W0*0\n    for i in range(len(w0)):\n        for j in range(len(w1)):\n            LOSS[i,j]=torch.sum((y-w0[i]-w1[j]*x)**2)\n    ax.plot_surface(W0, W1, LOSS, rstride=1, cstride=1, color='b',alpha=0.1)\n    ax.azim = 30  ## 3d plotì˜ view ì¡°ì ˆ \n    ax.dist = 8   ## 3d plotì˜ view ì¡°ì ˆ \n    ax.elev = 5   ## 3d plotì˜ view ì¡°ì ˆ \n    ax.set_xlabel(r'$w_0$')  # xì¶• ë ˆì´ë¸” ì„¤ì •\n    ax.set_ylabel(r'$w_1$')  # yì¶• ë ˆì´ë¸” ì„¤ì •\n    ax.set_xticks([-5,0,5,10])  # xì¶• í‹± ê°„ê²© ì„¤ì •\n    ax.set_yticks([-5,0,5,10])  # yì¶• í‹± ê°„ê²© ì„¤ì •\n    plt.close(fig)  # ìë™ ì¶œë ¥ ë°©ì§€\n    return fig\n\n\n# ì†ì‹¤ 8587.6875 ë¥¼ ê³„ì‚°í•˜ëŠ” ë˜ ë‹¤ë¥¸ ë°©ì‹\ndef l(w0hat,w1hat):\n    yhat = w0hat + w1hat*x\n    return torch.sum((y-yhat)**2)\n\n\nfig = plot_loss()\nax = fig.gca()\nax.scatter(2.5, 4, l(2.5,4), s=200, marker='*', color='red', label=r\"${\\bf W}=[2.5, 4]'$\")\nax.scatter(-5, 10, l(-5,10), s=200, marker='*', color='blue', label=r\"initial $\\hat{\\bf W}=[-5, 10]'$\")\nax.legend()\nfig\n\n\n\n\n\n\n\n\n\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad=True)\nalpha = 0.001\nfor epoc in range(30):\n    yhat = X @ What\n    loss = torch.sum((y-yhat)**2)\n    loss.backward()\n    What.data = What.data - 0.001 * What.grad\n    w0,w1 = What.data.reshape(-1) \n    ax.scatter(w0,w1,l(w0,w1),s=5,marker='o',color='blue')\n    What.grad = None\n\n\nfig\n\n\n\n\n\n\n\n\n\nğŸ—£ï¸\n\nBì˜ ì‹œê°í™”ì—ì„œ ìµœì´ˆì˜ ì§ì„ ì— ëŒ€ì‘í•˜ëŠ” ì ì´ íŒŒë€ìƒ‰ ì \nì ë“¤ì´ ë¹¨ê°„ìƒ‰ ì ìœ¼ë¡œ ì´ë™í•˜ëŠ” ê³¼ì •ì€ ì§ì„ ì´ ì˜¬ë¼ê°€ëŠ” ê³¼ì •ì— ëŒ€ì‘"
  },
  {
    "objectID": "posts/02wk-2.html#d.-ì• ë‹ˆë©”ì´ì…˜",
    "href": "posts/02wk-2.html#d.-ì• ë‹ˆë©”ì´ì…˜",
    "title": "02wk-2: (íšŒê·€) â€“ íŒŒë¼ë©”í„°ì˜ í•™ìŠµê³¼ì • ìŒë¯¸, MSE, íŒŒì´í† ì¹˜ì‹ ì½”ë”©íŒ¨í„´ (1)",
    "section": "D. ì• ë‹ˆë©”ì´ì…˜",
    "text": "D. ì• ë‹ˆë©”ì´ì…˜\n\nfrom matplotlib import animation\n\n\nplt.rcParams['figure.figsize'] = (7.5,2.5)\nplt.rcParams[\"animation.html\"] = \"jshtml\" \n\n\ndef show_animation(alpha=0.001):\n    ## 1. íˆìŠ¤í† ë¦¬ ê¸°ë¡ì„ ìœ„í•œ list ì´ˆê¸°í™”\n    loss_history = [] \n    yhat_history = [] \n    What_history = [] \n\n    ## 2. í•™ìŠµ + í•™ìŠµê³¼ì •ê¸°ë¡\n    What= torch.tensor([[-5.0],[10.0]],requires_grad=True)\n    What_history.append(What.data.tolist())\n    for epoc in range(30): \n        yhat=X@What ; yhat_history.append(yhat.data.tolist())\n        loss=torch.sum((y-yhat)**2); loss_history.append(loss.item())\n        loss.backward() \n        What.data = What.data - alpha * What.grad; What_history.append(What.data.tolist())\n        What.grad = None    \n\n    ## 3. ì‹œê°í™” \n    fig = plt.figure()\n    ax1 = fig.add_subplot(1, 2, 1)\n    ax2 = fig.add_subplot(1, 2, 2, projection='3d')\n\n    #### ax1: yhatì˜ ê´€ì ì—ì„œ.. \n    ax1.plot(x,y,'o',label=r\"$(x_i,y_i)$\")\n    line, = ax1.plot(x,yhat_history[0],label=r\"$(x_i,\\hat{y}_i)$\") \n    ax1.legend()\n    #### ax2: lossì˜ ê´€ì ì—ì„œ.. \n    w0 = np.arange(-6, 11, 0.5) \n    w1 = np.arange(-6, 11, 0.5)\n    W1,W0 = np.meshgrid(w1,w0)\n    LOSS=W0*0\n    for i in range(len(w0)):\n        for j in range(len(w1)):\n            LOSS[i,j]=torch.sum((y-w0[i]-w1[j]*x)**2)\n    ax2.plot_surface(W0, W1, LOSS, rstride=1, cstride=1, color='b',alpha=0.1)\n    ax2.azim = 30  ## 3d plotì˜ view ì¡°ì ˆ \n    ax2.dist = 8   ## 3d plotì˜ view ì¡°ì ˆ \n    ax2.elev = 5   ## 3d plotì˜ view ì¡°ì ˆ \n    ax2.set_xlabel(r'$w_0$')  # xì¶• ë ˆì´ë¸” ì„¤ì •\n    ax2.set_ylabel(r'$w_1$')  # yì¶• ë ˆì´ë¸” ì„¤ì •\n    ax2.set_xticks([-5,0,5,10])  # xì¶• í‹± ê°„ê²© ì„¤ì •\n    ax2.set_yticks([-5,0,5,10])  # yì¶• í‹± ê°„ê²© ì„¤ì •\n    ax2.scatter(2.5, 4, l(2.5,4), s=200, marker='*', color='red', label=r\"${\\bf W}=[2.5, 4]'$\")\n    ax2.scatter(-5, 10, l(-5,10), s=200, marker='*', color='blue')\n    ax2.legend()\n    def animate(epoc):\n        line.set_ydata(yhat_history[epoc])\n        ax2.scatter(np.array(What_history)[epoc,0],np.array(What_history)[epoc,1],loss_history[epoc],color='grey')\n        fig.suptitle(f\"alpha = {alpha} / epoch = {epoc}\")\n        return line\n\n    ani = animation.FuncAnimation(fig, animate, frames=30)\n    plt.close()\n    return ani\n\n\nğŸ—£ï¸ alpha:\n\ní•™ìŠµë¥ : updateë˜ëŠ” í­ (ML ê´€ì )\nstep size: ì˜¤ë¥¸ìª½ ê·¸ë¦¼ í•¨ìˆ˜ ê´€ì  (ì‚°ì—… ê³µí•™ ê´€ì )\n\n\n\nani = show_animation(alpha=0.001)\nani\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/02wk-2.html#e.-í•™ìŠµë¥ ì—-ë”°ë¥¸-ì‹œê°í™”",
    "href": "posts/02wk-2.html#e.-í•™ìŠµë¥ ì—-ë”°ë¥¸-ì‹œê°í™”",
    "title": "02wk-2: (íšŒê·€) â€“ íŒŒë¼ë©”í„°ì˜ í•™ìŠµê³¼ì • ìŒë¯¸, MSE, íŒŒì´í† ì¹˜ì‹ ì½”ë”©íŒ¨í„´ (1)",
    "section": "E. í•™ìŠµë¥ ì— ë”°ë¥¸ ì‹œê°í™”",
    "text": "E. í•™ìŠµë¥ ì— ë”°ë¥¸ ì‹œê°í™”\n- \\(\\alpha\\)ê°€ ë„ˆë¬´ ì‘ë‹¤ë©´ ë¹„íš¨ìœ¨ì ì„\n\nshow_animation(alpha=0.0001)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\nğŸ—£ï¸ 0.001 -&gt; 0.0001\n\nì•„ê¹Œë³´ë‹¤ ê°€ëŠ” ë‘¥ ë§ˆëŠ” ë‘¥ í•¨\n\n\n- \\(\\alpha\\)ê°€ í¬ë‹¤ê³  ë¬´ì¡°ê±´ ì¢‹ì€ê±´ ë˜ ì•„ë‹˜\n\nshow_animation(alpha=0.0083)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\nğŸ—£ï¸ 0.001 -&gt; 0.0083 (ì§ì ‘ ì°¾ì€ ìˆ«ì)\n\nì²˜ìŒë¶€í„° ìµœì†Œì ì„ ì§€ë‚˜ë²„ë¦¼ (ì§ì„ ì´ ì ë“¤ ìœ„ë¡œ ë°”ë¡œ ì˜¬ë¼ê°) -&gt; ë°”ëŒì§í•˜ì§€ ì•ŠìŒ\nì´í›„ ì§ì„ ì´ ë‹¤ì‹œ ì ë“¤ ì•„ë˜ë¡œ ë‚´ë ¤ì˜´\nì™”ë‹¤ê°”ë‹¤í•˜ë©´ì„œ ë‚´ë ¤ì˜¤ëŠ” ê²ƒ ê°™ê¸°ëŠ” í•˜ë‚˜ íš¨ìœ¨ì ì¸ ëŠë‚Œì€ ì•„ë‹˜\n\n\n- ìˆ˜í‹€ë¦¬ë©´ ìˆ˜ë ´ì•ˆí• ìˆ˜ë„??\n\nshow_animation(alpha=0.0085)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\nğŸ—£ï¸ 0.001 -&gt; 0.0085\n\nì§ì „ì˜ 0.0083ê³¼ ì–¼ë§ˆ ì°¨ì´ê°€ ë‚˜ì§€ë„ ì•ŠëŠ”ë°\nì´ë²ˆì—ëŠ” ì™”ë‹¤ê°”ë‹¤í•˜ë©´ì„œ ìˆ˜ë ´í•˜ì§€ë„ ì•ŠìŒ\nì˜¤íˆë ¤ ê°ˆìˆ˜ë¡ í¬ë¬¼ì„  ëª¨ì–‘ìœ¼ë¡œ ì ì  ì˜¬ë¼ê°\n\n\n- ê·¸ëƒ¥ ë§í• ìˆ˜ë„??\n\nshow_animation(alpha=0.01)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\nğŸ—£ï¸ 0.001 -&gt; 0.01\n\nê¸°ìš¸ê¸°ê°€ ë¬´í•œëŒ€ê°€ ë¨\nêµí›ˆ: alphaë¥¼ ì˜ ì„ íƒí•´ì•¼ ìˆ˜ë ´í•¨\n\n\n\n\nplt.rcdefaults()\nplt.rcParams['figure.figsize'] = 4.5,3.0"
  },
  {
    "objectID": "posts/02wk-2.html#a.-ê¸°ë³¸íŒ¨í„´",
    "href": "posts/02wk-2.html#a.-ê¸°ë³¸íŒ¨í„´",
    "title": "02wk-2: (íšŒê·€) â€“ íŒŒë¼ë©”í„°ì˜ í•™ìŠµê³¼ì • ìŒë¯¸, MSE, íŒŒì´í† ì¹˜ì‹ ì½”ë”©íŒ¨í„´ (1)",
    "section": "A. ê¸°ë³¸íŒ¨í„´",
    "text": "A. ê¸°ë³¸íŒ¨í„´\nğŸ—£ï¸ SSE ë§ê³  MSEë¡œ\n\n## -- ì™¸ìš°ì„¸ìš”!!! -- ##\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad = True)\nfor epoc in range(30):\n    # step1: yhat \n    yhat = X@What \n    # step2: loss\n    loss = torch.sum((y-yhat)**2)/100\n    # step3: ë¯¸ë¶„\n    loss.backward()\n    # step4: update\n    What.data = What.data - 0.1 * What.grad\n    What.grad = None\n\n\nplt.plot(x,y,'o')\nplt.plot(x,X@What.data,'--')\nplt.title(f'What={What.data.reshape(-1)}');"
  },
  {
    "objectID": "posts/02wk-2.html#b.-step2ì˜-ìˆ˜ì •-loss_fn-ì´ìš©",
    "href": "posts/02wk-2.html#b.-step2ì˜-ìˆ˜ì •-loss_fn-ì´ìš©",
    "title": "02wk-2: (íšŒê·€) â€“ íŒŒë¼ë©”í„°ì˜ í•™ìŠµê³¼ì • ìŒë¯¸, MSE, íŒŒì´í† ì¹˜ì‹ ì½”ë”©íŒ¨í„´ (1)",
    "section": "B. Step2ì˜ ìˆ˜ì • â€“ loss_fn ì´ìš©",
    "text": "B. Step2ì˜ ìˆ˜ì • â€“ loss_fn ì´ìš©\nğŸ—£ï¸(\n\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad = True)\n\n\nyhat=X@What\n\n\ntorch.sum((y-yhat)**2)/100 # MSE\n\ntensor(85.8769, grad_fn=&lt;DivBackward0&gt;)\n\n\n\ntorch.mean((y-yhat)**2) # MSE\n\ntensor(85.8769, grad_fn=&lt;MeanBackward0&gt;)\n\n\n\ndef loss_fn(yhat, y):\n    return torch.mean((y-yhat)**2)\n\n\nloss_fn(yhat,y)\n\ntensor(85.8769, grad_fn=&lt;MeanBackward0&gt;)\n\n\n\nloss_fn ì›ë¦¬ë¥¼ ì˜ ëª¨ë¥¸ë‹¤ë©´ pytorch í•¨ìˆ˜ ì´ìš©\n\n\nloss_fn = torch.nn.MSELoss()\nloss_fn(yhat,y) # ê²°ê³¼ëŠ” ë™ì¼\n\ntensor(85.8769, grad_fn=&lt;MseLossBackward0&gt;)\n\n\n\ní‹€ë¦° ì„¤ëª…\n\ntorch.nn.MSELossëŠ” í•¨ìˆ˜ì¸ë°, â€œNone -&gt; MSEë¥¼ ê³„ì‚°í•´ì£¼ëŠ” í•¨ìˆ˜â€ì¸ í•¨ìˆ˜\n\në§ëŠ” ì„¤ëª…\n\ntorch.nn.MSELossëŠ” callable objectë¥¼ ìƒì„±í•˜ëŠ” í´ë˜ìŠ¤\n\n\n)ğŸ—£ï¸\n\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad = True)\nloss_fn = torch.nn.MSELoss()\nfor epoc in range(30):\n    # step1: yhat \n    yhat = X@What \n    # step2: loss\n    #loss = torch.sum((y-yhat)**2)/100\n    loss = loss_fn(yhat,y) # ì—¬ê¸°ì„œëŠ” í° ìƒê´€ì—†ì§€ë§Œ ìŠµê´€ì ìœ¼ë¡œ yhatì„ ë¨¼ì €ë„£ëŠ” ì—°ìŠµì„ í•˜ì!!\n    # step3: ë¯¸ë¶„\n    loss.backward()\n    # step4: update\n    What.data = What.data - 0.1 * What.grad\n    What.grad = None\n\nğŸ—£ï¸ loss_fnì€ ë¬´ì¡°ê±´ yhat ë¨¼ì €\n\nplt.plot(x,y,'o')\nplt.plot(x,X@What.data,'--')\nplt.title(f'What={What.data.reshape(-1)}');"
  },
  {
    "objectID": "posts/02wk-2.html#c.-step1ì˜-ìˆ˜ì •-net-ì´ìš©",
    "href": "posts/02wk-2.html#c.-step1ì˜-ìˆ˜ì •-net-ì´ìš©",
    "title": "02wk-2: (íšŒê·€) â€“ íŒŒë¼ë©”í„°ì˜ í•™ìŠµê³¼ì • ìŒë¯¸, MSE, íŒŒì´í† ì¹˜ì‹ ì½”ë”©íŒ¨í„´ (1)",
    "section": "C. Step1ì˜ ìˆ˜ì • â€“ net ì´ìš©",
    "text": "C. Step1ì˜ ìˆ˜ì • â€“ net ì´ìš©\nğŸ—£ï¸ yhat = X@Whatë„ ì•Œê³  ì‹¶ì§€ ì•Šë‹¤ë©´ (ë„¤íŠ¸ì›Œí¬ ì´ìš©)\n# net â€“ net ì˜¤ë¸Œì íŠ¸ë€?\nì›ë˜ yhatì„ ì´ëŸ°ì‹ìœ¼ë¡œ êµ¬í–ˆëŠ”ë° ~\n\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad = True)\nyhat= X@What\nyhat[:5]\n\ntensor([[-29.8211],\n        [-28.6215],\n        [-24.9730],\n        [-21.2394],\n        [-19.7919]], grad_fn=&lt;SliceBackward0&gt;)\n\n\nì•„ë˜ì™€ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ì½”ë“œë¥¼ ì§œê³  ì‹¶ìŒ..\nyhat = net(X) # \nğŸ—£ï¸ Xë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ yhatì„ ì¶œë ¥í•˜ê³  ì‹¶ìŒ\nìœ„ì™€ ê°™ì€ ì½”ë“œë¥¼ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” netì€ torchì—ì„œ ì§€ì›í•˜ê³  ì•„ë˜ì™€ ê°™ì´ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ.\nğŸ—£ï¸(\n\n# torch.nn.Linear?\n\nnet = torch.nn.Linear(\n    in_features= ??,\n    out_features= ??,\n    bias= False # defaultëŠ” True\n)\n\nin_features: ì…ë ¥(X)ì— ëŒ€í•œ ì°¨ì› (featuresë¥¼ dimensionìœ¼ë¡œ ìƒê°)\n\n\nX.shape # 100ì€ ê´€ì¸¡ê°’ ê°œìˆ˜ì— ë”°ë¼ ë°”ë€” ìˆ˜ ìˆê³ , 2ëŠ” ëª¨í˜•ì´ ì •í•´ì§€ë©´ ì•ˆ ë°”ë€œ\n\ntorch.Size([100, 2])\n\n\n\nout_features: ì¶œë ¥(y)ì— ëŒ€í•œ ì°¨ì›\n\n\ny.shape # ë§ˆì°¬ê°€ì§€ë¡œ 1\n\ntorch.Size([100, 1])\n\n\n\nnet = torch.nn.Linear(\n    in_features= 2,\n    out_features= 1,\n    bias= False\n)\n\n\nyhat = net(X)\nyhat[:5]\n\ntensor([[-0.1600],\n        [-0.1362],\n        [-0.0639],\n        [ 0.0101],\n        [ 0.0388]], grad_fn=&lt;SliceBackward0&gt;)\n\n\n\nì›ë˜ êµ¬í•œ yhatê³¼ ë¹„êµí•˜ë©´ Whatì„ ì„¤ì •í•˜ì§€ ì•Šì•˜ìœ¼ë¯€ë¡œ ë‹¹ì—°íˆ ë‹¤ë¦„\n\n\nnet.weight # Whatê³¼ ê°™ë‹¤ê³  ìƒê°í•˜ë©´ ë¨\n\nParameter containing:\ntensor([[0.3320, 0.1982]], requires_grad=True)\n\n\n\nì—„ë°€íˆ ë§í•˜ë©´ net.weightëŠ” 2x1 matrixê°€ ì•„ë‹ˆë¼ 1x2 martix\n\nì»´í“¨í„° ê³µí•™ì  ì´ìœ ë¡œ ì´ë ‡ê²Œ ë˜ì–´ ìˆìŒ (column vectorë³´ë‹¤ row vector ì—°ì‚°ì´ ì‰½ë‹¤ê³  í•¨)\n\n\n\nnet.weight.T # ì´ê²Œ ì§„ì§œ Whatê³¼ ë™ì¼\n\ntensor([[0.3320],\n        [0.1982]], grad_fn=&lt;PermuteBackward0&gt;)\n\n\n\nnet.weight.data = torch.tensor([[-5.0],[10.0]]).T\nnet.weight.data\n\ntensor([[-5., 10.]])\n\n\n\nyhat= net(X)\nyhat[:5]\n\ntensor([[-29.8211],\n        [-28.6215],\n        [-24.9730],\n        [-21.2394],\n        [-19.7919]], grad_fn=&lt;SliceBackward0&gt;)\n\n\n\nì›ë˜ êµ¬í•œ yhatê³¼ ë™ì¼\n\n)ğŸ—£ï¸\n\n# yhat = net(X) \nnet = torch.nn.Linear(\n    in_features=2, # X:(n,2) --&gt; 2 \n    out_features=1, # yhat:(n,1) --&gt; 1 \n    bias=False \n)\n\n\nnet.weight.data = torch.tensor([[-5.0], [10.0]]).T # .T ë¥¼ í•´ì•¼í•¨. ì™¸ìš°ì„¸ìš” \nnet.weight\n\nParameter containing:\ntensor([[-5., 10.]], requires_grad=True)\n\n\n\nnet(X)[:5]\n\ntensor([[-29.8211],\n        [-28.6215],\n        [-24.9730],\n        [-21.2394],\n        [-19.7919]], grad_fn=&lt;SliceBackward0&gt;)\n\n\n\n(X@What)[:5]\n\ntensor([[-29.8211],\n        [-28.6215],\n        [-24.9730],\n        [-21.2394],\n        [-19.7919]], grad_fn=&lt;SliceBackward0&gt;)\n\n\n\n(X@net.weight.T)[:5]\n\ntensor([[-29.8211],\n        [-28.6215],\n        [-24.9730],\n        [-21.2394],\n        [-19.7919]], grad_fn=&lt;SliceBackward0&gt;)\n\n\n#\n- ìˆ˜ì •ëœì½”ë“œ\n\n# step1ì„ ìœ„í•œ ì‚¬ì „ì¤€ë¹„\nnet = torch.nn.Linear(\n    in_features=2,\n    out_features=1,\n    bias=False\n)\nnet.weight.data = torch.tensor([[-5.0,  10.0]])\n# step2ë¥¼ ìœ„í•œ ì‚¬ì „ì¤€ë¹„\nloss_fn = torch.nn.MSELoss()\nfor epoc in range(30):\n    # step1: yhat\n    # yhat = X@What \n    yhat = net(X)\n    # step2: loss\n    loss = loss_fn(yhat,y)\n    # step3: ë¯¸ë¶„\n    loss.backward()\n    # step4: update\n    net.weight.data = net.weight.data - 0.1 * net.weight.grad\n    net.weight.grad = None\n\nğŸ—£ï¸ What.data -&gt; net.weight.data, What.grad -&gt; net.weight.grad\n\nplt.plot(x,y,'o')\nplt.plot(x,net(X).data,'--')\nplt.title(f'net.weight={net.weight.data.reshape(-1)}');"
  },
  {
    "objectID": "posts/02wk-2.html#d.-step4ì˜-ìˆ˜ì •-optimizerì˜-ì´ìš©",
    "href": "posts/02wk-2.html#d.-step4ì˜-ìˆ˜ì •-optimizerì˜-ì´ìš©",
    "title": "02wk-2: (íšŒê·€) â€“ íŒŒë¼ë©”í„°ì˜ í•™ìŠµê³¼ì • ìŒë¯¸, MSE, íŒŒì´í† ì¹˜ì‹ ì½”ë”©íŒ¨í„´ (1)",
    "section": "D. Step4ì˜ ìˆ˜ì • â€“ optimizerì˜ ì´ìš©",
    "text": "D. Step4ì˜ ìˆ˜ì • â€“ optimizerì˜ ì´ìš©\n- ì†Œë§: ì•„ë˜ì˜ ê³¼ì •ì„ ì¢€ ë” í¸í•˜ê²Œ í–ˆìœ¼ë©´..\nnet.weight.data = net.weight.data - 0.1 * net.weight.grad\nnet.weight.data = None \n# optimizer â€“ ì´ê±¸ ì´ìš©í•˜ë©´ update ê³¼ì •ì„ ì†ì‰½ê²Œ í•  ìˆ˜ ìˆìŒ\nê¸°ì¡´ì½”ë“œ\n\n## -- ì¤€ë¹„ê³¼ì • -- ## \n# step1ì„ ìœ„í•œ ì‚¬ì „ì¤€ë¹„\nnet = torch.nn.Linear(\n    in_features=2,\n    out_features=1,\n    bias=False\n)\nnet.weight.data = torch.tensor([[-5.0,  10.0]])\n# step2ë¥¼ ìœ„í•œ ì‚¬ì „ì¤€ë¹„\nloss_fn = torch.nn.MSELoss()\n\n\n## -- 1ì—í­ì§„í–‰ -- ## \n# step1: \nyhat = net(X)\n# step2: loss\nloss = loss_fn(yhat,y)\n# step3: ë¯¸ë¶„\nloss.backward()\n# step4: update\nprint(net.weight.data)\nnet.weight.data = net.weight.data - 0.1 * net.weight.grad\nprint(net.weight.data)\nnet.weight.grad = None\n\ntensor([[-5., 10.]])\ntensor([[-3.6577,  8.8111]])\n\n\n\n## -- 2ì—í­ì§„í–‰ -- ## \n# step1: 2ì—í­ì§„í–‰\nyhat = net(X)\n# step2: loss\nloss = loss_fn(yhat,y)\n# step3: ë¯¸ë¶„\nloss.backward()\n# step4: update\nprint(net.weight.data)\nnet.weight.data = net.weight.data - 0.1 * net.weight.grad\nprint(net.weight.data)\nnet.weight.grad = None\n\ntensor([[-3.6577,  8.8111]])\ntensor([[-2.5548,  7.8612]])\n\n\nìƒˆë¡œìš´ì½”ë“œ â€“ optimizer ì´ìš©\nğŸ—£ï¸(\n\ntorch.optim.SGD : optimizerë¥¼ ë§Œë“¤ì–´ì¤Œ\ntorch.optim.SGDëŠ” net.weightë¥¼ ê°–ê³  ìˆì–´ì•¼ í•¨(What)\n\nnet.weightëŠ” net.parameters()ë¡œ ë³¼ ìˆ˜ ìˆìŒ\nnet.parameters()ëŠ” generator: iterable object -&gt; listí™” ê°€ëŠ¥\n\ntorch.optim.SGDëŠ” í•™ìŠµë¥  lrë„ ê°–ê³  ìˆì–´ì•¼ í•¨\n\n\nnet.weight\n\nParameter containing:\ntensor([[-2.5548,  7.8612]], requires_grad=True)\n\n\n\nnet.parameters()\n\n&lt;generator object Module.parameters at 0x7fc922ade820&gt;\n\n\n\nlist(net.parameters()) # ê°’ì„ ë³´ë ¤ë©´\n\n[Parameter containing:\n tensor([[-2.5548,  7.8612]], requires_grad=True)]\n\n\n\n# optimizr = torch.optim.SGD(net.parameters(), lr=0.1) # net.parameters(): generator\n\n\nnet.weight.data = net.weight.data - 0.1 * net.weight.grad\n\n=&gt; optimizr.step()\n\nnet.weight.grad = None\n\n=&gt; optimizr.zero_grad()\n\n\n)ğŸ—£ï¸\n\n## -- ì¤€ë¹„ê³¼ì • -- ## \n# step1ì„ ìœ„í•œ ì‚¬ì „ì¤€ë¹„\nnet = torch.nn.Linear(\n    in_features=2,\n    out_features=1,\n    bias=False\n)\nnet.weight.data = torch.tensor([[-5.0,  10.0]])\n# step2ë¥¼ ìœ„í•œ ì‚¬ì „ì¤€ë¹„\nloss_fn = torch.nn.MSELoss()\n# step4ë¥¼ ìœ„í•œ ì‚¬ì „ì¤€ë¹„\noptimizr = torch.optim.SGD(net.parameters(),lr=0.1)\n\n\n## -- 1ì—í­ì§„í–‰ -- ## \nyhat = net(X)\n# step2: loss\nloss = loss_fn(yhat,y)\n# step3: ë¯¸ë¶„\nloss.backward()\n# step4: update\nprint(net.weight.data)\n#net.weight.data = net.weight.data - 0.1 * net.weight.grad\noptimizr.step()\nprint(net.weight.data)\n#net.weight.grad = None\noptimizr.zero_grad()\n\ntensor([[-5., 10.]])\ntensor([[-3.6577,  8.8111]])\n\n\n\n## -- 2ì—í­ì§„í–‰ -- ## \nyhat = net(X)\n# step2: loss\nloss = loss_fn(yhat,y)\n# step3: ë¯¸ë¶„\nloss.backward()\n# step4: update\nprint(net.weight.data)\n#net.weight.data = net.weight.data - 0.1 * net.weight.grad\noptimizr.step()\nprint(net.weight.data)\n#net.weight.grad = None\noptimizr.zero_grad()\n\ntensor([[-3.6577,  8.8111]])\ntensor([[-2.5548,  7.8612]])\n\n\n#\n- ìˆ˜ì •ëœì½”ë“œ\n\n# step1ì„ ìœ„í•œ ì‚¬ì „ì¤€ë¹„\nnet = torch.nn.Linear(\n    in_features=2,\n    out_features=1,\n    bias=False\n)\nnet.weight.data = torch.tensor([[-5.0,  10.0]])\n# step2ë¥¼ ìœ„í•œ ì‚¬ì „ì¤€ë¹„\nloss_fn = torch.nn.MSELoss()\n# step4ë¥¼ ìœ„í•œ ì‚¬ì „ì¤€ë¹„ \noptimizr = torch.optim.SGD(net.parameters(),lr=0.1)\nfor epoc in range(30):\n    # step1: yhat \n    yhat = net(X)\n    # step2: loss\n    loss = loss_fn(yhat,y)\n    # step3: ë¯¸ë¶„\n    loss.backward()\n    # step4: update\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'o')\nplt.plot(x,yhat.data,'--')\nplt.title(f'net.weight={net.weight.data.reshape(-1)}');"
  },
  {
    "objectID": "posts/01wk-2.html",
    "href": "posts/01wk-2.html",
    "title": "01wk-2, 02wk-1: (íšŒê·€) â€“ íšŒê·€ëª¨í˜•, ì†ì‹¤í•¨ìˆ˜, íŒŒì´í† ì¹˜ë¥¼ ì´ìš©í•œ ì¶”ì •",
    "section": "",
    "text": "ğŸ“˜ Note Format Guide\nThis format serves as a structured guide for organizing lecture content, personal interpretation, experiments, and study-related questions.\nğŸ“ ğŸ—£ï¸ âœï¸ ğŸ”¬ â“"
  },
  {
    "objectID": "posts/01wk-2.html#a.-ì•„ì´ìŠ¤-ì•„ë©”ë¦¬ì¹´ë…¸-ê°€ì§œìë£Œ",
    "href": "posts/01wk-2.html#a.-ì•„ì´ìŠ¤-ì•„ë©”ë¦¬ì¹´ë…¸-ê°€ì§œìë£Œ",
    "title": "01wk-2, 02wk-1: (íšŒê·€) â€“ íšŒê·€ëª¨í˜•, ì†ì‹¤í•¨ìˆ˜, íŒŒì´í† ì¹˜ë¥¼ ì´ìš©í•œ ì¶”ì •",
    "section": "A. ì•„ì´ìŠ¤ ì•„ë©”ë¦¬ì¹´ë…¸ (ê°€ì§œìë£Œ)",
    "text": "A. ì•„ì´ìŠ¤ ì•„ë©”ë¦¬ì¹´ë…¸ (ê°€ì§œìë£Œ)\n- ì¹´í˜ì£¼ì¸ì¸ ë°•í˜œì›ì”¨ëŠ” ì˜¨ë„ì™€ ì•„ì´ìŠ¤ì•„ë©”ë¦¬ì¹´ë…¸ íŒë§¤ëŸ‰ì´ ê´€ê³„ê°€ ìˆë‹¤ëŠ” ê²ƒì„ ì•Œì•˜ë‹¤. êµ¬ì²´ì ìœ¼ë¡œëŠ”\n\nâ€œì˜¨ë„ê°€ ë†’ì•„ì§ˆ ìˆ˜ë¡ (=ë‚ ì”¨ê°€ ë”ìš¸ìˆ˜ë¡) ì•„ì´ìŠ¤ì•„ë©”ë¦¬ì¹´ë…¸ì˜ íŒë§¤ëŸ‰ì´ ì¦ê°€â€\n\ní•œë‹¤ëŠ” ì‚¬ì‹¤ì„ ì•Œê²Œ ë˜ì—ˆë‹¤. ì´ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•´ì„œ ì•„ë˜ì™€ ê°™ì´ 100ê°œì˜ ë°ì´í„°ë¥¼ ëª¨ì•˜ë‹¤.\n\ntemp = [-2.4821, -2.3621, -1.9973, -1.6239, -1.4792, -1.4635, -1.4509, -1.4435,\n        -1.3722, -1.3079, -1.1904, -1.1092, -1.1054, -1.0875, -0.9469, -0.9319,\n        -0.8643, -0.7858, -0.7549, -0.7421, -0.6948, -0.6103, -0.5830, -0.5621,\n        -0.5506, -0.5058, -0.4806, -0.4738, -0.4710, -0.4676, -0.3874, -0.3719,\n        -0.3688, -0.3159, -0.2775, -0.2772, -0.2734, -0.2721, -0.2668, -0.2155,\n        -0.2000, -0.1816, -0.1708, -0.1565, -0.1448, -0.1361, -0.1057, -0.0603,\n        -0.0559, -0.0214,  0.0655,  0.0684,  0.1195,  0.1420,  0.1521,  0.1568,\n         0.2646,  0.2656,  0.3157,  0.3220,  0.3461,  0.3984,  0.4190,  0.5443,\n         0.5579,  0.5913,  0.6148,  0.6469,  0.6469,  0.6523,  0.6674,  0.7059,\n         0.7141,  0.7822,  0.8154,  0.8668,  0.9291,  0.9804,  0.9853,  0.9941,\n         1.0376,  1.0393,  1.0697,  1.1024,  1.1126,  1.1532,  1.2289,  1.3403,\n         1.3494,  1.4279,  1.4994,  1.5031,  1.5437,  1.6789,  2.0832,  2.2444,\n         2.3935,  2.6056,  2.6057,  2.6632]\n\n\nsales= [-8.5420, -6.5767, -5.9496, -4.4794, -4.2516, -3.1326, -4.0239, -4.1862,\n        -3.3403, -2.2027, -2.0262, -2.5619, -1.3353, -2.0466, -0.4664, -1.3513,\n        -1.6472, -0.1089, -0.3071, -0.6299, -0.0438,  0.4163,  0.4166, -0.0943,\n         0.2662,  0.4591,  0.8905,  0.8998,  0.6314,  1.3845,  0.8085,  1.2594,\n         1.1211,  1.9232,  1.0619,  1.3552,  2.1161,  1.1437,  1.6245,  1.7639,\n         1.6022,  1.7465,  0.9830,  1.7824,  2.1116,  2.8621,  2.1165,  1.5226,\n         2.5572,  2.8361,  3.3956,  2.0679,  2.8140,  3.4852,  3.6059,  2.5966,\n         2.8854,  3.9173,  3.6527,  4.1029,  4.3125,  3.4026,  3.2180,  4.5686,\n         4.3772,  4.3075,  4.4895,  4.4827,  5.3170,  5.4987,  5.4632,  6.0328,\n         5.2842,  5.0539,  5.4538,  6.0337,  5.7250,  5.7587,  6.2020,  6.5992,\n         6.4621,  6.5140,  6.6846,  7.3497,  8.0909,  7.0794,  6.8667,  7.4229,\n         7.2544,  7.1967,  9.5006,  9.0339,  7.4887,  9.0759, 11.0946, 10.3260,\n        12.2665, 13.0983, 12.5468, 13.8340]\n\nğŸ—£ï¸ ìŒìˆ˜ íŒë§¤ëŸ‰ì€ ì¼ë‹¨ ë¬´ì‹œ\nì—¬ê¸°ì—ì„œ tempëŠ” í‰ê· ê¸°ì˜¨ì´ê³ , salesëŠ” ì•„ì´ìŠ¤ì•„ë©”ë¦¬ì¹´ë…¸ íŒë§¤ëŸ‰ì´ë‹¤. í‰ê· ê¸°ì˜¨ê³¼ íŒë§¤ëŸ‰ì˜ ê·¸ë˜í”„ë¥¼ ê·¸ë ¤ë³´ë©´ ì•„ë˜ì™€ ê°™ë‹¤.\n\nplt.plot(temp,sales,'o')\n\n\n\n\n\n\n\n\nğŸ—£ï¸ ì•½ê°„ì˜ ì˜¤ì°¨ëŠ” ìˆì§€ë§Œ ì„ ìœ¼ë¡œ ë³´ì„\nì˜¤ëŠ˜ ë°”ê¹¥ì˜ ì˜¨ë„ëŠ” 0.5ë„ ì´ë‹¤. ì•„ì´ìŠ¤ ì•„ë©”ë¼ì¹´ë…¸ë¥¼ ëª‡ì”ì •ë„ ë§Œë“¤ì–´ ë‘ë©´ ì¢‹ì„ê¹Œ?\nğŸ—£ï¸ ì´ ê·¸ë˜í”„ë¥¼ ë³´ê³  4.5ì” ì •ë„ë¡œ ì§ì‘ ê°€ëŠ¥"
  },
  {
    "objectID": "posts/01wk-2.html#b.-ê°€ì§œìë£Œë¥¼-ë§Œë“ -ë°©ë²•",
    "href": "posts/01wk-2.html#b.-ê°€ì§œìë£Œë¥¼-ë§Œë“ -ë°©ë²•",
    "title": "01wk-2, 02wk-1: (íšŒê·€) â€“ íšŒê·€ëª¨í˜•, ì†ì‹¤í•¨ìˆ˜, íŒŒì´í† ì¹˜ë¥¼ ì´ìš©í•œ ì¶”ì •",
    "section": "B. ê°€ì§œìë£Œë¥¼ ë§Œë“  ë°©ë²•",
    "text": "B. ê°€ì§œìë£Œë¥¼ ë§Œë“  ë°©ë²•\n- ë°©ë²•1: \\(y_i= w_0+w_1 x_i +\\epsilon_i = 2.5 + 4x_i +\\epsilon_i, \\quad i=1,2,\\dots,n\\)\nğŸ—£ï¸(\nxi = ì˜¨ë„ = temp\nyi = íŒë§¤ëŸ‰ = sales\níŒë§¤ëŸ‰ = 2.5 + 4*ì˜¨ë„ + ì˜¤ì°¨\n\ntorch.randn(10) # í‘œì¤€ì •ê·œë¶„í¬ì—ì„œ 10ê°œ ê°’ ì¶”ì¶œ, ê¸¸ì´ê°€ 10ì¸ vector (column vectorì¸ì§€ row vectorì¸ì§€ëŠ” ëª¨ë¦„)\n\ntensor([-0.4351, -0.4066,  1.2577, -1.1443,  0.3941, -0.2229, -0.4337,  0.8736,\n         0.6216,  1.0963])\n\n\n\ntorch.randn(100).sort() # 100ê°œ ê°’ì„ ì •ë ¬ / ì•ì€ ì •ë ¬ëœ ê°’, ë’¤ëŠ” ì¸ë±ìŠ¤\n\ntorch.return_types.sort(\nvalues=tensor([-3.3450e+00, -2.3363e+00, -1.7533e+00, -1.6534e+00, -1.4996e+00,\n        -1.4218e+00, -1.3757e+00, -1.3314e+00, -1.1898e+00, -1.1594e+00,\n        -1.1386e+00, -1.0975e+00, -1.0961e+00, -1.0899e+00, -1.0250e+00,\n        -9.7851e-01, -9.1254e-01, -8.8307e-01, -8.7845e-01, -8.4915e-01,\n        -7.4344e-01, -7.0972e-01, -7.0845e-01, -6.8746e-01, -6.7488e-01,\n        -6.6512e-01, -6.0503e-01, -5.8921e-01, -5.4838e-01, -5.1363e-01,\n        -5.0996e-01, -4.7537e-01, -4.3955e-01, -3.5707e-01, -3.4237e-01,\n        -3.4013e-01, -3.2890e-01, -3.2078e-01, -3.0216e-01, -2.9112e-01,\n        -2.8083e-01, -2.4387e-01, -2.4171e-01, -2.0109e-01, -1.9779e-01,\n        -1.9549e-01, -5.8397e-02, -2.5842e-02, -2.2056e-02,  2.0055e-03,\n         1.0348e-02,  2.2201e-02,  2.5445e-02,  2.6868e-02,  6.2116e-02,\n         1.3408e-01,  1.5172e-01,  2.0091e-01,  2.3218e-01,  2.5000e-01,\n         2.7442e-01,  2.8144e-01,  3.4857e-01,  3.7494e-01,  4.4520e-01,\n         4.8013e-01,  4.9466e-01,  5.0311e-01,  5.7595e-01,  6.2995e-01,\n         6.3221e-01,  6.5666e-01,  6.5788e-01,  6.6027e-01,  6.7909e-01,\n         7.1635e-01,  7.1752e-01,  7.2141e-01,  8.0059e-01,  8.0419e-01,\n         8.0801e-01,  8.1830e-01,  8.9444e-01,  9.6222e-01,  9.9973e-01,\n         1.1303e+00,  1.1527e+00,  1.2046e+00,  1.2086e+00,  1.2469e+00,\n         1.2752e+00,  1.2872e+00,  1.3125e+00,  1.4296e+00,  1.4390e+00,\n         1.5448e+00,  1.6129e+00,  1.6454e+00,  1.6769e+00,  1.7580e+00]),\nindices=tensor([81, 19, 56, 18, 89, 54, 27, 31, 65, 85, 94, 47,  0,  7,  8, 57, 14, 92,\n         3, 12, 86, 48,  9, 82, 62, 78,  1, 28, 32, 67, 21, 53, 10, 30, 23,  5,\n        88, 24, 63, 40, 20, 77, 34, 87, 99, 80, 41,  4, 69, 90, 35, 72, 58, 11,\n        22, 42, 76, 95, 74, 38, 46, 59, 91, 68, 43, 44, 50, 96, 51,  6, 29, 13,\n        66, 49, 73,  2, 70, 93, 97, 16, 15, 98, 55, 33, 39, 84, 25, 61, 17, 64,\n        45, 26, 75, 71, 79, 37, 60, 83, 36, 52]))\n\n\n\na = torch.randn(100).sort()\ntype(a)\n\ntorch.return_types.sort\n\n\n\na[0]\n\ntensor([-2.8188e+00, -2.7746e+00, -2.5355e+00, -2.4374e+00, -2.2716e+00,\n        -2.1492e+00, -1.8555e+00, -1.8281e+00, -1.6228e+00, -1.6164e+00,\n        -1.5151e+00, -1.5046e+00, -1.4989e+00, -1.4708e+00, -1.4605e+00,\n        -1.3748e+00, -1.3521e+00, -1.3183e+00, -1.2710e+00, -1.2416e+00,\n        -1.1459e+00, -1.0949e+00, -1.0907e+00, -1.0903e+00, -1.0481e+00,\n        -1.0313e+00, -1.0079e+00, -1.0003e+00, -9.9874e-01, -9.9081e-01,\n        -9.8943e-01, -9.7448e-01, -9.4772e-01, -9.4282e-01, -9.1282e-01,\n        -8.8605e-01, -8.6893e-01, -8.5283e-01, -7.8566e-01, -7.7867e-01,\n        -7.6961e-01, -7.4827e-01, -6.6928e-01, -6.3990e-01, -5.9842e-01,\n        -5.8057e-01, -5.5388e-01, -5.1941e-01, -5.1005e-01, -4.9040e-01,\n        -4.7796e-01, -3.9862e-01, -3.9854e-01, -3.8835e-01, -3.7719e-01,\n        -3.6587e-01, -3.0923e-01, -3.0278e-01, -2.5337e-01, -2.1358e-01,\n        -1.7441e-01, -1.4875e-01, -5.6163e-02, -3.3250e-02, -2.6646e-02,\n         2.1082e-03,  1.3442e-02,  9.5665e-02,  1.0434e-01,  1.2852e-01,\n         1.8255e-01,  2.2326e-01,  2.3160e-01,  2.5853e-01,  2.6803e-01,\n         3.3640e-01,  3.6288e-01,  3.7120e-01,  3.8451e-01,  4.0117e-01,\n         4.3763e-01,  4.5193e-01,  5.2404e-01,  6.1333e-01,  6.7461e-01,\n         6.8081e-01,  8.0477e-01,  9.1538e-01,  9.5395e-01,  1.0907e+00,\n         1.1139e+00,  1.1281e+00,  1.2559e+00,  1.2686e+00,  1.3258e+00,\n         1.3563e+00,  1.3864e+00,  1.5558e+00,  1.6258e+00,  2.1654e+00])\n\n\n\nx,_ = torch.randn(100).sort() # ì–¸íŒ¨í‚¹\nx\n\ntensor([-2.8984, -2.6607, -2.2449, -2.2072, -2.1918, -2.1538, -1.9428, -1.9416,\n        -1.8612, -1.6956, -1.6357, -1.4785, -1.4322, -1.2127, -1.1737, -0.9456,\n        -0.9244, -0.8456, -0.8190, -0.7925, -0.7609, -0.7305, -0.7011, -0.6806,\n        -0.6442, -0.6117, -0.6059, -0.5994, -0.4920, -0.4066, -0.3879, -0.3867,\n        -0.3612, -0.3604, -0.3142, -0.3112, -0.2940, -0.2812, -0.2753, -0.2665,\n        -0.2145, -0.2106, -0.1864, -0.1633, -0.1470, -0.1331, -0.1316, -0.0994,\n        -0.0954, -0.0717, -0.0586, -0.0329,  0.0095,  0.0182,  0.0214,  0.0915,\n         0.0952,  0.1077,  0.1124,  0.1612,  0.1614,  0.1969,  0.2003,  0.3242,\n         0.3424,  0.3925,  0.4078,  0.4468,  0.4536,  0.5199,  0.5238,  0.5563,\n         0.5595,  0.6236,  0.6372,  0.6451,  0.6630,  0.7122,  0.7335,  0.7569,\n         0.7589,  0.8969,  0.9318,  0.9552,  1.0023,  1.0198,  1.1083,  1.1978,\n         1.2752,  1.2928,  1.3265,  1.3825,  1.4325,  1.5292,  1.6095,  1.6239,\n         1.7316,  2.0886,  2.3070,  3.2682])\n\n\n\ntorch.manual_seed(43052) # ê°’ ê³ ì •\nx,_ = torch.randn(100).sort()\nx\n\ntensor([-2.4821, -2.3621, -1.9973, -1.6239, -1.4792, -1.4635, -1.4509, -1.4435,\n        -1.3722, -1.3079, -1.1904, -1.1092, -1.1054, -1.0875, -0.9469, -0.9319,\n        -0.8643, -0.7858, -0.7549, -0.7421, -0.6948, -0.6103, -0.5830, -0.5621,\n        -0.5506, -0.5058, -0.4806, -0.4738, -0.4710, -0.4676, -0.3874, -0.3719,\n        -0.3688, -0.3159, -0.2775, -0.2772, -0.2734, -0.2721, -0.2668, -0.2155,\n        -0.2000, -0.1816, -0.1708, -0.1565, -0.1448, -0.1361, -0.1057, -0.0603,\n        -0.0559, -0.0214,  0.0655,  0.0684,  0.1195,  0.1420,  0.1521,  0.1568,\n         0.2646,  0.2656,  0.3157,  0.3220,  0.3461,  0.3984,  0.4190,  0.5443,\n         0.5579,  0.5913,  0.6148,  0.6469,  0.6469,  0.6523,  0.6674,  0.7059,\n         0.7141,  0.7822,  0.8154,  0.8668,  0.9291,  0.9804,  0.9853,  0.9941,\n         1.0376,  1.0393,  1.0697,  1.1024,  1.1126,  1.1532,  1.2289,  1.3403,\n         1.3494,  1.4279,  1.4994,  1.5031,  1.5437,  1.6789,  2.0832,  2.2444,\n         2.3935,  2.6056,  2.6057,  2.6632])\n\n\n\n# temp # ìœ„ì˜ tempì™€ xëŠ” ë™ì¼\n\n\nsales[0] # -2.4821 * 4 + 2.5 + ì˜¤ì°¨\n\n-8.542\n\n\n\ntorch.manual_seed(43052)\nx,_ = torch.randn(100).sort()\neps = torch.randn(100)*0.5 # ì˜¤ì°¨ ë§Œë“¤ê¸° (ë¶„ì‚° ì‘ê²Œí•˜ë ¤ê³  0.5ë¥¼ ê³±í•¨)\n\n\n-2.4821 * 4 + 2.5 + eps[0] # sales[0]ê³¼ ë™ì¼\n\ntensor(-8.5420)\n\n\n\nx[1] * 4 + 2.5 + eps[1] # ë‘ ë²ˆì§¸ ê°’\n\ntensor(-6.5767)\n\n\n\nsales[1]\n\n-6.5767\n\n\n\ntorch.manual_seed(43052)\nx,_ = torch.randn(100).sort()\neps = torch.randn(100)*0.5\ny = x * 4 + 2.5 + eps # ë¸Œë¡œë“œìºìŠ¤íŒ… ì´ìš©\n\n\ntemp[:5],sales[:5]\n\n([-2.4821, -2.3621, -1.9973, -1.6239, -1.4792],\n [-8.542, -6.5767, -5.9496, -4.4794, -4.2516])\n\n\n\nx[:5], y[:5] # ìœ„ì™€ ë™ì¼\n\n(tensor([-2.4821, -2.3621, -1.9973, -1.6239, -1.4792]),\n tensor([-8.5420, -6.5767, -5.9496, -4.4794, -4.2516]))\n\n\n)ğŸ—£ï¸\n\ntorch.manual_seed(43052)\nx,_ = torch.randn(100).sort()\neps = torch.randn(100)*0.5\ny = x * 4 + 2.5 + eps\n\n\nx[:5], y[:5]\n\n(tensor([-2.4821, -2.3621, -1.9973, -1.6239, -1.4792]),\n tensor([-8.5420, -6.5767, -5.9496, -4.4794, -4.2516]))\n\n\n- ë°©ë²•2: \\({\\bf y}={\\bf X}{\\bf W} +\\boldsymbol{\\epsilon}\\)\n\n\\({\\bf y}=\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\dots \\\\ y_n\\end{bmatrix}, \\quad {\\bf X}=\\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\dots \\\\ 1 & x_n\\end{bmatrix}, \\quad {\\bf W}=\\begin{bmatrix} 2.5 \\\\ 4 \\end{bmatrix}, \\quad \\boldsymbol{\\epsilon}= \\begin{bmatrix} \\epsilon_1 \\\\ \\dots \\\\ \\epsilon_n\\end{bmatrix}\\)\n\nğŸ—£ï¸(\n\\(y_1 = 2.5 + 4x_1 + \\epsilon_1\\)\n\\(y_2 = 2.5 + 4x_2 + \\epsilon_2\\)\n\\(y_3 = 2.5 + 4x_3 + \\epsilon_3\\) â€¦ ì„ ìœ„ì™€ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆìŒ\në°©ë²•1ì€ scalarë¡œ í‘œí˜„, ë°©ë²•2ëŠ” matrixë¡œ í‘œí˜„\n\ny # ê¸¸ì´ê°€ 100ì¸ vector (ë°©ë²•1) / ë°©ë²•2ëŠ” (100,1) matrixë¡œ í‘œí˜„ë˜ì–´ì•¼ í•¨\n\ntensor([-8.5420, -6.5767, -5.9496, -4.4794, -4.2516, -3.1326, -4.0239, -4.1862,\n        -3.3403, -2.2027, -2.0262, -2.5619, -1.3353, -2.0466, -0.4664, -1.3513,\n        -1.6472, -0.1089, -0.3071, -0.6299, -0.0438,  0.4163,  0.4166, -0.0943,\n         0.2662,  0.4591,  0.8905,  0.8998,  0.6314,  1.3845,  0.8085,  1.2594,\n         1.1211,  1.9232,  1.0619,  1.3552,  2.1161,  1.1437,  1.6245,  1.7639,\n         1.6022,  1.7465,  0.9830,  1.7824,  2.1116,  2.8621,  2.1165,  1.5226,\n         2.5572,  2.8361,  3.3956,  2.0679,  2.8140,  3.4852,  3.6059,  2.5966,\n         2.8854,  3.9173,  3.6527,  4.1029,  4.3125,  3.4026,  3.2180,  4.5686,\n         4.3772,  4.3075,  4.4895,  4.4827,  5.3170,  5.4987,  5.4632,  6.0328,\n         5.2842,  5.0539,  5.4538,  6.0337,  5.7250,  5.7587,  6.2020,  6.5992,\n         6.4621,  6.5140,  6.6846,  7.3497,  8.0909,  7.0794,  6.8667,  7.4229,\n         7.2544,  7.1967,  9.5006,  9.0339,  7.4887,  9.0759, 11.0946, 10.3260,\n        12.2665, 13.0983, 12.5468, 13.8340])\n\n\n\nx # ê¸¸ì´ê°€ 100ì¸ vector (ë°©ë²•1) / ë°©ë²•2ëŠ” [1 x] ì´ëŸ°ì‹ìœ¼ë¡œ í‘œí˜„ë˜ì–´ì•¼ í•¨\n\ntensor([-2.4821, -2.3621, -1.9973, -1.6239, -1.4792, -1.4635, -1.4509, -1.4435,\n        -1.3722, -1.3079, -1.1904, -1.1092, -1.1054, -1.0875, -0.9469, -0.9319,\n        -0.8643, -0.7858, -0.7549, -0.7421, -0.6948, -0.6103, -0.5830, -0.5621,\n        -0.5506, -0.5058, -0.4806, -0.4738, -0.4710, -0.4676, -0.3874, -0.3719,\n        -0.3688, -0.3159, -0.2775, -0.2772, -0.2734, -0.2721, -0.2668, -0.2155,\n        -0.2000, -0.1816, -0.1708, -0.1565, -0.1448, -0.1361, -0.1057, -0.0603,\n        -0.0559, -0.0214,  0.0655,  0.0684,  0.1195,  0.1420,  0.1521,  0.1568,\n         0.2646,  0.2656,  0.3157,  0.3220,  0.3461,  0.3984,  0.4190,  0.5443,\n         0.5579,  0.5913,  0.6148,  0.6469,  0.6469,  0.6523,  0.6674,  0.7059,\n         0.7141,  0.7822,  0.8154,  0.8668,  0.9291,  0.9804,  0.9853,  0.9941,\n         1.0376,  1.0393,  1.0697,  1.1024,  1.1126,  1.1532,  1.2289,  1.3403,\n         1.3494,  1.4279,  1.4994,  1.5031,  1.5437,  1.6789,  2.0832,  2.2444,\n         2.3935,  2.6056,  2.6057,  2.6632])\n\n\n[1 x] ë§Œë“¤ê¸°\n\ntorch.ones(100) , x # ê¸¸ì´ê°€ 100ì¸ vector\n\n(tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n tensor([-2.4821, -2.3621, -1.9973, -1.6239, -1.4792, -1.4635, -1.4509, -1.4435,\n         -1.3722, -1.3079, -1.1904, -1.1092, -1.1054, -1.0875, -0.9469, -0.9319,\n         -0.8643, -0.7858, -0.7549, -0.7421, -0.6948, -0.6103, -0.5830, -0.5621,\n         -0.5506, -0.5058, -0.4806, -0.4738, -0.4710, -0.4676, -0.3874, -0.3719,\n         -0.3688, -0.3159, -0.2775, -0.2772, -0.2734, -0.2721, -0.2668, -0.2155,\n         -0.2000, -0.1816, -0.1708, -0.1565, -0.1448, -0.1361, -0.1057, -0.0603,\n         -0.0559, -0.0214,  0.0655,  0.0684,  0.1195,  0.1420,  0.1521,  0.1568,\n          0.2646,  0.2656,  0.3157,  0.3220,  0.3461,  0.3984,  0.4190,  0.5443,\n          0.5579,  0.5913,  0.6148,  0.6469,  0.6469,  0.6523,  0.6674,  0.7059,\n          0.7141,  0.7822,  0.8154,  0.8668,  0.9291,  0.9804,  0.9853,  0.9941,\n          1.0376,  1.0393,  1.0697,  1.1024,  1.1126,  1.1532,  1.2289,  1.3403,\n          1.3494,  1.4279,  1.4994,  1.5031,  1.5437,  1.6789,  2.0832,  2.2444,\n          2.3935,  2.6056,  2.6057,  2.6632]))\n\n\n\n# torch.stack([torch.ones(100) , x]) # ì¢Œìš°ë¡œ í•©ì¹˜ê¸° ìœ„í•´ stack ì‚¬ìš©\nprint(torch.stack([torch.ones(100) , x]).shape)\n# torch.stack([torch.ones(100) , x], axis=1) # ì›í–ˆë˜ ê²°ê³¼\nprint(torch.stack([torch.ones(100) , x], axis=1).shape)\n\n# torch.stack([torch.ones(100) , x]).T # ë‹¤ë¥¸ ë°©ë²•\nprint(torch.stack([torch.ones(100) , x]).T.shape)\n\ntorch.Size([2, 100])\ntorch.Size([100, 2])\ntorch.Size([100, 2])\n\n\n\nX = torch.stack([torch.ones(100) , x], axis=1)\nW = torch.tensor([[2.5],[4.0]])\ny = X@W + eps.reshape(100,1)\ny.shape\n\ntorch.Size([100, 1])\n\n\n\nsales[:5]\n\n[-8.542, -6.5767, -5.9496, -4.4794, -4.2516]\n\n\n\ny[:5,0]\n\ntensor([-8.5420, -6.5767, -5.9496, -4.4794, -4.2516])\n\n\nsalesì™€ y ë™ì¼\nğŸ”¬ğŸ—£ï¸(\n\n(ì°¸ê³ ) ì¸ë±ì‹± ê´€ë ¨ ì„¤ëª…\n\n\ny[:5]\n\ntensor([[-8.5420],\n        [-6.5767],\n        [-5.9496],\n        [-4.4794],\n        [-4.2516]])\n\n\nyëŠ” matrix ì´ë¯€ë¡œ\n\ny[:5,[0]] # column vectorì²˜ëŸ¼ ë¨\n\ntensor([[-8.5420],\n        [-6.5767],\n        [-5.9496],\n        [-4.4794],\n        [-4.2516]])\n\n\n\n# y[:,:] # yê°€ ê·¸ëŒ€ë¡œ ë‚˜ì˜´\n\n\ny[:5,:] # ê·¸ ì¤‘ 5ê°œë§Œ\n\ntensor([[-8.5420],\n        [-6.5767],\n        [-5.9496],\n        [-4.4794],\n        [-4.2516]])\n\n\në‚˜ì—´ ë°©ì‹ë§Œ ë‹¤ë¥´ê³  ê°’ì€ salesì™€ ë˜‘ê°™ìŒ\n)ğŸ”¬ğŸ—£ï¸\n\nX = torch.stack([torch.ones(100) , x], axis=1) # (100, 2)\nW = torch.tensor([[2.5],[4.0]]) # (2, 1)\ny = X@W + eps.reshape(100,1) # (100, 1)\nx # ì•„ë§ˆë„ (100,) \n\ntensor([-2.4821, -2.3621, -1.9973, -1.6239, -1.4792, -1.4635, -1.4509, -1.4435,\n        -1.3722, -1.3079, -1.1904, -1.1092, -1.1054, -1.0875, -0.9469, -0.9319,\n        -0.8643, -0.7858, -0.7549, -0.7421, -0.6948, -0.6103, -0.5830, -0.5621,\n        -0.5506, -0.5058, -0.4806, -0.4738, -0.4710, -0.4676, -0.3874, -0.3719,\n        -0.3688, -0.3159, -0.2775, -0.2772, -0.2734, -0.2721, -0.2668, -0.2155,\n        -0.2000, -0.1816, -0.1708, -0.1565, -0.1448, -0.1361, -0.1057, -0.0603,\n        -0.0559, -0.0214,  0.0655,  0.0684,  0.1195,  0.1420,  0.1521,  0.1568,\n         0.2646,  0.2656,  0.3157,  0.3220,  0.3461,  0.3984,  0.4190,  0.5443,\n         0.5579,  0.5913,  0.6148,  0.6469,  0.6469,  0.6523,  0.6674,  0.7059,\n         0.7141,  0.7822,  0.8154,  0.8668,  0.9291,  0.9804,  0.9853,  0.9941,\n         1.0376,  1.0393,  1.0697,  1.1024,  1.1126,  1.1532,  1.2289,  1.3403,\n         1.3494,  1.4279,  1.4994,  1.5031,  1.5437,  1.6789,  2.0832,  2.2444,\n         2.3935,  2.6056,  2.6057,  2.6632])\n\n\n(100,)ì„ (100,1)ë¡œ ë°”ê¾¸ê³  ì‹¶ìŒ\n\ntorch.manual_seed(43052)\nx,_ = torch.randn(100).sort()\neps = torch.randn(100)*0.5\ny = x * 4 + 2.5 + eps\n\nX = torch.stack([torch.ones(100) , x], axis=1) # (100, 2)\nW = torch.tensor([[2.5],[4.0]]) # (2, 1)\ny = X@W + eps.reshape(100,1) # (100, 1)\nx = X[:,[1]]\n\n\nx[:5], y[:5]\n\n(tensor([[-2.4821],\n         [-2.3621],\n         [-1.9973],\n         [-1.6239],\n         [-1.4792]]),\n tensor([[-8.5420],\n         [-6.5767],\n         [-5.9496],\n         [-4.4794],\n         [-4.2516]]))\n\n\n\ntemp[:5], sales[:5]\n\n([-2.4821, -2.3621, -1.9973, -1.6239, -1.4792],\n [-8.542, -6.5767, -5.9496, -4.4794, -4.2516])\n\n\në°©ë²• 2ì²˜ëŸ¼ matrixë¡œë„ ê°€ëŠ¥í•˜ë‹¤ëŠ” ê²ƒì„ í™•ì¸\n)ğŸ—£ï¸\nğŸ“(\nX = torch.stack([torch.ones(100),x],axis=1)\nW = torch.tensor([[2.5],[4.0]])\ny = X@W + eps.reshape(100,1)\nx = X[:,[1]]\nâœï¸ í¸ì˜ìƒ ìœ„ì˜ ì½”ë“œëŠ” ì‹¤í–‰ì‹œí‚¤ì§€ ì•ŠìŒ\n\nX[:5,:], y[:5,:]\n\n(tensor([[ 1.0000, -2.4821],\n         [ 1.0000, -2.3621],\n         [ 1.0000, -1.9973],\n         [ 1.0000, -1.6239],\n         [ 1.0000, -1.4792]]),\n tensor([[-8.5420],\n         [-6.5767],\n         [-5.9496],\n         [-4.4794],\n         [-4.2516]]))\n\n\n)ğŸ“\n- tureì™€ observed dataë¥¼ ë™ì‹œì— ì‹œê°í™”\nğŸ—£ï¸(\n\nplt.plot(temp, sales) # ì´ëŸ¬í•œ ë°ì´í„°ë¥¼ ê´€ì¸¡í–ˆë‹¤ê³  ìƒê°\n\n\n\n\n\n\n\n\n\nplt.plot(temp, sales, 'o') # scatter plot\n\n\n\n\n\n\n\n\n\nplt.plot(x, y, 'o') # ìœ„ì™€ ë™ì¼\n\n\n\n\n\n\n\n\nxì—ì„œ yë¡œ ê°€ëŠ” íŒ¨í„´ì„ ì°¾ê³  ì‹¶ìŒ\n\nplt.plot(x, y, 'o', label=\"observed data\") # ê´€ì¸¡í•œ ê°’\nplt.legend()\n\n\n\n\n\n\n\n\n\nplt.plot(x, y, 'o', label=\"observed data\") # ì ì„  + epsilon(í†µê³„ì ìœ¼ë¡œ ì„¤ëª…í•  ìˆ˜ ì—†ëŠ” í˜„ìƒ, random)\nplt.plot(x, 2.5 + 4*x, '--', label=\"true\") # ì›ë˜ ê´€ì¸¡ë˜ì–´ì•¼ í–ˆë˜ ê°’\nplt.legend()\n\n\n\n\n\n\n\n\n\ní•˜ê³  ì‹¶ì€ ê²ƒ\n\nì¹´í˜ ì£¼ì¸: ì˜¨ë„ê°€ 0.5ì¼ ë•Œ ì–¼ë§ˆë‚˜ íŒ”ë¦´ì§€ ì•Œê³  ì‹¶ìŒ\nê°€ì¥ ê°„ë‹¨: 0.5ë¥¼ ì ì„  ìœ„ì— ì˜¬ë¦° í›„ y ê°’ì„ ì˜ˆì¸¡ (0.5 * 4 + 2.5 = 4.5)\ní•˜ì§€ë§Œ ì‹¤ì œë¡œëŠ” íŒŒë€ìƒ‰ë§Œ ì•Œê³  ìˆìœ¼ë¯€ë¡œ ìœ„ì˜ ë°©ë²•ì€ cheating\n\n\n\nplt.plot(x,y,'o',label=r\"observed data: $(x_i,y_i)$\")\n#plt.plot(x,2.5+4*x,'--',label=r\"true: $(x_i, 4x_i+2.5)$ // $y=4x+2.5$ \")\nplt.legend()\n\n\n\n\n\n\n\n\n\ní•˜ê³  ì‹¶ì€ ê²ƒ\n\nìœ„ì˜ ìƒíƒœì—ì„œ ì ë‹¹í•œ ì¶”ì„¸ì„ ì„ ê·¸ë ¤ì„œ ì¶”ì •\n\n\n)ğŸ—£ï¸\n\nplt.plot(x,y,'o',label=r\"observed data: $(x_i,y_i)$\")\n#plt.plot(x,2.5+4*x,'--',label=r\"true: $(x_i, 4x_i+2.5)$ // $y=4x+2.5$ \")\nplt.legend()"
  },
  {
    "objectID": "posts/01wk-2.html#c.-íšŒê·€ë¶„ì„ì´ë€",
    "href": "posts/01wk-2.html#c.-íšŒê·€ë¶„ì„ì´ë€",
    "title": "01wk-2, 02wk-1: (íšŒê·€) â€“ íšŒê·€ëª¨í˜•, ì†ì‹¤í•¨ìˆ˜, íŒŒì´í† ì¹˜ë¥¼ ì´ìš©í•œ ì¶”ì •",
    "section": "C. íšŒê·€ë¶„ì„ì´ë€?",
    "text": "C. íšŒê·€ë¶„ì„ì´ë€?\n- í´ë¦¬ì…°: ê´€ì¸¡í•œ ìë£Œ \\((x_i,y_i)\\) ê°€ ìˆìŒ \\(\\to\\) ìš°ë¦¬ëŠ” \\((x_i,y_i)\\)ì˜ ê´€ê³„ë¥¼ íŒŒì•…í•˜ì—¬ ìƒˆë¡œìš´ \\(x\\)ê°€ ì™”ì„ë•Œ ê·¸ê²ƒì— ëŒ€í•œ ì˜ˆì¸¡ê°’(predicted value) \\(\\hat{y}\\)ì„ ì•Œì•„ë‚´ëŠ” ë²•ì¹™ì„ ì•Œê³  ì‹¶ìŒ \\(\\to\\) ê´€ê³„ë¥¼ íŒŒì•…í•˜ê¸° ìœ„í•´ì„œ \\((x_i, y_i)\\)ì˜ ì‚°ì ë„ë¥¼ ê·¸ë ¤ë³´ë‹ˆ \\(x_i\\)ì™€ \\(y_i\\)ëŠ” ì„ í˜•ì„±ì„ ê°€ì§€ê³  ìˆë‹¤ëŠ” ê²ƒì´ íŒŒì•…ë¨ \\(\\to\\) ì˜¤ì°¨í•­ì´ ë“±ë¶„ì‚°ì„±ì„ ê°€ì§€ê³  ì–´ì©Œê³  ì €ì©Œê³ â€¦ \\(\\to\\) í•˜ì—¬íŠ¼ \\((x_i,y_i)\\) ë¥¼ â€œì ë‹¹íˆ ì˜ ê´€í†µí•˜ëŠ”â€ ì–´ë– í•œ í•˜ë‚˜ì˜ ì¶”ì„¸ì„ ì„ ì˜ ì¶”ì •í•˜ë©´ ëœë‹¤.\n- íšŒê·€ë¶„ì„ì´ë€ ì‚°ì ë„ë¥¼ ë³´ê³  ì ë‹¹í•œ ì¶”ì„¸ì„ ì„ ì°¾ëŠ” ê²ƒì´ë‹¤. ì¢€ ë” ì •í™•í•˜ê²Œ ë§í•˜ë©´ \\((x_1,y_1) \\dots (x_n,y_n)\\) ìœ¼ë¡œ \\(\\begin{bmatrix} \\hat{w}_0 \\\\ \\hat{w}_1 \\end{bmatrix}\\) ë¥¼ ìµœëŒ€í•œ \\(\\begin{bmatrix} 2.5 \\\\ 4 \\end{bmatrix}\\)ì™€ ë¹„ìŠ·í•˜ê²Œ ì°¾ëŠ” ê²ƒ.\n\ngiven data : \\(\\big\\{(x_i,y_i) \\big\\}_{i=1}^{n}\\)\nparameter: \\({\\bf W}=\\begin{bmatrix} w_0 \\\\ w_1 \\end{bmatrix}\\)\nestimated parameter: \\({\\bf \\hat{W}}=\\begin{bmatrix} \\hat{w}_0 \\\\ \\hat{w}_1 \\end{bmatrix}\\)\n\nğŸ—£ï¸ y = ax + b ê¼´ì—ì„œ a, bë¥¼ ì •í•¨\n- ë” ì‰½ê²Œ ë§í•˜ë©´ ì•„ë˜ì˜ ê·¸ë¦¼ì„ ë³´ê³  â€œì ë‹¹í•œâ€ ì¶”ì„¸ì„ ì„ ì°¾ëŠ” ê²ƒì´ë‹¤.\n\nplt.plot(x,y,'o',label=r\"observed data: $(x_i,y_i)$\")\nplt.legend()\n\n\n\n\n\n\n\n\n- ì¶”ì„¸ì„ ì„ ê·¸ë¦¬ëŠ” í–‰ìœ„ = \\((w_0,w_1)\\)ì„ ì„ íƒí•˜ëŠ”ì¼"
  },
  {
    "objectID": "posts/01wk-2.html#a.-1ë‹¨ê³„-ìµœì´ˆì˜-ì ì„ ",
    "href": "posts/01wk-2.html#a.-1ë‹¨ê³„-ìµœì´ˆì˜-ì ì„ ",
    "title": "01wk-2, 02wk-1: (íšŒê·€) â€“ íšŒê·€ëª¨í˜•, ì†ì‹¤í•¨ìˆ˜, íŒŒì´í† ì¹˜ë¥¼ ì´ìš©í•œ ì¶”ì •",
    "section": "A. 1ë‹¨ê³„ â€“ ìµœì´ˆì˜ ì ì„ ",
    "text": "A. 1ë‹¨ê³„ â€“ ìµœì´ˆì˜ ì ì„ \nğŸ—£ï¸(\n\nWhat = torch.tensor([[-5.0],[10.0]], requires_grad=True)\nWhat\n\ntensor([[-5.],\n        [10.]], requires_grad=True)\n\n\n\nyhat = X@What\n\n\n# plt.plot(x, y, 'o')\n# plt.plot(x, yhat, '--')\n\n\nì‹¤í–‰ì‹œí‚¤ë©´ error\nrequires_grad=Trueë¥¼ ì—†ì• ë©´ error ë°œìƒ X\nrequires_grad=True\n\në¯¸ë¶„ì´ í•„ìš”í•¨ì„ ë‚˜íƒ€ë‚´ëŠ” ì˜µì…˜\nì§€ê¸ˆì€ ì˜ë¯¸ë¥¼ ì •í™•í•˜ê²Œ ì•Œ ìˆ˜ ì—†ì§€ë§Œ í¸ì˜ìƒ ì´ë¦„ì„ ë¯¸ë¶„ê¼¬ë¦¬í‘œë¼ê³  ë¶€ë¥´ê² ìŒ\n\n\n\nWhat+1\n\ntensor([[-4.],\n        [11.]], grad_fn=&lt;AddBackward0&gt;)\n\n\n\nê¼¬ë¦¬í‘œê°€ ë°”ë€Œê¸´ í•˜ë‚˜ í° ì§€ì¥ì€ ì—†ìŒ\n\n\n# yhat\n\n\nyhatì„ ì‹¤í–‰ì‹œì¼œë„ ê³„ì‚°ì„ ì˜ ë˜ë‚˜ ê¼¬ë¦¬í‘œê°€ ìˆìŒ\nê¼¬ë¦¬í‘œ ë•Œë¬¸ì— ê·¸ë˜í”„ë¥¼ ê·¸ë¦¬ë©´ errorê°€ ë°œìƒ\ní•´ê²°ì±… (ê¼¬ë¦¬í‘œë¥¼ ì œê±°í•œë‹¤ê³  ìƒê°, ê¼¬ë¦¬í‘œê°€ ìˆìœ¼ë©´ ê³„ì‚°ì€ ê°€ëŠ¥í•˜ë‚˜ ê·¸ë˜í”„ ê·¸ë¦¬ê¸° ë¶ˆê°€ëŠ¥)\n\nRuntimeError: Canâ€™t call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.\n.data\n\n\n\n# yhat.detach()\n\n\n# yhat.data\n\n\nplt.plot(x, y, 'o')\nplt.plot(x, yhat.detach(), '--') # ê·¸ë¦¼ì„ ê·¸ë¦¬ê¸° ìœ„í•´ì„œ yhatì˜ ë¯¸ë¶„ê¼¬ë¦¬í‘œë¥¼ ì œê±°\n\n\n\n\n\n\n\n\n)ğŸ—£ï¸\nğŸ—£ï¸ ê·¸ëƒ¥ ì•„ë¬´ ì§ì„ ì„ ê·¸ìŒ (2ë‹¨ê³„ë§Œ ì˜ ë˜ë©´ ìƒê´€ X)\n\nWhat = torch.tensor([[-5.0],[10.0]])\nWhat\n\ntensor([[-5.],\n        [10.]])\n\n\n\nyhat = X@What \n\n\nplt.plot(x,y,'o')\nplt.plot(x,yhat.data,'--')"
  },
  {
    "objectID": "posts/01wk-2.html#b.-2ë‹¨ê³„-update",
    "href": "posts/01wk-2.html#b.-2ë‹¨ê³„-update",
    "title": "01wk-2, 02wk-1: (íšŒê·€) â€“ íšŒê·€ëª¨í˜•, ì†ì‹¤í•¨ìˆ˜, íŒŒì´í† ì¹˜ë¥¼ ì´ìš©í•œ ì¶”ì •",
    "section": "B. 2ë‹¨ê³„ â€“ update",
    "text": "B. 2ë‹¨ê³„ â€“ update\n- â€™ì ë‹¹í•œ ì •ë„â€™ë¥¼ íŒë‹¨í•˜ê¸° ìœ„í•œ ì¥ì¹˜: loss function ë„ì…!\n\\[loss=\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2=\\sum_{i=1}^{n}(y_i-(\\hat{w}_0+\\hat{w}_1x_i))^2=({\\bf y}-{\\bf\\hat{y}})^\\top({\\bf y}-{\\bf\\hat{y}})=({\\bf y}-{\\bf X}{\\bf \\hat{W}})^\\top({\\bf y}-{\\bf X}{\\bf \\hat{W}})\\]\nğŸ—£ï¸ lossëŠ” \\((\\hat{w}_0, \\hat{w}_1)\\)ì„ ì…ë ¥ìœ¼ë¡œ ë°›ìŒ. loss ê°’ì„ ìµœì†Œë¡œ ë§Œë“œëŠ” \\((\\hat{w}_0, \\hat{w}_1)\\)ì„ ì°¾ìœ¼ë©´ ë¨.\n- loss í•¨ìˆ˜ì˜ íŠ¹ì§•: ìœ„ ê·¸ë¦¼ì˜ ì£¼í™©ìƒ‰ ì ì„ ì´ â€˜ì ë‹¹í•  ìˆ˜ë¡â€™ lossê°’ì´ ì‘ë‹¤.\n\nplt.plot(x,y,'o')\nplt.plot(x,yhat)\n\n\n\n\n\n\n\n\n\nloss = torch.sum((y-yhat)**2)\nloss\n\ntensor(8587.6875)\n\n\n- ìš°ë¦¬ì˜ ëª©í‘œ: ì´ loss(=8587.6275)ì„ ë” ì¤„ì´ì.\n\nê¶ê·¹ì ìœ¼ë¡œëŠ” ì•„ì˜ˆ ëª¨ë“  ì¡°í•© \\((\\hat{w}_0,\\hat{w}_1)\\)ì— ëŒ€í•˜ì—¬ ê°€ì¥ ì‘ì€ lossë¥¼ ì°¾ìœ¼ë©´ ì¢‹ê² ë‹¤.\n\n- ë¬¸ì œì˜ ì¹˜í™˜: ìƒê°í•´ë³´ë‹ˆê¹Œ ìš°ë¦¬ì˜ ë¬¸ì œëŠ” ì•„ë˜ì™€ ê°™ì´ ìˆ˜í•™ì ìœ¼ë¡œ ë‹¨ìˆœí™” ë˜ì—ˆë‹¤.\n\nê°€ì¥ ì ë‹¹í•œ ì£¼í™©ìƒ‰ ì„ ì„ ì°¾ì \\(\\to\\) \\(loss(\\hat{w}_0,\\hat{w}_1)\\)ë¥¼ ìµœì†Œë¡œí•˜ëŠ” \\((\\hat{w}_0,\\hat{w}_1)\\)ì˜ ê°’ì„ ì°¾ì.\n\n- ìˆ˜ì •ëœ ëª©í‘œ: \\(loss(\\hat{w}_0,\\hat{w}_1)\\)ë¥¼ ìµœì†Œë¡œ í•˜ëŠ” \\((\\hat{w}_0,\\hat{w}_1)\\)ì„ êµ¬í•˜ë¼.\n\në‹¨ìˆœí•œ ìˆ˜í•™ë¬¸ì œê°€ ë˜ì—ˆë‹¤. ì´ê²ƒì€ ë§ˆì¹˜ \\(f(x,y)\\)ë¥¼ ìµœì†Œí™”í•˜ëŠ” \\((x,y)\\)ë¥¼ ì°¾ìœ¼ë¼ëŠ” ê²ƒì„.\ní•¨ìˆ˜ì˜ ìµœëŒ€ê°’ í˜¹ì€ ìµœì†Œê°’ì„ ì»´í“¨í„°ë¥¼ ì´ìš©í•˜ì—¬ ì°¾ëŠ”ê²ƒì„ â€œìµœì í™”â€ë¼ê³  í•˜ë©° ì´ëŠ” ì‚°ê³µêµìˆ˜ë‹˜ë“¤ì´ ê°€ì¥ ì˜í•˜ëŠ” ë¶„ì•¼ì„. (ì‚°ê³µêµìˆ˜ë‹˜ë“¤ì—ê²Œ ë¶€íƒí•˜ë©´ ì˜í•´ì¤Œ, ì‚°ê³µêµìˆ˜ë‹˜ë“¤ì€ ë³´í†µ ìµœì í™”í•´ì„œ ì–´ë””ì— ì“¸ì§€ë³´ë‹¤ ìµœì í™” ìì²´ì— ë” ê´€ì‹¬ì„ ê°€ì§€ê³  ì—°êµ¬í•˜ì‹¬)\nìµœì í™”ë¥¼ í•˜ëŠ” ë°©ë²•? ê²½ì‚¬í•˜ê°•ë²•\n\n# ê²½ì‚¬í•˜ê°•ë²• ì•„ì´ë””ì–´ (1ì°¨ì›)\n\nì„ì˜ì˜ ì ì„ ì°ëŠ”ë‹¤.\nê·¸ ì ì—ì„œ ìˆœê°„ê¸°ìš¸ê¸°ë¥¼ êµ¬í•œë‹¤. (ì ‘ì„ ) &lt;â€“ ë¯¸ë¶„\nìˆœê°„ê¸°ìš¸ê¸°(=ë¯¸ë¶„ê³„ìˆ˜)ì˜ ë¶€í˜¸ë¥¼ ì‚´í´ë³´ê³  ë¶€í˜¸ì™€ ë°˜ëŒ€ë°©í–¥ìœ¼ë¡œ ì›€ì§ì¸ë‹¤.\n\n\níŒ: ê¸°ìš¸ê¸°ì˜ ì ˆëŒ€ê°’ í¬ê¸°ì™€ ë¹„ë¡€í•˜ì—¬ ë³´í­(=ì›€ì§ì´ëŠ” ì •ë„)ì„ ì¡°ì ˆí•œë‹¤. \\(\\to\\) \\(\\alpha\\)ë¥¼ ë„ì…\n\n\nìµœì¢…ìˆ˜ì‹: \\(\\hat{w} \\leftarrow \\hat{w} - \\alpha \\times \\frac{\\partial}{\\partial w}loss(w)\\)\n\n#\nğŸ—£ï¸(\n\në³´í­: step size\ní•¨ìˆ˜ë¥¼ ìµœê³ ì°¨í•­ì´ ì–‘ìˆ˜ì¸ 2ì°¨ í•¨ìˆ˜ë¡œ ìƒê°í•˜ë©´ ì´í•´í•˜ê¸° ì‰¬ì›€\n\nxì—ì„œ aë§Œí¼ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì´ë™: x + a\nxì—ì„œ aë§Œí¼ ì™¼ìª½ìœ¼ë¡œ ì´ë™: x - a\në¯¸ë¶„ê³„ìˆ˜ê°€ 0ì¸ìª½ìœ¼ë¡œ ì›€ì§ì¼ ë•Œ\n\nxê°€ ì˜¤ë¥¸ìª½ì— ìˆìœ¼ë©´ ë¯¸ë¶„ê³„ìˆ˜ &gt; 0\nxê°€ ì™¼ìª½ì— ìˆìœ¼ë©´ ë¯¸ë¶„ê³„ìˆ˜ &lt; 0\n\në¯¸ë¶„ê³„ìˆ˜ê°€ 0ì¸ìª½ê³¼ ê°€ê¹Œìš¸ìˆ˜ë¡ ì ‘ì„  ê¸°ìš¸ê¸°ì˜ ì ˆëŒ€ê°’ì´ ì‘ì•„ì§ -&gt; \\(\\alpha\\)ë¡œ ì¡°ì ˆ\n\n\\(\\alpha\\)ê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ ìˆ˜ë ´ ì†ë„ê°€ ëŠë¦´ ìˆ˜ ìˆê³ , ë„ˆë¬´ í¬ë©´ ìˆ˜ë ´ì„ ì•ˆí•  ìˆ˜ ìˆìŒ\n\nì˜ˆì‹œ) \\(f(x) = x^2\\) ì—ì„œ \\(x=2\\)ì¼ ë•Œ \\(\\alpha = 1\\)ì´ë©´ \\(x\\)ëŠ” \\(-2\\)ì™€ \\(2\\)ë§Œ ì™”ë‹¤ê°”ë‹¤ í•¨\n\n\n)ğŸ—£ï¸\n# ê²½ì‚¬í•˜ê°•ë²• ì•„ì´ë””ì–´ (2ì°¨ì›)\n\n\nì„ì˜ì˜ ì ì„ ì°ëŠ”ë‹¤.\nê·¸ ì ì—ì„œ ìˆœê°„ê¸°ìš¸ê¸°ë¥¼ êµ¬í•œë‹¤. (ì ‘í‰ë©´) &lt;â€“ í¸ë¯¸ë¶„\nìˆœê°„ê¸°ìš¸ê¸°(=ë¯¸ë¶„ê³„ìˆ˜)ì˜ ë¶€í˜¸ë¥¼ ì‚´í´ë³´ê³  ë¶€í˜¸ì™€ ë°˜ëŒ€ë°©í–¥ìœ¼ë¡œ ê°ê° ì›€ì§ì¸ë‹¤.\n\n\níŒ: ì—¬ê¸°ì„œë„ ê¸°ìš¸ê¸°ì˜ ì ˆëŒ€ê°’ í¬ê¸°ì™€ ë¹„ë¡€í•˜ì—¬ ë³´í­(=ì›€ì§ì´ëŠ” ì •ë„)ì„ ê°ê° ì¡°ì ˆí•œë‹¤. \\(\\to\\) \\(\\alpha\\)ë¥¼ ë„ì….\n\n#\nğŸ—£ï¸(\n\nì—¬ê¸°ì„œ ì„ì˜ì˜ ì ì€ 2ì°¨ì›\ní¸ë¯¸ë¶„: í•˜ë‚˜ë§Œ ë³€ìˆ˜ë¡œ ë³´ê³  ë‚˜ë¨¸ì§€ ê³ ì •\n\nì´í›„ 1ì°¨ì› ë°©ì‹ê³¼ ë™ì¼\nì–´ë–¤ ë°©í–¥(ì™¼ìª½, ì˜¤ë¥¸ìª½)ìœ¼ë¡œ ì–¼ë§ˆë‚˜ ê°ˆ ì§€(\\(\\alpha\\))\n\n\n)ğŸ—£ï¸\n- ê²½ì‚¬í•˜ê°•ë²• = lossë¥¼ ì¤„ì´ë„ë¡ \\({\\bf \\hat{W}}\\)ë¥¼ ê°œì„ í•˜ëŠ” ë°©ë²•\n\nì—…ë°ì´íŠ¸ ê³µì‹: ìˆ˜ì •ê°’ = ì›ë˜ê°’ - \\(\\alpha\\) \\(\\times\\) ê¸°ìš¸ì–´ì§„í¬ê¸°(=ë¯¸ë¶„ê³„ìˆ˜)\nì—¬ê¸°ì—ì„œ \\(\\alpha\\)ëŠ” ì „ì²´ì ì¸ ë³´í­ì˜ í¬ê¸°ë¥¼ ê²°ì •í•œë‹¤. ì¦‰ \\(\\alpha\\)ê°’ì´ í´ìˆ˜ë¡ í•œë²ˆì˜ updateì— ì›€ì§ì´ëŠ” ì–‘ì´ í¬ë‹¤.\n\nğŸ—£ï¸ \\(\\alpha\\)ë¥¼ MLì—ì„œëŠ” í•™ìŠµë¥ ì´ë¼ê³  í•¨\n- lossëŠ” \\(\\hat{\\bf W} =\\begin{bmatrix} \\hat{w}_0 \\\\ \\hat{w}_1 \\end{bmatrix}\\) ì— ë”°ë¼ì„œ ê°’ì´ ë°”ë€ŒëŠ” í•¨ìˆ˜ë¡œ í•´ì„ê°€ëŠ¥í•˜ê³  êµ¬ì²´ì ì¸ í˜•íƒœëŠ” ì•„ë˜ì™€ ê°™ìŒ.\n\\[ loss(\\hat{w}_0,\\hat{w}_1) := loss(\\hat{\\bf W})=\\sum_{i=1}^{n}(y_i-(\\hat{w}_0+\\hat{w}_1x_i))^2=({\\bf y}-{\\bf X}{\\bf \\hat{W}})^\\top({\\bf y}-{\\bf X}{\\bf \\hat{W}})\\]\në”°ë¼ì„œ êµ¬í•˜ê³  ì‹¶ì€ê²ƒì€ ì•„ë˜ì™€ ê°™ìŒ\n\\[\\hat{\\bf W}^{LSE} = \\underset{\\bf \\hat{W}}{\\operatorname{argmin}} ~ loss(\\hat{\\bf W})\\]\n\n\n\n\n\n\nWarning\n\n\n\nì•„ë˜ì˜ ìˆ˜ì‹\n\\[\\hat{\\bf W}^{LSE} = \\underset{\\bf \\hat{W}}{\\operatorname{argmin}} ~ loss(\\hat{\\bf W})\\]\nì€ ì•„ë˜ì™€ ê°™ì´ í‘œí˜„í•´ë„ ë¬´ë°©í•©ë‹ˆë‹¤.\n\\[\\hat{\\bf W} = \\underset{\\bf W}{\\operatorname{argmin}} ~ loss({\\bf W})\\]\në§ˆì¹˜ í•¨ìˆ˜ \\(f(\\hat{x})=({\\hat x}-1)^2\\) ì„ \\(f(x)=(x-1)^2\\) ì´ë¼ê³  í‘œí˜„í•  ìˆ˜ ìˆëŠ” ê²ƒ ì²˜ëŸ¼ìš”..\n\n\nì—¬ê¸°ê¹Œì§€ 01wk-2ì—ì„œ ìˆ˜ì—…í–ˆìŠµë‹ˆë‹¤~\n\nì—¬ê¸°ë¶€í„°ëŠ” 02wk-1ì—ì„œ..\n# ì§€ë‚œì‹œê°„ ë³µìŠµ\n\n# x,X,W,y // X = [1 x], W = [w0, w1]' # íšŒê·€ë¶„ì„ì—ì„œëŠ” W=Î²\n# íšŒê·€ëª¨í˜•: y=X@W+Ïµ = X@Î²+Ïµ\n# true: E(y)=X@W\n# observed: (x,y)\n# estimated W = What = [w0hat, w1hat]' &lt;-- ì•„ë¬´ê°’ì´ë‚˜ë„£ì—ˆìŒ.. \n# estimated y = yhat = X@What = X@Î²Ì‚ \n# loss = yhatì´ë‘ yë‘ ì–¼ë§ˆë‚˜ ë¹„ìŠ·í•œì§€ = sum((y-yhat)^2)\n# (x,y) ë³´ê³  ìµœì ì˜ ì„ ë¶„ì„ ê·¸ë¦¬ëŠ”ê²ƒ = lossë¥¼ ê°€ì¥ ì‘ê²Œ ë§Œë“œëŠ” What = [w0hat, w1hat] ë¥¼ ì°¾ëŠ”ê²ƒ\n# ì „ëµ: (1) ì•„ë¬´ Whatë‚˜ ì°ëŠ”ë‹¤ (2) ê·¸ê±°ë³´ë‹¤ ë” ë‚˜ì€ Whatì„ ì°¾ëŠ”ë‹¤. (3) 1-2ë¥¼ ë°˜ë³µí•œë‹¤. \n# ì „ëµ2ê°€ ì–´ë ¤ìš´ë°, ì´ë¥¼ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì´ ê²½ì‚¬í•˜ê°•ë²• \n# ê²½ì‚¬í•˜ê°•ë²• ì•Œê³ ë¦¬ì¦˜: ë”ë‚˜ì€What = ì›ë˜What - 0.1*ë¯¸ë¶„ê°’\n\n\nWhat = torch.tensor([[-5.0],[10.0]])\nWhat\n\ntensor([[-5.],\n        [10.]])\n\n\n\nyhat = X@What \nplt.plot(x,y,'o')\nplt.plot(x,yhat,'--')\n\n\n\n\n\n\n\n\n\nloss = torch.sum((y-yhat)**2)\nloss\n\ntensor(8587.6875)\n\n\në³µìŠµë~\n#\n- ë” ë‚˜ì€ ì„ ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•˜ê¸° ìœ„í•´ì„œëŠ” ê³µì‹ â€œë”ë‚˜ì€What = ì›ë˜What - 0.1*ë¯¸ë¶„ê°’â€ ë¥¼ ì ìš©í•´ì•¼í•˜ê³  ì´ë¥¼ ìœ„í•´ì„œëŠ” ë¯¸ë¶„ê°’ì„ ê³„ì‚°í•  ìˆ˜ ìˆì–´ì•¼ í•¨.\n\n\n\n\n\n\nImportant\n\n\n\nê²½ì‚¬í•˜ê°•ë²•ì„ ì¢€ ë” ì—„ë°€í•˜ê²Œ ì¨ë³´ì. ê²½ì‚¬í•˜ê°•ë²•ì€ \\(loss(\\hat{\\bf W})\\)ë¥¼ ìµœì†Œë¡œ ë§Œë“œëŠ” \\(\\hat{\\bf W}\\)ë¥¼ ì»´í“¨í„°ë¡œ êµ¬í•˜ëŠ” ë°©ë²•ì¸ë°, êµ¬ì²´ì ìœ¼ë¡œëŠ” ì•„ë˜ì™€ ê°™ë‹¤.\n1. ì„ì˜ì˜ ì  \\(\\hat{\\bf W}\\)ë¥¼ ì°ëŠ”ë‹¤.\n2. ê·¸ ì ì—ì„œ ìˆœê°„ê¸°ìš¸ê¸°ë¥¼ êµ¬í•œë‹¤. ì¦‰ \\(\\left.\\frac{\\partial}{\\partial {\\bf W}}loss({\\bf W})\\right|_{{\\bf W}=\\hat{\\bf W}}\\) ë¥¼ ê³„ì‚°í•œë‹¤.\n3. \\(\\hat{\\bf W}\\)ì—ì„œì˜ ìˆœê°„ê¸°ìš¸ê¸°ì˜ ë¶€í˜¸ë¥¼ ì‚´í´ë³´ê³  ë¶€í˜¸ì™€ ë°˜ëŒ€ë°©í–¥ìœ¼ë¡œ ì›€ì§ì¸ë‹¤. ì´ë•Œ ê¸°ìš¸ê¸°ì˜ ì ˆëŒ€ê°’ í¬ê¸°ì™€ ë¹„ë¡€í•˜ì—¬ ë³´í­(=ì›€ì§ì´ëŠ” ì •ë„)ì„ ê°ê° ì¡°ì ˆí•œë‹¤. ì¦‰ ì•„ë˜ì˜ ìˆ˜ì‹ì— ë”°ë¼ ì—…ë°ì´íŠ¸ í•œë‹¤.\n\\[\\hat{\\bf W} \\leftarrow \\hat{\\bf W} - \\alpha \\times \\left.\\frac{\\partial}{\\partial {\\bf W}}loss({\\bf W})\\right|_{{\\bf W}=\\hat{\\bf W}}\\]\nì—¬ê¸°ì—ì„œ ë§¨ ë§ˆì§€ë§‰ ìˆ˜ì‹ì„ ê°„ë‹¨í•˜ê²Œ ì“´ ê²ƒì´ ë”ë‚˜ì€What = ì›ë˜What - 0.1*ë¯¸ë¶„ê°’ ì´ë‹¤.\n\n\n- ë¯¸ë¶„ê°’ì„ ê³„ì‚°í•˜ëŠ” ë°©ë²•1\n\n# ì†ì‹¤ 8587.6875 ë¥¼ ê³„ì‚°í•˜ëŠ” ë˜ ë‹¤ë¥¸ ë°©ì‹\ndef l(w0,w1):\n    yhat = w0 + w1*x\n    return torch.sum((y-yhat)**2)\n\n\nl(-5,10)\n\ntensor(8587.6875)\n\n\nğŸ—£ï¸(\n\nêµ³ì´ í•¨ìˆ˜ë¥¼ ë§Œë“  ì´ìœ : ë¯¸ë¶„í•˜ë ¤ê³ \ní¸ë¯¸ë¶„ êµ¬í˜„\n\nl(-5,10)\n(l(w0+h,w1) - l(w0,w1))/h: ë„í•¨ìˆ˜\n\n\n)ğŸ—£ï¸\n\nh=0.001\nprint((l(-5+h,10) - l(-5,10))/h)\nprint((l(-5,10+h) - l(-5,10))/h)\n\ntensor(-1341.7968)\ntensor(1190.4297)\n\n\nì¼ë‹¨ ì´ê±°ë¡œ ì—…ë°ì´íŠ¸í•´ë³¼ê¹Œ?\n\n# ë”ë‚˜ì€What = ì›ë˜What - 0.1*ë¯¸ë¶„ê°’\n# [-5,10] - 0.001 * [-1341.7968,1190.4297]\n\n\nsssss = What - 0.001 * torch.tensor([[-1341.7968],[1190.4297]])\nsssss\n\ntensor([[-3.6582],\n        [ 8.8096]])\n\n\n\nplt.plot(x,y,'o')\nplt.plot(x,X@What,'-') # ì›ë˜What: ì£¼í™©ìƒ‰\nplt.plot(x,X@sssss,'-') # ë”ë‚˜ì€What: ì´ˆë¡ìƒ‰\n\n\n\n\n\n\n\n\n\nì˜ ëœ ê²ƒ ê°™ê¸´í•œë°..\në¯¸ë¶„êµ¬í•˜ëŠ”ê²Œ ë„ˆë¬´ ì–´ë ¤ì›Œ..\në‹¤ë¥¸ ë°©ë²• ì—†ì„ê¹Œ?\n\n\n\n\n\n\n\nImportant\n\n\n\nì‚¬ì‹¤ ì´ ë°©ë²•ì€\n\n\\(\\frac{\\partial}{\\partial w_0}loss(w_0,w_1) \\approx \\frac{loss(w_0+h,w_1)-loss(w_0,w_1)}{h}\\)\n\\(\\frac{\\partial}{\\partial w_1}loss(w_0,w_1) \\approx \\frac{loss(w_0,w_1+h)-loss(w_0,w_1)}{h}\\)\n\nì´ ê³„ì‚°ì„ ì´ìš©í•˜ì—¬\n\\[\\frac{\\partial}{\\partial {\\bf W}}loss({\\bf W}):= \\begin{bmatrix} \\frac{\\partial}{\\partial w_0} \\\\ \\frac{\\partial}{\\partial w_1}\\end{bmatrix}loss({\\bf W}) =  \\begin{bmatrix} \\frac{\\partial}{\\partial w_0}loss({\\bf W}) \\\\ \\frac{\\partial}{\\partial w_1}loss({\\bf W})\\end{bmatrix}  =  \\begin{bmatrix} \\frac{\\partial}{\\partial w_0}loss(w_0,w_1) \\\\ \\frac{\\partial}{\\partial w_1}loss(w_0,w_1)\\end{bmatrix}\\]\në¥¼ ê³„ì‚°í•œ ê²ƒì´ë¼ ë³¼ ìˆ˜ ìˆì£ \n\n\n- ë¯¸ë¶„ê°’ì„ ê³„ì‚°í•˜ëŠ” ë°©ë²•2\n\n## ì•½ê°„ì˜ ì§€ì‹ì´ í•„ìš”í•¨. \n# loss = (y-XWhat)'(y-XWhat)\n# = (y'-What'X')(y-XWhat)\n# = y'y-y'XWhat -What'X'y + What'X'XWhat \n# lossë¥¼ Whatìœ¼ë¡œ ë¯¸ë¶„\n# loss' = -X'y - X'y + 2X'XWhat\n\nâ“ í–‰ë ¬ ë¯¸ë¶„ ë³µìŠµ í•„ìš”\n\n-2*X.T@y + 2*X.T@X@What\n\ntensor([[-1342.2524],\n        [ 1188.9302]])\n\n\nğŸ—£ï¸ ì•½ê°„ì˜ ì˜¤ì°¨ëŠ” ìˆì§€ë§Œ ìœ„ì™€ ë¹„ìŠ· (ê·¸ëŸ¬ë‚˜ ë°©ë²•1, ë°©ë²•2 ë§ê³  ë‹¤ë¥¸ ë°©ë²•ì„ ì“°ê³  ì‹¶ìŒ)\n\n\n\n\n\n\nImportant\n\n\n\nì´ ë°©ë²•ì€ \\(loss({\\bf W})\\)ì˜ ë¯¸ë¶„ì„ êµ¬í• ìˆ˜ ìˆì–´ì•¼ ì‚¬ìš©ê°€ëŠ¥í•©ë‹ˆë‹¤. ì¦‰\n\\[\\frac{\\partial}{\\partial {\\bf W}}loss({\\bf W})= -2{\\bf X}^\\top {\\bf y} + 2{\\bf X}^\\top {\\bf X}{\\bf W}\\]\në¥¼ ê³„ì‚°í•  ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.\n\n\n- ë¯¸ë¶„ê°’ì„ ê³„ì‚°í•˜ëŠ” ë°©ë²•3 â€“ ì´ íŒ¨í„´ì„ ì™¸ìš°ì„¸ì—¬\n\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad=True)\nWhat\n\ntensor([[-5.],\n        [10.]], requires_grad=True)\n\n\n\nyhat = X@What\nloss = torch.sum((y-yhat)**2)\nloss\n\ntensor(8587.6875, grad_fn=&lt;SumBackward0&gt;)\n\n\nğŸ—£ï¸ ê¼¬ë¦¬í‘œê°€ ìˆê¸´í•˜ì§€ë§Œ ê²°ê³¼ëŠ” ìœ„ì™€ ë™ì¼\n\nloss.backward() # lossë¥¼ ë¯¸ë¶„í•˜ë¼.. ê¼¬ë¦¬í‘œê°€ ìˆê²Œ í•œ Whatìœ¼ë¡œ.. \n\nğŸ—£ï¸(\n\nlossë¥¼ Whatìœ¼ë¡œ ë¯¸ë¶„\nì¼ë°˜ì ìœ¼ë¡œ ë¯¸ë¶„ì„ í•˜ë©´ ë„í•¨ìˆ˜ê°€ ë‚˜ì˜¤ì§€ë§Œ, ì´ ê²½ìš°ëŠ” ë„í•¨ìˆ˜ì—ì„œ í˜„ì¬ Whatê°’ì„ ëŒ€ì…í•œ ê²°ê³¼ê°€ ë‚˜ì˜´\nì •í™•íˆ ë§í•˜ë©´ Whatì— í•´ë‹¹í•˜ëŠ” ì ‘ì„ ì˜ ê¸°ìš¸ê¸°\nì‹¤í–‰í•´ë„ ì‹¤í–‰ê²°ê³¼ëŠ” ë‚˜ì˜¤ì§€ ì•ŠìŒ. ê²°ê³¼ëŠ” What.gradì— ì €ì¥ë˜ì–´ ìˆìŒ\n\n)ğŸ—£ï¸\n\nWhat.grad\n\ntensor([[-1342.2524],\n        [ 1188.9305]])\n\n\n- ìœ„ì˜ ì½”ë“œë¥¼ ë‹¤ì‹œ ë³µìŠµí•´ë³´ì.\nâ€“ loss.backward()ì‹¤í–‰ì „ â€“\n\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad=True)\nyhat = X@What\nloss = torch.sum((y-yhat)**2)\n\n\nWhat.data, What.grad\n\n(tensor([[-5.],\n         [10.]]),\n None)\n\n\nğŸ—£ï¸ .backward()ë¥¼ ì‹¤í–‰í•˜ì§€ ì•Šì•„ì„œ .gradì— ì•„ë¬´ ê°’ë„ ì—†ìŒ(Noneìœ¼ë¡œ ì´ˆê¸°í™” ë¨)\nâ€“ loss.backward()ì‹¤í–‰í›„ â€“\n\nloss.backward()\n\n\nWhat.data, What.grad\n\n(tensor([[-5.],\n         [10.]]),\n tensor([[-1342.2524],\n         [ 1188.9305]]))\n\n\nğŸ—£ï¸(\n\n.backward()ë¥¼ ì‹¤í–‰í•˜ë‹ˆ .gradì— ê¸°ìš¸ê¸° ê°’ì´ ê³„ì‚°ë˜ì–´ ì—…ë°ì´íŠ¸ ë¨\nloss.backward(): What.grad &lt;- Whatì—ì„œ ë¯¸ë¶„ê°’ ì¸ì¤„ ì•Œì•˜ìœ¼ë‚˜ ì‚¬ì‹¤ì€\nloss.backward(): What.grad &lt;- What.grad + Whatì—ì„œ ë¯¸ë¶„ê°’ (ì¦‰, ëˆ„ì ì„ ì‹œì¼œì„œ ë”í•¨)\n\n\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad=True)\n\n\nyhat = X@What\nloss = torch.sum((y-yhat)**2)\nloss.backward()\n\n\nWhat.data, What.grad\n\n(tensor([[-5.],\n         [10.]]),\n tensor([[-1342.2524],\n         [ 1188.9305]]))\n\n\n\nyhat = X@What\nloss = torch.sum((y-yhat)**2)\nloss.backward()\n\n\nWhat.data, What.grad\n\n(tensor([[-5.],\n         [10.]]),\n tensor([[-2684.5049],\n         [ 2377.8611]]))\n\n\n\në‘ ë°°ê°€ ë¨\nì™œ?\n\nì‚°ê³µ: ì•Œê³ ë¦¬ì¦˜ ìƒì—ì„œëŠ” What.gradì˜ ê°’ì€ loss.backward()ë¥¼ í• ë•Œë§ˆë‹¤ ì´ˆê¸°í™”ê°€ ë§ìŒ (ì´ë¡ ì ìœ¼ë¡œëŠ” ì´ê²Œ ë§ìŒ)\nì»´ê³µ: ê·¸ëŸ¬ë©´ ë‚˜ì¤‘ì— ê³„ì‚° íš¨ìœ¨ì´ ì•ˆ ì¢‹ì•„ì§ (ì›¬ë§Œí•˜ë©´ ê³„ì‚°í•œ ë¯¸ë¶„ê°’ì„ ê°–ê³  ìˆê³  ì‹¶ìŒ, í•„ìš” ì—†ìœ¼ë©´ ë”°ë¡œ ì´ˆê¸°í™”í•˜ë©´ ë¨)\ní†µê³„: ìµœì í™”ì™€ ë¯¸ë¶„ ë¹¨ë¦¬í•˜ëŠ” ê²ƒì— ê´€ì‹¬ X\n\n\n)ğŸ—£ï¸\nâœï¸ ì´í›„ ì›í™œí•œ ì½”ë“œ ì‹¤í–‰ì„ ìœ„í•œ ì½”ë“œ (ì˜ë¯¸X)\n\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad=True)\nyhat = X@What\nloss = torch.sum((y-yhat)**2)\n\nWhat.data, What.grad\n\nloss.backward()\n\nWhat.data, What.grad\n\n(tensor([[-5.],\n         [10.]]),\n tensor([[-1342.2524],\n         [ 1188.9305]]))\n\n\n# 1íšŒ ì—…ë°ì´íŠ¸ ê³¼ì •ì„ ì°¨ê·¼ì°¨ê·¼ ì‹œê°í™”í•˜ë©° ì •ë¦¬í•´ë³´ì.\n\nalpha = 0.001 \nprint(f\"{What.data} -- ìˆ˜ì •ì „\")\nprint(f\"{-alpha*What.grad} -- ìˆ˜ì •í•˜ëŠ”í­\")\nprint(f\"{What.data-alpha*What.grad} -- ìˆ˜ì •í›„\")\nprint(f\"{torch.tensor([[2.5],[4]])} -- ì°¸ê°’(ì´ê±´ ë¹„ë°€~~)\")\n\ntensor([[-5.],\n        [10.]]) -- ìˆ˜ì •ì „\ntensor([[ 1.3423],\n        [-1.1889]]) -- ìˆ˜ì •í•˜ëŠ”í­\ntensor([[-3.6577],\n        [ 8.8111]]) -- ìˆ˜ì •í›„\ntensor([[2.5000],\n        [4.0000]]) -- ì°¸ê°’(ì´ê±´ ë¹„ë°€~~)\n\n\nğŸ—£ï¸(\n\n\\(\\alpha\\)ë¥¼ 0.001ë¡œ ì¡ì€ ì´ìœ : ë¯¸ë¶„ê°’ì´ 1000 ë‹¨ìœ„ë¡œ ë‚˜ì™€ì„œ ê·¸ëŒ€ë¡œ ë„£ìœ¼ë©´ ì›í•˜ëŠ” ê²°ê³¼ê°€ ì•ˆ ë‚˜ì˜¬ ê²ƒ ê°™ìŒ\n\nì˜ ìˆ˜ë ´ë ë•Œê¹Œì§€ ì‹œí–‰ì°©ì˜¤ë¥¼ ê²ªìœ¼ë©° í•´ë´ì•¼ í•¨\n\nìˆ˜ì •í•˜ëŠ” í­: ìœ„ ê·¸ë˜í”„ì—ì„œ ì£¼í™©ìƒ‰ ì„ \nìˆ˜ì • í›„: ìœ„ ê·¸ë˜í”„ì—ì„œ ì´ˆë¡ìƒ‰ ì„ \nìˆ˜ì • ì „ë³´ë‹¤ ìˆ˜ì • í›„ê°€ ì°¸ê°’ì— ê°€ê¹Œìš°ë¯€ë¡œ ì˜¬ë°”ë¥¸ ë°©í–¥ì„ ì§„í–‰ë˜ê³  ìˆìŒì„ ì•Œ ìˆ˜ ìˆìŒ\n\n)ğŸ—£ï¸\n\nWbefore = What.data\nWafter = What.data - alpha * What.grad \nWbefore, Wafter\n\n(tensor([[-5.],\n         [10.]]),\n tensor([[-3.6577],\n         [ 8.8111]]))\n\n\n\nplt.plot(x,y,'o',label=r'observed data')\nplt.plot(x,X@Wbefore,'--', label=r\"$\\hat{\\bf y}_{before}={\\bf X}@\\hat{\\bf W}_{before}$\")\nplt.plot(x,X@Wafter,'--', label=r\"$\\hat{\\bf y}_{after}={\\bf X}@\\hat{\\bf W}_{after}$\")\nplt.legend()\n\n\n\n\n\n\n\n\n#"
  },
  {
    "objectID": "posts/01wk-2.html#c.-3ë‹¨ê³„-iteration-learn-estimate-bfhat-w",
    "href": "posts/01wk-2.html#c.-3ë‹¨ê³„-iteration-learn-estimate-bfhat-w",
    "title": "01wk-2, 02wk-1: (íšŒê·€) â€“ íšŒê·€ëª¨í˜•, ì†ì‹¤í•¨ìˆ˜, íŒŒì´í† ì¹˜ë¥¼ ì´ìš©í•œ ì¶”ì •",
    "section": "C. 3ë‹¨ê³„ â€“ iteration (=learn = estimate \\(\\bf{\\hat W}\\))",
    "text": "C. 3ë‹¨ê³„ â€“ iteration (=learn = estimate \\(\\bf{\\hat W}\\))\n- ì´ì œ 1ë‹¨ê³„ì™€ 2ë‹¨ê³„ë¥¼ ë°˜ë³µë§Œí•˜ë©´ëœë‹¤. ê·¸ë˜ì„œ ì•„ë˜ì™€ ê°™ì€ ì½”ë“œë¥¼ ì‘ì„±í•˜ë©´ ë  ê²ƒ ê°™ì€ë°â€¦\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad=True) # ìµœì´ˆì˜ ì§ì„ ì„ ë§Œë“œëŠ” ê°’\nfor epoc in range(30):\n    yhat = X@What \n    loss = torch.sum((y-yhat)**2)\n    loss.backward()\n    What.data = What.data - 0.001 * What.grad\nëŒë ¤ë³´ë©´ ì˜ ì•ˆëœë‹¤.\nğŸ—£ï¸ ì›ë˜ ì² ìëŠ” epochì´ì§€ë§Œ í¸ì˜ìƒ epocìœ¼ë¡œ ì‘ì„±, ì˜ ë˜ê¸° ìœ„í•´ì„œëŠ” ë§ˆì§€ë§‰ì— ì´ˆê¸°í™”ë¥¼ í•´ì¤˜ì•¼ í•¨\n- ì•„ë˜ì™€ ê°™ì´ í•´ì•¼í•œë‹¤.\n\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad=True) # ìµœì´ˆì˜ ì§ì„ ì„ ë§Œë“œëŠ” ê°’\nfor epoc in range(30):\n    yhat = X@What \n    loss = torch.sum((y-yhat)**2)\n    loss.backward()\n    What.data = What.data - 0.001 * What.grad\n    What.grad = None \n\n\nplt.plot(x,y,'o',label=r\"observed: $(x_i,y_i)$\")\nplt.plot(x,X@What.data,'--o', label=r\"estimated: $(x_i,\\hat{y}_i)$ -- after 30 iterations (=epochs)\", alpha=0.4 )\nplt.legend()\n\n\n\n\n\n\n\n\n- ì™œ? loss.backward() ëŠ” ì•„ë˜ì˜ ì—­í• ì„ í•˜ëŠ”ê²ƒ ì²˜ëŸ¼ ì´í•´ë˜ì—ˆì§€ë§Œ\n\nWhat.grad \\(\\leftarrow\\) Whatì—ì„œë¯¸ë¶„ê°’\n\nì‹¤ì œë¡œëŠ” ì•„ë˜ì˜ ì—­í• ì„ ìˆ˜í–‰í•˜ê¸° ë•Œë¬¸ì´ë‹¤. (ì»´í“¨í„°ê³µí•™ì ì¸ ì´ìœ ë¡œ..)\n\nWhat.grad \\(\\leftarrow\\) What.grad + Whatì—ì„œë¯¸ë¶„ê°’\n\n\n\n\n\n\n\nNote\n\n\n\nWhat.grad \\(\\leftarrow\\) What.grad + Whatì—ì„œë¯¸ë¶„ê°’ ì„ì„ í™•ì¸í•˜ê¸° ìœ„í•´ì„œ.. ì•½ê°„ì˜ í…ŒìŠ¤íŠ¸ë¥¼ í–ˆìŠµë‹ˆë‹¤.\në¨¼ì €\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad=True) # ìµœì´ˆì˜ ì§ì„ ì„ ë§Œë“œëŠ” ê°’\nprint(What.data)\nprint(What.grad)\në¥¼ í™•ì¸í•œë’¤ ì•„ë˜ë¥¼ ë°˜ë³µì‹¤í–‰í•´ë´¤ì„ë•Œ\nyhat = X@What \nloss = torch.sum((y-yhat)**2)\nloss.backward() # \nprint(What.data)\nprint(What.grad)\nWhat.dataì™€ What.grad ê°’ì´ ê³„ì† ì¼ì •í•˜ê²Œ ë‚˜ì˜¨ë‹¤ë©´\n\nWhat.grad \\(\\leftarrow\\) Whatì—ì„œë¯¸ë¶„ê°’\n\nì´ì™€ ê°™ì€ ê³„ì‚°ì´ ì§„í–‰ë˜ëŠ” ê²ƒì´ê² ê³ , What.gradì˜ ê°’ì´ ìê¾¸ ì»¤ì§„ë‹¤ë©´\n\nWhat.grad \\(\\leftarrow\\) What.grad + Whatì—ì„œë¯¸ë¶„ê°’\n\nì´ì™€ ê°™ì€ ê³„ì‚°ì´ ì§„í–‰ë˜ëŠ” ê²ƒì´ê² ì£ ?"
  },
  {
    "objectID": "posts/04wk-1.html",
    "href": "posts/04wk-1.html",
    "title": "04wk-1: (ì‹ ê²½ë§) â€“ ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„ ê·¹ë³µ",
    "section": "",
    "text": "ğŸ“˜ Note Format Guide\nThis format serves as a structured guide for organizing lecture content, personal interpretation, experiments, and study-related questions.\n\n\n\n\n\n\n\n\nType\nWhat It Means\nWhen I Use It\n\n\n\n\nğŸ“ Lecture\nOriginal material from the professorâ€™s notes\nWhen Iâ€™m referencing core concepts or provided code\n\n\nğŸ—£ï¸ In-Class Note\nVerbal explanations shared during the lecture\nWhen I want to record something the professor said in class but didnâ€™t include in the official notes\n\n\nâœï¸ My Note\nMy thoughts, interpretations, or additional explanations\nWhen I reflect on or explain something in my own words\n\n\nğŸ”¬ Experiment\nCode I tried out or changed to explore further\nWhen I test variations or go beyond the original example\n\n\nâ“ Question\nQuestions I had while studying\nWhen I want to revisit or research something more deeply\n\n\n\nğŸ“ ğŸ—£ï¸ âœï¸ ğŸ”¬ â“\n\n1. ê°•ì˜ë…¸íŠ¸ ì›ë³¸ ë° ì˜ìƒ ë§í¬\nhttps://guebin.github.io/DL2025/posts/04wk-1.html\n\n\n2. Imports ğŸ“\n\nimport torch\nimport matplotlib.pyplot as plt \nimport pandas as pd\n\n\nplt.rcParams['figure.figsize'] = (4.5, 3.0)\n\n\n\n3. êº½ì¸ì§ì„ ì„ ë§Œë“œëŠ” ë°©ë²• ğŸ“\nì§€ë‚œì‹œê°„ë³µìŠµ\n\n# ì˜¤ëŠ˜ì˜ ì”ì†Œë¦¬.. \n## íšŒê·€(ì¹´í˜ì˜ˆì œ): yhat=ì§ì„ =linr(x), ì •ê·œë¶„í¬, MSEloss\n## ë¡œì§€ìŠ¤í‹±(ìŠ¤í™ê³¼ì·¨ì—…): yhat=ê³¡ì„ =sig(ì§ì„ )=sig(linr(x)), ë² ë¥´ëˆ„ì´, BCELoss\n## ì´ë¦„ì—†ìŒ(ìŠ¤í™ì˜ì—­ì„¤): yhat=êº½ì¸ê³¡ì„ =sig(êº½ì¸ì§ì„ )=sig(??), ë² ë¥´ëˆ„ì´, BCELOss\n\n- ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ì„œëŠ” ì‹œê·¸ëª¨ì´ë“œë¥¼ ì·¨í•˜ê¸° ì „ì— êº½ì¸ ê·¸ë˜í”„ ëª¨ì–‘ì„ ë§Œë“œëŠ” ê¸°ìˆ ì´ ìˆì–´ì•¼ê² ìŒ.\n- ì•„ë˜ì™€ ê°™ì€ ë²¡í„° \\({\\bf x}\\)ë¥¼ ê°€ì •í•˜ì.\n\nx = torch.linspace(-1,1,1001).reshape(-1,1)\nx\n\ntensor([[-1.0000],\n        [-0.9980],\n        [-0.9960],\n        ...,\n        [ 0.9960],\n        [ 0.9980],\n        [ 1.0000]])\n\n\n- ëª©í‘œ: ì•„ë˜ì™€ ê°™ì€ ë²¡í„° \\({\\bf y}\\)ë¥¼ ë§Œë“¤ì–´ë³´ì.\n\\[{\\bf y} = [y_1,y_2,\\dots,y_{n}]^\\top, \\quad y_i = \\begin{cases} 9x_i +4.5& x_i &lt;0 \\\\ -4.5x_i + 4.5& x_i &gt;0 \\end{cases}\\]\n\n\n\n\n\n\nCaution\n\n\n\nì¼ë°˜ì ìœ¼ë¡œ ì œ ê°•ì˜ë…¸íŠ¸ì—ì„œ\n\në…ë¦½ë³€ìˆ˜ = ì„¤ëª…ë³€ìˆ˜ = \\({\\bf x}\\), \\({\\bf X}\\)\nì¢…ì†ë³€ìˆ˜ = ë°˜ì‘ë³€ìˆ˜ = \\({\\bf y}\\)\n\në¥¼ ì˜ë¯¸í•˜ëŠ”ë°ìš”, ì—¬ê¸°ì—ì„œ \\(({\\bf x},{\\bf y})\\) ëŠ” (ë…ë¦½ë³€ìˆ˜,ì¢…ì†ë³€ìˆ˜) í˜¹ì€ (ì„¤ëª…ë³€ìˆ˜,ë°˜ì‘ë³€ìˆ˜) ë¥¼ ì˜ë¯¸í•˜ëŠ”ê²Œ ì•„ë‹™ë‹ˆë‹¤.\n\n\n# ë°©ë²•1 â€“ ìˆ˜ì‹ ê·¸ëŒ€ë¡œ êµ¬í˜„\nğŸ—£ï¸(\n\nplt.plot(x,x,color=\"red\")\nplt.plot(x,9*x+4.5,color=\"blue\")\nplt.plot(x,-4.5*x+4.5,color=\"orange\")\n\n\n\n\n\n\n\n\n\n# (9*x+4.5)[x&lt;0]\n\n\nlen(9*x+4.5)\n\n1001\n\n\n\nlen((9*x+4.5)[x&lt;0])\n\n501\n\n\n)ğŸ—£ï¸\n\nplt.plot(x,9*x+4.5,color=\"blue\",alpha=0.1)\nplt.plot(x[x&lt;0], (9*x+4.5)[x&lt;0],color=\"blue\")\nplt.plot(x,-4.5*x+4.5,color=\"orange\",alpha=0.1)\nplt.plot(x[x&gt;0], (-4.5*x+4.5)[x&gt;0],color=\"orange\")\n\n\n\n\n\n\n\n\n\ny = x*0\ny[x&lt;0] = (9*x+4.5)[x&lt;0]\ny[x&gt;0] = (-4.5*x+4.5)[x&gt;0]\nplt.plot(x,y)\n\n\n\n\n\n\n\n\n#\n# ë°©ë²•2 â€“ ë ë£¨ì´ìš©\nğŸ—£ï¸(\n\nrelu = torch.nn.ReLU()\nplt.plot(x,x,color=\"red\")\nplt.plot(x,relu(x),color=\"blue\")\n\n\n\n\n\n\n\n\n\nxê°€ 0ë³´ë‹¤ ì‘ìœ¼ë©´ yë¥¼ 0ìœ¼ë¡œ ë§Œë“¦\n\n\nrelu = torch.nn.ReLU()\nplt.plot(x,x,color=\"red\")\nplt.plot(x,relu(-x),color=\"blue\")\n\n\n\n\n\n\n\n\n\nyì¶• ëŒ€ì¹­\n\n\nrelu = torch.nn.ReLU()\nplt.plot(x,relu(x),color=\"red\")\nplt.plot(x,relu(-x),color=\"blue\")\n\n\n\n\n\n\n\n\n\nrelu = torch.nn.ReLU()\nplt.plot(x,-relu(x),color=\"red\")\nplt.plot(x,relu(-x),color=\"blue\")\n\n\n\n\n\n\n\n\n\nrelu = torch.nn.ReLU()\nplt.plot(x,-relu(x),color=\"red\")\nplt.plot(x,-relu(-x),color=\"blue\")\n\n\n\n\n\n\n\n\n\níŒŒë€ìƒ‰ì˜ ê¸°ìš¸ê¸°ë¥¼ 9, ë¹¨ê°„ìƒ‰ì˜ ê¸°ìš¸ê¸°ë¥¼ 4.5ë¡œ ë§Œë“¤ë©´\n\n\nrelu = torch.nn.ReLU()\nplt.plot(x,-4.5*relu(x),color=\"red\")\nplt.plot(x,-9*relu(-x),color=\"blue\")\n\n\n\n\n\n\n\n\n\nyì ˆí¸ì´ 4.5ì´ë¯€ë¡œ\n\n\nrelu = torch.nn.ReLU()\n# plt.plot(x,-4.5*relu(x),color=\"red\")\n# plt.plot(x,-9*relu(-x),color=\"blue\")\ny = -4.5*relu(x) + -9*relu(-x)\nplt.plot(x,y)\n\n\n\n\n\n\n\n\n\nrelu = torch.nn.ReLU()\n# plt.plot(x,-4.5*relu(x),color=\"red\")\n# plt.plot(x,-9*relu(-x),color=\"blue\")\ny = -4.5*relu(x) + -9*relu(-x) + 4.5\nplt.plot(x,y)\n\n\n\n\n\n\n\n\n)ğŸ—£ï¸\n\nrelu = torch.nn.ReLU()\n#plt.plot(x,-4.5*relu(x),color=\"red\")\n#plt.plot(x,-9*relu(-x),color=\"blue\")\ny = -4.5*relu(x) + -9*relu(-x) + 4.5\nplt.plot(x,y)\n\n\n\n\n\n\n\n\n- ì¢€ ë” ì¤‘ê°„ê³¼ì •ì„ ì‹œê°í™” â€“ (ê°•ì˜ë•Œ ì„¤ëª…ì•ˆí–ˆìŒ)\n\nfig = plt.figure(figsize=(6, 4))\nspec = fig.add_gridspec(4, 3)\nax1 = fig.add_subplot(spec[:2,0]); ax1.set_title(r'$x$'); ax1.set_ylim(-1,1)\nax2 = fig.add_subplot(spec[2:,0]); ax2.set_title(r'$-x$'); ax2.set_ylim(-1,1)\nax3 = fig.add_subplot(spec[:2,1]); ax3.set_title(r'$relu(x)$'); ax3.set_ylim(-1,1)\nax4 = fig.add_subplot(spec[2:,1]); ax4.set_title(r'$relu(-x)$'); ax4.set_ylim(-1,1)\nax5 = fig.add_subplot(spec[1:3,2]); ax5.set_title(r'$-4.5 relu(x)-9 relu(-x)+4.5$')\n#---#\nax1.plot(x,'--',color='C0')\nax2.plot(-x,'--',color='C1')\nax3.plot(relu(x),'--',color='C0')\nax4.plot(relu(-x),'--',color='C1')\nax5.plot(-4.5*relu(x)-9*relu(-x)+4.5,'--',color='C2')\nfig.tight_layout()\n\n\n\n\n\n\n\n\n#\n# ë°©ë²•3 â€“ reluì˜ ë¸Œë¡œë“œìºìŠ¤íŒ… í™œìš©\nğŸ—£ï¸(\n\ntorch.tensor([[1,2],[2,3],[4,-4]]) \n\ntensor([[ 1,  2],\n        [ 2,  3],\n        [ 4, -4]])\n\n\n\nplt.plot(torch.tensor([[1,2],[2,3],[4,-4]]), '--o') \n\n\n\n\n\n\n\n\n\ncolumnë³„ë¡œ plotì´ ë¨\n\n\ntorch.concat([x,-x], axis=1)\n\ntensor([[-1.0000,  1.0000],\n        [-0.9980,  0.9980],\n        [-0.9960,  0.9960],\n        ...,\n        [ 0.9960, -0.9960],\n        [ 0.9980, -0.9980],\n        [ 1.0000, -1.0000]])\n\n\n\nplt.plot(torch.concat([x,-x], axis=1))\n\n\n\n\n\n\n\n\n\nì—¬ê¸°ì„œ reluë¥¼ í•˜ë©´? reluëŠ” column wiseí•˜ê²Œ ë¸Œë¡œë“œìºìŠ¤íŒ… ë¨\n\n\nplt.plot(relu(torch.concat([x,-x], axis=1)))\n\n\n\n\n\n\n\n\n\nu = torch.concat([x,-x], axis=1)\nv = relu(u)\nplt.plot(v)\n\n\n\n\n\n\n\n\n\nu = torch.concat([x,-x], axis=1)\nv = relu(u)\nv[:,[0]] # ì²«ë²ˆì§¸ ì—´\n\ntensor([[0.0000],\n        [0.0000],\n        [0.0000],\n        ...,\n        [0.9960],\n        [0.9980],\n        [1.0000]])\n\n\n)ğŸ—£ï¸\n- ìš°ë¦¬ê°€ í•˜ê³  ì‹¶ì€ ê²ƒ\n\n# y = -4.5*relu(x) + -9*relu(-x) + 4.5\n\n- ì•„ë˜ì™€ ê°™ì€ ì•„ì´ë””ì–´ë¡œ yë¥¼ ê³„ì‚°í•´ë„ ëœë‹¤.\n\nx, relu ì¤€ë¹„\nu = [x -x]\nv = relu(u) = [relu(x), relu(-x)] = [v1 v2]\ny = -4.5*v1 + -9*v2 + 4.5\n\n\nu = torch.concat([x,-x],axis=1)\nv = relu(u)\nv1 = v[:,[0]]\nv2 = v[:,[1]]\ny = -4.5*v1 -9*v2 + 4.5 \nplt.plot(x,y)\n\n\n\n\n\n\n\n\nğŸ—£ï¸(\n\nBonus\n\n\nv # nx2\n\ntensor([[0.0000, 1.0000],\n        [0.0000, 0.9980],\n        [0.0000, 0.9960],\n        ...,\n        [0.9960, 0.0000],\n        [0.9980, 0.0000],\n        [1.0000, 0.0000]])\n\n\n\nv.T # 2xn, ì¤‘ì²© ë¦¬ìŠ¤íŠ¸ë¡œ í•´ì„ ê°€ëŠ¥\n\ntensor([[0.0000, 0.0000, 0.0000,  ..., 0.9960, 0.9980, 1.0000],\n        [1.0000, 0.9980, 0.9960,  ..., 0.0000, 0.0000, 0.0000]])\n\n\n\nv1, v2 = v.T # ì–¸íŒ¨í‚¹, v1ê³¼ v2ëŠ” length nì¸ vector\ny = -4.5*v1 -9*v2 + 4.5 \ny # y ì—­ì‹œ vector\n\ntensor([-4.5000, -4.4820, -4.4640,  ...,  0.0180,  0.0090,  0.0000])\n\n\n\nyê°€ nx1ì´ ë˜ì–´ì•¼í•˜ë¯€ë¡œ\n\n\nv1, v2 = v.T\ny = -4.5*v1 -9*v2 + 4.5 \ny = y.reshape(-1,1)\nplt.plot(x,y)\n\n\n\n\n\n\n\n\n)ğŸ—£ï¸\n#\n# ë°©ë²•4 â€“ y = linr(v)\nğŸ—£ï¸(\n\nv\n\ntensor([[0.0000, 1.0000],\n        [0.0000, 0.9980],\n        [0.0000, 0.9960],\n        ...,\n        [0.9960, 0.0000],\n        [0.9980, 0.0000],\n        [1.0000, 0.0000]])\n\n\n\nv @ torch.tensor([[-4.5],[-9]])\n\ntensor([[-9.0000],\n        [-8.9820],\n        [-8.9640],\n        ...,\n        [-4.4820],\n        [-4.4910],\n        [-4.5000]])\n\n\n\ny = v @ torch.tensor([[-4.5],[-9]]) + 4.5\nplt.plot(x,y)\n\n\n\n\n\n\n\n\n)ğŸ—£ï¸\n\n# 4. y = -4.5*v1 + -9*v2 + 4.5 = [v1 v2] @ [[-4.5],[-9]] + 4.5 \n# y = -4 + 3*x = [1 x] @ [[-4],[3]]\n\n\nx \nu = torch.concat([x,-x],axis=1)\nv = relu(u) \ny = v @ torch.tensor([[-4.5],[-9]]) + 4.5 \n\n\nplt.plot(x,y)\n\n\n\n\n\n\n\n\n#\n# ë°©ë²•5 â€“ u = linr(x)\nğŸ—£ï¸(\n\n#u = [x -x] = x @ [[1 -1]]\n\n)ğŸ—£ï¸\n\n# x \n# u = torch.concat([x,-x],axis=1)\n# v = relu(u) \n# y = v @ torch.tensor([[-4.5],[-9]]) + 4.5 \n\n\nx \nu = x @ torch.tensor([[1.0, -1.0]])\nv = relu(u) \ny = v @ torch.tensor([[-4.5],[-9]]) + 4.5 \n\n\nplt.plot(x,y)\n\n\n\n\n\n\n\n\n#\n# ë°©ë²•6 â€“ torch.nn.Linear()ë¥¼ ì´ìš©\nğŸ—£ï¸(\n\n# x \n# u = x @ torch.tensor([[1.0, -1.0]]) = linr(x) =&gt; l1(x) \n# v = relu(u) = a1(u)\n# y = v @ torch.tensor([[-4.5],[-9]]) + 4.5 = linr(x) =&gt; l2(v) \n\n\nlì€ linearì˜ ì•½ì, aëŠ” activation functionì˜ ì•½ì\n\n\n# u = l1(x) # l1ì€ x-&gt;uì¸ ì„ í˜•ë³€í™˜: (n,1) -&gt; (n,2) ì¸ ì„ í˜•ë³€í™˜\nl1 = torch.nn.Linear(1,2,bias=False)\nl1.weight.data = torch.tensor([[1.0, -1.0]]).T \nl1(x)\n\ntensor([[-1.0000,  1.0000],\n        [-0.9980,  0.9980],\n        [-0.9960,  0.9960],\n        ...,\n        [ 0.9960, -0.9960],\n        [ 0.9980, -0.9980],\n        [ 1.0000, -1.0000]], grad_fn=&lt;MmBackward0&gt;)\n\n\n\nu\n\ntensor([[-1.0000,  1.0000],\n        [-0.9980,  0.9980],\n        [-0.9960,  0.9960],\n        ...,\n        [ 0.9960, -0.9960],\n        [ 0.9980, -0.9980],\n        [ 1.0000, -1.0000]])\n\n\n\n# u = l1(x) # l1ì€ x-&gt;uì¸ ì„ í˜•ë³€í™˜: (n,1) -&gt; (n,2) ì¸ ì„ í˜•ë³€í™˜\nl1 = torch.nn.Linear(1,2,bias=False)\nl1.weight.data = torch.tensor([[1.0, -1.0]]).T \na1 = relu \nl2 = torch.nn.Linear(2,1,bias=True) # + 4.5 =&gt; bias\nl2.weight.data = torch.tensor([[-4.5],[-9]]).T \nl2.bias.data = torch.tensor([4.5])\nu = l1(x)\nv = a1(u) \ny = l2(v) \n\n\ny\n\ntensor([[-4.5000],\n        [-4.4820],\n        [-4.4640],\n        ...,\n        [ 0.0180],\n        [ 0.0090],\n        [ 0.0000]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\n\nplt.plot(x,y.data)\n\n\n\n\n\n\n\n\n\npiecewise linear function ì •ì˜ (x -&gt; l1 -&gt; a1 -&gt; l2) =&gt; í•œ ë²ˆì— ê·¸ë˜í”„ ê·¸ë¦¬ê¸° ê°€ëŠ¥\n\n\npwlinr = torch.nn.Sequential(l1,a1,l2)\nplt.plot(x,pwlinr(x).data)\n\n\n\n\n\n\n\n\n)ğŸ—£ï¸\n\n# x \n# u = x @ torch.tensor([[1.0, -1.0]]) = l1(x) \n# v = relu(u) = a1(u)\n# y = v @ torch.tensor([[-4.5],[-9]]) + 4.5 = l2(v) \n\n\n# u = l1(x) # l1ì€ x-&gt;uì¸ ì„ í˜•ë³€í™˜: (n,1) -&gt; (n,2) ì¸ ì„ í˜•ë³€í™˜\nl1 = torch.nn.Linear(1,2,bias=False)\nl1.weight.data = torch.tensor([[1.0, -1.0]]).T \na1 = relu \nl2 = torch.nn.Linear(2,1,bias=True)\nl2.weight.data = torch.tensor([[-4.5],[-9]]).T \nl2.bias.data = torch.tensor([4.5])\n#---#\nx\nu = l1(x)\nv = a1(u) \ny = l2(v) \n\n\nplt.plot(x,y.data)\n\n\n\n\n\n\n\n\n\npwlinr = torch.nn.Sequential(l1,a1,l2)\nplt.plot(x,pwlinr(x).data)\n\n\n\n\n\n\n\n\n#\n\n\n\n\n\n\nNote\n\n\n\nìˆ˜ì‹í‘œí˜„\n(1) \\({\\bf X}=\\begin{bmatrix} x_1 \\\\ \\dots \\\\ x_n \\end{bmatrix}\\)\n(2) \\(l_1({\\bf X})={\\bf X}{\\bf W}^{(1)}\\overset{bc}{+} {\\boldsymbol b}^{(1)}=\\begin{bmatrix} x_1 & -x_1 \\\\ x_2 & -x_2 \\\\ \\dots & \\dots \\\\ x_n & -x_n\\end{bmatrix}\\)\n\n\\({\\bf W}^{(1)}=\\begin{bmatrix} 1 & -1 \\end{bmatrix}\\)\n\\({\\boldsymbol b}^{(1)}=\\begin{bmatrix} 0 & 0 \\end{bmatrix}\\)\n\n(3) \\((a_1\\circ l_1)({\\bf X})=\\text{relu}\\big({\\bf X}{\\bf W}^{(1)}\\overset{bc}{+}{\\boldsymbol b}^{(1)}\\big)=\\begin{bmatrix} \\text{relu}(x_1) & \\text{relu}(-x_1) \\\\ \\text{relu}(x_2) & \\text{relu}(-x_2) \\\\ \\dots & \\dots \\\\ \\text{relu}(x_n) & \\text{relu}(-x_n)\\end{bmatrix}\\)\n(4) \\((l_2 \\circ a_1\\circ l_1)({\\bf X})=\\text{relu}\\big({\\bf X}{\\bf W}^{(1)}\\overset{bc}{+}{\\boldsymbol b}^{(1)}\\big){\\bf W}^{(2)}\\overset{bc}{+}b^{(2)}\\)\n\\(\\quad=\\begin{bmatrix} -4.5\\times\\text{relu}(x_1) -9.0 \\times \\text{relu}(-x_1) +4.5 \\\\ -4.5\\times\\text{relu}(x_2) -9.0 \\times\\text{relu}(-x_2) + 4.5 \\\\ \\dots \\\\ -4.5\\times \\text{relu}(x_n) -9.0 \\times\\text{relu}(-x_n)+4.5 \\end{bmatrix}\\)\n\n\\({\\bf W}^{(2)}=\\begin{bmatrix} -4.5 \\\\ -9 \\end{bmatrix}\\)\n\\(b^{(2)}=4.5\\)\n\n(5) \\(\\textup{pwlinr}({\\bf X})=(l_2 \\circ a_1\\circ l_1)({\\bf X})=\\text{relu}\\big({\\bf X}{\\bf W}^{(1)}\\overset{bc}{+}{\\boldsymbol b}^{(1)}\\big){\\bf W}^{(2)}\\overset{bc}{+}b^{(2)}\\)\n\\(\\quad =\\begin{bmatrix} -4.5\\times\\text{relu}(x_1) -9.0 \\times \\text{relu}(-x_1) +4.5 \\\\ -4.5\\times\\text{relu}(x_2) -9.0 \\times\\text{relu}(-x_2) + 4.5 \\\\ \\dots \\\\ -4.5\\times \\text{relu}(x_n) -9.0 \\times\\text{relu}(-x_n)+4.5 \\end{bmatrix}\\)\n\n\n\n\n4. ìŠ¤í™ì˜ì—­ì„¤ ì í•© ğŸ“\n- ë‹¤ì‹œí•œë²ˆ ë°ì´í„° ì •ë¦¬\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DL2025/main/posts/ironyofspec.csv\")\n\nğŸ—£ï¸(\n\ntorch.tensor(df.x)\n\ntensor([-1.0000, -0.9990, -0.9980,  ...,  0.9980,  0.9990,  1.0000],\n       dtype=torch.float64)\n\n\n\ndtype=torch.float64ì„ ë³´ê¸° ì‹«ìœ¼ë©´ ë‹¤ìŒê³¼ ê°™ì´ í•˜ë©´ ë¨ (pytorchëŠ” ê¸°ë³¸ì ìœ¼ë¡œ 32í˜•ìœ¼ë¡œ ì €ì¥ë˜ëŠ” ê²ƒì„ ì›í•¨)\n\n\ntorch.tensor(df.x).float() # vector\n\ntensor([-1.0000, -0.9990, -0.9980,  ...,  0.9980,  0.9990,  1.0000])\n\n\n\nx = torch.tensor(df.x).float().reshape(-1,1)\ny = torch.tensor(df.y).float().reshape(-1,1)\nprob = torch.tensor(df.prob).float().reshape(-1,1)\n\n\nplt.plot(x,y,'.',alpha=0.03)\n\n\n\n\n\n\n\n\n\nprob: ì°¸ê°’, ê´€ì¸¡ ë¶ˆê°€\n\n)ğŸ—£ï¸\n\nx = torch.tensor(df.x).float().reshape(-1,1)\ny = torch.tensor(df.y).float().reshape(-1,1)\nprob = torch.tensor(df.prob).float().reshape(-1,1)\n\n\nplt.plot(x,y,'.',alpha=0.03)\nplt.plot(x,prob,'--')\n\n\n\n\n\n\n\n\n- Step1ì— ëŒ€í•œ ìƒê°: ë„¤íŠ¸ì›Œí¬ë¥¼ ì–´ë–»ê²Œ ë§Œë“¤ê¹Œ? = ì•„í‚¤í…ì²˜ë¥¼ ì–´ë–»ê²Œ ë§Œë“¤ê¹Œ? = ëª¨ë¸ë§\n\\[\\underset{(n,1)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,2)}{\\boldsymbol u^{(1)}} \\overset{a_1}{\\to} \\underset{(n,2)}{\\boldsymbol v^{(1)}} \\overset{l_1}{\\to} \\underset{(n,1)}{\\boldsymbol u^{(2)}} \\overset{a_2}{\\to} \\underset{(n,1)}{\\boldsymbol v^{(2)}}=\\underset{(n,1)}{\\hat{\\boldsymbol y}}\\]\n\n\\(l_1\\): torch.nn.Linear(1,2,bias=False)\n\\(a_1\\): torch.nn.ReLU()\n\\(l_2\\): torch.nn.Linear(2,1,bias=True)\n\\(a_2\\): torch.nn.Sigmoid()\n\nğŸ—£ï¸ l2ê¹Œì§€ëŠ” êº¾ì¸ ì„ \n- Step1-4\n\ntorch.manual_seed(1)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,2,bias=False),\n    torch.nn.ReLU(),\n    torch.nn.Linear(2,1,bias=True),\n    torch.nn.Sigmoid()\n)\nloss_fn = torch.nn.BCELoss() \noptimizr = torch.optim.Adam(net.parameters())\n\nğŸ—£ï¸ lr ë”°ë¡œ ì„¤ì • ì•ˆí•˜ë©´ defaultë¡œ ë“¤ì–´ê°\n\nfor epoc in range(5000):\n    ## step1\n    yhat = net(x)\n    ## step2\n    loss = loss_fn(yhat,y)\n    ## step3\n    loss.backward()\n    ## step4\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'.',alpha=0.03)\nplt.plot(x,prob,'--')\nplt.plot(x,yhat.data,'--')\n\n\n\n\n\n\n\n\ní•œë²ˆë”~\n\nfor epoc in range(5000):\n    ## step1\n    yhat = net(x)\n    ## step2\n    loss = loss_fn(yhat,y)\n    ## step3\n    loss.backward()\n    ## step4\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'.',alpha=0.03)\nplt.plot(x,prob,'--')\nplt.plot(x,yhat.data,'--')\n\n\n\n\n\n\n\n\nğŸ—£ï¸(\n\n???\n\n\nnet\n\nSequential(\n  (0): Linear(in_features=1, out_features=2, bias=False)\n  (1): ReLU()\n  (2): Linear(in_features=2, out_features=1, bias=True)\n  (3): Sigmoid()\n)\n\n\n\nnet[0](x) # ì²˜ìŒ linear transform í†µê³¼\n\ntensor([[-2.8167,  3.9404],\n        [-2.8139,  3.9364],\n        [-2.8111,  3.9325],\n        ...,\n        [ 2.8111, -3.9325],\n        [ 2.8139, -3.9364],\n        [ 2.8167, -3.9404]], grad_fn=&lt;MmBackward0&gt;)\n\n\n\nplt.plot(x,net[0](x).data)\n\n\n\n\n\n\n\n\n\nê¸°ìš¸ê¸° íŠœë‹ì´ ì´ë¯¸ ë˜ì–´ ìˆìŒ (ìƒê°ëŒ€ë¡œë¼ë©´ ë‚˜ì¤‘ì— ë˜ì–´ì•¼ í•¨)\n\n\nplt.plot(x,net[1](net[0](x)).data) # ë ë£¨\n\n\n\n\n\n\n\n\n\nplt.plot(x,net[2](net[1](net[0](x))).data) # 2ë²ˆì§¸ linear transform\n\n\n\n\n\n\n\n\n\nplt.plot(x,net[3](net[2](net[1](net[0](x)))).data) # sigmoid\n\n\n\n\n\n\n\n\n\nì›ë˜ë¼ë©´ u = x @ [1 -1] ì²˜ëŸ¼ ê·¸ë˜í”„ í‹€ì„ ë§ì¶°ë†“ê³  ê¸°ìš¸ê¸°ë¥¼ ë¯¸ì„¸ì¡°ì •í•˜ì˜€ì§€ë§Œ\nê¸°ìš¸ê¸°ë¥¼ ì²˜ìŒë¶€í„° ë¯¸ì„¸ì¡°ì •í•˜ë©´ì„œ í•´ë„ ì˜ ë§ì„ ìˆ˜ ìˆìŒ\n\nì´ ë§ì€ global minì„ í•˜ë‚˜ë§Œ ê°–ëŠ” ê²ƒì´ ì•„ë‹˜ (ì—¬ëŸ¬ ê°œì˜ ìµœì €ê°’ì´ ìˆì„ ìˆ˜ ìˆìŒ)\n\n\n)ğŸ—£ï¸"
  },
  {
    "objectID": "posts/03wk-1.html",
    "href": "posts/03wk-1.html",
    "title": "03wk-1: (íšŒê·€, ë¡œì§€ìŠ¤í‹±) â€“ íŒŒì´í† ì¹˜ì‹ ì½”ë”©íŒ¨í„´ (2), ë¡œì§€ìŠ¤í‹± ëª¨í˜•",
    "section": "",
    "text": "ğŸ“˜ Note Format Guide\nThis format serves as a structured guide for organizing lecture content, personal interpretation, experiments, and study-related questions.\nğŸ“ ğŸ—£ï¸ âœï¸ ğŸ”¬ â“"
  },
  {
    "objectID": "posts/03wk-1.html#a.-biasì˜-ì‚¬ìš©",
    "href": "posts/03wk-1.html#a.-biasì˜-ì‚¬ìš©",
    "title": "03wk-1: (íšŒê·€, ë¡œì§€ìŠ¤í‹±) â€“ íŒŒì´í† ì¹˜ì‹ ì½”ë”©íŒ¨í„´ (2), ë¡œì§€ìŠ¤í‹± ëª¨í˜•",
    "section": "A. biasì˜ ì‚¬ìš©",
    "text": "A. biasì˜ ì‚¬ìš©\nğŸ—£ï¸(\n\nì €ë²ˆ ì‹œê°„ ì½”ë“œ\n\n\nnet = torch.nn.Linear(2, 1, bias=False)\nnet.weight.data = torch.tensor([[-5.0, 10.0]])\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD(net.parameters(), lr=0.1) # lr: learning rate\n\n# step 1~4\nfor epoc in range(30):\n    # 1\n    yhat = net(X)\n    # 2\n    loss = loss_fn(yhat,y)\n    # 3\n    loss.backward()\n    # 4\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nnet.weight # ì§€ë‚œ ì‹œê°„ ê²°ê³¼ì™€ ë™ì¼\n\nParameter containing:\ntensor([[2.4290, 4.0144]], requires_grad=True)\n\n\n\nì´ì œ bias=True\n\n\n# net(X) = X@net.weight.T # í˜„ì¬ ì´ë ‡ê²Œ ì•Œê³  ìˆìœ¼ë‚˜ ì‚¬ì‹¤ì€ ì•„ë‹˜\n\n\nnet.weight\n\nParameter containing:\ntensor([[2.4290, 4.0144]], requires_grad=True)\n\n\n\nprint(net.bias) # í˜„ì¬ëŠ” bias=False\n\nNone\n\n\n\n# net(X) = X@net.weight.T + net.bias # ì‚¬ì‹¤ì€ ì´ê²Œ ë§ìŒ\n\n\në‘˜ì€ ë™ì¼\n\ny = X@W + Ïµ # y = net(X) + Ïµ\ny = w0hat + x*w1hat + Ïµ # y = net(x) + Ïµ\n\nnet(X) = X@net.weight.T + net.bias ì—ì„œ Xê°€ xë¡œ ë°”ë€Œë©´\n\nnet(x) = x@net.weight.T + net.bias\nnet(x) = w0hat + x*w1hat ì´ë¯€ë¡œ\nnet.biasì— í•´ë‹¹í•˜ëŠ” ê²ƒì€ w0hat\nnet.weight.Tì— í•´ë‹¹í•˜ëŠ” ê²ƒì€ w1hat ìœ¼ë¡œ ìƒê° ê°€ëŠ¥\n\nìœ„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ net(x)ë¥¼ ë§Œë“¤ë©´\n\nxëŠ” (n,1)ì´ë¯€ë¡œ input ì°¨ì›ì€ 1\n\n\n\nnet = torch.nn.Linear(1,1,bias=True)\nnet\n\nLinear(in_features=1, out_features=1, bias=True)\n\n\n\nnet.weight # 1x1 matrix\n\nParameter containing:\ntensor([[0.3480]], requires_grad=True)\n\n\n\nnet.bias # length 1ì¸ vector\n\nParameter containing:\ntensor([0.7757], requires_grad=True)\n\n\n\nnet.weight.T # net(x) = x@net.weight.T + net.bias ì—ì„œ net.weight.TëŠ” w1hat\n\ntensor([[0.3480]], grad_fn=&lt;PermuteBackward0&gt;)\n\n\n\nnet.weight.data = torch.tensor([[10.0]])\nnet.weight.data\n\ntensor([[10.]])\n\n\n\nnet.bias.data = torch.tensor([[-5.0]]) # net(x) = x@net.weight.T + net.bias ì—ì„œ net.biasëŠ” w0hat\nnet.bias.data\n\ntensor([[-5.]])\n\n\n\nìœ„ì˜ ë‚´ìš©ì„ ì €ë²ˆ ì‹œê°„ ì½”ë“œì— ë°˜ì˜í•˜ë©´\n\nnet ìˆ˜ì •, weight ë° bias ê°’ ìˆ˜ì •\nnet(X) -&gt; net(x)\n\n\n\nnet = torch.nn.Linear(1, 1, bias=True)\nnet.weight.data = torch.tensor([[10.0]])\nnet.bias.data = torch.tensor([[-5.0]])\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD(net.parameters(), lr=0.1) # lr: learning rate\n\n# step 1~4\nfor epoc in range(30):\n    # 1\n    yhat = net(x)\n    # 2\n    loss = loss_fn(yhat,y)\n    # 3\n    loss.backward()\n    # 4\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nnet.weight\n\nParameter containing:\ntensor([[4.0144]], requires_grad=True)\n\n\n\nnet.bias\n\nParameter containing:\ntensor([[2.4290]], requires_grad=True)\n\n\n\nì €ë²ˆ ì‹œê°„ ê²°ê³¼ì™€ ë™ì¼\n\n)ğŸ—£ï¸\nnetì—ì„œ biasë¥¼ ì‚¬ìš©\n\n# step1ì„ ìœ„í•œ ì‚¬ì „ì¤€ë¹„\nnet = torch.nn.Linear(\n    in_features=1,\n    out_features=1,\n    bias=True\n) # net(x) = x@net.weight.T + net.bias \nnet.bias.data = torch.tensor([-5.0])\nnet.weight.data = torch.tensor([[10.0]])\n# step2ë¥¼ ìœ„í•œ ì‚¬ì „ì¤€ë¹„\nloss_fn = torch.nn.MSELoss()\n# step4ë¥¼ ìœ„í•œ ì‚¬ì „ì¤€ë¹„ \noptimizr = torch.optim.SGD(net.parameters(),lr=0.1)\nfor epoc in range(30):\n    # step1: yhat \n    yhat = net(x)\n    # step2: loss\n    loss = loss_fn(yhat,y)\n    # step3: ë¯¸ë¶„\n    loss.backward()\n    # step4: update\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nnet.bias.data, net.weight.data\n\n(tensor([2.4290]), tensor([[4.0144]]))\n\n\n#"
  },
  {
    "objectID": "posts/03wk-1.html#b.-ì˜ëª»ëœ-ì½”ë“œ",
    "href": "posts/03wk-1.html#b.-ì˜ëª»ëœ-ì½”ë“œ",
    "title": "03wk-1: (íšŒê·€, ë¡œì§€ìŠ¤í‹±) â€“ íŒŒì´í† ì¹˜ì‹ ì½”ë”©íŒ¨í„´ (2), ë¡œì§€ìŠ¤í‹± ëª¨í˜•",
    "section": "B. ì˜ëª»ëœ(?) ì½”ë“œ",
    "text": "B. ì˜ëª»ëœ(?) ì½”ë“œ\nğŸ—£ï¸ biasì˜ defaultëŠ” Trueì´ë¯€ë¡œ ì €ë²ˆ ì‹œê°„ ì½”ë“œì—ì„œ biasë¥¼ ì§€ìš°ë©´ bias=Trueê°€ ë¨\n\n# step1ì„ ìœ„í•œ ì‚¬ì „ì¤€ë¹„\nnet = torch.nn.Linear(\n    in_features=2,\n    out_features=1,\n)\nnet.weight.data = torch.tensor([[-5.0,  10.0]])\n# step2ë¥¼ ìœ„í•œ ì‚¬ì „ì¤€ë¹„\nloss_fn = torch.nn.MSELoss()\n# step4ë¥¼ ìœ„í•œ ì‚¬ì „ì¤€ë¹„ \noptimizr = torch.optim.SGD(net.parameters(),lr=0.1)\nfor epoc in range(30):\n    # step1: yhat \n    yhat = net(X)\n    # step2: loss\n    loss = loss_fn(yhat,y)\n    # step3: ë¯¸ë¶„\n    loss.backward()\n    # step4: update\n    optimizr.step()\n    optimizr.zero_grad()\n\nğŸ—£ï¸(\n\nnet.weight # ê²°ê³¼ê°€ ë§ì´ ë‹¬ë¼ì§\n\nParameter containing:\ntensor([[-1.1114,  4.0080]], requires_grad=True)\n\n\n\nplt.plot(x,y,'o')\nplt.plot(x,net(X).data, '--')\n\n\n\n\n\n\n\n\n\nê·¸ëŸ°ë° ê²°ê³¼ë¥¼ ì‹œê°í™”í•´ë³´ë©´ ë‚˜ì˜ì§€ ì•ŠìŒ\n\n)ğŸ—£ï¸\n- ê²°ê³¼ì‹œê°í™”\n\nplt.plot(x,y,'o')\nplt.plot(x,yhat.data,'--')\nplt.title(f'net.weight={net.weight.data.reshape(-1)}');\n\n\n\n\n\n\n\n\n- ë‚˜ì˜ì§€ ì•Šì€ ì´ìœ ?\nâœï¸ ë°”ë¡œ ë°‘ì˜ ì½”ë“œëŠ” í¸ì˜ìƒ ì‹¤í–‰ X\n# step1ì„ ìœ„í•œ ì‚¬ì „ì¤€ë¹„\nnet = torch.nn.Linear(\n    in_features=2,\n    out_features=1,\n)\nyhat = net(X) = X@net.weight.T + net.bias\n\nnet.weight\n\nParameter containing:\ntensor([[-1.1114,  4.0080]], requires_grad=True)\n\n\n\nnet.bias\n\nParameter containing:\ntensor([3.5562], requires_grad=True)\n\n\nğŸ—£ï¸(\n\nì›ë˜ëŒ€ë¡œë¼ë©´ ì ˆí¸, ê¸°ìš¸ê¸° ì´ 2ê°œì˜ parameterë§Œ í•™ìŠµí•´ì•¼í•˜ëŠ”ë° ìœ„ì˜ ê²°ê³¼ëŠ” 3ê°œë¥¼ í•™ìŠµí•¨\nyhat ê³„ì‚° ê³¼ì •ì„ ì‚´í´ë³´ë©´\n\n\nX[[0],:] # nx2 martixì—ì„œ ì²« ë²ˆì§¸ observationë§Œ ë½‘ìŒ\n\ntensor([[ 1.0000, -2.4821]])\n\n\n\nyhat[:1] # ì´ yhatì´ ì–´ë–»ê²Œ ë‚˜ì™”ëŠ”ì§€ ë³´ë©´\n\ntensor([[-7.5063]], grad_fn=&lt;SliceBackward0&gt;)\n\n\n\nX[[0],:] @ net.weight.T + net.bias\n\n\n-1.1114 * 1.0000 + 4.0080 * (-2.4821) + 3.5562 # ì•½ê°„ì˜ ì°¨ì´ëŠ” ì†Œìˆ˜ì  ì°¨ì´\n\n-7.503456799999999\n\n\n\n-2.4821ì€ x, ë‹¤ìŒê³¼ ê°™ì´ ì •ë¦¬í•˜ë©´\n\n\n-1.1114 * 1.0000 + 3.5562\n\n2.4448\n\n\n\nì ˆí¸ì— ëŒ€í•œ True ê°’: 2.5, ê¸°ìš¸ê¸°ì— ëŒ€í•œ True ê°’: 4\n\nì¦‰, ì ˆí¸ì„ 2ê°œë¡œ ë‚˜ëˆ ì„œ í•™ìŠµí•¨ (ë¹„íš¨ìœ¨ì )\n\nê·¸ëŸ¬ë©´ ì´ê²Œ í‹€ë¦° ê²ƒì¸ê°€?\n\níšŒê·€ë¶„ì„ì—ì„œ ì´ë ‡ê²Œ ëª¨ë¸ë§í•˜ë©´ í‹€ë¦¼ (í†µê³„í•™ì  ê´€ì )\ní•˜ì§€ë§Œ í•™ìŠµ ê²°ê³¼ ìì²´ëŠ” ë§ìŒ (ë¹„íš¨ìœ¨ì ì¼ë¿)\nAIë‚˜ DL ê´€ì ì—ì„œëŠ” ìµœì ì˜ parameter ê°œìˆ˜ê°€ ì •í•´ì§€ì§€ ì•Šì€ ê²½ìš°ê°€ ë§ì•„ì„œ\në¹„íš¨ìœ¨ì ì´ê¸´í•´ë„ ì˜ëª»ìœ¼ë¡œ ê¹Œì§€ëŠ” ìƒê° X\n\n\n)ğŸ—£ï¸"
  },
  {
    "objectID": "posts/03wk-1.html#a.-hatbf-y",
    "href": "posts/03wk-1.html#a.-hatbf-y",
    "title": "03wk-1: (íšŒê·€, ë¡œì§€ìŠ¤í‹±) â€“ íŒŒì´í† ì¹˜ì‹ ì½”ë”©íŒ¨í„´ (2), ë¡œì§€ìŠ¤í‹± ëª¨í˜•",
    "section": "A. \\(\\hat{\\bf y} = ??\\)",
    "text": "A. \\(\\hat{\\bf y} = ??\\)\nğŸ—£ï¸(\n\nì¼ë°˜ì ìœ¼ë¡œ íšŒê·€ë¶„ì„ì—ì„œ ì„¤ëª… ë³€ìˆ˜, ë°˜ì‘ ë³€ìˆ˜ ëª¨ë‘ ì—°ì†í˜• ë³€ìˆ˜ì´ì§€ë§Œ,\nyê°€ ìƒíƒœë¥¼ ì˜ë¯¸í•  ë•Œê°€ ìˆìŒ (ex. X = ì ìˆ˜, y = í•©ê²©/ë¶ˆí•©ê²©)\n\ní•©ê²©ì„ 1, ë¶ˆí•©ê²©ì„ 0ìœ¼ë¡œ ìˆ«ìí™”í•˜ë©´\nyëŠ” 0 ë˜ëŠ” 1ë§Œ ê°€ì§\n\nì´ëŸ¬í•œ ìë£ŒëŠ” ë§¤ìš° ë§ìŒ\n\n)ğŸ—£ï¸\n- \\({\\bf X}\\)ë¥¼ ê°€ì§€ê³  \\({\\bf y}\\)ë¥¼ ë§ì¶”ëŠ” ì•„ë˜ì™€ ê°™ì€ ë¬¸ì œ\n\nx = torch.tensor([-6,-5,-4,-3,-2,-1, 0, 1, 2, 3, 4, 5, 6.0]).reshape(-1,1)\ny = torch.tensor([ 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1]).reshape(-1,1)\nplt.plot(x,y,'o')\n\n\n\n\n\n\n\n\nğŸ—£ï¸(\n\nxê°€ ì¦ê°€í•¨ì— ë”°ë¼ yê°€ 1ì´ ë  ê²ƒ ê°™ê³ , xê°€ ê°ì†Œí•¨ì— ë”°ë¼ yê°€ 0ë  ê²ƒ ê°™ìŒ\nëª¨ë¸ë§ì„ ì–´ë–»ê²Œ?\nëª¨ë¸ë§: observed dataë¥¼ ë³´ê³  error-freeí•œ structureë¥¼ ì°¾ëŠ” ê²ƒ\nì—¬ê¸°ì„œ error-freeí•œ structureëŠ”?\n\nerror-free: ìš´ì ì¸ ìš”ì†Œê°€ ì—†ìŒ\n\nìš´ì ì¸ ìš”ì†Œ?\n\nì´ ê²½ìš° 0ì ì¸ë° í•©ê²©, 1ì ì¸ë° ë¶ˆí•©ê²©\në„ì €íˆ ë°›ì•„ë“¤ì´ì§€ ëª»í•  ìˆ˜ ìˆìŒ\n(ì´ë ‡ê²Œ ê²½ê³„ì— ìˆëŠ”ë° ìš´ì ì¸ ìš”ì†Œë¡œ ê²°ì •ë˜ëŠ” ê²½ìš°)\n\nì´ê²ƒì„ ì¼ë°˜ì ì¸ íšŒê·€ë¶„ì„ì²˜ëŸ¼ underlying(error-free)ì´ ìˆê³  ì˜¤ì°¨í•­ì„ ì •ê·œë¶„í¬ì—ì„œ errorë¥¼ ë½‘ì€ ê²ƒìœ¼ë¡œ ì„¤ëª…í•˜ë©´ X\n\nì´ì „ì˜ cafe ë°ì´í„°ëŠ” ì´ë ‡ê²Œ ì„¤ëª… ê°€ëŠ¥\n\nì°¨ë¼ë¦¬ underlyingì—ì„œ xê°’ì— ëŒ€ì‘í•˜ëŠ” yê°’ì„ ì„±ê³µ í™•ë¥ ë¡œ í•˜ëŠ” ë² ë¥´ëˆ„ì´ ì‹œí–‰ìœ¼ë¡œ ì„¤ëª…í•˜ë©´ ê·¸ëŸ´ë“¯ í•¨\n\nunderlying: ì—¬ê¸°ì„œëŠ” ê´€ì¸¡ê°’ì´ ì•„ë‹ˆê³  í™•ë¥ ì„ ì˜ë¯¸í•˜ëŠ” ê³¡ì„ ìœ¼ë¡œ í•´ì„\nì„±ê³µ í™•ë¥ ì´ 0.9ì¸ ë² ë¥´ëˆ„ì´ ì‹œí–‰ì„ í–ˆëŠ”ë° 0.1ì¸ í™•ë¥ ì˜ ê²°ê³¼ê°€ ë‚˜ì™€ë„ ì–´ì©” ìˆ˜ ì—†ìŒ (ìš´ì ì¸ ìš”ì†Œ)\nì˜¤ì°¨: ë² ë¥´ëˆ„ì´ ì‹œí–‰ì— ì˜í•´ ìƒì„±ë˜ëŠ” ëœë¤ì„±\n\ní†µê³„í•™ê³¼ì‹ ëª¨ë¸ë§\n\nstructure(error-free)ë¿ë§Œ ì•„ë‹ˆë¼ (ì´ê²ƒë„ ì–´ë ¤ì›€, ì—¬ê¸°ê¹Œì§€ëŠ” ë¹„í†µê³„í•™ê³¼ì‹)\nê´€ì¸¡ì¹˜ë¥¼ error termì„ ì´ìš©í•´ ì„¤ëª… (ìš´ì ì¸ ìš”ì†Œê°€ ì–´ë–»ê²Œ ì‘ìš©í•˜ëŠ”ì§€)\n\nyhat\n\nunderlying\nyê°€ 0 ë˜ëŠ” 1ë§Œ ê°€ì§€ë¯€ë¡œ yhatë„ ê·¸ë˜ì•¼í•˜ë‚˜ ì‹¶ì§€ë§Œ X (íšŒê·€ë¶„ì„ì—ì„œ ì˜¤ì°¨í•­ì´ í¬í•¨ëœ ê´€ì¸¡ì¹˜ë¥¼ ë”°ë¼ê°€ëŠ” ê²ƒê³¼ ë™ì¼)\nyhatì€ 0ê³¼ 1ì‚¬ì´ì˜ ìˆ«ì (ëª¨ë¸ë§ ëŒ€ìƒ: ê´€ì¸¡ì¹˜ê°€ ì•„ë‹ˆë¼ ì¶”ì„¸ì„ )\n\n\në‹¤ìŒê³¼ ê°™ì´ ëª¨ë¸ë§ì„ í•´ë³´ë©´\n\n\nprob = torch.exp(x) / (torch.exp(x) + 1)\nplt.plot(x,y,'o')\nplt.plot(x,prob,'--')\n\n\n\n\n\n\n\n\n\n\\(\\frac{e^x}{e^x + 1}\\)\n\n\\(x\\)ê°€ ì»¤ì§€ë©´ 1ì— ê°€ê¹Œì›Œì§€ê³ \n\\(x=0\\)ì´ë©´ 1/2\n\\(x\\)ê°€ ì‘ì•„ì§€ë©´ 0ì— ê°€ê¹Œì›Œì§\n\ní•˜ì§€ë§Œ ì´ ìˆ˜ì‹ì€ ì´ ê²½ìš°ì—ë§Œ ë§ê³  í™•ì¥ì„±ì´ ë–¨ì–´ì§\n\n)ğŸ—£ï¸\n- ì•„ë˜ì™€ ê°™ì´ ëª¨í˜•í™” í•˜ë©´?\n\nplt.plot(x,y,'o', label=r\"observed data (with error) = $(x_i,y_i)$\")\nplt.plot(x,torch.exp(x)/(1+torch.exp(x)),'o--', label = \"underlying (without error)\")\nplt.legend()"
  },
  {
    "objectID": "posts/03wk-1.html#b.-hatbf-y-fracexptextlinrbf-x1exptextlinrbf-x",
    "href": "posts/03wk-1.html#b.-hatbf-y-fracexptextlinrbf-x1exptextlinrbf-x",
    "title": "03wk-1: (íšŒê·€, ë¡œì§€ìŠ¤í‹±) â€“ íŒŒì´í† ì¹˜ì‹ ì½”ë”©íŒ¨í„´ (2), ë¡œì§€ìŠ¤í‹± ëª¨í˜•",
    "section": "B. \\(\\hat{\\bf y} = \\frac{\\exp(\\text{linr}({\\bf X}))}{1+\\exp(\\text{linr}({\\bf X}))}\\)",
    "text": "B. \\(\\hat{\\bf y} = \\frac{\\exp(\\text{linr}({\\bf X}))}{1+\\exp(\\text{linr}({\\bf X}))}\\)\n- ê±±ì •: ì‚°ì ë„ê°€ ê¼­ ì•„ë˜ì™€ ê°™ì€ ë°©ì‹ì´ ì•„ë‹ˆë¼ë©´ ì–´ì©Œì§€?\n\nplt.plot(x,y,'o')\n\n\n\n\n\n\n\n\n\n\\(x\\)ê°€ ì¦ê°€í• ìˆ˜ë¡ \\(y\\)ê°€ 0ì´ ëœë‹¤ë©´?\n0ê·¼ì²˜ì—ì„œ ë³€í™”ê°€ ì¼ì–´ë‚˜ì§€ ì•Šê³  2ê·¼ì²˜ì—ì„œ ë³€í™”ê°€ ì¼ì–´ë‚œë‹¤ë©´?\në³€í™”ê°€ ì¢€ ë” ê¸‰í•˜ê²Œ (í˜¹ì€ ì™„ë§Œí•˜ê²Œ ì¼ì–´ë‚œë‹¤ë©´?)\n\nğŸ—£ï¸(\n\n\\(\\frac{e^{-x}}{e^{-x} + 1}\\)\ní•©ê²©ë¥ ì´ ë‚®ì€ ê²½ìš°\nstrictí•˜ê²Œ ê²°ê³¼ê°€ ë‚˜ë‰˜ëŠ” ê²½ìš°(ex. ì¥í•™ê¸ˆ)\n\n\nplt.plot(x,y,'o', label=r\"observed data (with error) = $(x_i,y_i)$\")\nplt.plot(x,torch.exp(-x)/(1+torch.exp(-x)),'o--', label = \"underlying (without error)\")\nplt.legend()\n\n\n\n\n\n\n\n\n\nplt.plot(x,y,'o', label=r\"observed data (with error) = $(x_i,y_i)$\")\nplt.plot(x,torch.exp(-x+3)/(1+torch.exp(-x+3)),'o--', label = \"underlying (without error)\")\nplt.legend()\n\n\n\n\n\n\n\n\n\nplt.plot(x,y,'o', label=r\"observed data (with error) = $(x_i,y_i)$\")\nplt.plot(x,torch.exp(5*x+3)/(1+torch.exp(5*x+3)),'o--', label = \"underlying (without error)\")\nplt.legend()\n\n\n\n\n\n\n\n\n\nì´ëŸ¬í•œ 5*x+3 ë“±ì„ ì¼ë°˜í™”í•˜ë©´\n\n5x+3 = w0hat + w1hat  x : íšŒê·€ë¶„ì„ ì„ í˜• ëª¨í˜•\n= w0hat + w1hat * x = linr(x) # xë¥¼ linear transformì‹œí‚´\n\n\nğŸ”¬ 0ê·¼ì²˜ì—ì„œ ë³€í™”ê°€ ì¼ì–´ë‚˜ì§€ ì•Šê³  2ê·¼ì²˜ì—ì„œ ë³€í™”ê°€ ì¼ì–´ë‚œë‹¤ë©´?\n\nplt.plot(x,y,'o', label=r\"observed data (with error) = $(x_i,y_i)$\")\nplt.plot(x,torch.exp(x-2)/(1+torch.exp(x-2)),'o--', label = \"underlying (without error)\")\nplt.legend()\n\n\n\n\n\n\n\n\nğŸ”¬ ë³€í™”ê°€ ì¢€ ë” ê¸‰í•˜ê²Œ ì¼ì–´ë‚œë‹¤ë©´?\n\nplt.plot(x,y,'o', label=r\"observed data (with error) = $(x_i,y_i)$\")\nplt.plot(x,torch.exp(3*x)/(1+torch.exp(3*x)),'o--', label = \"underlying (without error)\")\nplt.legend()\n\n\n\n\n\n\n\n\nğŸ”¬ ë³€í™”ê°€ ì¢€ ë” ì™„ë§Œí•˜ê²Œ ì¼ì–´ë‚œë‹¤ë©´?\n\nplt.plot(x,y,'o', label=r\"observed data (with error) = $(x_i,y_i)$\")\nplt.plot(x,torch.exp(x/3)/(1+torch.exp(x/3)),'o--', label = \"underlying (without error)\")\nplt.legend()\n\n\n\n\n\n\n\n\n)ğŸ—£ï¸\n\nplt.plot(x,y,'o', label=r\"observed data (with error) = $(x_i,y_i)$\")\nplt.plot(x,torch.exp(5*x+3)/(1+torch.exp(5*x+3)),'o--', label = \"underlying (without error)\")\nplt.legend()\n\n\n\n\n\n\n\n\n- ê±±ì •í•´ê²°\n\n#plt.plot(x,y,'o', label=r\"observed data (with error) = $(x_i,y_i)$\")\nplt.plot(x,torch.exp(x)/(1+torch.exp(x)),'o--', label = \"underlying type1 (without error)\", color=\"C1\")\nplt.plot(x,torch.exp(5*x)/(1+torch.exp(5*x)),'o--', label = \"underlying type2 (without error)\", color=\"C2\")\nplt.legend()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\níšŒê·€ vs ë¡œì§€ìŠ¤í‹±\n\n\\({\\bf X} \\to {\\bf y}\\) ì— ëŒ€í•œ íŒ¨í„´ì´ \\(\\text{linr}({\\bf X}) \\approx {\\bf y}\\) ì´ë¼ë©´ íšŒê·€!\n\\({\\bf X} \\to {\\bf y}\\) ì— ëŒ€í•œ íŒ¨í„´ì´ \\(\\frac{\\exp(\\text{linr}({\\bf X}))}{1+\\exp(\\text{linr}({\\bf X}))} \\approx {\\bf y}\\) ì´ë¼ë©´ ë¡œì§€ìŠ¤í‹±!\n\n\n\nğŸ—£ï¸(\n\nXë¥¼ linear transformí–ˆë”ë‹ˆ ì„  ìì²´ê°€ yì™€ ë¹„ìŠ· =&gt; íšŒê·€\nìœ„ì˜ ê·¸ë˜í”„ë¥¼ ê·¸ë¦¬ëŠ” ì‹ìœ¼ë¡œ í–ˆë”ë‹ˆ yì™€ ë¹„ìŠ· =&gt; ë¡œì§€ìŠ¤í‹±\n\nì •í™•íˆëŠ” í™•ë¥ ì´ yì™€ ë¹„ìŠ·í•˜ë‹¤ë©´ (y ìì²´ëŠ” 0 ë˜ëŠ” 1)\n\n\n)ğŸ—£ï¸"
  },
  {
    "objectID": "posts/03wk-1.html#c.-ë¡œì§€ìŠ¤í‹±-ëª¨í˜•",
    "href": "posts/03wk-1.html#c.-ë¡œì§€ìŠ¤í‹±-ëª¨í˜•",
    "title": "03wk-1: (íšŒê·€, ë¡œì§€ìŠ¤í‹±) â€“ íŒŒì´í† ì¹˜ì‹ ì½”ë”©íŒ¨í„´ (2), ë¡œì§€ìŠ¤í‹± ëª¨í˜•",
    "section": "C. ë¡œì§€ìŠ¤í‹± ëª¨í˜•",
    "text": "C. ë¡œì§€ìŠ¤í‹± ëª¨í˜•\n- \\(x\\)ê°€ ì»¤ì§ˆìˆ˜ë¡ (í˜¹ì€ ì‘ì•„ì§ˆìˆ˜ë¡) \\(y=1\\)ì´ ì˜ë‚˜ì˜¤ëŠ” ëª¨í˜•ì€ ì•„ë˜ì™€ ê°™ì´ ì„¤ê³„í•  ìˆ˜ ìˆìŒ &lt;â€” ì™¸ìš°ì„¸ìš”!!!\n\n\\(y_i \\sim {\\cal B}(\\pi_i),\\quad\\) where \\(\\pi_i = \\frac{\\exp(w_0+w_1x_i)}{1+\\exp(w_0+w_1x_i)} = \\frac{1}{1+\\exp(-w_0-w_1x_i)}\\)\n\\(\\hat{y}_i= \\frac{\\exp(\\hat{w}_0+\\hat{w}_1x_i)}{1+\\exp(\\hat{w}_0+\\hat{w}_1x_i)}=\\frac{1}{1+\\exp(-\\hat{w}_0-\\hat{w}_1x_i)}\\)\nğŸ—£ï¸\n\n\\(\\pi_i\\)ëŠ” í™•ë¥ ì„ ì˜ë¯¸\n\\(\\frac{e^{x}}{1 + e^{x}}\\) = \\(\\frac{1}{e^{-x} + 1}\\) ì—ì„œ \\(x\\) ëŒ€ì‹  \\(w_0+w_1x_i\\)\nì±… ë§ˆë‹¤ ë‹¤ë¥´ì§€ë§Œ ì˜¤ë¥¸ìª½ì²˜ëŸ¼ ë§ì´ ì”€\n\n\n- íšŒê·€ëª¨í˜•ê³¼ ë¡œì§€ìŠ¤í‹± ëª¨í˜•ì˜ ë¹„êµ\n\níšŒê·€ëª¨í˜•: \\(y_i \\sim {\\cal N}(w_0+w_1x_i, \\sigma^2)\\)1\në¡œì§€ìŠ¤í‹±: \\(y_i \\sim {\\cal B}\\big(\\frac{\\exp(w_0+w_1x_i)}{1+\\exp(w_0+w_1x_i)}\\big)\\)\nğŸ—£ï¸\n\níšŒê·€ëª¨í˜•: ì˜¤ì°¨í•­ì˜ ê´€ì ì—ì„œ í•´ì„\në¡œì§€ìŠ¤í‹±(yê°€ 0 ë˜ëŠ” 1): ìœ„ì˜ ê³¡ì„ ì„ ë‚˜íƒ€ë‚´ëŠ” ì¼ë°˜ì ì¸ ìˆ˜ì‹\n\n=&gt; ì´ ìˆ˜ì‹ê°’ì„ í† ëŒ€ë¡œ ë² ë¥´ëˆ„ì´ ì‹œí–‰ì„ í•˜ë©´ ì˜¤ì°¨í•­ê¹Œì§€ ì„¤ëª… ê°€ëŠ¥í•œ ëª¨ë¸ì´ ë¨\n\n\n\n- ìš°ë¦¬ê°€ ì˜ˆì¸¡í•˜ê³  ì‹¶ì€ê²ƒ\n\níšŒê·€ëª¨í˜•: ì •ê·œë¶„í¬ì˜ í‰ê· ì„ ì˜ˆì¸¡í•˜ê³  ì‹¶ìŒ. ì¦‰ \\(w_0+w_1x_i\\)ë¥¼ ì˜ˆì¸¡í•˜ê³  ì‹¶ìŒ. ì˜ˆì¸¡ê°’ìœ¼ë¡œëŠ” \\(\\hat{w}_0 + \\hat{w}_1x_i\\)ë¥¼ ì‚¬ìš©!\në¡œì§€ìŠ¤í‹±: ë² ë¥´ëˆ„ì´ì˜ í‰ê· ì„ ì˜ˆì¸¡í•˜ê³  ì‹¶ìŒ. ì¦‰ \\(\\frac{\\exp(w_0+w_1x_i)}{1+\\exp(w_0+w_1x_i)}\\)ë¥¼ ì˜ˆì¸¡í•˜ê³  ì‹¶ìŒ. ì˜ˆì¸¡ê°’ìœ¼ë¡œëŠ” \\(\\frac{\\exp(\\hat{w}_0+\\hat{w}_1x_i)}{1+\\exp(\\hat{w}_0+\\hat{w}_1x_i)}\\)ë¥¼ ì‚¬ìš©!\nğŸ—£ï¸\n\në‘˜ ë‹¤ \\(\\hat{w}_0\\), \\(\\hat{w}_1\\)ë¥¼ ì¶”ì •í•˜ë©´ ê°ê° ì§ì„ ê³¼ ê³¡ì„ ì´ ê²°ì •ë¨\në² ë¥´ëˆ„ì´ì˜ í‰ê· ì€ \\(p\\)\n\nì¦‰, í™•ë¥ ì„ ì˜ˆì¸¡í•˜ê³  ì‹¶ìŒ"
  },
  {
    "objectID": "posts/03wk-1.html#footnotes",
    "href": "posts/03wk-1.html#footnotes",
    "title": "03wk-1: (íšŒê·€, ë¡œì§€ìŠ¤í‹±) â€“ íŒŒì´í† ì¹˜ì‹ ì½”ë”©íŒ¨í„´ (2), ë¡œì§€ìŠ¤í‹± ëª¨í˜•",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nì›ë˜ëŠ” ì´ë ‡ê²Œ ì¼ì—ˆì§€.. \\(y_i = w_0 + w_1x_i + \\epsilon_i \\quad \\epsilon_i \\sim {\\cal N}(0,\\sigma^2)\\)â†©ï¸"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deep Learning",
    "section": "",
    "text": "Based on: https://guebin.github.io/DL2025/\n\n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nApr 9, 2025\n\n\n06wk-1: (ì‹ ê²½ë§) â€“ ë°ì´í„°ë¶„ì„ ì½”ë”©íŒ¨í„´\n\n\nsw1kwon \n\n\n\n\nApr 7, 2025\n\n\n05wk-2: (ì‹ ê²½ë§) â€“ ì‹ ê²½ë§ì˜ í‘œí˜„, GPUì‚¬ìš©ë²•, í™•ë¥ ì ê²½ì‚¬í•˜ê°•ë²•\n\n\nsw1kwon \n\n\n\n\nApr 2, 2025\n\n\n05wk-1: (ì‹ ê²½ë§) â€“ ì˜ˆì¸¡, ì‹œë²¤ì½”ì •ë¦¬ì˜ ì´ë©´, ë“œëì•„ì›ƒ\n\n\nsw1kwon \n\n\n\n\nMar 31, 2025\n\n\n04wk-2: (ì‹ ê²½ë§) â€“ êº½ì¸ê·¸ë˜í”„ì˜ í•œê³„(?), ì‹œë²¤ì½”ì •ë¦¬, MNIST\n\n\nsw1kwon \n\n\n\n\nMar 26, 2025\n\n\n04wk-1: (ì‹ ê²½ë§) â€“ ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„ ê·¹ë³µ\n\n\nsw1kwon \n\n\n\n\nMar 24, 2025\n\n\n03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„\n\n\nsw1kwon \n\n\n\n\nMar 19, 2025\n\n\n03wk-1: (íšŒê·€, ë¡œì§€ìŠ¤í‹±) â€“ íŒŒì´í† ì¹˜ì‹ ì½”ë”©íŒ¨í„´ (2), ë¡œì§€ìŠ¤í‹± ëª¨í˜•\n\n\nsw1kwon \n\n\n\n\nMar 17, 2025\n\n\n02wk-2: (íšŒê·€) â€“ íŒŒë¼ë©”í„°ì˜ í•™ìŠµê³¼ì • ìŒë¯¸, MSE, íŒŒì´í† ì¹˜ì‹ ì½”ë”©íŒ¨í„´ (1)\n\n\nsw1kwon \n\n\n\n\nMar 10, 2025\n\n\n01wk-2, 02wk-1: (íšŒê·€) â€“ íšŒê·€ëª¨í˜•, ì†ì‹¤í•¨ìˆ˜, íŒŒì´í† ì¹˜ë¥¼ ì´ìš©í•œ ì¶”ì •\n\n\nsw1kwon \n\n\n\n\nMar 5, 2025\n\n\n01wk-1: (í† ì¹˜) â€“ ê°•ì˜ì†Œê°œ, íŒŒì´í† ì¹˜ ê¸°ë³¸\n\n\nsw1kwon \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/06wk-1.html",
    "href": "posts/06wk-1.html",
    "title": "06wk-1: (ì‹ ê²½ë§) â€“ ë°ì´í„°ë¶„ì„ ì½”ë”©íŒ¨í„´",
    "section": "",
    "text": "ğŸ“˜ Note Format Guide\nThis format serves as a structured guide for organizing lecture content, personal interpretation, experiments, and study-related questions.\nğŸ“ ğŸ—£ï¸ âœï¸ ğŸ”¬ â“"
  },
  {
    "objectID": "posts/06wk-1.html#a.-ì¼ë°˜ì ì¸-traintest-ì…‹íŒ…",
    "href": "posts/06wk-1.html#a.-ì¼ë°˜ì ì¸-traintest-ì…‹íŒ…",
    "title": "06wk-1: (ì‹ ê²½ë§) â€“ ë°ì´í„°ë¶„ì„ ì½”ë”©íŒ¨í„´",
    "section": "A. ì¼ë°˜ì ì¸ train/test ì…‹íŒ…",
    "text": "A. ì¼ë°˜ì ì¸ train/test ì…‹íŒ…\n- Step1: ë°ì´í„°ì •ë¦¬\n\ntrain_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True)\ntest_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True)\nto_tensor = torchvision.transforms.ToTensor()\nX0 = torch.stack([to_tensor(img) for img, lbl in train_dataset if lbl==0])\nX1 = torch.stack([to_tensor(img) for img, lbl in train_dataset if lbl==1])\nX = torch.concat([X0,X1],axis=0).reshape(-1,784)\ny = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1)\nXX0 = torch.stack([to_tensor(img) for img, lbl in test_dataset if lbl==0])\nXX1 = torch.stack([to_tensor(img) for img, lbl in test_dataset if lbl==1])\nXX = torch.concat([XX0,XX1],axis=0).reshape(-1,784)\nyy = torch.tensor([0.0]*len(XX0) + [1.0]*len(XX1)).reshape(-1,1)\n\nğŸ—£ï¸(\n\nX[0].shape\n\ntorch.Size([784])\n\n\n\nplt.imshow(X[0].reshape(28,28), cmap=\"gray\")\n\n\n\n\n\n\n\n\n\nprint(y[0]) # labelì´ tensor í˜•íƒœë¡œ ì €ì¥ë˜ì–´ ìˆìŒ\n\ntensor([0.])\n\n\n\nXë§Œ ê°€ì§€ê³  í•™ìŠµì„ í•œ ë’¤ XXë¥¼ ê°€ì§€ê³  í™•ì¸\n\n\nX.shape, y.shape\n\n(torch.Size([12665, 784]), torch.Size([12665, 1]))\n\n\n\nXX.shape, yy.shape\n\n(torch.Size([2115, 784]), torch.Size([2115, 1]))\n\n\n)ğŸ—£ï¸\n- Step2: í•™ìŠµê°€ëŠ¥í•œ ì˜¤ë¸Œì íŠ¸ë“¤ì˜ ì„¤ì • (ëª¨ë¸ë§ê³¼ì • í¬í•¨)\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(784,32),\n    torch.nn.ReLU(), # (n,32)\n    torch.nn.Linear(32,1),\n    torch.nn.Sigmoid() # yëŠ” 0 ë˜ëŠ” 1\n)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD(net.parameters()) # Adamì€ ë„ˆë¬´ ì˜ ë§ì¶°ì„œ SGD\n\n- Step3: í•™ìŠµ (=ì í•©)\n\nfor epoc in range(1,501):\n    #---ì—í­ì‹œì‘---# \n    # 1 \n    yhat = net(X) \n    # 2 \n    loss = loss_fn(yhat,y) \n    # 3 \n    loss.backward()\n    # 4 \n    optimizr.step()\n    optimizr.zero_grad()\n    #---ì—í­ë---# \n    #ì—í­ë§ˆë‹¤ ë‚´ê°€ ë³´ê³ ì‹¶ì€ê²ƒë“¤ì„ ë³´ì—¬ì£¼ëŠ” ì½”ë“œ\n    if (epoc % 50) ==0: # 50ìœ¼ë¡œ ë‚˜ëˆˆ ë‚˜ë¨¸ì§€ = 0 =&gt; 50ì˜ ë°°ìˆ˜\n        acc = ((net(X).data &gt; 0.5) == y).float().mean().item() # item: tensor -&gt; float\n        print(f\"# of epochs={epoc}   \\t train_acc = {acc:.4f}\")\n\n# of epochs=50       train_acc = 0.4677\n# of epochs=100      train_acc = 0.4677\n# of epochs=150      train_acc = 0.4757\n# of epochs=200      train_acc = 0.5295\n# of epochs=250      train_acc = 0.6632\n# of epochs=300      train_acc = 0.7929\n# of epochs=350      train_acc = 0.8731\n# of epochs=400      train_acc = 0.9206\n# of epochs=450      train_acc = 0.9465\n# of epochs=500      train_acc = 0.9634\n\n\nğŸ—£ï¸ ì˜¤ë²„í”¼íŒ… ë¹„íŒ ê°€ëŠ¥ì„± ì¡´ì¬\n- Step4: ì˜ˆì¸¡ & ê²°ê³¼ë¶„ì„\ntrain acc\n\n((net(X).data &gt; 0.5) == y).float().mean()\n\ntensor(0.9634)\n\n\ntest acc\n\n((net(XX).data&gt;0.5) == yy).float().mean()\n\ntensor(0.9749)\n\n\nğŸ—£ï¸ ì‹¤ì „ì—ì„œ ë” ê´œì°®ìŒ\nğŸ—£ï¸ Step4: acc, recall, F1 score, ì‹œê°í™” ë“±\n#ì—í­ë§ˆë‹¤ ë‚´ê°€ ë³´ê³ ì‹¶ì€ê²ƒë“¤ì„ ë³´ì—¬ì£¼ëŠ” ì½”ë“œ\n    if (epoc % 50) ==0: # 50ìœ¼ë¡œ ë‚˜ëˆˆ ë‚˜ë¨¸ì§€ = 0 =&gt; 50ì˜ ë°°ìˆ˜\n        acc = ((net(X).data &gt; 0.5) == y).float().mean().item() # item: tensor -&gt; float\n        Xval --&gt; # train data ìì²´ì—ì„œ test ë°ì´í„°ë¥¼ ë‚˜ëˆ„ê³  ì •í™•ë„ë¥¼ ë¹„êµí•˜ë©° early stopping í•  ìˆ˜ë„ ìˆìŒ (ì˜¤ë²„í”¼íŒ… ë°©ì§€)\n        print(f\"# of epochs={epoc}   \\t train_acc = {acc:.4f}\")"
  },
  {
    "objectID": "posts/06wk-1.html#b.-dropout-ì‚¬ìš©",
    "href": "posts/06wk-1.html#b.-dropout-ì‚¬ìš©",
    "title": "06wk-1: (ì‹ ê²½ë§) â€“ ë°ì´í„°ë¶„ì„ ì½”ë”©íŒ¨í„´",
    "section": "B. Dropout ì‚¬ìš©",
    "text": "B. Dropout ì‚¬ìš©\n- Step1: ë°ì´í„°ì •ë¦¬\n\npass\n\n- Step2: í•™ìŠµê°€ëŠ¥í•œ ì˜¤ë¸Œì íŠ¸ë“¤ì˜ ì„¤ì • (ëª¨ë¸ë§ê³¼ì • í¬í•¨)\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(784,32),\n    torch.nn.Dropout(0.9),\n    torch.nn.ReLU(),\n    torch.nn.Linear(32,1),\n    torch.nn.Sigmoid()\n)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD(net.parameters())\n\nğŸ—£ï¸ ì›ë˜ëŠ” í™œì„±í™” í•¨ìˆ˜ ë‹¤ìŒì´ì§€ë§Œ ReLU í•œì • ì „ì—ë„ ì‚¬ìš© ê°€ëŠ¥\n- Step3: í•™ìŠµ (=ì í•©)\n\nfor epoc in range(1,501):\n    net.train()\n    #---ì—í­ì‹œì‘---# \n    # 1 \n    yhat = net(X) \n    # 2 \n    loss = loss_fn(yhat,y) \n    # 3 \n    loss.backward()\n    # 4 \n    optimizr.step()\n    optimizr.zero_grad()\n    #---ì—í­ë---# \n    net.eval()\n    #ì—í­ë§ˆë‹¤ ë‚´ê°€ ë³´ê³ ì‹¶ì€ê²ƒë“¤ì„ ë³´ì—¬ì£¼ëŠ” ì½”ë“œ\n    if (epoc % 50) ==0:\n        acc = ((net(X).data &gt; 0.5) == y).float().mean().item()\n        print(f\"# of epochs={epoc}   \\t train_acc = {acc:.4f}\")\n\n# of epochs=50       train_acc = 0.4677\n# of epochs=100      train_acc = 0.4677\n# of epochs=150      train_acc = 0.4744\n# of epochs=200      train_acc = 0.5215\n# of epochs=250      train_acc = 0.6435\n# of epochs=300      train_acc = 0.7675\n# of epochs=350      train_acc = 0.8468\n# of epochs=400      train_acc = 0.8978\n# of epochs=450      train_acc = 0.9301\n# of epochs=500      train_acc = 0.9492\n\n\n\n# of epochs=50       train_acc = 0.4677\n# of epochs=100      train_acc = 0.4677\n# of epochs=150      train_acc = 0.4757 # ìœ„ì˜ ê²°ê³¼ì™€ ì‚´ì§ ë‹¤ë¦„\n# of epochs=200      train_acc = 0.5295\n# of epochs=250      train_acc = 0.6632\n# of epochs=300      train_acc = 0.7929\n# of epochs=350      train_acc = 0.8731\n# of epochs=400      train_acc = 0.9206\n# of epochs=450      train_acc = 0.9465\n# of epochs=500      train_acc = 0.9634\n\n- Step4: ì˜ˆì¸¡ & ê²°ê³¼ë¶„ì„\ntrain acc\n\n((net(X).data &gt; 0.5) == y).float().mean()\n\ntensor(0.9492)\n\n\ntest acc\n\n((net(XX).data&gt;0.5) == yy).float().mean()\n\ntensor(0.9626)\n\n\nğŸ—£ï¸ ì‹¤ì „ì—ì„œ ë” ê´œì°®ìŒ"
  },
  {
    "objectID": "posts/06wk-1.html#c.-gpuë„-ì‚¬ìš©",
    "href": "posts/06wk-1.html#c.-gpuë„-ì‚¬ìš©",
    "title": "06wk-1: (ì‹ ê²½ë§) â€“ ë°ì´í„°ë¶„ì„ ì½”ë”©íŒ¨í„´",
    "section": "C. GPUë„ ì‚¬ìš©",
    "text": "C. GPUë„ ì‚¬ìš©\n- Step1: ë°ì´í„°ì •ë¦¬\n\npass\n\n- Step2: í•™ìŠµê°€ëŠ¥í•œ ì˜¤ë¸Œì íŠ¸ë“¤ì˜ ì„¤ì • (ëª¨ë¸ë§ê³¼ì • í¬í•¨)\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(784,32),\n    torch.nn.Dropout(0.9),\n    torch.nn.ReLU(),\n    torch.nn.Linear(32,1),\n    torch.nn.Sigmoid()\n).to(\"cuda:0\")\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD(net.parameters())\n\n- Step3: í•™ìŠµ (=ì í•©)\n\nfor epoc in range(1,501):\n    net.train()\n    #---ì—í­ì‹œì‘---# \n    X = X.to(\"cuda:0\")\n    y = y.to(\"cuda:0\")\n    # 1 \n    yhat = net(X) \n    # 2 \n    loss = loss_fn(yhat,y) \n    # 3 \n    loss.backward()\n    # 4 \n    optimizr.step()\n    optimizr.zero_grad()\n    #---ì—í­ë---# \n    net.eval()\n    #ì—í­ë§ˆë‹¤ ë‚´ê°€ ë³´ê³ ì‹¶ì€ê²ƒë“¤ì„ ë³´ì—¬ì£¼ëŠ” ì½”ë“œ\n    if (epoc % 50) ==0:\n        acc = ((net(X).data &gt; 0.5) == y).float().mean().item()\n        print(f\"# of epochs={epoc}   \\t train_acc = {acc:.4f}\")\n\n# of epochs=50       train_acc = 0.4677\n# of epochs=100      train_acc = 0.4677\n# of epochs=150      train_acc = 0.4745\n# of epochs=200      train_acc = 0.5223\n# of epochs=250      train_acc = 0.6441\n# of epochs=300      train_acc = 0.7686\n# of epochs=350      train_acc = 0.8469\n# of epochs=400      train_acc = 0.8979\n# of epochs=450      train_acc = 0.9302\n# of epochs=500      train_acc = 0.9492\n\n\nğŸ—£ï¸ ë¹ ë¦„\n- Step4: ì˜ˆì¸¡ & ê²°ê³¼ë¶„ì„\ntrain acc\n\n((net(X).data &gt; 0.5) == y).float().mean()\n\ntensor(0.9492, device='cuda:0')\n\n\ntest acc\n\n# ((net(XX).data&gt;0.5) == yy).float().mean() # net(XX)ê°€ ë¬¸ì œ\n\nğŸ—£ï¸ XXë¥¼ GPUì— ì˜¬ë¦¬ë“ ê°€, netë¥¼ CPUì— ë‚´ë¦¬ë“ ê°€\n\nXX = XX.to(\"cuda:0\")\nyy = yy.to(\"cuda:0\") \n\n\n((net(XX).data&gt;0.5) == yy).float().mean()\n\ntensor(0.9626, device='cuda:0')"
  },
  {
    "objectID": "posts/06wk-1.html#d.-ë¯¸ë‹ˆë°°ì¹˜ë„-ì‚¬ìš©",
    "href": "posts/06wk-1.html#d.-ë¯¸ë‹ˆë°°ì¹˜ë„-ì‚¬ìš©",
    "title": "06wk-1: (ì‹ ê²½ë§) â€“ ë°ì´í„°ë¶„ì„ ì½”ë”©íŒ¨í„´",
    "section": "D. ë¯¸ë‹ˆë°°ì¹˜ë„ ì‚¬ìš©",
    "text": "D. ë¯¸ë‹ˆë°°ì¹˜ë„ ì‚¬ìš©\n- Step1: ë°ì´í„°ì •ë¦¬\nğŸ—£ï¸ ë‹¤ì‹œ CPUë¡œ ë‚´ë¦¼\n\nX = X.to(\"cpu\")\ny = y.to(\"cpu\")\nXX = XX.to(\"cpu\")\nyy = yy.to(\"cpu\")\n\nğŸ—£ï¸(\n\nX.shape\n\ntorch.Size([12665, 784])\n\n\n\nds  = torch.utils.data.TensorDataset(X,y)\ndl = torch.utils.data.DataLoader(ds,batch_size = 16) \n\n\n# for Xm, ym in dl: # m: ë¯¸ë‹ˆ ë°°ì¹˜\n#     print(Xm) # í•˜ë‚˜í•˜ë‚˜ê°€ ë¯¸ë‹ˆë°°ì¹˜\n\n\n# for Xm, ym in dl: # m: ë¯¸ë‹ˆ ë°°ì¹˜\n#     print(Xm.shape) # ê²°ê³¼: torch.Size([16, 784])\n\n\nds  = torch.utils.data.TensorDataset(X,y)\ndl = torch.utils.data.DataLoader(ds,batch_size = 16, shuffle=True) \n\n\n# for Xm, ym in dl: # m: ë¯¸ë‹ˆ ë°°ì¹˜\n#     print(ym) # ì„ì—¬ ìˆìŒ\n\n\n# for Xm, ym in dl: # m: ë¯¸ë‹ˆ ë°°ì¹˜\n#     print(ym.shape) # ê²°ê³¼: torch.Size([16, 1])\n\n)ğŸ—£ï¸\n\nds  = torch.utils.data.TensorDataset(X,y)\ndl = torch.utils.data.DataLoader(ds,batch_size = 16, shuffle=True) \n\n- Step2: í•™ìŠµê°€ëŠ¥í•œ ì˜¤ë¸Œì íŠ¸ë“¤ì˜ ì„¤ì • (ëª¨ë¸ë§ê³¼ì • í¬í•¨)\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(784,32),\n    torch.nn.Dropout(0.9),\n    torch.nn.ReLU(),\n    torch.nn.Linear(32,1),\n    torch.nn.Sigmoid()\n).to(\"cuda:0\")\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD(net.parameters())\n\n- Step3: í•™ìŠµ (=ì í•©)\n\nğŸ—£ï¸\n\nX=X.to(â€œcuda:0â€), y=y.to(â€œcuda:0â€)ë¥¼ í•  ìˆ˜ëŠ” ì—†ìœ¼ë¯€ë¡œ ë¯¸ë‹ˆë°°ì¹˜ ë³„ë¡œ GPUì— ì˜¬ë¦¼\nepochì„ 500ë²ˆì”© ëŒë¦´ í•„ìš”ëŠ” ì—†ìœ¼ë¯€ë¡œ 2ë²ˆë§Œ ëŒë¦¼\n\n\n\nfor epoc in range(1,3):\n    net.train()\n    #---ì—í­ì‹œì‘---# \n    for Xm,ym in dl:         \n        Xm = Xm.to(\"cuda:0\")\n        ym = ym.to(\"cuda:0\")\n        # 1 \n        ym_hat = net(Xm) \n        # 2 \n        loss = loss_fn(ym_hat,ym) \n        # 3 \n        loss.backward()\n        # 4 \n        optimizr.step()\n        optimizr.zero_grad()\n    #---ì—í­ë---# \n    net.eval()\n    #ì—í­ë§ˆë‹¤ ë‚´ê°€ ë³´ê³ ì‹¶ì€ê²ƒë“¤ì„ ë³´ì—¬ì£¼ëŠ” ì½”ë“œ\n    s = 0 \n    for Xm, ym in dl:\n        Xm = Xm.to(\"cuda:0\")\n        ym = ym.to(\"cuda:0\")\n        s = s + ((net(Xm) &gt; 0.5) == ym).float().sum()\n    acc = s/12665        \n    print(f\"# of epochs={epoc}   \\t train_acc = {acc:.4f}\")\n\n# of epochs=1        train_acc = 0.9860\n# of epochs=2        train_acc = 0.9931\n\n\nğŸ—£ï¸ ë‹¤ë¥¸ ë°©ë²• (ì´ë ‡ê²Œ í•˜ë©´ ì‰¬ì›€)\nfor epoc in range(1,3):\n    net.train()\n    net.gpu()\n    #---ì—í­ì‹œì‘---# \n    for Xm,ym in dl:         \n        Xm = Xm.to(\"cuda:0\")\n        ym = ym.to(\"cuda:0\")\n        # 1 \n        ym_hat = net(Xm) \n        # 2 \n        loss = loss_fn(ym_hat,ym) \n        # 3 \n        loss.backward()\n        # 4 \n        optimizr.step()\n        optimizr.zero_grad()\n    #---ì—í­ë---# \n    net.eval()\n    net.to(\"cpu\")\n    #ì—í­ë§ˆë‹¤ ë‚´ê°€ ë³´ê³ ì‹¶ì€ê²ƒë“¤ì„ ë³´ì—¬ì£¼ëŠ” ì½”ë“œ\n    acc = ((net(X).data &gt; 0.5) == y).float().mean().item()\n    print(f\"# of epochs={epoc}   \\t train_acc = {acc:.4f}\")\n\nğŸ—£ï¸ ë‹¤ë¥¸ ë°©ë²• (netë¥¼ GPUë¡œ ìœ ì§€í•˜ê³  ì‹¶ìœ¼ë©´) =&gt; ê°•ì˜ì•ˆ ì½”ë“œ\n\nmean ëŒ€ì‹  sum\ní•˜ë‚˜ì˜ ë¯¸ë‹ˆë°°ì¹˜ì—ì„œ ë§ì€ ê²ƒì˜ ê°œìˆ˜ë¥¼ sì— ê³„ì† ëˆ„ì  ì‹œí‚´ (forë¬¸)\nfor ë¬¸ì´ ì¢…ë£Œë˜ê³  së¥¼ ì´ ê°œìˆ˜(X.shape)ë¡œ ë‚˜ëˆ„ë©´ accuracyê°€ ê³„ì‚°ë¨\n\n\n- Step4: ì˜ˆì¸¡ & ê²°ê³¼ë¶„ì„\nğŸ—£ï¸(\n\n# net(X) # error\n\n\nnetì€ cudaì— ìˆê³  XëŠ” cpuì— ìˆìŒ\n\nnetì„ cpuë¡œ ë‚´ë¦´ ê²ƒì¸ì§€, Xë¥¼ cudaë¡œ ì˜¬ë¦´ ê²ƒì¸ì§€ ì„ íƒ\n\nXë¥¼ cudaë¡œ ì˜¬ë¦¬ê¸° ì‹«ì–´ì„œ ë¯¸ë‹ˆë°°ì¹˜ë¥¼ ì‚¬ìš©í•˜ì˜€ìœ¼ë¯€ë¡œ netì„ cpuë¡œ ë‚´ë¦¼\n\n)ğŸ—£ï¸\n\nnet.to(\"cpu\")\n\nSequential(\n  (0): Linear(in_features=784, out_features=32, bias=True)\n  (1): Dropout(p=0.9, inplace=False)\n  (2): ReLU()\n  (3): Linear(in_features=32, out_features=1, bias=True)\n  (4): Sigmoid()\n)\n\n\ntrain acc\n\n((net(X) &gt; 0.5) == y).float().mean()\n\ntensor(0.9931)\n\n\ntest acc\n\n((net(XX) &gt; 0.5) == yy).float().mean()\n\ntensor(0.9967)\n\n\nğŸ—£ï¸ testë„ ì˜ ë‚˜ì˜¤ë¯€ë¡œ ì˜¤ë²„í”¼íŒ… X\n\nì ì  ë¹„ë³¸ì§ˆì ì¸ ì½”ë“œê°€ ëŠ˜ì–´ë‚¨ (=ì½”ë“œê°€ ë“œëŸ½ë‹¤ëŠ” ì†Œë¦¬ì—ìš”) â€“&gt; Trainerì˜ ê°œë… ë“±ì¥\n\n\nğŸ—£ï¸\n\në”¥ëŸ¬ë‹ ê°€ì§€ê³  ë¶„ì„í•˜ë©´ â€“&gt; íŠ¸ë ˆì´ë„ˆê°€ ìˆëŠ” ë‹¤ë¥¸ íŒ¨í‚¤ì§€ë¥¼ ì¨ì•¼í•¨ (í•™ë¶€ ìˆ˜ì¤€)\n\níŒŒì´í† ì¹˜ ë¼ì´íŠ¸ë‹\ní—ˆê¹…í˜ì´ìŠ¤\n\nì—°êµ¬í• ë•ŒëŠ” íŒŒì´í† ì¹˜ ì´í•´í•´ì•¼ í•¨"
  },
  {
    "objectID": "posts/04wk-2.html",
    "href": "posts/04wk-2.html",
    "title": "04wk-2: (ì‹ ê²½ë§) â€“ êº½ì¸ê·¸ë˜í”„ì˜ í•œê³„(?), ì‹œë²¤ì½”ì •ë¦¬, MNIST",
    "section": "",
    "text": "ğŸ“˜ Note Format Guide\nThis format serves as a structured guide for organizing lecture content, personal interpretation, experiments, and study-related questions.\nğŸ“ ğŸ—£ï¸ âœï¸ ğŸ”¬ â“"
  },
  {
    "objectID": "posts/04wk-2.html#a.-stepì€-í‘œí˜„-ë¶ˆê°€ëŠ¥í•˜ì§€-ì•Šë‚˜",
    "href": "posts/04wk-2.html#a.-stepì€-í‘œí˜„-ë¶ˆê°€ëŠ¥í•˜ì§€-ì•Šë‚˜",
    "title": "04wk-2: (ì‹ ê²½ë§) â€“ êº½ì¸ê·¸ë˜í”„ì˜ í•œê³„(?), ì‹œë²¤ì½”ì •ë¦¬, MNIST",
    "section": "A. Stepì€ í‘œí˜„ ë¶ˆê°€ëŠ¥í•˜ì§€ ì•Šë‚˜?",
    "text": "A. Stepì€ í‘œí˜„ ë¶ˆê°€ëŠ¥í•˜ì§€ ì•Šë‚˜?\n# ì˜ˆì œ1 â€“ ì¼ë¶€ëŸ¬ ì´ìƒí•˜ê²Œ ë§Œë“  ì·¨ì—…í•©ê²©ë¥  ê³¡ì„ \n\ntorch.manual_seed(43052)\nx = torch.linspace(-1,1,2000).reshape(-1,1)\nu = 0*x-3\nu[x&lt;-0.2] = (15*x+6)[x&lt;-0.2]\nu[(-0.2&lt;x)&(x&lt;0.4)] = (0*x-1)[(-0.2&lt;x)&(x&lt;0.4)]\nsig = torch.nn.Sigmoid()\nv = Ï€ = sig(u)\ny = torch.bernoulli(v)\n\n\nplt.plot(x,y,'.',alpha=0.03, label=\"observed\")\nplt.plot(x,v,'--', label=\"unobserved\")\nplt.legend()\n\n\n\n\n\n\n\n\nğŸ—£ï¸ ëš ë–¨ì–´ì§€ëŠ” ë¶€ë¶„ì€ ì–´ë–»ê²Œ í•´ì•¼í•˜ì§€? ê¸°ìš¸ê¸°ë¥¼ ê¸‰í•˜ê²Œ ê·¼ì‚¬í•˜ëŠ” ì‹ìœ¼ë¡œ ì ‘ê·¼\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,512),\n    torch.nn.ReLU(),\n    torch.nn.Linear(512,1),\n    torch.nn.Sigmoid()\n)\nloss_fn = torch.nn.BCELoss()\noptimizr = torch.optim.Adam(net.parameters())\n#---#\nfor epoc in range(5000):\n    ## 1\n    yhat = net(x)\n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\nğŸ—£ï¸ bias ì—¬ë¶€ê°€ ì§ì„ ì˜ ê°œìˆ˜ì— ì˜í–¥ì„ ì£¼ì§€ëŠ” X, (1,2)ê°€ ì•„ë‹ˆë¼ (1,512)ë¡œ í•˜ë©´ ì—¬ëŸ¬ ë²ˆ êº¾ì¼ ê²ƒì„\n\nplt.plot(x,y,'.',alpha=0.03, label=\"observed\")\nplt.plot(x,v, label=\"true\")\nplt.plot(x,net(x).data,'--', label=\"estimated\")\nplt.legend()\n\n\n\n\n\n\n\n\nğŸ—£ï¸ trueëŠ” ê´€ì¸¡í•  ìˆ˜ ì—†ìŒ, estimatedëŠ” trueì™€ ì°¨ì´ê°€ ìˆì–´ë„ ì“¸ë§Œ í•¨\nğŸ—£ï¸(\n\nê³¼ì • ì‚´í´ë³´ê¸°\n\n\nnet\n\nSequential(\n  (0): Linear(in_features=1, out_features=512, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=512, out_features=1, bias=True)\n  (3): Sigmoid()\n)\n\n\n\nnet[:1]\n\nSequential(\n  (0): Linear(in_features=1, out_features=512, bias=True)\n)\n\n\n\nnet[:3]\n\nSequential(\n  (0): Linear(in_features=1, out_features=512, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=512, out_features=1, bias=True)\n)\n\n\n\nplt.plot(net[:3](x).data) # êº¾ì¸ ì„ \n\n\n\n\n\n\n\n\n\nì •ë‹µì€ ì•„ë‹ˆì§€ë§Œ ì ë‹¹íˆ ê·¼ì‚¬ì ìœ¼ë¡œ ì“¸ ìˆ˜ ìˆì„ ê²ƒ ê°™ìŒ\n\n\nplt.plot(net[:4](x).data) # sigmoid ê²°ê³¼\n\n\n\n\n\n\n\n\n)ğŸ—£ï¸\n#"
  },
  {
    "objectID": "posts/04wk-2.html#b.-ê³¡ì„ ì€-í‘œí˜„-ë¶ˆê°€ëŠ¥í•˜ì§€-ì•Šë‚˜",
    "href": "posts/04wk-2.html#b.-ê³¡ì„ ì€-í‘œí˜„-ë¶ˆê°€ëŠ¥í•˜ì§€-ì•Šë‚˜",
    "title": "04wk-2: (ì‹ ê²½ë§) â€“ êº½ì¸ê·¸ë˜í”„ì˜ í•œê³„(?), ì‹œë²¤ì½”ì •ë¦¬, MNIST",
    "section": "B. ê³¡ì„ ì€ í‘œí˜„ ë¶ˆê°€ëŠ¥í•˜ì§€ ì•Šë‚˜?",
    "text": "B. ê³¡ì„ ì€ í‘œí˜„ ë¶ˆê°€ëŠ¥í•˜ì§€ ì•Šë‚˜?\n# ì˜ˆì œ2 â€“ 2024ë…„ ìˆ˜ëŠ¥ ë¯¸ì 30ë²ˆ ë¬¸ì œì— ë‚˜ì˜¨ ê³¡ì„ \n\\[y_i = e^{-x_i} \\times  |\\cos(5x_i)| \\times \\sin(5x) + \\epsilon_i, \\quad \\epsilon_i \\sim N(0,\\sigma^2)\\]\n\ntorch.manual_seed(43052)\nx = torch.linspace(0,2,2000).reshape(-1,1)\neps = torch.randn(2000).reshape(-1,1)*0.05\nfx = torch.exp(-1*x)* torch.abs(torch.cos(3*x))*(torch.sin(3*x))\ny = fx + eps\n\n\nplt.plot(x,y,label=\"observed\",alpha=0.5)\nplt.plot(x,fx,label=\"true\")\n\n\n\n\n\n\n\n\nğŸ—£ï¸ ë˜ê²Œ ì„¸ë°€í•˜ê²Œ ë§ì´ êº¾ìœ¼ë©´ ê³¡ì„ ì€ ì•„ë‹ˆì§€ë§Œ ê³¡ì„ ì²˜ëŸ¼ ë³´ì¼ ìˆ˜ ìˆì„ ê²ƒ ê°™ìŒ (ì´ ìƒí™©ì—ì„œ bias ì—¬ë¶€ëŠ” ì˜ë¯¸ X)\nğŸ—£ï¸(\n\nêµ³ì´ 0ê³¼ 1 ì‚¬ì´ë¡œ ëˆ„ë¥¼ í•„ìš”ë„ ì—†ê³  - ê°’ë„ ê°–ê³  ìˆìœ¼ë¯€ë¡œ sigmoid ì·¨í•  í•„ìš”ëŠ” ì—†ì„ë“¯\nyê°€ 0 ë˜ëŠ” 1ì´ ì•„ë‹ˆê³  ì—°ì†ì ì¸ ì–´ë–¤ ê°’ì„ ê³„ì† ê°€ì§ˆ ìˆ˜ ìˆìŒ\n\níšŒê·€ë‘ ë¹„ìŠ·í•˜ë¯€ë¡œ MSELoss ì‚¬ìš©\nBCELossë¥¼ ì—¬ê¸°ì„œ ì‚¬ìš©í•œë‹¤ë©´ ì´ ê²½ìš° logì— ìŒìˆ˜ê°€ ë“¤ì–´ê°ˆ ìˆ˜ë„ ìˆìœ¼ë¯€ë¡œ X\n\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1024), # êº½ì´ì§€ì•Šì€ 1024ê°œì˜ ì§ì„ \n    torch.nn.ReLU(), # êº½ì¸(ë ë£¨ëœ) 1024ê°œì˜ ì§ì„  \n    torch.nn.Linear(1024,1), # í•©ì³ì§„ í•˜ë‚˜ì˜ êº½ì¸ ì§ì„  \n)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n## \nfor epoc in range(1000):\n    ## 1\n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,label=\"observed\",alpha=0.5)\nplt.plot(x,fx,label=\"true\")\nplt.plot(x,net(x).data,'--',label=\"estimated\")\nplt.legend()\n\n\n\n\n\n\n\n\n\nì˜ ë³´ë©´ ì§ì„  ëŠë‚Œì´ ìˆê¸°ëŠ” í•˜ë‚˜ ì´ ì •ë„ë©´ ê·¸ëŸ­ì €ëŸ­ ê´œì°®ìŒ\n\n)ğŸ—£ï¸\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,2048), # êº½ì´ì§€ì•Šì€ 1024ê°œì˜ ì§ì„ \n    torch.nn.ReLU(), # êº½ì¸(ë ë£¨ëœ) 1024ê°œì˜ ì§ì„  \n    torch.nn.Linear(2048,1), # í•©ì³ì§„ í•˜ë‚˜ì˜ êº½ì¸ ì§ì„  \n)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n## \nfor epoc in range(1000):\n    ## 1\n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,label=\"observed\",alpha=0.5)\nplt.plot(x,fx,label=\"true\")\nplt.plot(x,net(x).data,'--',label=\"estimated\")\nplt.legend()\n\n\n\n\n\n\n\n\n#"
  },
  {
    "objectID": "posts/04wk-2.html#a.-ì‹œë²¤ì½”ì •ë¦¬-ì†Œê°œ",
    "href": "posts/04wk-2.html#a.-ì‹œë²¤ì½”ì •ë¦¬-ì†Œê°œ",
    "title": "04wk-2: (ì‹ ê²½ë§) â€“ êº½ì¸ê·¸ë˜í”„ì˜ í•œê³„(?), ì‹œë²¤ì½”ì •ë¦¬, MNIST",
    "section": "A. ì‹œë²¤ì½”ì •ë¦¬ ì†Œê°œ",
    "text": "A. ì‹œë²¤ì½”ì •ë¦¬ ì†Œê°œ\n\n\n\n\n\n\nUniversal Approximation Thm [@cybenko1989approximation]\n\n\n\ní•˜ë‚˜ì˜ ì€ë‹‰ì¸µì„ ê°€ì§€ëŠ” ì•„ë˜ì™€ ê°™ì€ ê¼´ì˜ ë„¤íŠ¸ì›Œí¬ \\(net: {\\bf X}_{n \\times p} \\to {\\bf y}_{n\\times q}\\)ëŠ”\nnet = torch.nn.Sequential(\n    torch.nn.Linear(p,???),\n    torch.nn.Sigmoid(),\n    torch.nn.Linear(???,q)\n)\nëª¨ë“  ë³´ë  ê°€ì¸¡í•¨ìˆ˜ (Borel measurable function)\n\\[f: {\\bf X}_{n \\times p} \\to {\\bf y}_{n\\times q}\\]\në¥¼ ì›í•˜ëŠ” ì •í™•ë„ë¡œ â€œê·¼ì‚¬â€ì‹œí‚¬ ìˆ˜ ìˆë‹¤. ì‰½ê²Œ ë§í•˜ë©´ \\({\\bf X} \\to {\\bf y}\\) ì¸ ì–´ë– í•œ ë³µì¡í•œ ê·œì¹™ë¼ë„ í•˜ë‚˜ì˜ ì€ë‹‰ì¸µì„ ê°€ì§„ ì‹ ê²½ë§ì´ ì›í•˜ëŠ” ì •í™•ë„ë¡œ ê·¼ì‚¬ì‹œí‚¨ë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤. ì˜ˆë¥¼ë“¤ë©´ ì•„ë˜ì™€ ê°™ì€ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆë‹¤.\n\n\\({\\bf X}_{n\\times 2}\\)ëŠ” í† ìµì ìˆ˜, GPA ì´ê³  \\({\\bf y}_{n\\times 1}\\)ëŠ” ì·¨ì—…ì—¬ë¶€ì¼ ê²½ìš° \\({\\bf X} \\to {\\bf y}\\)ì¸ ê·œì¹™ì„ ì‹ ê²½ë§ì€ í•­ìƒ ì°¾ì„ ìˆ˜ ìˆë‹¤.\n\\({\\bf X}_{n \\times p}\\)ëŠ” ì£¼íƒì´ë¯¸ì§€, ì§€ì—­ì •ë³´, ì£¼íƒë©´ì , ì£¼íƒì— ëŒ€í•œ ì„¤ëª… ì´ê³  \\({\\bf y}_{n\\times 1}\\)ëŠ” ì£¼íƒê°€ê²©ì¼ ê²½ìš° \\({\\bf X} \\to {\\bf y}\\)ì¸ ê·œì¹™ì„ ì‹ ê²½ë§ì€ í•­ìƒ ì°¾ì„ ìˆ˜ ìˆë‹¤.\n\nì¦‰ í•˜ë‚˜ì˜ ì€ë‹‰ì¸µì„ ê°€ì§„ ì‹ ê²½ë§ì˜ í‘œí˜„ë ¥ì€ ê±°ì˜ ë¬´í•œëŒ€ë¼ ë³¼ ìˆ˜ ìˆë‹¤.\n\n\n\nğŸ—£ï¸\n\nì‹œë²¤ì½”ê°€ Sigmoidë¡œ ì¦ëª…í–ˆìœ¼ë‚˜ ReLUë¥¼ ë„£ì–´ë„ ìƒê´€ X\nxëŠ” pì˜ ì°¨ì›ì„ ê°–ê³  yëŠ” qì˜ ì°¨ì›ì„ ê°€ì ¸ë„ ë¨ (ê°™ì„ í•„ìš” X)\n???: ì•„ë¬´ ìˆ«ìë¥¼ ë„£ì–´ë„ ìƒê´€ì—†ìœ¼ë‚˜ 2^n ìœ¼ë¡œ ì“°ëŠ”ê²Œ ë©”ëª¨ë¦¬ì— íš¨ìœ¨ì ì´ë¼ê³  ì•Œë ¤ì ¸ ìˆìŒ\në³´ë ê°€ì¸¡í•¨ìˆ˜: ì¼ë°˜ì¸ë“¤ì´ ìƒìƒí•  ìˆ˜ ìˆëŠ” ê±°ì˜ ëª¨ë“  í•¨ìˆ˜\nì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë„ ìˆ«ìë¡œ ë°”ê¿€ ìˆ˜ ìˆìŒ =&gt; Xë¥¼ nxpë¡œ ì •ë¦¬ ê°€ëŠ¥\n\n\n\në³´ë ê°€ì¸¡í•¨ìˆ˜ì— ëŒ€í•œ ì •ì˜ëŠ” ì¸¡ë„ë¡ ì— ëŒ€í•œ ì´í•´ê°€ ìˆì–´ì•¼ ê°€ëŠ¥í•¨. ì¸¡ë„ë¡ ì— ëŒ€í•œ ë‚´ìš©ì´ ê¶ê¸ˆí•˜ë‹¤ë©´ https://guebin.github.io/SS2024/ ì„ ê³µë¶€í•´ë³´ì„¸ìš”"
  },
  {
    "objectID": "posts/04wk-2.html#b.-ì™œ-ê°€ëŠ¥í•œê°€",
    "href": "posts/04wk-2.html#b.-ì™œ-ê°€ëŠ¥í•œê°€",
    "title": "04wk-2: (ì‹ ê²½ë§) â€“ êº½ì¸ê·¸ë˜í”„ì˜ í•œê³„(?), ì‹œë²¤ì½”ì •ë¦¬, MNIST",
    "section": "B. ì™œ ê°€ëŠ¥í•œê°€??",
    "text": "B. ì™œ ê°€ëŠ¥í•œê°€??\n- ì¤€ë¹„\n\nx = torch.linspace(-10,10,200).reshape(-1,1)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=2),\n    torch.nn.Sigmoid(),\n    torch.nn.Linear(in_features=2,out_features=1)\n)\nl1,a1,l2 = net\n\nğŸ—£ï¸ 2ê°œì˜ ì§ì„  -&gt; 2ê°œì˜ ê³¡ì„ (Sigmoid) -&gt; 1ê°œë¡œ í•©ì¹¨\n\nnet\n\nSequential(\n  (0): Linear(in_features=1, out_features=2, bias=True)\n  (1): Sigmoid()\n  (2): Linear(in_features=2, out_features=1, bias=True)\n)\n\n\nğŸ—£ï¸(\n\nplt.plot(x, net[0](x).data)\n\n\n\n\n\n\n\n\n\nplt.plot(x, net[:2](x).data) # Sigmoid ê¹Œì§€\n\n\n\n\n\n\n\n\n\nplt.plot(x, net[:3](x).data) # í•©ì³ì„œ í•˜ë‚˜ì˜ Sigmoid\n\n\n\n\n\n\n\n\n)ğŸ—£ï¸\n# ìƒê°1 â€“ 2ê°œì˜ ì‹œê·¸ëª¨ì´ë“œë¥¼ ìš°ì—°íˆ ì˜ ì¡°í•©í•˜ë©´ í•˜ë‚˜ì˜ ê³„ë‹¨í•¨ìˆ˜ë¥¼ ë§Œë“¤ ìˆ˜ ìˆë‹¤.\nğŸ—£ï¸ ìˆ«ìë¥¼ ì˜ ë•Œë ¤ë§ì¶”ë‹¤ë³´ë©´..\n\nl1.weight.data = torch.tensor([[-5.00],[5.00]])\nl1.bias.data = torch.tensor([+10.00,+10.00])\n\n\nl2.weight.data = torch.tensor([[1.00,1.00]])\nl2.bias.data = torch.tensor([-1.00])\n\n\nfig,ax = plt.subplots(1,3,figsize=(9,3))\nax[0].plot(x,l1(x)[:,[0]].data,label=r\"$-5x+10$\")\nax[0].plot(x,l1(x)[:,[1]].data,label=r\"$5x+10$\")\nax[0].set_title('$l_1(x)$')\nax[0].legend()\nax[1].plot(x,a1(l1(x))[:,[0]].data,label=r\"$v_1=sig(-5x+10)$\")\nax[1].plot(x,a1(l1(x))[:,[1]].data,label=r\"$v_2=sig(5x+10)$\")\nax[1].set_title('$(a_1 \\circ l_1)(x)$')\nax[1].legend()\nax[2].plot(x,l2(a1(l1(x))).data,color='C2',label=r\"$v_1+v_2-1$\")\nax[2].set_title('$(l_2 \\circ a_1 \\circ \\l_1)(x)$')\nax[2].legend()\n\n\n\n\n\n\n\n\n#\n# ìƒê°2 â€“ ê³„ë‹¨í•¨ìˆ˜ì˜ ëª¨ì–‘ì´ ê¼­ ìƒê°1ê³¼ ê°™ì„ í•„ìš”ëŠ” ì—†ë‹¤. ì¤‘ì‹¬ì€ ì´ë™ê°€ëŠ¥í•˜ê³ , ë†’ì´ë„ ì¡°ì ˆê°€ëŠ¥í•˜ë‹¤.\nê°€ëŠ¥í•œ ì˜ˆì‹œ1\n\nl1.weight.data = torch.tensor([[-5.00],[5.00]])\nl1.bias.data = torch.tensor([+0.00,+20.00])\nl2.weight.data = torch.tensor([[1.00,1.00]])\nl2.bias.data = torch.tensor([-1.00])\nfig,ax = plt.subplots(1,3,figsize=(9,3))\nax[0].plot(x,l1(x).data.numpy(),'--',color='C0'); ax[0].set_title('$l_1(x)$')\nax[1].plot(x,a1(l1(x)).data.numpy(),'--',color='C0'); ax[1].set_title('$(a_1 \\circ l_1)(x)$')\nax[2].plot(x,l2(a1(l1(x))).data,'--',color='C0'); ax[2].set_title('$(l_2 \\circ a_1 \\circ \\l_1)(x)$');\nax[2].set_ylim(-0.1,2.6)\n\n\n\n\n\n\n\n\nê°€ëŠ¥í•œ ì˜ˆì‹œ2\n\nl1.weight.data = torch.tensor([[-5.00],[5.00]])\nl1.bias.data = torch.tensor([+20.00,+00.00])\nl2.weight.data = torch.tensor([[2.50,2.50]])\nl2.bias.data = torch.tensor([-2.50])\nfig,ax = plt.subplots(1,3,figsize=(9,3))\nax[0].plot(x,l1(x).data.numpy(),'--',color='C1'); ax[0].set_title('$l_1(x)$')\nax[1].plot(x,a1(l1(x)).data.numpy(),'--',color='C1'); ax[1].set_title('$(a_1 \\circ l_1)(x)$')\nax[2].plot(x,l2(a1(l1(x))).data,'--',color='C1'); ax[2].set_title('$(l_2 \\circ a_1 \\circ \\l_1)(x)$');\nax[2].set_ylim(-0.1,2.6)\n\n\n\n\n\n\n\n\n#\n# ìƒê°3: ì²«ë²ˆì§¸ ì„ í˜•ë³€í™˜(=\\(l_1\\))ì—ì„œ out_features=4ë¡œ í•˜ê³  ì ë‹¹í•œ ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì •í•˜ë©´ \\((l_2\\circ a_1 \\circ l_1)(x)\\)ì˜ ê²°ê³¼ë¡œ ìƒê°2ì˜ ì˜ˆì‹œ1,2ë¥¼ ì¡°í•©í•œ í˜•íƒœë„ ê°€ëŠ¥í•  ê²ƒ ê°™ë‹¤. ì¦‰ 4ê°œì˜ ì‹œê·¸ëª¨ì´ë“œë¥¼ ì˜ ì¡°í•©í•˜ë©´ 2ë‹¨ê³„ ê³„ë‹¨í•¨ìˆ˜ë¥¼ ë§Œë“¤ ìˆ˜ ìˆë‹¤.\n\nl1 = torch.nn.Linear(in_features=1,out_features=4)\na1 = torch.nn.Sigmoid()\nl2 = torch.nn.Linear(in_features=4,out_features=1)\n\n\nl1.weight.data = torch.tensor([[-5.00],[5.00],[-5.00],[5.00]])\nl1.bias.data = torch.tensor([0.00, 20.00, 20.00, 0])\nl2.weight.data = torch.tensor([[1.00,  1.00, 2.50,  2.50]])\nl2.bias.data = torch.tensor([-1.0-2.5])\n\nğŸ—£ï¸ ìˆ«ìë¥¼ ë°”ê¾¸ë©´ ëª¨ì–‘ì´ ë‹¬ë¼ì§\n\nplt.plot(l2(a1(l1(x))).data,'--')\nplt.title(r\"$(l_2 \\circ a_1 \\circ l_1)(x)$\")\n\nText(0.5, 1.0, '$(l_2 \\\\circ a_1 \\\\circ l_1)(x)$')\n\n\n\n\n\n\n\n\n\n\nì´ëŸ¬í•œ í•¨ìˆ˜ëŠ” ê³„ë‹¨ëª¨ì–‘ì´ë©°, 0ì„ ì œì™¸í•œ ì„œë¡œë‹¤ë¥¸ ê³„ë‹¨ì˜ ë†’ì´ëŠ” 2ê°œê°€ ëœë‹¤. ì´ë¥¼ ê°„ë‹¨íˆ â€œ2ë‹¨ê³„-ê³„ë‹¨í•¨ìˆ˜â€ë¼ê³  ì¹­í•˜ì.\n\n#\n# ìƒê°4 â€“ \\(2m\\)ê°œì˜ ì‹œê·¸ëª¨ì´ë“œë¥¼ ìš°ì—°íˆ ì˜ ì¡°í•©í•˜ë©´ \\(m\\)ë‹¨ê³„ ê³„ë‹¨í•¨ìˆ˜ë¥¼ ë§Œë“¤ ìˆ˜ ìˆë‹¤.\n- ì •ë¦¬1: 2ê°œì˜ ì‹œê·¸ëª¨ì´ë“œë¥¼ ìš°ì—°íˆ ì˜ ê²°í•©í•˜ë©´ ì•„ë˜ì™€ ê°™ì€ â€œ1ë‹¨ê³„-ê³„ë‹¨í•¨ìˆ˜â€ í•¨ìˆ˜ \\(h\\)ë¥¼ ë§Œë“¤ ìˆ˜ ìˆë‹¤.\nğŸ—£ï¸(\n\nsig = torch.nn.Sigmoid()\n\n\nê³±í•˜ëŠ” ìˆ«ìê°€ ì»¤ì§ˆìˆ˜ë¡ ê¸‰í•˜ê²Œ ì˜¬ë¼ê°\n\n\nplt.plot(x, sig(0.5*(x-0.5)))\n\n\n\n\n\n\n\n\n\nplt.plot(x, sig(3*(x-0.5)))\n\n\n\n\n\n\n\n\n\nplt.plot(x, sig(200*(x-0.5)))\n\n\n\n\n\n\n\n\n\nplt.plot(x, sig(200*(x-0.5)))\nplt.plot(x, -sig(200*(x+0.5)))\n\n\n\n\n\n\n\n\n\nv1 = sig(200*(x-0.5))\nv2 = -sig(200*(x+0.5))\nplt.plot(x,v1+v2)\n\n\n\n\n\n\n\n\n\nplt.plot(x, -sig(200*(x-0.5)))\nplt.plot(x, sig(200*(x+0.5)))\n\n\n\n\n\n\n\n\n)ğŸ—£ï¸\n\ndef h(x):\n    sig = torch.nn.Sigmoid()\n    v1 = -sig(200*(x-0.5))\n    v2 = sig(200*(x+0.5))\n    return v1+v2 \n\n\nplt.plot(x,h(x))\nplt.title(\"$h(x)$\")\n\nText(0.5, 1.0, '$h(x)$')\n\n\n\n\n\n\n\n\n\n- ì •ë¦¬2: ìœ„ì™€ ê°™ì€ í•¨ìˆ˜ \\(h\\)ë¥¼ ì´ìš©í•œ ì•„ë˜ì˜ ë„¤íŠ¸ì›Œí¬ë¥¼ ê³ ë ¤í•˜ì. ì´ëŠ” â€œmë‹¨ê³„-ê³„ë‹¨í•¨ìˆ˜â€ë¥¼ ë§Œë“ ë‹¤.\n\\[\\underset{(n,1)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,m)}{\\boldsymbol u^{(1)}} \\overset{h}{\\to} \\underset{(n,m)}{\\boldsymbol v^{(1)}} \\overset{l_2}{\\to} \\underset{(n,1)}{\\hat{\\boldsymbol y}}\\]\nê·¸ë¦¬ê³  ìœ„ì˜ ë„¤íŠ¸ì›Œí¬ì™€ ë™ì¼í•œ íš¨ê³¼ë¥¼ ì£¼ëŠ” ì•„ë˜ì˜ ë„¤íŠ¸ì›Œí¬ê°€ í•­ìƒ ì¡´ì¬í•¨.\nğŸ—£ï¸ 2ê°œì˜ Sigmoidë¥¼ ê°ê° ì·¨í•¨\n\\[\\underset{(n,1)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,2m)}{\\boldsymbol u^{(1)}} \\overset{sig}{\\to} \\underset{(n,2m)}{\\boldsymbol v^{(1)}} \\overset{l_2}{\\to} \\underset{(n,1)}{\\hat{\\boldsymbol y}}\\]\n#\n# ìƒê°5 â€“ ê·¸ëŸ°ë° ì–´ì§€ê°„í•œ í•¨ìˆ˜í˜•íƒœëŠ” êµ¬ë¶ˆêµ¬ë¶ˆí•œ â€œmë‹¨ê³„-ê³„ë‹¨í•¨ìˆ˜â€ë¡œ ë‹¤ ê·¼ì‚¬í•  ìˆ˜ ìˆì§€ ì•Šë‚˜?\nê·¸ë ‡ë‹¤ë©´ ì•„ë˜ì˜ ë„¤íŠ¸ì›Œí¬ì—ì„œ (1) ?? ë¥¼ ì¶©ë¶„íˆ í‚¤ìš°ê³  (2) ì ì ˆí•˜ê²Œ í•™ìŠµë§Œ ì˜ ëœë‹¤ë©´\nnet = torch.nn.Sequential(\n    torch.nn.Linear(p,???),\n    torch.nn.Sigmoid(),\n    torch.nn.Linear(???,q)\n)\nìœ„ì˜ ë„¤íŠ¸ì›Œí¬ëŠ” ê±°ì˜ ë¬´í•œí•œ í‘œí˜„ë ¥ì„ ê°€ì§„ë‹¤. â€“&gt; ì´ëŸ°ì‹ìœ¼ë¡œ ì¦ëª…í•˜ë©´ ë©ë‹ˆë‹¹\n#"
  },
  {
    "objectID": "posts/04wk-2.html#c.-hì˜-ìœ„ë ¥",
    "href": "posts/04wk-2.html#c.-hì˜-ìœ„ë ¥",
    "title": "04wk-2: (ì‹ ê²½ë§) â€“ êº½ì¸ê·¸ë˜í”„ì˜ í•œê³„(?), ì‹œë²¤ì½”ì •ë¦¬, MNIST",
    "section": "C. \\(h\\)ì˜ ìœ„ë ¥",
    "text": "C. \\(h\\)ì˜ ìœ„ë ¥\nğŸ—£ï¸ Sigmoid ëŒ€ì‹  hë¥¼ í•˜ê³  ì‹¶ìŒ\n- ì†Œë§: ì•„ë˜ì™€ ê°™ì´ netì„ ì„¤ê³„í•´ì„œ, ê·¸ ìœ„ë ¥ì„ ì²´ê°í•´ë³´ê³  ì‹¶ì€ë°..\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,??),\n    torch.nn.H(),\n    torch.nn.Linear(??,1)\n)\n- \\(h(x)\\)ë¥¼ ìƒì„±í•˜ëŠ” í´ë˜ìŠ¤ë¥¼ ë§Œë“¤ì–´ë³´ì.\nğŸ—£ï¸ Module: ìƒì† / ì˜ ëª¨ë¥´ê² ìœ¼ë©´ ë‹¤ìŒì„ templeteìœ¼ë¡œ ìƒê°í•˜ê³  ì™¸ìš°ê¸°\nclass H(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self,x):\n        # out = h(x)\n        return out\nğŸ—£ï¸(\n\nclass H(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self,x):\n        def h(x):\n            sig = torch.nn.Sigmoid()\n            v1 = -sig(200*(x-0.5))\n            v2 = sig(200*(x+0.5))\n            return v1+v2 \n        out = h(x)\n        return out \n\n\nmy_h = H()\n\n\nplt.plot(x, my_h(x))\n\n\n\n\n\n\n\n\n)ğŸ—£ï¸\n\nclass H(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self,x):\n        def h(x):\n            sig = torch.nn.Sigmoid()\n            v1 = -sig(200*(x-0.5))\n            v2 = sig(200*(x+0.5))\n            return v1+v2 \n        out = h(x)\n        return out \n\n\nh = H()\n\n- \\(h\\)ì˜ ìœ„ë ¥ì„ ì²´ê°í•´ë³´ì.\n# ì˜ˆì œ1 â€“ ìŠ¤í™ì˜ ì—­ì„¤\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DL2025/main/posts/ironyofspec.csv\")\nx = torch.tensor(df.x).float().reshape(-1,1)\ny = torch.tensor(df.y).float().reshape(-1,1)\nprob = torch.tensor(df.prob).float().reshape(-1,1)\n\nğŸ—£ï¸(\n\në‹¤ìŒì„ ì í•©ì‹œí‚¤ë ¤ê³  í•¨\n\n\nplt.plot(x,prob)\n\n\n\n\n\n\n\n\n\nnetwork ë§Œë“  ì´ì „ ë°©ì‹\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,2,bias=False),\n    torch.nn.ReLu(),\n    torch.nn.Linear(2,1),\n    torch.Sigmoid()\n)\n\nì´ì œ ì´ë ‡ê²Œ í•˜ì§€ ì•Šê³  ë‹¤ìŒê³¼ ê°™ì´ í•˜ë ¤ê³  í•¨\n\n)ğŸ—£ï¸\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,2048),\n    H(),\n    torch.nn.Linear(2048,1),\n    torch.nn.Sigmoid()\n)\nloss_fn = torch.nn.BCELoss()\noptimizr = torch.optim.Adam(net.parameters())\n#---#\nfor epoc in range(200):\n    ## 1 \n    yhat = net(x)\n    ## 2\n    loss = loss_fn(yhat,y)\n    ## 3\n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,prob)\nplt.plot(x,net(x).data,'--')\n\n\n\n\n\n\n\n\nğŸ—£ï¸ ì í•©ëœ ê²ƒì„ ë³´ë©´ ì•ˆ ë§ê¸°ëŠ” í•˜ë‚˜ ë”°ë¼ê°€ê³ ëŠ” ìˆìŒ\n#\n# ì˜ˆì œ2 â€“ ìˆ˜ëŠ¥ê³¡ì„ \n\ntorch.manual_seed(43052)\nx = torch.linspace(0,2,2000).reshape(-1,1)\neps = torch.randn(2000).reshape(-1,1)*0.05\nfx = torch.exp(-1*x)* torch.abs(torch.cos(3*x))*(torch.sin(3*x))\ny = fx + eps\n\n\nplt.plot(x,y,alpha=0.5)\nplt.plot(x,fx)\n\n\n\n\n\n\n\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,2048),\n    H(),\n    torch.nn.Linear(2048,1)\n)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n#---#\nfor epoc in range(200):\n    ## 1 \n    yhat = net(x)\n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\nğŸ—£ï¸ Sigmoidë¥¼ í•  í•„ìš”ëŠ” X\n\nplt.plot(x,y,alpha=0.5)\nplt.plot(x,fx)\nplt.plot(x,net(x).data,'--')\n\n\n\n\n\n\n\n\n#"
  },
  {
    "objectID": "posts/04wk-2.html#d.-ì˜ë¬¸ì ",
    "href": "posts/04wk-2.html#d.-ì˜ë¬¸ì ",
    "title": "04wk-2: (ì‹ ê²½ë§) â€“ êº½ì¸ê·¸ë˜í”„ì˜ í•œê³„(?), ì‹œë²¤ì½”ì •ë¦¬, MNIST",
    "section": "D. ì˜ë¬¸ì ",
    "text": "D. ì˜ë¬¸ì \nğŸ—£ï¸ ë°˜ë°•ì€ ë‹¤ìŒ ì‹œê°„\n- ì´ ìˆ˜ì—…ì„ ì˜ ì´í•´í•œ ì‚¬ëŒ: ê·¸ëƒ¥ í™œì„±í™”í•¨ìˆ˜ë¥¼ \\(h\\)ë¡œ ì“°ë©´ ë ì•„ë‹ˆì•¼? ë­í•˜ëŸ¬ relu ë¥¼ ì“°ëŠ”ê±°ì§€?\n- ë”¥ëŸ¬ë‹ì„ ì¢€ ê³µë¶€í•´ë³¸ì‚¬ëŒ1: ì™œ ë”¥ëŸ¬ë‹ì´ 2010ë…„ì´ ì§€ë‚˜ì„œì•¼ ë–³ì§€? 1989ë…„ì— ì„¸ìƒì˜ ëª¨ë“  ë¬¸ì œê°€ í’€ë ¤ì•¼ í•˜ëŠ”ê²ƒ ì•„ë‹Œê°€?\n- ë”¥ëŸ¬ë‹ì„ ì¢€ ê³µë¶€í•´ë³¸ì‚¬ëŒ2: í•˜ë‚˜ì˜ ì€ë‹‰ì¸µì„ ê°€ì§„ ë„¤í¬ì›Œí¬ëŠ” ì˜ ì•ˆì“°ì§€ ì•Šë‚˜? ì€ë‹‰ì¸µì´ ê¹Šì„ìˆ˜ë¡ ì¢‹ë‹¤ê³  ë“¤ì—ˆëŠ”ë°?\n- ì•½ê°„ì˜ ì˜êµ¬ì‹¬ì´ ìˆì§€ë§Œ ì•„ë¬´íŠ¼ ìš°ë¦¬ëŠ” ì•„ë˜ì˜ ë¬´ê¸°ë¥¼ ê°€ì§„ ê¼´ì´ ë˜ì—ˆë‹¤.\n\n\n\n\n\n\nìš°ë¦¬ì˜ ë¬´ê¸°\n\n\n\ní•˜ë‚˜ì˜ ì€ë‹‰ì¸µì„ ê°€ì§€ëŠ” ì•„ë˜ì™€ ê°™ì€ ê¼´ì˜ ë„¤íŠ¸ì›Œí¬ë¡œ,\nnet = torch.nn.Sequential(\n    torch.nn.Linear(p,???),\n    torch.nn.Sigmoid(),\n    torch.nn.Linear(???,q)\n)\n\\(f: {\\bf X}_{n \\times p} \\to {\\bf y}_{n\\times q}\\) ì¸ ëª¨ë“  ë³´ë  ê°€ì¸¡ í•¨ìˆ˜ \\(f\\) ì„ ì›í•˜ëŠ” ì •í™•ë„ë¡œ â€œê·¼ì‚¬â€ì‹œí‚¬ ìˆ˜ ìˆë‹¤."
  },
  {
    "objectID": "posts/04wk-2.html#a.-ì˜ˆë¹„í•™ìŠµ-plt.imshow",
    "href": "posts/04wk-2.html#a.-ì˜ˆë¹„í•™ìŠµ-plt.imshow",
    "title": "04wk-2: (ì‹ ê²½ë§) â€“ êº½ì¸ê·¸ë˜í”„ì˜ í•œê³„(?), ì‹œë²¤ì½”ì •ë¦¬, MNIST",
    "section": "A. ì˜ˆë¹„í•™ìŠµ â€“ plt.imshow()",
    "text": "A. ì˜ˆë¹„í•™ìŠµ â€“ plt.imshow()\n- plt.imshow(..., cmap=\"gray\") ì—ì„œ ...ì´ shapeì´ (??,??)ì´ë©´ í‘ë°±ì´ë¯¸ì§€ë¥¼ ì¶œë ¥\nğŸ—£ï¸(\n\nimg = torch.tensor([[255,100],\n                    [255,0]])\nplt.imshow(img)\n\n\n\n\n\n\n\n\n\nimg.shape # 2x2 í”½ì…€\n\ntorch.Size([2, 2])\n\n\n)ğŸ—£ï¸\n\nimg = torch.tensor([[255,100],\n                    [255,0]])\nplt.imshow(img,cmap=\"gray\")\n\n\n\n\n\n\n\n\nğŸ—£ï¸ ìˆ«ìê°€ í´ìˆ˜ë¡ í°ìƒ‰, ì‘ì„ìˆ˜ë¡ ê²€ì •ìƒ‰\n- plt.imshow(...) ì—ì„œ ...ì˜ shapeì´ (??,??,3)ì´ë©´ ì¹¼ë¼ì´ë¯¸ì§€ë¥¼ ì¶œë ¥\nğŸ—£ï¸(\n\nr = torch.tensor([[255,0],\n                  [255,0]])\ng = r*0\nb = r*0\ng\n\ntensor([[0, 0],\n        [0, 0]])\n\n\n\nr.shape\n\ntorch.Size([2, 2])\n\n\n\ntorch.stack([r,g,b],axis=-1)\n\ntensor([[[255,   0,   0],\n         [  0,   0,   0]],\n\n        [[255,   0,   0],\n         [  0,   0,   0]]])\n\n\n\ntorch.stack([r,g,b],axis=-1).shape\n\ntorch.Size([2, 2, 3])\n\n\n)ğŸ—£ï¸\n\nr = torch.tensor([[255,0],\n                  [255,0]])\ng = torch.tensor([[0,255],\n                  [0,0]])\nb = torch.tensor([[0,0],\n                  [0,255]])\nimg = torch.stack([r,g,b],axis=-1)\nplt.imshow(img)\n\n\n\n\n\n\n\n\nğŸ”¬\n\nimg\n\ntensor([[[255,   0,   0],\n         [  0, 255,   0]],\n\n        [[255,   0,   0],\n         [  0,   0, 255]]])\n\n\n- plt.imshow(...) ì—ì„œ ...ì˜ ìë£Œí˜•ì´ intì¸ì§€ floatì¸ì§€ì— ë”°ë¼ì„œ ì¸ì‹ì´ ë‹¤ë¦„\nğŸ—£ï¸ int: maxë¥¼ 255ë¡œ ê·¸ë¦¼, float: maxë¥¼ 1ë¡œ ê·¸ë¦¼\n\nr = torch.tensor([[1,0],\n                  [1,0]])\ng = torch.tensor([[0,1],\n                  [0,0]])\nb = torch.tensor([[0,0],\n                  [0,1]])\nimg = torch.stack([r,g,b],axis=-1)\nplt.imshow(img)\n\n\n\n\n\n\n\n\n\nimg[0]\n\ntensor([[1, 0, 0],\n        [0, 1, 0]])\n\n\n\nr = torch.tensor([[255,0],\n                  [255,0]])/255\ng = torch.tensor([[0,255],\n                  [0,0]])/255\nb = torch.tensor([[0,0],\n                  [0,255]])/255\nimg = torch.stack([r,g,b],axis=-1)\nplt.imshow(img)\n\n\n\n\n\n\n\n\n\nimg[0]\n\ntensor([[1., 0., 0.],\n        [0., 1., 0.]])"
  },
  {
    "objectID": "posts/04wk-2.html#b.-ë°ì´í„°",
    "href": "posts/04wk-2.html#b.-ë°ì´í„°",
    "title": "04wk-2: (ì‹ ê²½ë§) â€“ êº½ì¸ê·¸ë˜í”„ì˜ í•œê³„(?), ì‹œë²¤ì½”ì •ë¦¬, MNIST",
    "section": "B. ë°ì´í„°",
    "text": "B. ë°ì´í„°\n- ë°ì´í„° ì •ë¦¬ì½”ë“œ\n\ntrain_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True)\nto_tensor = torchvision.transforms.ToTensor()\nX3 = torch.stack([to_tensor(Xi) for Xi, yi in train_dataset if yi==3])\nX7 = torch.stack([to_tensor(Xi) for Xi, yi in train_dataset if yi==7])\nX = torch.concat([X3,X7],axis=0)\ny = torch.tensor([0.0]*len(X3) + [1.0]*len(X7))\n\n100.0%\n100.0%\n100.0%\n100.0%\n\n\nğŸ—£ï¸(\n\nX.shape # 4ì°¨ì›\n\ntorch.Size([12396, 1, 28, 28])\n\n\n\nX[0].shape\n\ntorch.Size([1, 28, 28])\n\n\n\nX[0][0].shape\n\ntorch.Size([28, 28])\n\n\n\nplt.imshow(X[0][0],cmap=\"gray\")\n\n\n\n\n\n\n\n\n\nplt.imshow(X[0].reshape(28,28),cmap=\"gray\")\n\n\n\n\n\n\n\n\n\nplt.imshow(X[1].reshape(28,28),cmap=\"gray\")\n\n\n\n\n\n\n\n\n\nplt.imshow(X[-1].reshape(28,28),cmap=\"gray\") # ëì— ìˆëŠ” ê´€ì¸¡ì¹˜\n\n\n\n\n\n\n\n\n\nplt.imshow(X[-2].reshape(28,28),cmap=\"gray\") # ëì— ìˆëŠ” ê´€ì¸¡ì¹˜\n\n\n\n\n\n\n\n\n\ny\n\ntensor([0., 0., 0.,  ..., 1., 1., 1.])\n\n\n\n0ì€ 3ì˜ ì´ë¯¸ì§€, 1ì€ 7ì˜ ì´ë¯¸ì§€\n\n\nlen(y)\n\n12396\n\n\n)ğŸ—£ï¸\n\nplt.plot(y,'.')\n\n\n\n\n\n\n\n\n- ìš°ë¦¬ëŠ” \\({\\bf X}: (n,1,28,28)\\) ì—ì„œ \\({\\bf y}: (n,1)\\)ìœ¼ë¡œ ê°€ëŠ” ë§µí•‘ì„ ë°°ìš°ê³  ì‹¶ìŒ. \\(\\to\\) ì´ëŸ°ê±´ ë°°ìš´ì ì´ ì—†ëŠ”ë°?.. \\(\\to\\) ê·¸ë ‡ë‹¤ë©´ \\({\\bf X}:(n,784) \\to {\\bf y}:(n,1)\\) ìœ¼ë¡œ ê°€ëŠ” ë§µí•‘ì„ í•™ìŠµí•˜ì.\n)ğŸ—£ï¸\n\n28*28\n\n784\n\n\n[img for img in X] = [X[0], X[1], ..., X[-1]]\n\nX[0].shape\n\ntorch.Size([1, 28, 28])\n\n\n\ntorch.stack([img.reshape(-1) for img in X])\n\ntensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]])\n\n\n\ntorch.stack([img.reshape(-1) for img in X]).shape\n\ntorch.Size([12396, 784])\n\n\n\ny.shape # vector\n\ntorch.Size([12396])\n\n\n)ğŸ—£ï¸\n\nX = torch.stack([img.reshape(-1) for img in X])\ny = y.reshape(-1,1)\n\n\nX.shape,y.shape\n\n(torch.Size([12396, 784]), torch.Size([12396, 1]))"
  },
  {
    "objectID": "posts/04wk-2.html#c.-í•™ìŠµ",
    "href": "posts/04wk-2.html#c.-í•™ìŠµ",
    "title": "04wk-2: (ì‹ ê²½ë§) â€“ êº½ì¸ê·¸ë˜í”„ì˜ í•œê³„(?), ì‹œë²¤ì½”ì •ë¦¬, MNIST",
    "section": "C. í•™ìŠµ",
    "text": "C. í•™ìŠµ\nğŸ—£ï¸ Hê°€ ë” ì¢‹ì€ ê²ƒì„ ì•Œê³  ìˆì§€ë§Œ ì‚¬ëŒë“¤ì´ ë§ì´ ì“°ëŠ” ReLUë¡œ\nğŸ—£ï¸(\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(784,32),\n    torch.nn.ReLU(),\n    torch.nn.Linear(32,1),\n    torch.nn.Sigmoid()\n)\n\n\nnet(X)\n\ntensor([[0.5066],\n        [0.5152],\n        [0.4821],\n        ...,\n        [0.5168],\n        [0.5087],\n        [0.5066]], grad_fn=&lt;SigmoidBackward0&gt;)\n\n\n\nyì™€ ë¹„ìŠ·í•œ í˜•íƒœë¡œ ì¶œë ¥ë˜ëŠ” ê²ƒì´ ì¤‘ìš”\n\n)ğŸ—£ï¸\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(784,32),\n    torch.nn.ReLU(),\n    torch.nn.Linear(32,1),\n    torch.nn.Sigmoid()\n)\nloss_fn = torch.nn.BCELoss()\noptimizr = torch.optim.Adam(net.parameters())\n#---#\nfor epoc in range(200):\n    ## 1 \n    yhat = net(X) \n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3\n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(y,'.')\nplt.plot(net(X).data,'.',alpha=0.2)\n\n\n\n\n\n\n\n\nğŸ—£ï¸ í‹€ë¦° ê²ƒë„ ìˆì§€ë§Œ ë§ì€ ê²ƒì´ ë” ë§ìŒ (2,000ë²ˆí•˜ë©´ ë” ë§ì´ ë§ì¶¤)\nğŸ—£ï¸(Accuracy\n\nRule ì •í•˜ê¸°\n\n\nnet(X).data &gt; 0.5\n\ntensor([[False],\n        [False],\n        [False],\n        ...,\n        [ True],\n        [ True],\n        [ True]])\n\n\n\n(net(X).data &gt; 0.5)*1.0\n\ntensor([[0.],\n        [0.],\n        [0.],\n        ...,\n        [1.],\n        [1.],\n        [1.]])\n\n\n\ny\n\ntensor([[0.],\n        [0.],\n        [0.],\n        ...,\n        [1.],\n        [1.],\n        [1.]])\n\n\n\n(y == (net(X).data &gt; 0.5)*1.0)\n\ntensor([[True],\n        [True],\n        [True],\n        ...,\n        [True],\n        [True],\n        [True]])\n\n\n\n(y == (net(X).data &gt; 0.5)*1.0).sum()\n\ntensor(12264)\n\n\n\nlen(y)\n\n12396\n\n\n\n12264/12396\n\n0.989351403678606\n\n\n\n((y == (net(X).data &gt; 0.5))*1.0).mean()\n\ntensor(0.9894)\n\n\n)ğŸ—£ï¸\n\n((y == (net(X).data &gt; 0.5))*1.0).mean()\n\ntensor(0.9894)\n\n\n\n\n\n\n\n\nNote\n\n\n\nì´ë¯¸ì§€ìë£Œì˜ ì°¨ì›\n\nì¹¼ë¼ì´ë¯¸ì§€ë°ì´í„° \\({\\bf X}\\)ëŠ” (n,3,h,w) ì˜ ì°¨ì›ì„ ê°€ì§€ê±°ë‚˜ (n,h,w,3)ì˜ ì°¨ì›ì„ ê°€ì§„ë‹¤.\ní‘ë°±ì´ë¯¸ì§€ë°ì´í„° \\({\\bf X}\\)ëŠ” (n,h,w) ì˜ ì°¨ì›ì„ ê°€ì§€ê±°ë‚˜ (n,1,h,w)ì˜ ì°¨ì›ì„ ê°€ì§€ê±°ë‚˜ (n,h,w,1)ì˜ ì°¨ì›ì„ ê°€ì§„ë‹¤."
  },
  {
    "objectID": "posts/05wk-1.html",
    "href": "posts/05wk-1.html",
    "title": "05wk-1: (ì‹ ê²½ë§) â€“ ì˜ˆì¸¡, ì‹œë²¤ì½”ì •ë¦¬ì˜ ì´ë©´, ë“œëì•„ì›ƒ",
    "section": "",
    "text": "ğŸ“˜ Note Format Guide\nThis format serves as a structured guide for organizing lecture content, personal interpretation, experiments, and study-related questions.\nğŸ“ ğŸ—£ï¸ âœï¸ ğŸ”¬ â“"
  },
  {
    "objectID": "posts/05wk-1.html#a.-ë°ì´í„°",
    "href": "posts/05wk-1.html#a.-ë°ì´í„°",
    "title": "05wk-1: (ì‹ ê²½ë§) â€“ ì˜ˆì¸¡, ì‹œë²¤ì½”ì •ë¦¬ì˜ ì´ë©´, ë“œëì•„ì›ƒ",
    "section": "A. ë°ì´í„°",
    "text": "A. ë°ì´í„°\n\ntorch.manual_seed(43052)\nx,_ = torch.randn(100).sort()\neps = torch.randn(100)*0.5\ny = x * 4 + 2.5 + eps\nx,y = x.reshape(-1,1), y.reshape(-1,1)\n\n\nplt.plot(x,y,'o')"
  },
  {
    "objectID": "posts/05wk-1.html#b.-í•™ìŠµ",
    "href": "posts/05wk-1.html#b.-í•™ìŠµ",
    "title": "05wk-1: (ì‹ ê²½ë§) â€“ ì˜ˆì¸¡, ì‹œë²¤ì½”ì •ë¦¬ì˜ ì´ë©´, ë“œëì•„ì›ƒ",
    "section": "B. í•™ìŠµ",
    "text": "B. í•™ìŠµ\n\nğŸ—£ï¸ í˜„ì¬ ìˆ˜ì¤€ì—ì„œëŠ” ë‹¤ìŒì²˜ëŸ¼ ìƒê°\n\nyê°€ ì—°ì†: MSELoss\nyê°€ 0 ë˜ëŠ” 1ë§Œ: BCELoss\n\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1)\n)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters(),lr=0.1)\n## \nfor epoc in range(200):\n    ## step1 \n    yhat = net(x) \n    ## step2 \n    loss = loss_fn(yhat,y)\n    ## step3 \n    loss.backward()\n    ## step4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'o')\nplt.plot(x,net(x).data,'--')\n\n\n\n\n\n\n\n\n\nnet[0].weight, net[0].bias\n\n(Parameter containing:\n tensor([[4.0042]], requires_grad=True),\n Parameter containing:\n tensor([2.4459], requires_grad=True))"
  },
  {
    "objectID": "posts/05wk-1.html#c.-ì˜ˆì¸¡",
    "href": "posts/05wk-1.html#c.-ì˜ˆì¸¡",
    "title": "05wk-1: (ì‹ ê²½ë§) â€“ ì˜ˆì¸¡, ì‹œë²¤ì½”ì •ë¦¬ì˜ ì´ë©´, ë“œëì•„ì›ƒ",
    "section": "C. ì˜ˆì¸¡",
    "text": "C. ì˜ˆì¸¡\nì˜¨ë„ê°€ 0.1 ë„ì¼ë•Œ, ì»¤í”¼ë¥¼ ì–¼ë§ˆë‚˜ íŒ”ê¹Œ?\n\n0.1 * 4.0042 + 2.4459 \n\n2.84632\n\n\n\nxx = torch.tensor([[0.1]])\nnet(xx)\n\ntensor([[2.8463]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\nì˜¨ë„ê°€ 0.2ë„ì¼ë•Œ ì»¤í”¼ë¥¼ ì–¼ë§ˆë‚˜ íŒ”ê¹Œ?\n\n0.2 * 4.0042 + 2.4459 \n\n3.24674\n\n\n\nxx = torch.tensor([[0.2]])\nnet(xx)\n\ntensor([[3.2467]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\nì˜¨ë„ê°€ [0.1, 0.2] ì¼ë•Œì˜ ì˜ˆì¸¡ê°’ì„ í•œë²ˆì— ë³´ê³  ì‹¶ë‹¤ë©´?\n\nxx = torch.tensor([[0.1],\n                   [0.2]])\nnet(xx)\n\ntensor([[2.8463],\n        [3.2467]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\n\n\n\n\n\n\nNote\n\n\n\nì´ê±° ì§ˆë¬¸ì´ ì™€ì„œ ì¢€ ë” ìì„¸íˆ ì„¤ëª…í•˜ê² ìŠµë‹ˆë‹¤. (ì•„ì§ net(x)ì˜ ê³„ì‚° ê³¼ì •ì„ ì„ í˜• ë³€í™˜ ê´€ì ì—ì„œ ìˆ˜ì‹ìœ¼ë¡œ ì •ë¦¬í•˜ëŠ” ë° ìµìˆ™í•˜ì§€ ì•Šìœ¼ì…”ì„œ ê·¸ëŸ´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ê±´ ë‹¨ìˆœ ì‚°ìˆ˜ë¼ì„œ í•˜ë‚˜ì”© ì°¨ê·¼ì°¨ê·¼ ë”°ë¼ê°€ë‹¤ ë³´ë©´ ì¶©ë¶„íˆ ì´í•´í•˜ì‹¤ ìˆ˜ ìˆì–´ìš”. ì²˜ìŒë¶€í„° ë°”ë¡œ ì´í•´ë˜ì§€ ì•Šë”ë¼ë„ ì „í˜€ ê±±ì •í•˜ì‹¤ í•„ìš” ì—†ìŠµë‹ˆë‹¤.)\ní•˜ë‚˜ì˜ ê°’ \\(x\\)ì— ëŒ€í•˜ì—¬ \\(net(x)\\)ëŠ” ì•„ë˜ë¥¼ ì˜ë¯¸í•˜ëŠ” ì—°ì‚°ì„ í•©ë‹ˆë‹¤.\nnet(x) = 4.0042 * x + 2.4459  = net[0].weight * x + net[0].bias\nì‚¬ì‹¤ ìœ„ì˜ ê³¼ì •ì„ ìˆ˜ì‹ìœ¼ë¡œ ì—„ë°€í•˜ê²Œ ì“°ë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.\n\\[net(\\begin{bmatrix} x \\end{bmatrix}) = 2.4459 + \\begin{bmatrix} x \\end{bmatrix} \\begin{bmatrix} 4.0042 \\end{bmatrix}\\]\nì—¬ê¸°ì—ì„œ \\(\\begin{bmatrix} x \\end{bmatrix}\\) ì™€ \\(\\begin{bmatrix} 4.0042  \\end{bmatrix}\\) ëŠ” ëª¨ë‘ \\(1\\times 1\\) matrixë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ë§Œì•½ì— \\(2 \\times 1\\) matrix \\({\\bf x} = \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}\\)ë¥¼ ë„¤íŠ¸ì›Œí¬ì˜ ì…ë ¥ìœ¼ë¡œ ê³ ë ¤í•œë‹¤ë©´ ì•„ë˜ì™€ ê°™ì´ ë©ë‹ˆë‹¤.\n\\[net({\\bf x})=net\\left(\\begin{bmatrix}x_1 \\\\ x_2 \\end{bmatrix}\\right) = 2.4459 + \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} \\begin{bmatrix} 4.0042 \\end{bmatrix} = \\begin{bmatrix} 2.4459 + 4.0042 x_1 \\\\ 2.4459 + 4.0042 x_2\\end{bmatrix} \\]\në”°ë¼ì„œ \\({\\bf xx} = \\begin{bmatrix} 0.1 \\\\ 0.2 \\end{bmatrix}\\) ë¥¼ ë„¤íŠ¸ì›Œí¬ì˜ ì…ë ¥ìœ¼ë¡œ ë„£ìœ¼ë©´\n\\[net({\\bf xx})= \\begin{bmatrix} 2.4459 + 4.0042 \\times 0.1 \\\\ 2.4459 + 4.0042 \\times 0.2\\end{bmatrix}= \\begin{bmatrix} 2.8463 \\\\ 3.2467 \\end{bmatrix}\\]\nì™€ ê°™ì´ ê³„ì‚°ë˜ê² ì£ ."
  },
  {
    "objectID": "posts/05wk-1.html#a.-ì˜¤ë²„í”¼íŒ…",
    "href": "posts/05wk-1.html#a.-ì˜¤ë²„í”¼íŒ…",
    "title": "05wk-1: (ì‹ ê²½ë§) â€“ ì˜ˆì¸¡, ì‹œë²¤ì½”ì •ë¦¬ì˜ ì´ë©´, ë“œëì•„ì›ƒ",
    "section": "A. ì˜¤ë²„í”¼íŒ…",
    "text": "A. ì˜¤ë²„í”¼íŒ…\n- ì˜¤ë²„í”¼íŒ…ì´ë€?\n\nìœ„í‚¤: In mathematical modeling, overfitting is â€œthe production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit to additional data or predict future observations reliablyâ€. (ìˆ˜í•™ì  ëª¨ë¸ë§ì—ì„œ ê³¼ì í•©ì´ë€ â€œì–´ë–¤ ëª¨ë¸ì´ ì£¼ì–´ì§„ ë°ì´í„°ì— ë„ˆë¬´ ê¼­ ë§ì¶°ì ¸ ìˆì–´ì„œ, ìƒˆë¡œìš´ ë°ì´í„°ë‚˜ ë¯¸ë˜ì˜ ê²°ê³¼ë¥¼ ì˜ ì˜ˆì¸¡í•˜ì§€ ëª»í•  ìˆ˜ ìˆëŠ” ìƒíƒœâ€ë¥¼ ì˜ë¯¸í•œë‹¤.)\nì œ ê°œë…: ë°ì´í„°ë¥¼ â€œë°ì´í„° = ì–¸ë”ë¼ì‰ + ì˜¤ì°¨â€ë¼ê³  ìƒê°í• ë•Œ ìš°ë¦¬ê°€ ë°ì´í„°ë¡œë¶€í„° ì í•©í•  ê²ƒì€ ì–¸ë”ë¼ì‰ì¸ë° ì˜¤ì°¨í•­ì„ ì í•©í•˜ê³  ìˆëŠ” í˜„ìƒ."
  },
  {
    "objectID": "posts/05wk-1.html#b.-ì˜¤ë²„í”¼íŒ…-ì˜ˆì‹œ",
    "href": "posts/05wk-1.html#b.-ì˜¤ë²„í”¼íŒ…-ì˜ˆì‹œ",
    "title": "05wk-1: (ì‹ ê²½ë§) â€“ ì˜ˆì¸¡, ì‹œë²¤ì½”ì •ë¦¬ì˜ ì´ë©´, ë“œëì•„ì›ƒ",
    "section": "B. ì˜¤ë²„í”¼íŒ… ì˜ˆì‹œ",
    "text": "B. ì˜¤ë²„í”¼íŒ… ì˜ˆì‹œ\nğŸ—£ï¸ ë„¤íŠ¸ì›Œí¬ì˜ í‘œí˜„ë ¥ì´ ë„ˆë¬´ ì¢‹ì„ ë•Œ\n- \\(m\\)ì´ ë§¤ìš° í´ë•Œ ì•„ë˜ì˜ ë„¤íŠ¸ì›Œí¬ ê±°ì˜ ë¬´ì—‡ì´ë“  ë§ì¶œ ìˆ˜ ìˆë‹¤ê³  ë³´ë©´ ëœë‹¤.\n\n\\(\\underset{(n,1)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,m)}{\\boldsymbol u^{(1)}} \\overset{h}{\\to} \\underset{(n,m)}{\\boldsymbol v^{(1)}} \\overset{l_2}{\\to} \\underset{(n,1)}{\\hat{\\boldsymbol y}}\\)\n\\(\\underset{(n,1)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,m)}{\\boldsymbol u^{(1)}} \\overset{sig}{\\to} \\underset{(n,m)}{\\boldsymbol v^{(1)}} \\overset{l_2}{\\to} \\underset{(n,1)}{\\hat{\\boldsymbol y}}\\)\n\\(\\underset{(n,1)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,m)}{\\boldsymbol u^{(1)}} \\overset{relu}{\\to} \\underset{(n,m)}{\\boldsymbol v^{(1)}} \\overset{l_2}{\\to} \\underset{(n,1)}{\\hat{\\boldsymbol y}}\\)\n\n- ê·¸ëŸ°ë° ì¢…ì¢… ë§ì¶”ì§€ ë§ì•„ì•¼ í•  ê²ƒë“¤ë„ ë§ì¶˜ë‹¤.\n\\[\\text{model:} \\quad y_i = (0\\times x_i) + \\epsilon_i,~~ \\text{where}~ \\epsilon_i \\sim N(0,0.01^2)\\]\nğŸ—£ï¸ yëŠ” xì— ëŒ€í•œ ì‹ X / underlying = 0 / structure: í—ˆêµ¬, ì˜¤ì°¨í•­ì´ ë§Œë“¤ì–´ë‚¸ ìš°ì—°\n\ntorch.manual_seed(5) \nx = torch.linspace(0,1,100).reshape(100,1)\ny = torch.randn(100).reshape(100,1)*0.01\nplt.plot(x,y,'--o',alpha=0.5)\n\n\n\n\n\n\n\n\n\ntorch.manual_seed(1)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,512),\n    torch.nn.ReLU(),\n    torch.nn.Linear(512,1)\n)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n#---#\nfor epoc in range(1000):\n    ## step1 \n    yhat = net(x) \n    ## step2 \n    loss = loss_fn(yhat,y)\n    ## step3 \n    loss.backward()\n    ## step4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'--o',alpha=0.5)\nplt.plot(x,net(x).data,'--')\n\n\n\n\n\n\n\n\nğŸ—£ï¸ 0ìœ¼ë¡œ ì í•©í•˜ì§€ ì•Šì€ ê²ƒì€ ë‹¤ í‹€ë¦¼"
  },
  {
    "objectID": "posts/05wk-1.html#c.-ì˜¤ë²„í”¼íŒ…ì´ë¼ëŠ”-ëšœë ·í•œ-ì¦ê±°-train-test",
    "href": "posts/05wk-1.html#c.-ì˜¤ë²„í”¼íŒ…ì´ë¼ëŠ”-ëšœë ·í•œ-ì¦ê±°-train-test",
    "title": "05wk-1: (ì‹ ê²½ë§) â€“ ì˜ˆì¸¡, ì‹œë²¤ì½”ì •ë¦¬ì˜ ì´ë©´, ë“œëì•„ì›ƒ",
    "section": "C. ì˜¤ë²„í”¼íŒ…ì´ë¼ëŠ” ëšœë ·í•œ ì¦ê±°! (train / test)",
    "text": "C. ì˜¤ë²„í”¼íŒ…ì´ë¼ëŠ” ëšœë ·í•œ ì¦ê±°! (train / test)\nğŸ—£ï¸ 0ë³´ë‹¤ ì£¼í™©ìƒ‰ ì„ ì´ ë” ì¢‹ì€ ê²ƒ ê°™ë‹¤ëŠ” ì£¼ì¥ì— ëŒ€í•œ ë°˜ë°• (ì˜ˆì¸¡)\n- ë°ì´í„°ì˜ ë¶„ë¦¬í•˜ì—¬ ë³´ì.\n\ntorch.manual_seed(5) \nx_all = torch.linspace(0,1,100).reshape(100,1)\ny_all = torch.randn(100).reshape(100,1)*0.01\nx,xx = x_all[:80], x_all[80:]\ny,yy = y_all[:80], y_all[80:]\nplt.plot(x,y,'--o',alpha=0.5,label=\"training\")\nplt.plot(xx,yy,'--o',alpha=0.5,label=\"test\")\nplt.legend()\n\n\n\n\n\n\n\n\n- trainë§Œ í•™ìŠµ\nğŸ—£ï¸ Bì™€ ë˜‘ê°™ì€ ì¡°ê±´\n\ntorch.manual_seed(1)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,512),\n    torch.nn.ReLU(),\n    torch.nn.Linear(512,1)\n)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n#---#\nfor epoc in range(1000):\n    ## step1 \n    yhat = net(x) \n    ## step2 \n    loss = loss_fn(yhat,y)\n    ## step3 \n    loss.backward()\n    ## step4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n- training dataë¡œ í•™ìŠµí•œ netë¥¼ training data ì— ì ìš©\n\nplt.plot(x_all,y_all,'--o',alpha=0.5,color=\"gray\")\nplt.plot(x,net(x).data,'--')\n\n\n\n\n\n\n\n\n\ntrainingì—ì„œëŠ” ê·¸ëŸ­ì €ëŸ­ ì˜ ë§ì¶¤\n\n- training dataë¡œ í•™ìŠµí•œ netë¥¼ test data ì— ì ìš©\n\nplt.plot(x_all,y_all,'--o',alpha=0.5,color=\"gray\")\nplt.plot(x,net(x).data,'--')\nplt.plot(xx,net(xx).data,'--')\n\n\n\n\n\n\n\n\n\ntrainì—ì„œëŠ” ê·¸ëŸ­ì €ëŸ­ ì˜ ë§ì¶”ëŠ”ë° testì—ì„œëŠ” ì—‰ë§ì´ë‹¤ = overfit\n\nğŸ—£ï¸ randomì´ê¸° ë•Œë¬¸ì— trendëŠ” ì—†ìŒ"
  },
  {
    "objectID": "posts/05wk-1.html#d.-ì‹œë²¤ì½”ì •ë¦¬ì˜-ì˜¬ë°”ë¥¸-ì´í•´",
    "href": "posts/05wk-1.html#d.-ì‹œë²¤ì½”ì •ë¦¬ì˜-ì˜¬ë°”ë¥¸-ì´í•´",
    "title": "05wk-1: (ì‹ ê²½ë§) â€“ ì˜ˆì¸¡, ì‹œë²¤ì½”ì •ë¦¬ì˜ ì´ë©´, ë“œëì•„ì›ƒ",
    "section": "D. ì‹œë²¤ì½”ì •ë¦¬ì˜ ì˜¬ë°”ë¥¸ ì´í•´",
    "text": "D. ì‹œë²¤ì½”ì •ë¦¬ì˜ ì˜¬ë°”ë¥¸ ì´í•´\n\n\n\n\n\n\nNote\n\n\n\nì‹œë²¤ì½”ì˜ í•­ë³€(?) [@cybenko1989approximation]\ní•˜ë‚˜ì˜ ì€ë‹‰ì¸µì„ ê°€ì§€ëŠ” ì•„ë˜ì™€ ê°™ì€ ê¼´ì˜ ë„¤íŠ¸ì›Œí¬ \\(net: {\\bf X}_{n \\times p} \\to {\\bf y}_{n\\times q}\\)ëŠ”\nnet = torch.nn.Sequential(\n    torch.nn.Linear(p,???),\n    torch.nn.Sigmoid(), ## &lt;-- ì—¬ê¸°ì— ë ë£¨ë¥¼ ì¨ë„ ëœë‹¤. \n    torch.nn.Linear(???,q)\n)\nëª¨ë“  ë³´ë ê°€ì¸¡í•¨ìˆ˜\n\\[f: {\\bf X}_{n \\times p} \\to {\\bf y}_{n\\times q}\\]\në¥¼ ì›í•˜ëŠ” ì •í™•ë„ë¡œ â€œê·¼ì‚¬â€ì‹œí‚¬ ìˆ˜ ìˆë‹¤. ì‰½ê²Œ ë§í•˜ë©´ \\({\\bf X} \\to {\\bf y}\\) ì¸ ì–´ë– í•œ ë³µì¡í•œ ê·œì¹™ë¼ë„ í•˜ë‚˜ì˜ ì€ë‹‰ì¸µì„ ê°€ì§„ ì‹ ê²½ë§ì´ ì›í•˜ëŠ” ì •í™•ë„ë¡œ ê·¼ì‚¬ì‹œí‚¨ë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤. ê·¸ë ‡ì§€ë§Œ ì´ëŸ¬í•œ ê·œì¹™ì´ ë„¤í¬ì›Œí¬ê°€ í•™ìŠµí•˜ì§€ ëª»í–ˆë˜ ìë£Œ (ì²˜ìŒ ë³´ëŠ” ìë£Œ, unseen data) \\({\\bf XX}_{m \\times p}\\), \\({\\bf yy}_{m \\times q}\\) ì— ëŒ€í•˜ì—¬ì„œë„ ì˜¬ë°”ë¥´ê²Œ ì ìš©ëœë‹¤ë¼ëŠ” ë³´ì¥ì€ ì—†ë‹¤. ì‹œë²¤ì½”ëŠ” ë‹¨ì§€ netê°€ ê°€ì§€ëŠ” í‘œí˜„ë ¥ì˜ í•œê³„ë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ ë°í˜”ì„ ë¿ì´ë‹¤."
  },
  {
    "objectID": "posts/05wk-1.html#a.-ì˜¤ë²„í”¼íŒ…ì˜-í•´ê²°",
    "href": "posts/05wk-1.html#a.-ì˜¤ë²„í”¼íŒ…ì˜-í•´ê²°",
    "title": "05wk-1: (ì‹ ê²½ë§) â€“ ì˜ˆì¸¡, ì‹œë²¤ì½”ì •ë¦¬ì˜ ì´ë©´, ë“œëì•„ì›ƒ",
    "section": "A. ì˜¤ë²„í”¼íŒ…ì˜ í•´ê²°",
    "text": "A. ì˜¤ë²„í”¼íŒ…ì˜ í•´ê²°\n- ì˜¤ë²„í”¼íŒ…ì˜ í•´ê²°ì±…: ë“œëì•„ì›ƒ\n- ë°ì´í„°\n\ntorch.manual_seed(5) \nx_all = torch.linspace(0,1,100).reshape(100,1)\ny_all = torch.randn(100).reshape(100,1)*0.01\n#plt.plot(x_all,y_all,'--o',alpha=0.5)\nx,y = x_all[:80], y_all[:80]\nxx,yy = x_all[80:], y_all[80:]\nplt.plot(x,y,'--o',color=\"C0\")\nplt.plot(xx,yy,'--o',color=\"C1\")\n\n\n\n\n\n\n\n\n- í•™ìŠµ\nğŸ—£ï¸ torch.nn.Dropout(0.8) ì¶”ê°€ (í•™ìŠµ ì‹œ ì¼ë¶€ë§Œ ì‚¬ìš©)\n\ntorch.manual_seed(1)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,512),\n    torch.nn.ReLU(),\n    torch.nn.Dropout(0.8),\n    torch.nn.Linear(512,1)\n)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n#---#\nfor epoc in range(1000):\n    ## step1 \n    yhat = net(x) \n    ## step2 \n    loss = loss_fn(yhat,y)\n    ## step3 \n    loss.backward()\n    ## step4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n- ê²°ê³¼ì‹œê°í™” (ì˜ëª»ëœ ì‚¬ìš©)\n\nplt.plot(x_all,y_all,'--o',alpha=0.5,color=\"gray\")\nplt.plot(x,net(x).data,'--')\nplt.plot(xx,net(xx).data,'--')\n\n\n\n\n\n\n\n\n- ê²°ê³¼ì‹œê°í™” (ì˜¬ë°”ë¥¸ ì‚¬ìš©)\n\nnet.training \n\nTrue\n\n\nğŸ—£ï¸ í‰ê°€ ëª¨ë“œë¡œ ë°”ê¾¸ê¸°\n\nnet.eval()\n\nSequential(\n  (0): Linear(in_features=1, out_features=512, bias=True)\n  (1): ReLU()\n  (2): Dropout(p=0.8, inplace=False)\n  (3): Linear(in_features=512, out_features=1, bias=True)\n)\n\n\n\nnet.training\n\nFalse\n\n\n\nplt.plot(x_all,y_all,'--o',alpha=0.5,color=\"gray\")\nplt.plot(x,net(x).data,'--')\nplt.plot(xx,net(xx).data,'--')\n\n\n\n\n\n\n\n\nğŸ—£ï¸ ì–´ëŠ ì •ë„ 0ìœ¼ë¡œ ë–¨ì–´ì§, ì˜¤ë²„í”¼íŒ… ë¬¸ì œê°€ ì™„ì „íˆ í•´ê²°ë˜ì§€ëŠ” ì•Šì•˜ì§€ë§Œ ì–´ëŠì •ë„ ì™„í™”"
  },
  {
    "objectID": "posts/05wk-1.html#b.-ë“œëì•„ì›ƒ-ë ˆì´ì–´",
    "href": "posts/05wk-1.html#b.-ë“œëì•„ì›ƒ-ë ˆì´ì–´",
    "title": "05wk-1: (ì‹ ê²½ë§) â€“ ì˜ˆì¸¡, ì‹œë²¤ì½”ì •ë¦¬ì˜ ì´ë©´, ë“œëì•„ì›ƒ",
    "section": "B. ë“œëì•„ì›ƒ ë ˆì´ì–´",
    "text": "B. ë“œëì•„ì›ƒ ë ˆì´ì–´\n- ë“œëì•„ì›ƒì˜ ì„±ì§ˆ1: ë“œëì•„ì›ƒì˜ ê³„ì‚°ë°©ì‹ì„ ì´í•´í•´ë³´ì.\nğŸ—£ï¸ default= 0.5\n\nu = torch.randn(10,2)\nd = torch.nn.Dropout(0.9)\nu\n\ntensor([[ 0.5951,  0.2245],\n        [ 0.8238,  0.5230],\n        [ 0.4772, -1.0465],\n        [-0.6826,  0.4257],\n        [ 0.5113,  0.4130],\n        [-0.3946,  0.0827],\n        [ 1.4149, -1.7569],\n        [ 0.3142, -0.9964],\n        [-0.4613,  0.3530],\n        [-0.2743, -0.5558]])\n\n\n\nd(u)\n\ntensor([[0.0000, 0.0000],\n        [0.0000, 0.0000],\n        [0.0000, -0.0000],\n        [-0.0000, 0.0000],\n        [5.1128, 4.1303],\n        [-0.0000, 0.0000],\n        [0.0000, -0.0000],\n        [0.0000, -0.0000],\n        [-0.0000, 3.5305],\n        [-0.0000, -0.0000]])\n\n\n\n90%ì˜ ë“œëì•„ì›ƒ: ë“œëì•„ì›ƒì¸µì˜ ì…ë ¥ ì¤‘ ì„ì˜ë¡œ 90%ë¥¼ ê³¨ë¼ì„œ ê²°ê³¼ë¥¼ 0ìœ¼ë¡œ ë§Œë“ ë‹¤. + ê·¸ë¦¬ê³  0ì´ ë˜ì§€ì•Šê³  ì‚´ì•„ë‚¨ì€ ê°’ë“¤ì€ 10ë°° ë§Œí¼ ê°’ì´ ì»¤ì§„ë‹¤.\në‚¨ì€ê°’ì„ 10ë°° í‚¤ìš°ëŠ” ì´ìœ ? ì¶œë ¥ì˜ í‰ê· ê°’ì„ ë³´ì •í•˜ê¸° ìœ„í•´ì„œ\n\n- ë“œëì•„ì›ƒì˜ ì„±ì§ˆ2: ë“œëì•„ì›ƒì„ on/off í•˜ëŠ” ë°©ë²•ì„ ì´í•´í•´ë³´ì.\n\nu = torch.randn(10,2)\nu\n\ntensor([[ 0.8395,  1.8825],\n        [-0.0415, -2.3987],\n        [-0.3658, -1.3403],\n        [-1.4066,  0.7178],\n        [-1.0465,  0.9663],\n        [-1.2350,  1.3424],\n        [-1.1903,  0.3955],\n        [ 0.4236, -0.7882],\n        [-0.4348,  0.2669],\n        [-0.9102, -0.3219]])\n\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Dropout(0.9)\n)\nnet\n\nSequential(\n  (0): Dropout(p=0.9, inplace=False)\n)\n\n\n\nu,net(u)\n\n(tensor([[ 0.8395,  1.8825],\n         [-0.0415, -2.3987],\n         [-0.3658, -1.3403],\n         [-1.4066,  0.7178],\n         [-1.0465,  0.9663],\n         [-1.2350,  1.3424],\n         [-1.1903,  0.3955],\n         [ 0.4236, -0.7882],\n         [-0.4348,  0.2669],\n         [-0.9102, -0.3219]]),\n tensor([[  0.0000,   0.0000],\n         [ -0.0000,  -0.0000],\n         [ -0.0000,  -0.0000],\n         [-14.0662,   0.0000],\n         [ -0.0000,   0.0000],\n         [-12.3497,   0.0000],\n         [ -0.0000,   0.0000],\n         [  4.2361,  -0.0000],\n         [ -0.0000,   0.0000],\n         [ -0.0000,  -3.2190]]))\n\n\nğŸ—£ï¸ ë§¤ë²ˆ ì–´ë–¤ ë…¸ë“œê°€ ì£½ì„ì§€ ëª¨ë¦„ (ë‹¤ 0ì´ ë˜ê¸°ë„ í•¨)\nâ“ ë¹„ìœ¨? í™•ë¥ ?\n\nnet.training\n\nTrue\n\n\n\nnet.eval() # ë“œëì•„ì›ƒì´ ë¬´ë ¥í™”\n\nSequential(\n  (0): Dropout(p=0.9, inplace=False)\n)\n\n\n\nu,net(u)\n\n(tensor([[ 0.8395,  1.8825],\n         [-0.0415, -2.3987],\n         [-0.3658, -1.3403],\n         [-1.4066,  0.7178],\n         [-1.0465,  0.9663],\n         [-1.2350,  1.3424],\n         [-1.1903,  0.3955],\n         [ 0.4236, -0.7882],\n         [-0.4348,  0.2669],\n         [-0.9102, -0.3219]]),\n tensor([[ 0.8395,  1.8825],\n         [-0.0415, -2.3987],\n         [-0.3658, -1.3403],\n         [-1.4066,  0.7178],\n         [-1.0465,  0.9663],\n         [-1.2350,  1.3424],\n         [-1.1903,  0.3955],\n         [ 0.4236, -0.7882],\n         [-0.4348,  0.2669],\n         [-0.9102, -0.3219]]))\n\n\n- ë“œëì•„ì›ƒë ˆì´ì–´ ì •ë¦¬\n\nê³„ì‚°: (1) ì…ë ¥ì˜ ì¼ë¶€ë¥¼ ì„ì˜ë¡œ 0ìœ¼ë¡œ ë§Œë“œëŠ” ì—­í•  (2) 0ì´ ì•ˆëœê²ƒë“¤ì€ ìŠ¤ì¹¼ë¼ë°°í•˜ì—¬ ë“œëì•„ì›ƒì„ í†µê³¼í•œ ëª¨ë“  ìˆ«ìë“¤ì˜ ì´í•©ì´ ëŒ€ì²´ë¡œ ì¼ì •í•˜ê²Œ ë˜ë„ë¡ ì¡°ì •\non/off: í•™ìŠµì‹œì—ëŠ” dropout on / í•™ìŠµì„ í•˜ì§€ ì•Šì„ ê²½ìš°ëŠ” dropout off\nëŠë‚Œ: ì¼ë¶€ëŸ¬ íŒ¨ë„í‹°ë¥¼ ì•ˆê³  í•™ìŠµí•˜ëŠ” ëŠë‚Œ..\níš¨ê³¼: ì˜¤ë²„í”¼íŒ…ì„ ì–µì œí•˜ëŠ” íš¨ê³¼ê°€ ìˆìŒ\n\nğŸ—£ï¸ ëœë¤ í¬ë ˆìŠ¤íŠ¸ì™€ ë™ì¼\n\nì°¸ê³ : ì˜¤ë²„í”¼íŒ…ì„ ì¡ëŠ” ë°©ë²•ì€ ë“œëì•„ì›ƒë§Œ ìˆëŠ”ê²Œ ì•„ë‹ˆë‹¤..\n\nğŸ—£ï¸ ê·¼ë³¸: ì‹œê°í™” í›„ ë°ì´í„°ì— ë§ì¶˜ ëª¨ë¸ì„ ì°¾ìŒ (ì–´ë ¤ì›€) / ì‹¤ì œ: ì‹œë²¤ì½” ì •ë¦¬ë¡œ ì í•©ì„ í•œ í›„ ë“œëì•„ì›ƒì„ ê±¸ì–´ ì˜¤ë²„í”¼íŒ… ë°©ì§€"
  },
  {
    "objectID": "posts/05wk-1.html#c.-ë“œëì•„ì›ƒ-ë ˆì´ì–´ì˜-ìœ„ì¹˜",
    "href": "posts/05wk-1.html#c.-ë“œëì•„ì›ƒ-ë ˆì´ì–´ì˜-ìœ„ì¹˜",
    "title": "05wk-1: (ì‹ ê²½ë§) â€“ ì˜ˆì¸¡, ì‹œë²¤ì½”ì •ë¦¬ì˜ ì´ë©´, ë“œëì•„ì›ƒ",
    "section": "C. ë“œëì•„ì›ƒ ë ˆì´ì–´ì˜ ìœ„ì¹˜",
    "text": "C. ë“œëì•„ì›ƒ ë ˆì´ì–´ì˜ ìœ„ì¹˜\n- ReLU,dropoutì˜ íŠ¹ì´í•œ ì„±ì§ˆ: \\(\\text{dropout}(\\text{relu}({\\bf x}))=\\text{relu}(\\text{dropout}({\\bf x}))\\)\nğŸ—£ï¸ ë‘˜ ë‹¤ xë¥¼ ê·¸ëŒ€ë¡œ ë‚´ë³´ë‚´ê±°ë‚˜ 0ìœ¼ë¡œ ë§Œë“¦ (ìˆœì„œ ìƒê´€ ì—†ì´ ê²°ê³¼ ë™ì¼)\n\nu = torch.randn(10,2)\nr = torch.nn.ReLU()\nd = torch.nn.Dropout()\n\n\ntorch.manual_seed(0)\nd(r(u))\n\ntensor([[0.0000, 0.0000],\n        [0.0000, 0.0000],\n        [0.0000, 0.0000],\n        [0.0000, 0.5372],\n        [2.6658, 2.1870],\n        [0.3798, 0.0000],\n        [0.0000, 1.6593],\n        [0.9300, 0.0000],\n        [0.0000, 0.0000],\n        [0.0000, 0.0000]])\n\n\n\ntorch.manual_seed(0)\nr(d(u))\n\ntensor([[0.0000, 0.0000],\n        [-0.0000, 0.0000],\n        [0.0000, 0.0000],\n        [0.0000, 0.5372],\n        [2.6658, 2.1870],\n        [0.3798, -0.0000],\n        [0.0000, 1.6593],\n        [0.9300, 0.0000],\n        [0.0000, 0.0000],\n        [-0.0000, 0.0000]])\n\n\n- ë‹¤ë¥¸ í™œì„±í™”í•¨ìˆ˜ëŠ” ì„±ë¦½ì•ˆí•¨\nğŸ—£ï¸ í™œì„±í™” í•¨ìˆ˜: ë¹„ì„ í˜• í•¨ìˆ˜, activation í•¨ìˆ˜\n\nu = torch.randn(10,2)\ns = torch.nn.Sigmoid()\nd = torch.nn.Dropout()\n\n\ntorch.manual_seed(0)\nd(s(u))\n\ntensor([[0.4801, 0.0000],\n        [0.0000, 1.4006],\n        [0.3487, 0.0000],\n        [0.0000, 1.2299],\n        [0.9213, 1.6180],\n        [1.1322, 0.0000],\n        [0.0000, 1.4407],\n        [0.6015, 1.4349],\n        [0.0000, 1.7626],\n        [0.0000, 0.0000]])\n\n\n\ntorch.manual_seed(0)\ns(d(u))\n\ntensor([[0.0907, 0.5000],\n        [0.5000, 0.8452],\n        [0.0427, 0.5000],\n        [0.5000, 0.7183],\n        [0.4218, 0.9472],\n        [0.6300, 0.5000],\n        [0.5000, 0.8691],\n        [0.1561, 0.8657],\n        [0.5000, 0.9822],\n        [0.5000, 0.5000]])\n\n\n- ê²°ë¡ : ë“œëì•„ì›ƒì€ í™œì„±í™” í•¨ìˆ˜ ë°”ë¡œ ë’¤ì— ì˜¤ëŠ”ê²Œ ë§ìŒ. (ê·¸ë ‡ì§€ ì•Šë‹¤ë©´ 0ì„ ë§Œë“¤ ìˆ˜ ì—†ëŠ”ê±¸?) ê·¸ë ‡ì§€ë§Œ ReLUì˜ ê²½ìš° í™œì„±í™” í•¨ìˆ˜ ì§ì „ì— ì·¨í•˜ê¸°ë„ í•¨.\nğŸ—£ï¸ ReLUëŠ” ìˆœì„œë¥¼ ë°”ê¾¸ëŠ” ê²ƒì´ ê³„ì‚° ìƒì— íš¨ìœ¨ì´ ìˆë‹¤ê³  í•¨"
  },
  {
    "objectID": "posts/05wk-1.html#d.-í‰ê· ë³´ì •ì˜-í•„ìš”ì„±-ì„ íƒí•™ìŠµ",
    "href": "posts/05wk-1.html#d.-í‰ê· ë³´ì •ì˜-í•„ìš”ì„±-ì„ íƒí•™ìŠµ",
    "title": "05wk-1: (ì‹ ê²½ë§) â€“ ì˜ˆì¸¡, ì‹œë²¤ì½”ì •ë¦¬ì˜ ì´ë©´, ë“œëì•„ì›ƒ",
    "section": "D. í‰ê· ë³´ì •ì˜ í•„ìš”ì„± (ì„ íƒí•™ìŠµ)",
    "text": "D. í‰ê· ë³´ì •ì˜ í•„ìš”ì„± (ì„ íƒí•™ìŠµ)\n\n\n\n\n\n\nNote\n\n\n\n90%ì˜ ë“œëì•„ì›ƒì—ì„œ ì¶œë ¥ê²°ê³¼ì— ì™œ x10í•˜ëŠ”ì§€ ì¢€ ë” ìì„¸íˆ ì„¤ëª…í•œ ì±•í„°ì…ë‹ˆë‹¤. ê¶ê¸ˆí•˜ì‹œë‹¤ë©´ ì½ì–´ë³´ì‹œê³  ì•„ë‹ˆë¼ë©´ ë„˜ì–´ê°€ì…”ë„ ë¬´ë°©í•©ë‹ˆë‹¤.\n\n\n- ì•„ë˜ì˜ ë°ì´í„°ë¥¼ ê´€ì°°í•˜ì.\n\nx,_ = torch.randn(300).sort()\ny = relu(20*x) + torch.randn(300)\nx,y = x.reshape(-1,1), y.reshape(-1,1)\n\n\nplt.plot(x,y,'o')\n\n\n\n\n\n\n\n\n- ì í•©í•´ë³´ì.\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1000),\n    torch.nn.ReLU(),\n    torch.nn.Dropout(0.1),\n    torch.nn.Linear(1000,1,bias=False),\n)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n#---#\nfor epoc in range(5000):\n    ## 1 \n    yhat = net(x)\n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nnet.eval()\n\nSequential(\n  (0): Linear(in_features=1, out_features=1000, bias=True)\n  (1): ReLU()\n  (2): Dropout(p=0.1, inplace=False)\n  (3): Linear(in_features=1000, out_features=1, bias=False)\n)\n\n\n\nnet.training\n\nFalse\n\n\n\nplt.plot(x,y,'o')\nplt.plot(x,net(x).data,'--')\n\n\n\n\n\n\n\n\n- ì£¼í™©ìƒ‰ì„ ì´ë‚˜ì˜¤ëŠ” ì´ìœ  ì„¤ëª…í•´ë³´ì.\n\nU = net[:-1](x).data \nW = net[-1].weight.T \n\nì•„ë˜3ê°œëŠ” ë™ì¼í•œì½”ë“œì„\n\nnet(x).reshape(-1)[:10] # ì½”ë“œ1\n\ntensor([-0.9858, -0.5127, -0.4687,  0.0514,  0.0558,  0.2089,  0.2213,  0.2619,\n         0.2691,  0.2823], grad_fn=&lt;SliceBackward0&gt;)\n\n\n\n(U@W).reshape(-1)[:10] # ì½”ë“œ2\n\ntensor([-0.9858, -0.5127, -0.4687,  0.0514,  0.0558,  0.2089,  0.2213,  0.2619,\n         0.2691,  0.2823], grad_fn=&lt;SliceBackward0&gt;)\n\n\n\n((U*W.reshape(-1)).sum(axis=1))[:10] # ì½”ë“œ3\n\ntensor([-0.9858, -0.5127, -0.4687,  0.0514,  0.0558,  0.2089,  0.2213,  0.2619,\n         0.2691,  0.2823], grad_fn=&lt;SliceBackward0&gt;)\n\n\në”°ë¼ì„œ ì•„ë˜ì˜ ì£¼í™©ìƒ‰ì„ ë“¤ì˜ .sum(axis=1) í•˜ê¸°ë§Œ í•˜ë©´ net(x)ì˜ ê²°ê³¼ê°€ ëœë‹¤.\n\nplt.plot(x,U*W.reshape(-1).data,color=\"C1\",alpha=0.02);\n\n\n\n\n\n\n\n\n- ì¦‰ ì™¼ìª½ì˜ ì£¼í™©ìƒ‰ì„ 1ì´ ëª¨ë‘ í•©ì³ì ¸ì„œ ì˜¤ë¥¸ìª½ì˜ ì ì„ ì´ëœë‹¤.\n\nfig,ax = plt.subplots(1,2,figsize=(9,3))\nax[0].plot(x,U*W.reshape(-1).data,color=\"C1\",alpha=0.02);\nax[0].set_title(\"1,000 ReLUs\")\nax[1].plot(x,net(x).data,'--',color=\"C1\")\nax[1].set_title(r\"$net({\\bf x})$=sum(1,000 ReLUs)\");\n\n\n\n\n\n\n\n\n\në§Œì•½ì— ì™¼ìª½ì˜ ì£¼í™©ìƒ‰ì„ ì´ 10%ë§Œ ì‚¬ìš©ë˜ì–´ì„œ 100ê°œì˜ ë ë£¨ë§Œ ì‚¬ìš©ë˜ì—ˆë‹¤ë©´? ëŒ€ì¶© x10ì„ í•´ì¤˜ì•¼ net(x) ê°€ ë‚˜ì˜¤ì§€ ì•Šê² ì–´ìš”?"
  },
  {
    "objectID": "posts/05wk-1.html#footnotes",
    "href": "posts/05wk-1.html#footnotes",
    "title": "05wk-1: (ì‹ ê²½ë§) â€“ ì˜ˆì¸¡, ì‹œë²¤ì½”ì •ë¦¬ì˜ ì´ë©´, ë“œëì•„ì›ƒ",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n1000ê°œê°€ ìˆìŒâ†©ï¸"
  },
  {
    "objectID": "posts/03wk-2.html",
    "href": "posts/03wk-2.html",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "",
    "text": "ğŸ“˜ Note Format Guide\nThis format serves as a structured guide for organizing lecture content, personal interpretation, experiments, and study-related questions.\nğŸ“ ğŸ—£ï¸ âœï¸ ğŸ”¬ â“"
  },
  {
    "objectID": "posts/03wk-2.html#a.-ë¡œì§€ìŠ¤í‹±-ëª¨í˜•",
    "href": "posts/03wk-2.html#a.-ë¡œì§€ìŠ¤í‹±-ëª¨í˜•",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "A. ë¡œì§€ìŠ¤í‹± ëª¨í˜•",
    "text": "A. ë¡œì§€ìŠ¤í‹± ëª¨í˜•\n- \\(x\\)ê°€ ì»¤ì§ˆìˆ˜ë¡ (í˜¹ì€ ì‘ì•„ì§ˆìˆ˜ë¡) \\(y=1\\)ì´ ì˜ë‚˜ì˜¤ëŠ” ëª¨í˜•ì€ ì•„ë˜ì™€ ê°™ì´ ì„¤ê³„í•  ìˆ˜ ìˆìŒ &lt;â€” ì™¸ìš°ì„¸ìš”!!!\n\n\\(y_i \\sim {\\cal B}(\\pi_i),\\quad\\) where \\(\\pi_i = \\frac{\\exp(w_0+w_1x_i)}{1+\\exp(w_0+w_1x_i)} = \\frac{1}{1+\\exp(-w_0-w_1x_i)}\\)\n\\(\\hat{y}_i= \\frac{\\exp(\\hat{w}_0+\\hat{w}_1x_i)}{1+\\exp(\\hat{w}_0+\\hat{w}_1x_i)}=\\frac{1}{1+\\exp(-\\hat{w}_0-\\hat{w}_1x_i)}\\)\n\n- íšŒê·€ëª¨í˜•ê³¼ ë¡œì§€ìŠ¤í‹± ëª¨í˜•ì˜ ë¹„êµ\n\níšŒê·€ëª¨í˜•: \\(y_i \\sim {\\cal N}(w_0+w_1x_i, \\sigma^2)\\)1\në¡œì§€ìŠ¤í‹±: \\(y_i \\sim {\\cal B}\\big(\\frac{\\exp(w_0+w_1x_i)}{1+\\exp(w_0+w_1x_i)}\\big)\\)\n\n- ìš°ë¦¬ê°€ ì˜ˆì¸¡í•˜ê³  ì‹¶ì€ê²ƒ\n\níšŒê·€ëª¨í˜•: ì •ê·œë¶„í¬ì˜ í‰ê· ì„ ì˜ˆì¸¡í•˜ê³  ì‹¶ìŒ. ì¦‰ \\(w_0+w_1x_i\\)ë¥¼ ì˜ˆì¸¡í•˜ê³  ì‹¶ìŒ. ì˜ˆì¸¡ê°’ìœ¼ë¡œëŠ” \\(\\hat{w}_0 + \\hat{w}_1x_i\\)ë¥¼ ì‚¬ìš©!\në¡œì§€ìŠ¤í‹±: ë² ë¥´ëˆ„ì´ì˜ í‰ê· ì„ ì˜ˆì¸¡í•˜ê³  ì‹¶ìŒ. ì¦‰ \\(\\frac{\\exp(w_0+w_1x_i)}{1+\\exp(w_0+w_1x_i)}\\)ë¥¼ ì˜ˆì¸¡í•˜ê³  ì‹¶ìŒ. ì˜ˆì¸¡ê°’ìœ¼ë¡œëŠ” \\(\\frac{\\exp(\\hat{w}_0+\\hat{w}_1x_i)}{1+\\exp(\\hat{w}_0+\\hat{w}_1x_i)}\\)ë¥¼ ì‚¬ìš©!"
  },
  {
    "objectID": "posts/03wk-2.html#b.-ë°ì´í„°-ìŠ¤í™ê³¼-ì·¨ì—…",
    "href": "posts/03wk-2.html#b.-ë°ì´í„°-ìŠ¤í™ê³¼-ì·¨ì—…",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "B. ë°ì´í„° â€“ ìŠ¤í™ê³¼ ì·¨ì—…",
    "text": "B. ë°ì´í„° â€“ ìŠ¤í™ê³¼ ì·¨ì—…\nğŸ—£ï¸(\n\në°ì´í„° ë§Œë“¤ê¸°\n\n\ntorch.linspace(-1,1,2000)\n\ntensor([-1.0000, -0.9990, -0.9980,  ...,  0.9980,  0.9990,  1.0000])\n\n\n\nlen(torch.linspace(-1,1,2000))\n\n2000\n\n\n\nx = torch.linspace(-1,1,2000).reshape(2000,1)\nx\n\ntensor([[-1.0000],\n        [-0.9990],\n        [-0.9980],\n        ...,\n        [ 0.9980],\n        [ 0.9990],\n        [ 1.0000]])\n\n\n\nì´ ìƒíƒœì—ì„œ ì„ í˜• ë³€í™˜ì„ í•œë‹¤ë©´\n\n-1 + x*5 : ì„ í˜• ëª¨ë¸\në¡œì§€ìŠ¤í‹± ëª¨í˜•ì€\n\n\n\nx = torch.linspace(-1,1,2000).reshape(2000,1)\nprob = torch.exp(-1 + x*5) / (1+ torch.exp(-1 + x*5))\nplt.plot(x,prob)\n\n\n\n\n\n\n\n\n\në‹¤ë¥¸ ë°©ë²• (ë³´ê¸° ì¢‹ê²Œ)\n\n\nx = torch.linspace(-1,1,2000).reshape(2000,1)\nw0, w1 = -1, 5\nprob = torch.exp(w0 + x*w1) / (1+ torch.exp(w0 + x*w1))\nplt.plot(x,prob)\n\n\n\n\n\n\n\n\n\nprob\n\ntensor([[0.0025],\n        [0.0025],\n        [0.0025],\n        ...,\n        [0.9818],\n        [0.9819],\n        [0.9820]])\n\n\n\nprob.shape\n\ntorch.Size([2000, 1])\n\n\n\ny ë§Œë“¤ê¸° (probë¡œ ë² ë¥´ëˆ„ì´ ì‹œí–‰)\n\n\ntorch.bernoulli(prob)\n\ntensor([[0.],\n        [0.],\n        [0.],\n        ...,\n        [1.],\n        [1.],\n        [1.]])\n\n\n\ntorch.bernoulli(prob).shape\n\ntorch.Size([2000, 1])\n\n\n\nseed ê³ ì • í›„ ì‹œê°í™”\n\n\ntorch.manual_seed(43052)\nx = torch.linspace(-1,1,2000).reshape(2000,1)\nw0, w1 = -1, 5\nprob = torch.exp(w0 + x*w1) / (1+ torch.exp(w0 + x*w1))\ny = torch.bernoulli(prob)\n\n\nplt.plot(x,y) # ë³´ê¸° ì‰½ì§€ëŠ” ì•ŠìŒ \n\n\n\n\n\n\n\n\n\nplt.plot(x,y, 'o') # ì ë“¤ì´ ë„ˆë¬´ ë§ì´ ê²¹ì¹¨\n\n\n\n\n\n\n\n\n\nplt.plot(x,y,'.',alpha=0.03) # íˆ¬ëª…ë„ ì¡°ì ˆ\n\n\n\n\n\n\n\n\n\nxê°€ ì¦ê°€í• ìˆ˜ë¡ yëŠ” 1ì´ ë‚˜ì˜¬ ê°€ëŠ¥ì„±ì´ ë†’ì•„ì§€ê³ \nxê°€ ê°ì†Œí• ìˆ˜ë¡ yëŠ” 0ì´ ë‚˜ì˜¬ ê°€ëŠ¥ì„±ì´ ë†’ì•„ì§\n\n\nplt.plot(x,y,'.',alpha=0.03) # ê´€ì¸¡(error í¬í•¨)\nplt.plot(x,prob,'--') # ì‹¤ì²´ ë°ì´í„°ì—ì„œëŠ” ê´€ì¸¡ ë¶ˆê°€ëŠ¥ (error-free structure)\n\n\n\n\n\n\n\n\n)ğŸ—£ï¸\n\ntorch.manual_seed(43052)\nx = torch.linspace(-1,1,2000).reshape(2000,1)\nw0,w1 = -1, 5\nprob = torch.exp(w0+w1*x) / (1+torch.exp(w0+w1*x)) \ny = torch.bernoulli(prob)\n\n\nplt.plot(x,y,'.',alpha=0.03)\nplt.plot(x[0],y[0],'.',label=r\"$(x_i,y_i)$\",color=\"C0\")\nplt.plot(x,prob,'--r',label=r\"prob (true, unknown) = $\\frac{exp(-1+5x)}{1+exp(-1+5x)}$\")\nplt.legend()\n\n\n\n\n\n\n\n\n\nğŸ—£ï¸\n\nprob: í™•ë¥ \níŒŒë€ìƒ‰ ì : ê´€ì¸¡ê°’\nëª©í‘œ: ë¹¨ê°„ìƒ‰ ì„  ì˜ ë§ì¶”ê¸°\n\në°©ë²•: ìµœì´ˆì˜ ê³¡ì„ ì„ ê·¸ë¦¬ê³  update"
  },
  {
    "objectID": "posts/03wk-2.html#c.-step1-net-ì„¤ê³„-ëª¨ë¸ë§",
    "href": "posts/03wk-2.html#c.-step1-net-ì„¤ê³„-ëª¨ë¸ë§",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "C. Step1: net ì„¤ê³„ (ëª¨ë¸ë§)",
    "text": "C. Step1: net ì„¤ê³„ (ëª¨ë¸ë§)\n- ìµœì´ˆì˜ ê³¡ì„ ì„ ê·¸ë ¤ë³´ì.\n\nìµœì´ˆì˜ì§ì„ : \\(\\hat{y}_i= \\hat{w}_0+\\hat{w}_1x_i\\) ì—ì„œ ì•„ë¬´ \\(\\hat{w}_0\\), \\(\\hat{w}_1\\) ì„ ì„¤ì •í•˜ë©´ ëœë‹¤.\nìµœì´ˆì˜ê³¡ì„ : \\(\\hat{y}_i= \\frac{\\exp(\\hat{w}_0+\\hat{w}_1x_i)}{1+\\exp(\\hat{w}_0+\\hat{w}_1x_i)}=\\frac{1}{1+\\exp(-\\hat{w}_0-\\hat{w}_1x_i)}\\) ì—ì„œ ì•„ë¬´ \\(\\hat{w}_0\\), \\(\\hat{w}_1\\) ì„ ì„¤ì •í•˜ë©´ ëœë‹¤.\n\n\n\n\n\n\n\nNote\n\n\n\nì¼ë‹¨ì€ ì´ˆê¸° ì„¤ì •ê°’ì„ \\(\\hat{w}_0 = -0.8\\), \\(\\hat{w}_1 = -0.3\\) ìœ¼ë¡œ í•˜ì. (ì‹¤ì œê°’ì€ \\(w_0=-1\\), \\(w_1=5\\) ì´ë‹¤)\n\n\n# ë°©ë²•1 â€“ l1, sigmoid\nğŸ—£ï¸(\n\nw0hat = -4\nw1hat = 10\nyhat = torch.exp(w0hat + w1hat*x) / (1+ torch.exp(w0hat + w1hat*x))\n\n\nyhat\n\ntensor([[8.3153e-07],\n        [8.3989e-07],\n        [8.4833e-07],\n        ...,\n        [9.9748e-01],\n        [9.9750e-01],\n        [9.9753e-01]])\n\n\n\nplt.plot(x,y,'.',alpha=0.03)\nplt.plot(x, yhat, '--')\n\n\n\n\n\n\n\n\n\nyhatì„ ë‹¤ìŒê³¼ ê°™ì´ í•  ìˆ˜ë„ ìˆìŒ\n\n\nlinr = torch.nn.Linear(1,1)\n# linr(x)\n\n\ndef sigmoid(x):\n    return torch.exp(x) / (1+ torch.exp(x)) #  í¸ì˜ìƒ linr(x) ëŒ€ì‹  xë¡œ ì‘ì„±\n\n\nlinr(x)\n\ntensor([[ 0.6311],\n        [ 0.6304],\n        [ 0.6297],\n        ...,\n        [-0.6902],\n        [-0.6909],\n        [-0.6916]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\n\nyhat = sigmoid(linr(x))\nplt.plot(x, yhat.data) # ê³¡ì„  ì¤‘ ì¼ë¶€ë§Œ ê·¸ë ¤ì ¸ ì§ì„ ì²˜ëŸ¼ ë³´ì„\n\n\n\n\n\n\n\n\n\nlinr(x)ê°€ ê³„ì‚°ë˜ëŠ” ê³¼ì •\n\n\nlinr.weight, linr.bias\n\n(Parameter containing:\n tensor([[-0.6613]], requires_grad=True),\n Parameter containing:\n tensor([-0.0303], requires_grad=True))\n\n\n\n-0.6613*x + -0.0303\n\ntensor([[ 0.6310],\n        [ 0.6303],\n        [ 0.6297],\n        ...,\n        [-0.6903],\n        [-0.6909],\n        [-0.6916]])\n\n\n\nê°’ì„ ì•„ê¹Œì²˜ëŸ¼ ì§€ì •í•´ì£¼ë©´\n\n\nlinr.weight.data = torch.tensor([[10.0]])\nlinr.bias.data = torch.tensor([-4.0])\n\nâ“ biasëŠ” [[-4.0]]ì´ ì•„ë‹ˆë¼ [-4.0]\nğŸ”¬(\n\nlinr.weight.data = torch.tensor([[10.0]])\nlinr.bias.data = torch.tensor([[-4.0]])\n\n\nlinr(x)\n\ntensor([[-14.0000],\n        [-13.9900],\n        [-13.9800],\n        ...,\n        [  5.9800],\n        [  5.9900],\n        [  6.0000]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\nâ“ ìƒê´€ì—†ëŠ”ë“¯?\n\nlinr.weight.data = torch.tensor([10.0])\nlinr.bias.data = torch.tensor([[-4.0]])\n\n\n# linr(x) # error: RuntimeError: mat2 must be a matrix, got 1-D tensor\n\n)ğŸ”¬\nğŸ”¬ ì°¸ê³ ) -4.0ì´ ì•„ë‹ˆë¼ -4ë¥¼ ì“°ë©´ error\n\nlinr.weight.data = torch.tensor([[10.0]])\nlinr.bias.data = torch.tensor([-4.0])\n\n\nlinr(x)\n\ntensor([[-14.0000],\n        [-13.9900],\n        [-13.9800],\n        ...,\n        [  5.9800],\n        [  5.9900],\n        [  6.0000]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\n\nw0hat + w1hat*x # ìœ„ì™€ ë™ì¼\n\ntensor([[-14.0000],\n        [-13.9900],\n        [-13.9800],\n        ...,\n        [  5.9800],\n        [  5.9900],\n        [  6.0000]])\n\n\n\n-4*x + 10 # ì´ê²ƒë„ ë™ì¼\n\ntensor([[14.0000],\n        [13.9960],\n        [13.9920],\n        ...,\n        [ 6.0080],\n        [ 6.0040],\n        [ 6.0000]])\n\n\n\në‹¤ì‹œ ì •ë¦¬í•˜ë©´\n\n\ndef sigmoid(x):\n    return torch.exp(x) / (1+ torch.exp(x)) #  í¸ì˜ìƒ linr(x) ëŒ€ì‹  xë¡œ ì‘ì„±\n\n\nl1 = torch.nn.Linear(1,1)\nl1.weight.data = torch.tensor([[10.0]])\nl1.bias.data = torch.tensor([-4.0])\n#yhat = torch.exp(l1(x)) / (1+ torch.exp(l1(x)))\nyhat = sigmoid(l1(x))\n\n\nplt.plot(x,yhat.data)\n\n\n\n\n\n\n\n\n\nê°’ì„ ë°”ê¾¸ê³  ì‹¶ìœ¼ë©´\n\n\nl1 = torch.nn.Linear(1,1)\nl1.weight.data = torch.tensor([[-0.3]])\nl1.bias.data = torch.tensor([-0.8])\n#yhat = torch.exp(l1(x)) / (1+ torch.exp(l1(x)))\nyhat = sigmoid(l1(x))\n\n\nplt.plot(x,yhat.data)\n\n\n\n\n\n\n\n\n\nplt.plot(x,y,'.',alpha=0.03)\nplt.plot(x, yhat.data, '--')\n\n\n\n\n\n\n\n\n)ğŸ—£ï¸\n\nl1 = torch.nn.Linear(1,1)\nl1(x) # w0hat + w1hat*x \n\ntensor([[ 0.4735],\n        [ 0.4728],\n        [ 0.4721],\n        ...,\n        [-0.9890],\n        [-0.9897],\n        [-0.9905]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\nğŸ—£ï¸ ì‹¤í–‰í•  ë•Œë§ˆë‹¤ ë‹¬ë¼ì§€ë¯€ë¡œ ì•„ë˜ì™€ ê°™ì´ ê³ ì •\n\nl1.weight.data = torch.tensor([[-0.3]])\nl1.bias.data = torch.tensor([-0.8])\n\n\ndef sigmoid(x):\n    return torch.exp(x)/(1+torch.exp(x))\n\n\nplt.plot(x,y,'.',alpha=0.03)\nplt.plot(x[0],y[0],'o',label=r\"$(x_i,y_i)$\",color=\"C0\")\nplt.plot(x,prob,'--r',label=r\"prob (true, unknown) = $\\frac{exp(-1+5x)}{1+exp(-1+5x)}$\")\nplt.plot(x,sigmoid(l1(x)).data,'--b', label=r\"prob (estimated) = $(x_i,\\hat{y}_i)$ -- first curve\")\nplt.legend()\n\n\n\n\n\n\n\n\n#\n# ë°©ë²•2 â€“ l1, a1\nğŸ—£ï¸(\nx -&gt; w0hat + w1hat*x  # ìµœì´ˆì˜ ê³¡ì„ ì„ ê·¸ë¦¬ê¸° ìœ„í•œ ì„ í˜• ë³€í™˜\nu = w0hat + w1hat*x  # ê²°ê³¼ë¥¼ uë¡œ ì €ì¥\nfirst_curve = yhat = prob_hat = sigmoid(u)\nu = w0hat + w1hat*x = l1(x) # l1ì„ ë§Œë“ ë‹¤ë©´ ì´ë ‡ê²Œë„ ì“¸ ìˆ˜ ìˆìŒ\nl1 = torch.nn.Linear(1,1)\nl1.weight.data = torch.tensor([[-0.3]])\nl1.bias.data = torch.tensor([-0.8])\nu = l1(x)\nyhat = sigmoid(u)\n\nsigmoidëŠ” ì§ì ‘ ë§Œë“¤ì—ˆìŒ\n\n\nsigmoid?\n\n\nSignature: sigmoid(x)\nDocstring: &lt;no docstring&gt;\nFile:      /tmp/ipykernel_30452/3273882758.py\nType:      function\n\n\n\n\nsigmoid??\n\n\nSignature: sigmoid(x)\nDocstring: &lt;no docstring&gt;\nSource:   \ndef sigmoid(x):\n    return torch.exp(x)/(1+torch.exp(x))\nFile:      /tmp/ipykernel_30452/3273882758.py\nType:      function\n\n\n\n\në‹¤ìŒê³¼ ê°™ì´ë„ í•  ìˆ˜ ìˆìŒ (torch.nnì˜ í´ë˜ìŠ¤ ì´ìš©)\n\n\nsig = torch.nn.Sigmoid()\nsig\n\nSigmoid()\n\n\n\nl1 = torch.nn.Linear(1,1)\nl1.weight.data = torch.tensor([[-0.3]])\nl1.bias.data = torch.tensor([-0.8])\nu = l1(x)\nyhat = sig(u)\n\n\nyhat\n\ntensor([[0.3775],\n        [0.3775],\n        [0.3774],\n        ...,\n        [0.2499],\n        [0.2498],\n        [0.2497]], grad_fn=&lt;SigmoidBackward0&gt;)\n\n\n\nl1 = torch.nn.Linear(1,1)\nl1.weight.data = torch.tensor([[-0.3]])\nl1.bias.data = torch.tensor([-0.8])\nyhat = sig(l1(x)) # x --&gt; l1 --&gt; sig ë¡œ ì´í•´\n\n\nplt.plot(x,y,'.',alpha=0.03)\nplt.plot(x[0],y[0],'o',label=r\"$(x_i,y_i)$\",color=\"C0\")\nplt.plot(x,prob,'--r',label=r\"prob (true, unknown) = $\\frac{exp(-1+5x)}{1+exp(-1+5x)}$\")\nplt.plot(x,sigmoid(l1(x)).data,'--b', label=r\"prob (estimated) = $(x_i,\\hat{y}_i)$ -- first curve\")\nplt.legend()\n\n\n\n\n\n\n\n\n\në°©ë²•1ê³¼ ë™ì¼í•œ ê²°ê³¼\n\n)ğŸ—£ï¸\n\nl1 = torch.nn.Linear(1,1)\nl1.weight.data = torch.tensor([[-0.3]])\nl1.bias.data = torch.tensor([-0.8])\n\n\na1 = torch.nn.Sigmoid()\n\n\nsigmoid(l1(x)), a1(l1(x)) # ë˜‘ê°™ì•„ìš”\n\n(tensor([[0.3775],\n         [0.3775],\n         [0.3774],\n         ...,\n         [0.2499],\n         [0.2498],\n         [0.2497]], grad_fn=&lt;DivBackward0&gt;),\n tensor([[0.3775],\n         [0.3775],\n         [0.3774],\n         ...,\n         [0.2499],\n         [0.2498],\n         [0.2497]], grad_fn=&lt;SigmoidBackward0&gt;))\n\n\n- ì§€ê¸ˆê¹Œì§€ì˜ êµ¬í˜„ í™•ì¸\n\nplt.plot(x,y,'.',alpha=0.03)\nplt.plot(x[0],y[0],'o',label=r\"$(x_i,y_i)$\",color=\"C0\")\nplt.plot(x,prob,'--r',label=r\"prob (true, unknown) = $\\frac{exp(-1+5x)}{1+exp(-1+5x)}$\")\nplt.plot(x,a1(l1(x)).data,'--b', label=r\"prob (estimated) = $(x_i,\\hat{y}_i)$ -- first curve with $(a_1 \\circ l_1)(x)$\")\nplt.legend()\n\n\n\n\n\n\n\n\n#\n# ë°©ë²•3 - l1, a1 ë§Œë“¤ê³  \\(\\to\\) net\nğŸ—£ï¸(\n\nyhat\n\ntensor([[0.3775],\n        [0.3775],\n        [0.3774],\n        ...,\n        [0.2499],\n        [0.2498],\n        [0.2497]], grad_fn=&lt;SigmoidBackward0&gt;)\n\n\n\na1(l1(x))\n\ntensor([[0.3775],\n        [0.3775],\n        [0.3774],\n        ...,\n        [0.2499],\n        [0.2498],\n        [0.2497]], grad_fn=&lt;SigmoidBackward0&gt;)\n\n\n\nnet = al \\(\\circ\\) l1 ì„ ì •ì˜í•˜ì—¬ net(x)ë„ ê°™ì€ ê²°ê³¼ë¥¼ ë‚˜ì˜¤ê²Œ í•˜ê³  ì‹¶ìŒ\n\n\ntorch.nn.Sequential(l1,a1)\n\nSequential(\n  (0): Linear(in_features=1, out_features=1, bias=True)\n  (1): Sigmoid()\n)\n\n\n\nnet = torch.nn.Sequential(l1,a1)\nnet(x) # a1(l1(x))\n\ntensor([[0.3775],\n        [0.3775],\n        [0.3774],\n        ...,\n        [0.2499],\n        [0.2498],\n        [0.2497]], grad_fn=&lt;SigmoidBackward0&gt;)\n\n\n\nì´ë ‡ê²Œ í•œ ì´ìœ : parameters()ë¥¼ ì´ìš©í•˜ì—¬ optimizerë¥¼ ë§Œë“¤ ìˆ˜ ìˆìŒ\n\n\nnet.parameters()\n\n&lt;generator object Module.parameters at 0x7f34682a17b0&gt;\n\n\n)ğŸ—£ï¸\n- ê´€ì°°: ì§€ê¸ˆ ì•„ë˜ì˜ êµ¬ì¡°ì´ë‹¤.\n\\[{\\bf x} \\overset{l_1}{\\to} {\\bf u} \\overset{a_1}{\\to} {\\bf v} = \\hat{\\bf y}\\]\n- ì†Œë§: í•¨ìˆ˜ \\(l_1, a_1\\) ì˜ í•©ì„±ì„ í•˜ë‚˜ë¡œ ë¬¶ì–´ì„œ\n\\[(a_1\\circ l_1)({\\bf x}) := net({\\bf x})\\]\nì´ëŸ¬í•œ ê¸°ëŠ¥ì„ í•˜ëŠ” í•˜ë‚˜ì˜ í•¨ìˆ˜ \\(net\\)ì„ ë§Œë“¤ ìˆ˜ ì—†ì„ê¹Œ?\n\nl1 = torch.nn.Linear(1,1)\nl1.weight.data = torch.tensor([[-0.3]])\nl1.bias.data = torch.tensor([-0.8])\na1 = torch.nn.Sigmoid()\n\n\nnet = torch.nn.Sequential(l1,a1) #l1ì„ ì·¨í•˜ê³  ê·¸ë‹¤ìŒì— a1ì„ ì·¨í•˜ë¼ëŠ” ì˜ë¯¸\n\n\nnet(x), a1(l1(x)), sigmoid(l1(x))\n\n(tensor([[0.3775],\n         [0.3775],\n         [0.3774],\n         ...,\n         [0.2499],\n         [0.2498],\n         [0.2497]], grad_fn=&lt;SigmoidBackward0&gt;),\n tensor([[0.3775],\n         [0.3775],\n         [0.3774],\n         ...,\n         [0.2499],\n         [0.2498],\n         [0.2497]], grad_fn=&lt;SigmoidBackward0&gt;),\n tensor([[0.3775],\n         [0.3775],\n         [0.3774],\n         ...,\n         [0.2499],\n         [0.2498],\n         [0.2497]], grad_fn=&lt;DivBackward0&gt;))\n\n\n* net êµ¬ì¡° ì ê¹ ì‚´í´ë³´ê¸°\nğŸ—£ï¸(\n\nnet\n\nSequential(\n  (0): Linear(in_features=1, out_features=1, bias=True)\n  (1): Sigmoid()\n)\n\n\n\nnet[0]\n\nLinear(in_features=1, out_features=1, bias=True)\n\n\n\nl1\n\nLinear(in_features=1, out_features=1, bias=True)\n\n\n\nnet[1]\n\nSigmoid()\n\n\n\naaa = torch.nn.Sigmoid()\naaa\n\nSigmoid()\n\n\n\nnetê°€ ë¦¬ìŠ¤íŠ¸ì²˜ëŸ¼ ë˜ì–´ ìˆì–´ ì²«ë²ˆì§¸ ì›ì†Œ net[0]ì€ l1 ì´ê³  ë‘ë²ˆì§¸ ì›ì†Œ net[1]ì€ aaaì¸ë“¯\ní™•ì¸ ë°©ë²•: ì•„ë˜\n\n\nl1 is net[0]\n\nTrue\n\n\n\na1 is net[1]\n\nTrue\n\n\n\në‹¤ë¥¸ í™•ì¸ ë°©ë²•\n\nì˜¤ë¸Œì íŠ¸: ë©”ëª¨ë¦¬ì— ì €ì¥\nì €ì¥ë˜ì–´ ìˆëŠ” ì£¼ì†Œê°€ ë™ì¼í•˜ë©´ ê°™ì€ ì˜¤ë¸Œì íŠ¸\n\n\n\nid(net[0]), id(l1)\n\n(139863062109248, 139863062109248)\n\n\n\nid(net[1]), id(a1)\n\n(139863062108960, 139863062108960)\n\n\n\nnet(x), a1(l1(x))\n\n(tensor([[0.3775],\n         [0.3775],\n         [0.3774],\n         ...,\n         [0.2499],\n         [0.2498],\n         [0.2497]], grad_fn=&lt;SigmoidBackward0&gt;),\n tensor([[0.3775],\n         [0.3775],\n         [0.3774],\n         ...,\n         [0.2499],\n         [0.2498],\n         [0.2497]], grad_fn=&lt;SigmoidBackward0&gt;))\n\n\n\nnet(x), net[1](net[0](x)) # ì´ê²ƒë„ ë™ì¼\n\n(tensor([[0.3775],\n         [0.3775],\n         [0.3774],\n         ...,\n         [0.2499],\n         [0.2498],\n         [0.2497]], grad_fn=&lt;SigmoidBackward0&gt;),\n tensor([[0.3775],\n         [0.3775],\n         [0.3774],\n         ...,\n         [0.2499],\n         [0.2498],\n         [0.2497]], grad_fn=&lt;SigmoidBackward0&gt;))\n\n\n)ğŸ—£ï¸\n\nnet[0], net[1]\n\n(Linear(in_features=1, out_features=1, bias=True), Sigmoid())\n\n\n\nl1 is net[0]\n\nTrue\n\n\n\na1 is net[1]\n\nTrue\n\n\n#\n# ë°©ë²•4 â€“ netì„ ë°”ë¡œ ë§Œë“¤ê¸°\nğŸ—£ï¸(\n# x --&gt; yhat: íšŒê·€ë¶„ì„ì—ì„œ ìµœì´ˆì˜ ì§ì„  ë°”ë¡œ ë§Œë“œëŠ” ë°©ë²•\nnet = torch.nn.Linear(1,1)\nyhat - net(x)\n# x --&gt; yhat: ë¡œì§€ìŠ¤í‹±ì—ì„œ ìµœì´ˆì˜ ê³¡ì„  ë°”ë¡œ ë§Œë“œëŠ” ë°©ë²•\nnet = torch.nn.Sequential(\n    l1,\n    a1\n)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n)\nyhat = net(x)\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n)\nyhat = net(x)\n\n\nnet[0].weight # ì•„ë¬´ parameterê°€ ë“¤ì–´ê°€ ìˆìŒ\n\nParameter containing:\ntensor([[0.4945]], requires_grad=True)\n\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n)\nnet[0].weight.data = torch.tensor([[-0.3]])\nnet[0].bias.data = torch.tensor([-0.8])\nyhat = net(x)\n\n\nnet(x)\n\ntensor([[0.3775],\n        [0.3775],\n        [0.3774],\n        ...,\n        [0.2499],\n        [0.2498],\n        [0.2497]], grad_fn=&lt;SigmoidBackward0&gt;)\n\n\n)ğŸ—£ï¸\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n)\nnet[0].weight.data = torch.tensor([[-0.3]])\nnet[0].bias.data = torch.tensor([-0.8])\nyhat = net(x)\n\n\nnet(x)\n\ntensor([[0.3775],\n        [0.3775],\n        [0.3774],\n        ...,\n        [0.2499],\n        [0.2498],\n        [0.2497]], grad_fn=&lt;SigmoidBackward0&gt;)\n\n\nğŸ—£ï¸ ê²°ë¡ : ìœ„ì˜ ë°©ë²•ìœ¼ë¡œ ì‚¬ìš©í•˜ë©´ ë¨\n#"
  },
  {
    "objectID": "posts/03wk-2.html#d.-step14",
    "href": "posts/03wk-2.html#d.-step14",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "D. Step1~4",
    "text": "D. Step1~4\nğŸ—£ï¸(\n\ní•™ìŠµ ì‹œì‘\n\n\nplt.plot(x,y,'.',alpha=0.03) # given data\n\n\n\n\n\n\n\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n)\nnet[0].weight.data = torch.tensor([[-0.3]])\nnet[0].bias.data = torch.tensor([-0.8])\nyhat = net(x)\n\n\nplt.plot(x,y,'.',alpha=0.03) # given data\nplt.plot(x,net(x).data, '--') # ìµœì´ˆì˜ ê³¡ì„  ê·¸ë¦¬ê¸°\n\n\n\n\n\n\n\n\n\nìµœì´ˆì˜ ê³¡ì„ ë³´ë‹¤ ë‚˜ì€ ê³¡ì„ ì„ ì°¾ìœ¼ë©° updateí•˜ë©´ ë¨\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n)\nnet[0].weight.data = torch.tensor([[-0.3]])\nnet[0].bias.data = torch.tensor([-0.8])\nyhat = net(x)\nloss = torch.mean((y-yhat)**2) # loss í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ ì¤Œ\nloss\n\ntensor(0.2747, grad_fn=&lt;MeanBackward0&gt;)\n\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n)\nnet[0].weight.data = torch.tensor([[-0.3]])\nnet[0].bias.data = torch.tensor([-0.8])\noptimizr = torch.optim.SGD(net.parameters(), lr=0.25)\nfor epoc in range(200):\n    yhat = net(x)\n    loss = torch.mean((y-yhat)**2)\n    loss.backward() # ë¯¸ë¶„\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'.',alpha=0.03)\nplt.plot(x,net(x).data, '--')\n\n\n\n\n\n\n\n\n\nìµœì´ˆì˜ ê³¡ì„ ë³´ë‹¤ëŠ” ê·¸ëŸ´ë“¯í•´ì§\nì•Œê³  ìˆëŠ” True ê°’ê³¼ ë¹„êµí•´ë³´ë©´\n\nì£¼í™©ìƒ‰ ì„ : True\n200ë²ˆ ì •ë„ ë°˜ë³µí•˜ë‹ˆ ì–´ëŠ ì •ë„ ì˜¨ ê²ƒ ê°™ì§€ë§Œ ë”± ë§ë‹¤ê³  ë³´ê¸°ëŠ” ì–´ë ¤ì›€\n\n\n\nplt.plot(x,y,'.',alpha=0.03)\nplt.plot(x,prob,'--')\nplt.plot(x,net(x).data, '--')\n\n\n\n\n\n\n\n\n\n200ë²ˆ ë” (ì´ 400ë²ˆ)\n\n\nfor epoc in range(200):\n    yhat = net(x)\n    loss = torch.mean((y-yhat)**2)\n    loss.backward() # ë¯¸ë¶„\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'.',alpha=0.03)\nplt.plot(x,prob,'--')\nplt.plot(x,net(x).data, '--')\n\n\n\n\n\n\n\n\n\n200ë²ˆ ë” (ì´ 600ë²ˆ)\n\n\nfor epoc in range(200):\n    yhat = net(x)\n    loss = torch.mean((y-yhat)**2)\n    loss.backward() # ë¯¸ë¶„\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'.',alpha=0.03)\nplt.plot(x,prob,'--')\nplt.plot(x,net(x).data, '--')\n\n\n\n\n\n\n\n\n\n200ë²ˆ ë” (ì´ 800ë²ˆ)\n\n\nfor epoc in range(200):\n    yhat = net(x)\n    loss = torch.mean((y-yhat)**2)\n    loss.backward() # ë¯¸ë¶„\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'.',alpha=0.03)\nplt.plot(x,prob,'--')\nplt.plot(x,net(x).data, '--')\n\n\n\n\n\n\n\n\n\nëŒë¦´ìˆ˜ë¡ ê°€ê¹Œì›Œì§ˆ ê²ƒ ê°™ìŒ\n\n)ğŸ—£ï¸\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1, out_features=1),\n    torch.nn.Sigmoid()\n)\nl1, a1 = net \nl1.weight.data = torch.tensor([[-0.3]])\nl1.bias.data = torch.tensor([-0.8])\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25)\n#---#\nfor epoc in range(100):\n    ## 1\n    yhat = net(x) \n    ## 2 \n    loss = torch.mean((y-yhat)**2)\n    ## 3\n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'.',alpha=0.05)\nplt.plot(x,prob,'--r')\nplt.plot(x,yhat.data,'--b')\nplt.title('after 100 epochs')\n\nText(0.5, 1.0, 'after 100 epochs')\n\n\n\n\n\n\n\n\n\n\nfor epoc in range(4900):\n    ## 1\n    yhat = net(x) \n    ## 2 \n    loss = torch.mean((y-yhat)**2)\n    ## 3\n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'.',alpha=0.05)\nplt.plot(x,prob,'--r')\nplt.plot(x,yhat.data,'--b')\nplt.title('after 5000 epochs')\n\nText(0.5, 1.0, 'after 5000 epochs')\n\n\n\n\n\n\n\n\n\nğŸ—£ï¸ ë¡œì§€ìŠ¤í‹±ì´ í•´ê²°ëœ ê²ƒì²˜ëŸ¼ ë³´ì„\nğŸ—£ï¸(\n\në‹¤ìŒê³¼ ê°™ì´ í•´ë„ ë§ˆì°¬ê°€ì§€ (ì´ˆê¸°ê°’ì€ í¬ê²Œ ì¤‘ìš”í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ)\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1, out_features=1),\n    torch.nn.Sigmoid()\n)\n# l1, a1 = net \n# l1.weight.data = torch.tensor([[-0.3]])\n# l1.bias.data = torch.tensor([-0.8])\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25)\n#---#\nfor epoc in range(100):\n    ## 1\n    yhat = net(x) \n    ## 2 \n    loss = torch.mean((y-yhat)**2)\n    ## 3\n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'.',alpha=0.05)\nplt.plot(x,prob,'--r')\nplt.plot(x,yhat.data,'--b')\nplt.title('after 100 epochs')\n\nText(0.5, 1.0, 'after 100 epochs')\n\n\n\n\n\n\n\n\n\n\nfor epoc in range(4900):\n    ## 1\n    yhat = net(x) \n    ## 2 \n    loss = torch.mean((y-yhat)**2)\n    ## 3\n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'.',alpha=0.05)\nplt.plot(x,prob,'--r')\nplt.plot(x,yhat.data,'--b')\nplt.title('after 5000 epochs')\n\nText(0.5, 1.0, 'after 5000 epochs')\n\n\n\n\n\n\n\n\n\n\nì„±ê³µí•œ ê²ƒ ê°™ì§€ë§Œ ì‹¤ìƒì€ ê·¸ë ‡ì§€ ì•ŠìŒ\n\nfor epoc in range(4900):\n    ## 1\n    yhat = net(x) \n    ## 2 \n    loss = torch.mean((y-yhat)**2) # ì´ ë¶€ë¶„ì— ë¬¸ì œê°€ ìˆì–´ ì„¤ëª…í•  ì˜ˆì •\n    ## 3\n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n)ğŸ—£ï¸"
  },
  {
    "objectID": "posts/03wk-2.html#a.-ì‹œê°í™”ë¥¼-ìœ„í•œ-ì¤€ë¹„",
    "href": "posts/03wk-2.html#a.-ì‹œê°í™”ë¥¼-ìœ„í•œ-ì¤€ë¹„",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "A. ì‹œê°í™”ë¥¼ ìœ„í•œ ì¤€ë¹„",
    "text": "A. ì‹œê°í™”ë¥¼ ìœ„í•œ ì¤€ë¹„\n\ndef plot_loss(loss_fn, ax=None, Wstar=[-1,5]):\n    w0hat,w1hat =torch.meshgrid(torch.arange(-10,3,0.1),torch.arange(-1,10,0.1),indexing='ij')\n    w0hat = w0hat.reshape(-1)\n    w1hat = w1hat.reshape(-1)\n    def l(w0hat,w1hat):\n        yhat = torch.exp(w0hat+w1hat*x)/(1+torch.exp(w0hat+w1hat*x))\n        return loss_fn(yhat,y) \n    loss = list(map(l,w0hat,w1hat))\n    #---#\n    if ax is None: \n        fig = plt.figure()\n        ax = fig.add_subplot(1,1,1,projection='3d')\n    ax.scatter(w0hat,w1hat,loss,s=0.001) \n    ax.scatter(w0hat[::20],w1hat[::20],loss[::20],s=0.1,color='C0') \n    w0star,w1star = np.array(Wstar).reshape(-1)\n    ax.scatter(w0star,w1star,l(w0star,w1star),s=200,marker='*',color='red',label=f\"W=[{w0star:.1f},{w1star:.1f}]\")\n    #---#\n    ax.elev = 15\n    ax.dist = -20\n    ax.azim = 75    \n    ax.legend()\n    ax.set_xlabel(r'$w_0$')  # xì¶• ë ˆì´ë¸” ì„¤ì •\n    ax.set_ylabel(r'$w_1$')  # yì¶• ë ˆì´ë¸” ì„¤ì •\n    ax.set_xticks([-10,-5,0])  # xì¶• í‹± ê°„ê²© ì„¤ì •\n    ax.set_yticks([-10,0,10])  # yì¶• í‹± ê°„ê²© ì„¤ì •\n\n\ndef _learn_and_record(net, loss_fn, optimizr):\n    yhat_history = [] \n    loss_history = []\n    What_history = []\n    Whatgrad_history = []\n    What_history.append([net[0].bias.data.item(), net[0].weight.data.item()])\n    for epoc in range(100): \n        ## step1 \n        yhat = net(x)\n        ## step2 \n        loss = loss_fn(yhat,y)\n        ## step3\n        loss.backward() \n        ## step4 \n        optimizr.step()\n        ## record \n        if epoc % 5 ==0: \n            yhat_history.append(yhat.reshape(-1).data.tolist())\n            loss_history.append(loss.item())\n            What_history.append([net[0].bias.data.item(), net[0].weight.data.item()])\n            Whatgrad_history.append([net[0].bias.grad.item(), net[0].weight.grad.item()])\n        optimizr.zero_grad() \n        \n    return yhat_history, loss_history, What_history, Whatgrad_history\n    \ndef show_animation(net, loss_fn, optimizr):\n    yhat_history,loss_history,What_history,Whatgrad_history = _learn_and_record(net,loss_fn,optimizr)\n    \n    fig = plt.figure(figsize=(7.5,3.5))\n    ax1 = fig.add_subplot(1, 2, 1)\n    ax2 = fig.add_subplot(1, 2, 2, projection='3d')\n    ## ax1: ì™¼ìª½ê·¸ë¦¼ \n    ax1.scatter(x,y,alpha=0.01)\n    ax1.scatter(x[0],y[0],color='C0',label=r\"observed data = $(x_i,y_i)$\")\n    ax1.plot(x,prob,'--',label=r\"prob (true) = $(x_i,\\frac{exp(-1+5x_i)}{1+exp(-1+5x_i)})$\")    \n    line, = ax1.plot(x,yhat_history[0],'--',label=r\"prob (estimated) = $(x_i,\\hat{y}_i)$\") \n    ax1.legend()\n    ## ax2: ì˜¤ë¥¸ìª½ê·¸ë¦¼ \n    plot_loss(loss_fn,ax2)\n    ax2.scatter(np.array(What_history)[0,0],np.array(What_history)[0,1],loss_history[0],color='blue',s=200,marker='*')    \n    def animate(epoc):\n        line.set_ydata(yhat_history[epoc])\n        w0hat = np.array(What_history)[epoc,0]\n        w1hat = np.array(What_history)[epoc,1]\n        w0hatgrad = np.array(Whatgrad_history)[epoc,0]\n        w1hatgrad = np.array(Whatgrad_history)[epoc,1]\n        ax2.scatter(w0hat,w1hat,loss_history[epoc],color='grey')\n        ax2.set_title(f\"What.grad=[{w0hatgrad:.4f},{w1hatgrad:.4f}]\",y=0.8)\n        fig.suptitle(f\"epoch={epoc*5} // What=[{w0hat:.2f},{w1hat:.2f}] // Loss={loss_fn.__class__.__name__} // Opt={optimizr.__class__.__name__}\")\n        return line\n    ani = animation.FuncAnimation(fig, animate, frames=20)    \n    plt.close()\n    return ani\n\n\nfrom matplotlib import animation\nplt.rcParams[\"animation.html\"] = \"jshtml\"\n\ní•¨ìˆ˜ì‚¬ìš©ë²•\n\nloss_fn = torch.nn.MSELoss()\nplot_loss(loss_fn)\n\n\n\n\n\n\n\n\nğŸ—£ï¸(\n\ndef loss_fn2(yhat,y):\n    return loss_fn(yhat,y)*2\n\n\nplot_loss(loss_fn2)\n\n\n\n\n\n\n\n\n\nzì¶•ë§Œ 2ë°° ì¦ê°€ (í•¨ìˆ˜: ê³¡ë©´ì„ ê·¸ë ¤ì£¼ëŠ” ì—­í• )\n\n\n# show_animation??\n\n\nSignature: show_animation(net, loss_fn, optimizr)\n\nnet: ì´ˆê¸° ì„¤ì • ê°’ (w0, w1)\nloss_fn: ê·¸ë¦¼\noptimizr: í•™ìŠµ ê³¼ì •\n\në°‘ ì½”ë“œ: ì–´ë– í•œ ì´ˆê¸°ê°’ì„ ë°›ì•„ í•™ìŠµí•˜ëŠ” ê³¼ì •ì„ ê·¸ë ¤ì¤Œ\n\nì‹¤í–‰í•  ë•Œë§ˆë‹¤ ì´ˆê¸°ê°’ ë‹¬ë¼ì§\n\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25)\nshow_animation(net,loss_fn,optimizr)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\nì´ˆê¸°ê°’ ê³ ì •\n\në§Œì•½ í•™ìŠµë¥ ì´ 2.5ë©´ ë” ë¹¨ë¦¬ ë–¨ì–´ì§\n\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n)\nnet[0].weight.data = torch.tensor([[-0.8]])\nnet[0].bias.data = torch.tensor([-0.3])\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25)\nshow_animation(net,loss_fn,optimizr)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n)ğŸ—£ï¸\n\ntorch.manual_seed(42)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25) \nshow_animation(net,loss_fn,optimizr)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\nğŸ—£ï¸\n\nì´ˆê¸°ê°’ì— ë”°ë¼ í•™ìŠµì´ ë‹¬ë¼ì§\në§Œì•½ ì´ˆê¸° ê°’ì´ ìš°ì¸¡ ìƒë‹¨ì´ë¼ë©´ í‰í‰í•˜ê¸° ë•Œë¬¸ì— updateê°€ ì•„ì£¼ ì¡°ê¸ˆì”© ì¼ì–´ë‚¨"
  },
  {
    "objectID": "posts/03wk-2.html#b.-ì¢‹ì€-ì´ˆê¸°ê°’",
    "href": "posts/03wk-2.html#b.-ì¢‹ì€-ì´ˆê¸°ê°’",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "B. ì¢‹ì€ ì´ˆê¸°ê°’",
    "text": "B. ì¢‹ì€ ì´ˆê¸°ê°’\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nnet[0].bias.data = torch.tensor([-0.8])\nnet[0].weight.data = torch.tensor([[-0.3]])\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25) \n#---#\nshow_animation(net,loss_fn,optimizr)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\nğŸ—£ï¸\n\nì´ ê²½ìš°ëŠ” ê¸°ë‹¤ë¦¬ë©´ í•™ìŠµì´ ì˜ ë  ê±° ê°™ìŒ"
  },
  {
    "objectID": "posts/03wk-2.html#c.-ê°€ëŠ¥ì„±-ìˆëŠ”-ì´ˆê¸°ê°’",
    "href": "posts/03wk-2.html#c.-ê°€ëŠ¥ì„±-ìˆëŠ”-ì´ˆê¸°ê°’",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "C. ê°€ëŠ¥ì„± ìˆëŠ” ì´ˆê¸°ê°’",
    "text": "C. ê°€ëŠ¥ì„± ìˆëŠ” ì´ˆê¸°ê°’\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nnet[0].bias.data = torch.tensor([-3.0])\nnet[0].weight.data = torch.tensor([[-1.0]])\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25) \n#---#\nshow_animation(net,loss_fn,optimizr)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\nğŸ—£ï¸\n\në§ˆì§€ë§‰ì— ì•½ê°„ í¬ë§ì´ ë³´ì„\në§ˆìŒ ë¨¹ê³  20,000ë²ˆ ì •ë„ ëŒë¦¬ë©´ ë  ê±° ê°™ìŒ"
  },
  {
    "objectID": "posts/03wk-2.html#d.-ìµœì•…ì˜-ì´ˆê¸°ê°’",
    "href": "posts/03wk-2.html#d.-ìµœì•…ì˜-ì´ˆê¸°ê°’",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "D. ìµœì•…ì˜ ì´ˆê¸°ê°’",
    "text": "D. ìµœì•…ì˜ ì´ˆê¸°ê°’\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nnet[0].bias.data = torch.tensor([-10.0])\nnet[0].weight.data = torch.tensor([[-1.0]])\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25) \n#---#\nshow_animation(net,loss_fn,optimizr)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\nğŸ—£ï¸\n\nì´ ê²½ìš°ëŠ” í¬ë§ì´ ì—†ìŒ\nê³¡ì„ ì´ ì•„ë˜ë¡œ ë³¼ë¡í•œ 2ì°¨ í•¨ìˆ˜ê°€ ì•„ë‹ˆê³  4ì°¨ í•¨ìˆ˜ë¼ë©´ í•™ìŠµë¥ ê³¼ ì´ˆê¸° ê°’ì— ë”°ë¼ ê°–í˜€ë²„ë¦´ ìˆ˜ë„ ìˆê³  ìš´ì— ë”°ë¼ ë‹¬ë¼ì§\n\n\ní•´ê²°í•˜ëŠ” ì ‘ê·¼ë²•:\n\nì»´ê³µìŠ¤íƒ€ì¼: ì—í­ì„ ëŠ˜ë ¤ë³¼ê¹Œ?\nì‚°ê³µìŠ¤íƒ€ì¼: ì˜µí‹°ë§ˆì´ì €ë¥¼ ë°”ê¿”ë³¼ê¹Œ?\ní†µê³„ìŠ¤íƒ€ì¼: Lossë¥¼ ë°”ê¿”ë³¼ê¹Œ?\nğŸ—£ï¸\n\nì´ˆê¸° ê°’ì„ ë°”ê¿”ê°€ë©° ë¬´ìˆ˜íˆ ì‹¤í–‰í•˜ë©° ì°¾ìŒ\nì´ ì–´ë ¤ìš´ ê³¡ë©´ì— ëŒ€í•´ ì˜µí‹°ë§ˆì´ì €ë¥¼ ìˆ˜ì •\nê³¡ë©´ ìì²´ë¥¼ ìµœì í™”ê°€ ì˜ ë˜ê²Œ ë°”ê¿ˆ (loss í•¨ìˆ˜ë¥¼ ë°”ê¿ˆ: MSE Loss ë§ê³  ë‹¤ë¥¸ Loss?)"
  },
  {
    "objectID": "posts/03wk-2.html#a.-bce-lossë¥¼-ì‚¬ìš©í•˜ì—¬-í•™ìŠµ",
    "href": "posts/03wk-2.html#a.-bce-lossë¥¼-ì‚¬ìš©í•˜ì—¬-í•™ìŠµ",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "A. BCE Lossë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ",
    "text": "A. BCE Lossë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ\n- BCE lossë¼ëŠ”ê²Œ ìˆìŒ.\n\n\\(loss= - \\sum_{i=1}^{n} \\big(y_i\\log(\\hat{y}_i)+(1-y_i)\\log(1-\\hat{y}_i)\\big)\\)\nhttps://en.wikipedia.org/wiki/Cross-entropy\n\nğŸ—£ï¸(\nyi = 0\nyi_hat = 0.001\nlog(1) = 0\nloss = 0\nyi = 0\nyi_hat = 0.9999\nlog(1-0.999) = log(0) = -ë¬´í•œëŒ€\nloss = ë¬´í•œëŒ€\nyi = 1\nyi_hat = 1\nloss = 0\nyi = 1\nyi_hat = 0.0001\nloss = ë¬´í•œëŒ€\n\në¹„ìŠ·í• ìˆ˜ë¡ 0, ë‹¤ë¥¼ìˆ˜ë¡ ë¬´í•œëŒ€ê¹Œì§€ ê° -&gt; lossì˜ ì—­í• ì€ í•¨\nì›ë¦¬: - log likelihoood\n\n)ğŸ—£ï¸\n\nğŸ—£ï¸\n\nnet[0] = torch.nn.Linear(in_features=1, out_features=1)\nnet[1] = torch.nn.Sigmoid()\nnet = [net[0], net[1]] ëŠë‚Œ\nl1, a1 = [net[0], net[1]] ëŠë‚Œ\n\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1, out_features=1),\n    torch.nn.Sigmoid()\n)\nl1, a1 = net \nl1.weight.data = torch.tensor([[-0.3]])\nl1.bias.data = torch.tensor([-0.8])\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25)\n#---#\nfor epoc in range(100):\n    ## 1\n    yhat = net(x) \n    ## 2 \n    #loss = torch.mean((y-yhat)**2) # loss_fn(yhat,y)\n    loss = -torch.mean(y*torch.log(yhat) + (1-y)*torch.log(1-yhat))\n    ## 3\n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'.',alpha=0.05)\nplt.plot(x,prob,'--r')\nplt.plot(x,yhat.data,'--b')\nplt.title('after 100 epochs')\n\nText(0.5, 1.0, 'after 100 epochs')\n\n\n\n\n\n\n\n\n\nê°™ì€ 100 ì—í­ì¸ë° í›¨ì”¬ ì˜ë§ì¶¤..\nğŸ—£ï¸ ë™ì¼í•œ ì´ˆê¸° ê°’\n- lossìˆ˜ì‹ì„ ëª»ì™¸ìš°ê² ë‹¤ë©´?\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1, out_features=1),\n    torch.nn.Sigmoid()\n)\nl1, a1 = net \nl1.weight.data = torch.tensor([[-0.3]])\nl1.bias.data = torch.tensor([-0.8])\nloss_fn = torch.nn.BCELoss()\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25)\n#---#\nfor epoc in range(100):\n    ## 1\n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y) # yhatë¶€í„° ì¨ì•¼í•¨\n    ## 3\n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'.',alpha=0.05)\nplt.plot(x,prob,'--r')\nplt.plot(x,yhat.data,'--b')\nplt.title('after 100 epochs')\n\nText(0.5, 1.0, 'after 100 epochs')"
  },
  {
    "objectID": "posts/03wk-2.html#b.-loss-function-ì‹œê°í™”",
    "href": "posts/03wk-2.html#b.-loss-function-ì‹œê°í™”",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "B. Loss Function ì‹œê°í™”",
    "text": "B. Loss Function ì‹œê°í™”\n\nplot_loss(torch.nn.MSELoss())\n\n\n\n\n\n\n\n\nğŸ—£ï¸ MSELossëŠ” ìš°ì¸¡ ìƒë‹¨ì— ìˆìœ¼ë©´ ì•ˆ ë  ê²ƒ ê°™ìŒ\n\nplot_loss(torch.nn.BCELoss())\n\n\n\n\n\n\n\n\n- ë¹„êµí•´ë³´ì.\n\nfig = plt.figure()\nax1 = fig.add_subplot(1,2,1,projection='3d')\nax2 = fig.add_subplot(1,2,2,projection='3d')\nplot_loss(torch.nn.MSELoss(),ax1)\nplot_loss(torch.nn.BCELoss(),ax2)\n\n\n\n\n\n\n\n\n\nğŸ—£ï¸\n\nì˜¤ë¥¸ìª½ê³¼ ê°™ì€ ê²½ìš°ë¥¼ ì–´ë ¤ìš´ ë§ë¡œ convex functionì´ë¼ê³  í•¨\nloss í•¨ìˆ˜ê°€ convex functionì´ë©´ ìˆ˜ë ´ì‹œí‚¤ê¸° ì‰¬ì›€"
  },
  {
    "objectID": "posts/03wk-2.html#c.-í•™ìŠµê³¼ì •-ì‹œê°í™”-ì¢‹ì€-ì´ˆê¸°ê°’",
    "href": "posts/03wk-2.html#c.-í•™ìŠµê³¼ì •-ì‹œê°í™”-ì¢‹ì€-ì´ˆê¸°ê°’",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "C. í•™ìŠµê³¼ì • ì‹œê°í™” â€“ ì¢‹ì€ ì´ˆê¸°ê°’",
    "text": "C. í•™ìŠµê³¼ì • ì‹œê°í™” â€“ ì¢‹ì€ ì´ˆê¸°ê°’\n- MSELoss\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nnet[0].bias.data = torch.tensor([-0.8])\nnet[0].weight.data = torch.tensor([[-0.3]])\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25) \n#---#\nshow_animation(net,loss_fn,optimizr)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n- BCELoss\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nnet[0].bias.data = torch.tensor([-0.8])\nnet[0].weight.data = torch.tensor([[-0.3]])\nloss_fn = torch.nn.BCELoss()\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25) \n#---#\nshow_animation(net,loss_fn,optimizr)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nğŸ—£ï¸ ê°™ì€ ì´ˆê¸°ê°’ì¸ë° BCELossê°€ ë” ìˆ˜ë ´ì„ ì˜ í•  ê²ƒ ê°™ìŒ"
  },
  {
    "objectID": "posts/03wk-2.html#d.-í•™ìŠµê³¼ì •-ì‹œê°í™”-ê°€ëŠ¥ì„±-ìˆëŠ”-ì´ˆê¸°ê°’",
    "href": "posts/03wk-2.html#d.-í•™ìŠµê³¼ì •-ì‹œê°í™”-ê°€ëŠ¥ì„±-ìˆëŠ”-ì´ˆê¸°ê°’",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "D. í•™ìŠµê³¼ì • ì‹œê°í™” â€“ ê°€ëŠ¥ì„± ìˆëŠ” ì´ˆê¸°ê°’",
    "text": "D. í•™ìŠµê³¼ì • ì‹œê°í™” â€“ ê°€ëŠ¥ì„± ìˆëŠ” ì´ˆê¸°ê°’\n- MSELoss\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nnet[0].bias.data = torch.tensor([-3.0])\nnet[0].weight.data = torch.tensor([[-1.0]])\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25) \n#---#\nshow_animation(net,loss_fn,optimizr)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n- BCELoss\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nnet[0].bias.data = torch.tensor([-3.0])\nnet[0].weight.data = torch.tensor([[-1.0]])\nloss_fn = torch.nn.BCELoss()\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25) \n#---#\nshow_animation(net,loss_fn,optimizr)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nğŸ—£ï¸ BCELossëŠ” ì²˜ìŒë¶€í„° ì˜ ë–¨ì–´ì§"
  },
  {
    "objectID": "posts/03wk-2.html#e.-í•™ìŠµê³¼ì •-ì‹œê°í™”-ìµœì•…ì˜-ì´ˆê¸°ê°’",
    "href": "posts/03wk-2.html#e.-í•™ìŠµê³¼ì •-ì‹œê°í™”-ìµœì•…ì˜-ì´ˆê¸°ê°’",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "E. í•™ìŠµê³¼ì • ì‹œê°í™” â€“ ìµœì•…ì˜ ì´ˆê¸°ê°’",
    "text": "E. í•™ìŠµê³¼ì • ì‹œê°í™” â€“ ìµœì•…ì˜ ì´ˆê¸°ê°’\n- MSELoss\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nnet[0].bias.data = torch.tensor([-10.0])\nnet[0].weight.data = torch.tensor([[-1.0]])\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25) \n#---#\nshow_animation(net,loss_fn,optimizr)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n- BCELoss\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nnet[0].bias.data = torch.tensor([-10.0])\nnet[0].weight.data = torch.tensor([[-1.0]])\nloss_fn = torch.nn.BCELoss()\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25) \n#---#\nshow_animation(net,loss_fn,optimizr)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nğŸ—£ï¸ BCELossëŠ” ì²˜ìŒë¶€í„° ì˜ ë–¨ì–´ì§"
  },
  {
    "objectID": "posts/03wk-2.html#a.-í•™ìŠµê³¼ì •-ì‹œê°í™”-ì¢‹ì€-ì´ˆê¸°ê°’",
    "href": "posts/03wk-2.html#a.-í•™ìŠµê³¼ì •-ì‹œê°í™”-ì¢‹ì€-ì´ˆê¸°ê°’",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "A. í•™ìŠµê³¼ì • ì‹œê°í™” â€“ ì¢‹ì€ ì´ˆê¸°ê°’",
    "text": "A. í•™ìŠµê³¼ì • ì‹œê°í™” â€“ ì¢‹ì€ ì´ˆê¸°ê°’\n- MSELoss + SGD\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nnet[0].bias.data = torch.tensor([-0.8470])\nnet[0].weight.data = torch.tensor([[-0.3467]])\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25) \n#---#\nshow_animation(net,loss_fn,optimizr)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n- MSELoss + Adam\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nnet[0].bias.data = torch.tensor([-0.8])\nnet[0].weight.data = torch.tensor([[-0.3]])\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters(),lr=0.25) \n#---#\nshow_animation(net,loss_fn,optimizr)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nğŸ—£ï¸ Adamì„ ì‚¬ìš©í•˜ë‹ˆ ë¹¨ë¦¬ ë–¨ì–´ì§€ë©´ì„œ ì˜ ìˆ˜ë ´í•¨ (í˜ìœ¼ë¡œ ë¯¸ëŠ” ëŠë‚Œ)"
  },
  {
    "objectID": "posts/03wk-2.html#b.-í•™ìŠµê³¼ì •-ì‹œê°í™”-ê°€ëŠ¥ì„±-ìˆëŠ”-ì´ˆê¸°ê°’",
    "href": "posts/03wk-2.html#b.-í•™ìŠµê³¼ì •-ì‹œê°í™”-ê°€ëŠ¥ì„±-ìˆëŠ”-ì´ˆê¸°ê°’",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "B. í•™ìŠµê³¼ì • ì‹œê°í™” â€“ ê°€ëŠ¥ì„± ìˆëŠ” ì´ˆê¸°ê°’",
    "text": "B. í•™ìŠµê³¼ì • ì‹œê°í™” â€“ ê°€ëŠ¥ì„± ìˆëŠ” ì´ˆê¸°ê°’\n- MSELoss + SGD\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nnet[0].bias.data = torch.tensor([-3.0])\nnet[0].weight.data = torch.tensor([[-1.0]])\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25) \n#---#\nshow_animation(net,loss_fn,optimizr)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n- MSELoss + Adam\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nnet[0].bias.data = torch.tensor([-3.0])\nnet[0].weight.data = torch.tensor([[-1.0]])\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters(),lr=0.25) \n#---#\nshow_animation(net,loss_fn,optimizr)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nğŸ—£ï¸ Adamì„ ì‚¬ìš©í•˜ë‹ˆ ë¹¨ë¦¬ ë–¨ì–´ì§€ë©´ì„œ ì˜ ìˆ˜ë ´í•¨ (ë§ˆì§€ë§‰ì€ ì‚´ì§ ëŒì•„ê°€ëŠ” ëŠë‚Œ)"
  },
  {
    "objectID": "posts/03wk-2.html#c.-í•™ìŠµê³¼ì •-ì‹œê°í™”-ìµœì•…ì˜-ì´ˆê¸°ê°’",
    "href": "posts/03wk-2.html#c.-í•™ìŠµê³¼ì •-ì‹œê°í™”-ìµœì•…ì˜-ì´ˆê¸°ê°’",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "C. í•™ìŠµê³¼ì • ì‹œê°í™” â€“ ìµœì•…ì˜ ì´ˆê¸°ê°’",
    "text": "C. í•™ìŠµê³¼ì • ì‹œê°í™” â€“ ìµœì•…ì˜ ì´ˆê¸°ê°’\n- MSELoss + SGD\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nnet[0].bias.data = torch.tensor([-10.0])\nnet[0].weight.data = torch.tensor([[-1.0]])\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD(net.parameters(),lr=0.05) \n#---#\nshow_animation(net,loss_fn,optimizr)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n- MSELoss + Adam\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nnet[0].bias.data = torch.tensor([-10.0])\nnet[0].weight.data = torch.tensor([[-1.0]])\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters(),lr=0.25) \n#---#\nshow_animation(net,loss_fn,optimizr)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nğŸ—£ï¸ Adamì„ ì‚¬ìš©í•˜ë‹ˆ ë¹¨ë¦¬ ë–¨ì–´ì§€ë©´ì„œ ì˜ ìˆ˜ë ´í•¨ (ë‚´ë ¤ì˜¤ëŠ” í˜ì´ ê°•í•´ì„œ ê·¸ëŸ°ì§€ ë§ˆì§€ë§‰ì€ ì‚´ì§ ëŒë‹¤ê°€ ê°€ëŠ” ëŠë‚Œ)\nğŸ—£ï¸ í˜„ì¬ ìµœì í™”ë¥¼ ì˜í•˜ê³  ì‹¶ìœ¼ë©´ Adamì„ ì‚¬ìš©í•˜ë©´ ë¨"
  },
  {
    "objectID": "posts/03wk-2.html#d.-ì°¸ê³ ìë£Œ",
    "href": "posts/03wk-2.html#d.-ì°¸ê³ ìë£Œ",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "D. ì°¸ê³ ìë£Œ",
    "text": "D. ì°¸ê³ ìë£Œ\nhttps://www.youtube.com/watch?v=MD2fYip6QsQ\n\n11:50 â€“ Momentum\n12:30 â€“ RMSprop\n15:55 â€“ Adam\nğŸ—£ï¸ local minê³¼ global minì´ ë”°ë¡œ ìˆì„ ë•Œ\n\nì¼ë°˜ì ì¸ ê²½ì‚¬í•˜ê°•ë²•ì€ ë³´í†µ local minì— ë¹ ì§\nAdamì€ local minì„ ì˜ íƒˆì¶œí•¨ (í•­ìƒì€ X)"
  },
  {
    "objectID": "posts/03wk-2.html#a.-ì‹ ë¬¸ê¸°ì‚¬-ë°ì´í„°ì˜-ëª¨í‹°ë¸Œ",
    "href": "posts/03wk-2.html#a.-ì‹ ë¬¸ê¸°ì‚¬-ë°ì´í„°ì˜-ëª¨í‹°ë¸Œ",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "A. ì‹ ë¬¸ê¸°ì‚¬ (ë°ì´í„°ì˜ ëª¨í‹°ë¸Œ)",
    "text": "A. ì‹ ë¬¸ê¸°ì‚¬ (ë°ì´í„°ì˜ ëª¨í‹°ë¸Œ)\n- ìŠ¤í™ì´ ë†’ì•„ë„ ì·¨ì—…ì´ ì•ˆëœë‹¤ê³  í•©ë‹ˆë‹¤..\nì¤‘ì†ŒÂ·ì§€ë°© ê¸°ì—… â€œë½‘ì•„ë´¤ì ê·¸ë§Œë‘ë‹ˆê¹Œâ€\nì¤‘ì†Œê¸°ì—… ê´€ê³„ìë“¤ì€ ê³ ìŠ¤í™ ì§€ì›ìë¥¼ êº¼ë¦¬ëŠ” ì´ìœ ë¡œ ë†’ì€ í‡´ì§ë¥ ì„ ê¼½ëŠ”ë‹¤. ì—¬ê±´ì´ ì¢‹ì€ ëŒ€ê¸°ì—…ìœ¼ë¡œ ì´ì§í•˜ê±°ë‚˜ íšŒì‚¬ë¥¼ ê´€ë‘ëŠ” ê²½ìš°ê°€ ë§ë‹¤ëŠ” í•˜ì†Œì—°ì´ë‹¤. ê³ ìš©ì •ë³´ì›ì´ ì§€ë‚œ 3ì¼ ê³µê°œí•œ ìë£Œì— ë”°ë¥´ë©´ ì¤‘ì†Œê¸°ì—… ì²­ë…„ì·¨ì—…ì ê°€ìš´ë° 49.5%ê°€ 2ë…„ ë‚´ì— íšŒì‚¬ë¥¼ ê·¸ë§Œë‘ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ë‹¤.\nì¤‘ì†Œ ITì—…ì²´ ê´€ê³„ìëŠ” â€œê¸°ì—… ì…ì¥ì—ì„œ ê°€ì¥ ë¼ˆì•„í”ˆ ê²Œ ì‹ ì…ì‚¬ì›ì´ ê·¸ë§Œë‘¬ì„œ ìƒˆë¡œ ë½‘ëŠ” ì¼â€ì´ë¼ë©° â€œëª…ë¬¸ëŒ€ ë‚˜ì˜¨ ìŠ¤í™ ì¢‹ì€ ì§€ì›ìë¥¼ ë½‘ì•„ë†”ë„ 1ë…„ì„ ì±„ìš°ì§€ ì•Šê³  ê·¸ë§Œë‘ëŠ” ì‚¬ì›ì´ ëŒ€ë¶€ë¶„ì´ë¼ ìš°ë¦¬ë„ ëˆˆì„ ë‚®ì¶° ì‚¬ëŒì„ ë½‘ëŠ”ë‹¤â€ê³  ë§í–ˆë‹¤."
  },
  {
    "objectID": "posts/03wk-2.html#b.-ê°€ì§œë°ì´í„°-ìŠ¤í™ì˜-ì—­ì„¤",
    "href": "posts/03wk-2.html#b.-ê°€ì§œë°ì´í„°-ìŠ¤í™ì˜-ì—­ì„¤",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "B. ê°€ì§œë°ì´í„° â€“ ìŠ¤í™ì˜ ì—­ì„¤",
    "text": "B. ê°€ì§œë°ì´í„° â€“ ìŠ¤í™ì˜ ì—­ì„¤\nğŸ—£ï¸ x: ìŠ¤í™, prob: í•©ê²©í•  í™•ë¥ \n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DL2025/main/posts/ironyofspec.csv\")\ndf\n\n\n\n\n\n\n\n\nx\nprob\ny\n\n\n\n\n0\n-1.000000\n0.000045\n0.0\n\n\n1\n-0.998999\n0.000046\n0.0\n\n\n2\n-0.997999\n0.000047\n0.0\n\n\n3\n-0.996998\n0.000047\n0.0\n\n\n4\n-0.995998\n0.000048\n0.0\n\n\n...\n...\n...\n...\n\n\n1995\n0.995998\n0.505002\n0.0\n\n\n1996\n0.996998\n0.503752\n0.0\n\n\n1997\n0.997999\n0.502501\n0.0\n\n\n1998\n0.998999\n0.501251\n1.0\n\n\n1999\n1.000000\n0.500000\n1.0\n\n\n\n\n2000 rows Ã— 3 columns\n\n\n\n\nx = torch.tensor(df.x).float().reshape(-1,1)\ny = torch.tensor(df.y).float().reshape(-1,1)\nprob = torch.tensor(df.prob).float().reshape(-1,1)\n\n\nplt.plot(x,y,'o',alpha=0.02)\nplt.plot(x[0],y[0],'o',label= r\"observed data = $(x_i,y_i)$\",color=\"C0\")\nplt.plot(x,prob,'--b',label= r\"prob (true, unknown)\")\nplt.legend()\n\n\n\n\n\n\n\n\nğŸ—£ï¸ ìŠ¤í™ì´ ë„ˆë¬´ ë†’ìœ¼ë©´ ì˜¤íˆë ¤ ë–¨ì–´ì§"
  },
  {
    "objectID": "posts/03wk-2.html#c.-ë¡œì§€ìŠ¤í‹±ìœ¼ë¡œ-ì í•©",
    "href": "posts/03wk-2.html#c.-ë¡œì§€ìŠ¤í‹±ìœ¼ë¡œ-ì í•©",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "C. ë¡œì§€ìŠ¤í‹±ìœ¼ë¡œ ì í•©",
    "text": "C. ë¡œì§€ìŠ¤í‹±ìœ¼ë¡œ ì í•©\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n)\nloss_fn = torch.nn.BCELoss()\noptimizr = torch.optim.Adam(net.parameters())\n#---# \nfor epoc in range(5000):\n    ## 1 \n    yhat = net(x)\n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'o',alpha=0.02)\nplt.plot(x[0],y[0],'o',label= r\"observed data = $(x_i,y_i)$\",color=\"C0\")\nplt.plot(x,prob,'--b',label= r\"prob (true, unknown)\")\nplt.plot(x,net(x).data, '--', label= r\"prob (estimated) = $(x_i,\\hat{y}_i)$\")\nplt.legend()\n\n\n\n\n\n\n\n\n- Epochì„ 10ì–µë²ˆìœ¼ë¡œ ì„¤ì •í•´ë„ ì´ê±´ ëª» ë§ì¶œê²ƒ ê°™ìŒ.\n\nğŸ—£ï¸\n\nì£¼í™©ìƒ‰ ì„ (model)ì´ ì˜¬ë¼ê°€ë‹¤ê°€ ë‚´ë ¤ì˜¤ëŠ” ê²ƒì€ ìµœì´ˆì˜ ê³¡ì„ ì´ ë°”ë€” ìˆ˜ ìˆëŠ” ë²”ìœ„ë¥¼ ë²—ì–´ë‚¨ (ìˆ˜ì‹ì ìœ¼ë¡œ)\nì´ëŸ° ê²½ìš° ëª¨í˜•ì˜ í‘œí˜„ë ¥ì´ ë‚®ë‹¤ê³  í‘œí˜„í•¨"
  },
  {
    "objectID": "posts/03wk-2.html#d.-ë¡œì§€ìŠ¤í‹±-í•œê³„ê·¹ë³µ-ì•„ì´ë””ì–´ë§Œ",
    "href": "posts/03wk-2.html#d.-ë¡œì§€ìŠ¤í‹±-í•œê³„ê·¹ë³µ-ì•„ì´ë””ì–´ë§Œ",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "D. ë¡œì§€ìŠ¤í‹± í•œê³„ê·¹ë³µ â€“ ì•„ì´ë””ì–´ë§Œ",
    "text": "D. ë¡œì§€ìŠ¤í‹± í•œê³„ê·¹ë³µ â€“ ì•„ì´ë””ì–´ë§Œ\nğŸ—£ï¸ ë°˜ë°˜ ì˜ë¼ì„œ í•˜ë©´ ë  ê²ƒ ê°™ìŒ\n- sigmoidë¥¼ ë„£ê¸° ì „ì˜ ìƒíƒœê°€ ì§ì„ ì´ ì•„ë‹ˆë¼ êº½ì´ëŠ” ì§ì„ ì´ì•¼ í•œë‹¤.\n\na = torch.nn.Sigmoid()\n\n\nfig,ax = plt.subplots(4,2,figsize=(8,8))\nu1 = torch.tensor([-6,-4,-2,0,2,4,6])\nu2 = torch.tensor([6,4,2,0,-2,-4,-6])\nu3 = torch.tensor([-6,-2,2,6,2,-2,-6])\nu4 = torch.tensor([-6,-2,2,6,4,2,0])\nax[0,0].plot(u1,'--o',color='C0',label = r\"$u_1$\")\nax[0,0].legend()\nax[0,1].plot(a(u1),'--o',color='C0',label = r\"$a(u_1)=\\frac{exp(u_1)}{exp(u_1)+1}$\")\nax[0,1].legend()\nax[1,0].plot(u2,'--o',color='C1',label = r\"$u_2$\")\nax[1,0].legend()\nax[1,1].plot(a(u2),'--o',color='C1',label = r\"$a(u_2)=\\frac{exp(u_2)}{exp(u_2)+1}$\")\nax[1,1].legend()\nax[2,0].plot(u3,'--o',color='C2', label = r\"$u_3$\")\nax[2,0].legend()\nax[2,1].plot(a(u3),'--o',color='C2', label = r\"$a(u_3)=\\frac{exp(u_3)}{exp(u_3)+1}$\")\nax[2,1].legend()\nax[3,0].plot(u4,'--o',color='C3', label = r\"$u_4$\")\nax[3,0].legend()\nax[3,1].plot(a(u4),'--o',color='C3', label = r\"$a(u_4)=\\frac{exp(u_4)}{exp(u_4)+1}$\")\nax[3,1].legend()"
  },
  {
    "objectID": "posts/03wk-2.html#footnotes",
    "href": "posts/03wk-2.html#footnotes",
    "title": "03wk-2: (ë¡œì§€ìŠ¤í‹±) â€“ sig(linr(x)), BCELoss, Adam, ë¡œì§€ìŠ¤í‹±ì˜ í•œê³„",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nì›ë˜ëŠ” ì´ë ‡ê²Œ ì¼ì—ˆì§€.. \\(y_i = w_0 + w_1x_i + \\epsilon_i \\quad \\epsilon_i \\sim {\\cal N}(0,\\sigma^2)\\)â†©ï¸"
  },
  {
    "objectID": "posts/05wk-2.html",
    "href": "posts/05wk-2.html",
    "title": "05wk-2: (ì‹ ê²½ë§) â€“ ì‹ ê²½ë§ì˜ í‘œí˜„, GPUì‚¬ìš©ë²•, í™•ë¥ ì ê²½ì‚¬í•˜ê°•ë²•",
    "section": "",
    "text": "ğŸ“˜ Note Format Guide\nThis format serves as a structured guide for organizing lecture content, personal interpretation, experiments, and study-related questions.\nğŸ“ ğŸ—£ï¸ âœï¸ ğŸ”¬ â“"
  },
  {
    "objectID": "posts/05wk-2.html#a.-ë¡œì§€ìŠ¤í‹±",
    "href": "posts/05wk-2.html#a.-ë¡œì§€ìŠ¤í‹±",
    "title": "05wk-2: (ì‹ ê²½ë§) â€“ ì‹ ê²½ë§ì˜ í‘œí˜„, GPUì‚¬ìš©ë²•, í™•ë¥ ì ê²½ì‚¬í•˜ê°•ë²•",
    "section": "A. ë¡œì§€ìŠ¤í‹±",
    "text": "A. ë¡œì§€ìŠ¤í‹±\n\\[\\underset{(n,1)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,1)}{\\boldsymbol u^{(1)}} \\overset{sig}{\\to} \\underset{(n,1)}{\\boldsymbol v^{(1)}} =\\underset{(n,1)}{\\hat{\\bf y}}\\]\nğŸ—£ï¸(\n\nìœ„ë¥¼ ë³´ê³  ë‹¤ìŒê³¼ ê°™ì€ í‘œí˜„ì´ ë– ì˜¬ë¼ì•¼ í•¨\n\në‹¤ìŒì€ ê´€ìŠµì  í‘œí˜„\nu: ë³´í†µ Xì— ì„ í˜•ë³€í™˜\nv: ë³´í†µ uì— ë¹„ì„ í˜•ë³€í™˜(Sigmoid, ReLU)\nv ê²°ê³¼ì— ì„ í˜•ë³€í™˜ì„ í•˜ë©´ ë˜ uë¼ê³  í•˜ê¸° ë•Œë¬¸ì— ìš°ì¸¡ìƒë‹¨ì— ìˆ«ìë¡œ êµ¬ë¶„\n\n\nnet torch.nn.Sequential(\n    torch.nn.Linear(1,1), # l1(X) = u\n    torch.nn.Sigmoid() # sig(u) = v\n)\nyhat = net(X)\n)ğŸ—£ï¸\n- ëª¨ë“  observationê³¼ ê°€ì¤‘ì¹˜ë¥¼ ëª…ì‹œí•œ ë²„ì „\n(í‘œí˜„1)\n\n\në‹¨ì : ë˜‘ê°™ì€ ê·¸ë¦¼ì˜ ë°˜ë³µì´ ë„ˆë¬´ ë§ìŒ\n\n- observation ë°˜ë³µì„ ìƒëµí•œ ë²„ì „ë“¤\n(í‘œí˜„2) ëª¨ë“  \\(i\\)ì— ëŒ€í•˜ì—¬ ì•„ë˜ì˜ ê·¸ë¦¼ì„ ë°˜ë³µí•œë‹¤ê³  í•˜ë©´ (í‘œí˜„1)ê³¼ ê°™ë‹¤.\n\n(í‘œí˜„3) ê·¸ëŸ°ë° (í‘œí˜„2)ì—ì„œ ì•„ë˜ì™€ ê°™ì´ \\(x_i\\), \\(y_i\\) ëŒ€ì‹ ì— ê°„ë‹¨íˆ \\(x\\), \\(y\\)ë¡œ ì“°ëŠ” ê²½ìš°ë„ ë§ìŒ\n\nğŸ—£ï¸ x: vector\n- 1ì„ ìƒëµí•œ ë²„ì „ë“¤\n(í‘œí˜„4) bais=False ëŒ€ì‹ ì— bias=Trueë¥¼ ì£¼ë©´ 1ì„ ìƒëµí•  ìˆ˜ ìˆìŒ\n\n(í‘œí˜„4ì˜ ìˆ˜ì •) \\(\\hat{w}_1\\)ëŒ€ì‹ ì— \\(\\hat{w}\\)ë¥¼ ì“°ëŠ” ê²ƒì´ ë” ìì—°ìŠ¤ëŸ¬ì›€\n\n(í‘œí˜„5) ì„ í˜•ë³€í™˜ì˜ ê²°ê³¼ëŠ” ì•„ë˜ì™€ ê°™ì´ \\(u\\)ë¡œ í‘œí˜„í•˜ê¸°ë„ í•œë‹¤.\n\n\në‹¤ì´ì–´ê·¸ë¨ì€ ê·¸ë¦¬ëŠ” ì‚¬ëŒì˜ ì·¨í–¥ì— ë”°ë¼ ê·¸ë¦¬ëŠ” ë°©ë²•ì´ ì¡°ê¸ˆì”© ë‹¤ë¦…ë‹ˆë‹¤. ì¦‰ êµì¬ë§ˆë‹¤ ë‹¬ë¼ìš”."
  },
  {
    "objectID": "posts/05wk-2.html#b.-ìŠ¤í™ì˜ì—­ì„¤",
    "href": "posts/05wk-2.html#b.-ìŠ¤í™ì˜ì—­ì„¤",
    "title": "05wk-2: (ì‹ ê²½ë§) â€“ ì‹ ê²½ë§ì˜ í‘œí˜„, GPUì‚¬ìš©ë²•, í™•ë¥ ì ê²½ì‚¬í•˜ê°•ë²•",
    "section": "B. ìŠ¤í™ì˜ì—­ì„¤",
    "text": "B. ìŠ¤í™ì˜ì—­ì„¤\n\\[\\underset{(n,1)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,2)}{\\boldsymbol u^{(1)}} \\overset{relu}{\\to} \\underset{(n,2)}{\\boldsymbol v^{(1)}} \\overset{l_2}{\\to} \\underset{(n,1)}{\\boldsymbol u^{(2)}} \\overset{sig}{\\to} \\underset{(n,1)}{\\boldsymbol v^{(2)}} =\\underset{(n,1)}{\\hat{\\bf y}}\\]\nì°¸ê³ : ì½”ë“œë¡œ í‘œí˜„\ntorch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=2),\n    torch.nn.ReLU(),\n    torch.nn.Linear(in_features=2,out_features=1),\n    torch.nn.Sigmoid()\n)\n- ì´í•´ë¥¼ ìœ„í•´ì„œ ì˜ˆì  ì— ë‹¤ë£¨ì—ˆë˜ ì•„ë˜ì˜ ìƒí™©ì„ ê³ ë ¤í•˜ì.\n\n(ê°•ì˜ë…¸íŠ¸ì˜ í‘œí˜„)\n\n(ì¢€ ë” ì¼ë°˜í™”ëœ í‘œí˜„) ìƒí™©ì„ ì¼ë°˜í™”í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.\n\n* Layerì˜ ê°œë…: \\({\\bf X}\\)ì—ì„œ \\(\\hat{\\boldsymbol y}\\)ë¡œ ê°€ëŠ” ê³¼ì •ì€ â€œì„ í˜•ë³€í™˜+ë¹„ì„ í˜•ë³€í™˜â€ì´ ë°˜ë³µë˜ëŠ” êµ¬ì¡°ì´ë‹¤. â€œì„ í˜•ë³€í™˜+ë¹„ì„ í˜•ë³€í™˜â€ì„ í•˜ë‚˜ì˜ ì„¸íŠ¸ë¡œ ë³´ë©´ ì•„ë˜ì™€ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.\n\n\\(\\underset{(n,1)}{\\bf X}  \\overset{l_1}{\\to} \\left( \\underset{(n,2)}{\\boldsymbol u^{(1)}} \\overset{relu}{\\to} \\underset{(n,2)}{\\boldsymbol v^{(1)}} \\right) \\overset{l_2}{\\to} \\left(\\underset{(n,1)}{\\boldsymbol u^{(2)}} \\overset{sig}{\\to} \\underset{(n,1)}{\\boldsymbol v^{(2)}}\\right), \\quad  \\underset{(n,1)}{\\boldsymbol v^{(2)}}=\\underset{(n,1)}{net({\\bf X})}=\\underset{(n,1)}{\\hat{\\bf y}}\\)\n\nğŸ—£ï¸ ì„ í˜•ë³€í™˜ -&gt; ì„ í˜•ë³€í™˜: ì•„ë¬´ëŸ° ì´ì ì´ ì—†ìŒ, ì˜ëª» ì„¤ê³„ (ë°”ë¡œ ê°€ëŠ” ì„ í˜•ë³€í™˜ì´ ìˆê¸° ë•Œë¬¸)\nì´ê²ƒì„ ë‹¤ì´ì–´ê·¸ë¨ìœ¼ë¡œ í‘œí˜„í•œë‹¤ë©´ ì•„ë˜ì™€ ê°™ë‹¤.\n(ì„ í˜•+ë¹„ì„ í˜•ì„ í•˜ë‚˜ì˜ Layerë¡œ ë¬¶ì€ í‘œí˜„)\n\nğŸ—£ï¸ Layer 0(Input Layerë¼ê³ ë„ í•¨)ì€ ì„¸ì§€ X\nLayerë¥¼ ì„¸ëŠ” ë°©ë²•\n\nì œ ë°©ì‹: í•™ìŠµê°€ëŠ¥í•œ íŒŒë¼ë©”í„°ê°€ ëª‡ì¸µìœ¼ë¡œ ìˆëŠ”ì§€â€¦ &lt;â€“ ì´ê²ƒë§Œ ê¸°ì–µí•˜ì„¸ì—¬\nì¼ë¶€ êµì¬ ì„¤ëª…: ì…ë ¥ì¸µì€ ê³„ì‚°í•˜ì§€ ì•ŠìŒ, activation layerëŠ” ê³„ì‚°í•˜ì§€ ì•ŠìŒ. &lt;â€“ ë¬´ì‹œí•˜ì„¸ìš”.. ì´ëŸ¬ë©´ í—·ê°ˆë¦½ë‹ˆë‹¤..\nìœ„ì˜ ì˜ˆì œì˜ ê²½ìš° number of layer = 2 ì´ë‹¤.\n\nHidden Layerì˜ ìˆ˜ë¥¼ ì„¸ëŠ” ë°©ë²•\n\nì œ ë°©ì‹: Hidden Layerì˜ ìˆ˜ = Layerì˜ ìˆ˜ -1 &lt;â€“ ì´ê±¸ ê¸°ì–µí•˜ì„¸ì—¬..\n\nì¼ë¶€ êµì¬ ì„¤ëª…: Layerì˜ ìˆ˜ = Hidden Layerì˜ ìˆ˜ + ì¶œë ¥ì¸µì˜ ìˆ˜ = Hidden Layerì˜ ìˆ˜ + 1 &lt;â€“ ê¸°ì–µí•˜ì§€ ë§ˆì„¸ì—¬\nìœ„ì˜ ì˜ˆì œì˜ ê²½ìš° number of hidden layer = 1 ì´ë‹¤.\n\nğŸ—£ï¸ ì´ ê²½ìš° Hiden Layer: Layer 1 / ì¶œë ¥ì¸µ: Layer 2 (yhat)\n\n\n\n\n\n\nImportant\n\n\n\në¬´ì¡°ê±´ í•™ìŠµê°€ëŠ¥í•œ íŒŒë¼ë©”í„°ê°€ ëª‡ê²¹ìœ¼ë¡œ ìˆëŠ”ì§€ë§Œ íŒë‹¨í•˜ì„¸ìš”. ë”´ê±° ì•„ë¬´ê²ƒë„ ìƒê°í•˜ì§€ë§ˆì„¸ì—¬\n## ì˜ˆì‹œ1 -- 2ì¸µ (íˆë“ ë ˆì´ì–´ëŠ” 1ì¸µ)\ntorch.nn.Sequential(\n    torch.nn.Linear(??,??), ## &lt;-- í•™ìŠµí•´ì•¼í•  ê°€ì¤‘ì¹˜ê°€ ìˆëŠ” ì¸µ\n    torch.nn.ReLU(),\n    torch.nn.Linear(??,??), ## &lt;-- í•™ìŠµí•´ì•¼í•  ê°€ì¤‘ì¹˜ê°€ ìˆëŠ” ì¸µ\n)\n## ì˜ˆì‹œ2 -- 2ì¸µ (íˆë“ ë ˆì´ì–´ëŠ” 1ì¸µ)\ntorch.nn.Sequential(\n    torch.nn.Linear(??,??), ## &lt;-- í•™ìŠµí•´ì•¼í•  ê°€ì¤‘ì¹˜ê°€ ìˆëŠ” ì¸µ\n    torch.nn.ReLU(),\n    torch.nn.Linear(??,??), ## &lt;-- í•™ìŠµí•´ì•¼í•  ê°€ì¤‘ì¹˜ê°€ ìˆëŠ” ì¸µ\n    torch.nn.Sigmoid(),\n)\n## ì˜ˆì‹œ3 -- 1ì¸µ (íˆë“ ë ˆì´ì–´ëŠ” ì—†ìŒ!!)\ntorch.nn.Sequential(\n    torch.nn.Linear(??,??), ## &lt;-- í•™ìŠµí•´ì•¼í•  ê°€ì¤‘ì¹˜ê°€ ìˆëŠ” ì¸µ\n) \n## ì˜ˆì‹œ4 -- 1ì¸µ (íˆë“ ë ˆì´ì–´ëŠ” ì—†ìŒ!!)\ntorch.nn.Sequential(\n    torch.nn.Linear(??,??), ## &lt;-- í•™ìŠµí•´ì•¼í•  ê°€ì¤‘ì¹˜ê°€ ìˆëŠ” ì¸µ\n    torch.nn.Sigmoid()\n) \n## ì˜ˆì‹œ5 -- 3ì¸µ (íˆë“ ë ˆì´ì–´ëŠ” 2ì¸µ)\ntorch.nn.Sequential(\n    torch.nn.Linear(??,??), ## &lt;-- í•™ìŠµí•´ì•¼í•  ê°€ì¤‘ì¹˜ê°€ ìˆëŠ” ì¸µ\n    torch.nn.Sigmoid()\n    torch.nn.Linear(??,??), ## &lt;-- í•™ìŠµí•´ì•¼í•  ê°€ì¤‘ì¹˜ê°€ ìˆëŠ” ì¸µ\n    torch.nn.Sigmoid()\n    torch.nn.Linear(??,??), ## &lt;-- í•™ìŠµí•´ì•¼í•  ê°€ì¤‘ì¹˜ê°€ ìˆëŠ” ì¸µ    \n) \n## ì˜ˆì‹œ6 -- 3ì¸µ (íˆë“ ë ˆì´ì–´ëŠ” 2ì¸µ)\ntorch.nn.Sequential(\n    torch.nn.Linear(??,??), ## &lt;-- í•™ìŠµí•´ì•¼í•  ê°€ì¤‘ì¹˜ê°€ ìˆëŠ” ì¸µ\n    torch.nn.ReLU()\n    torch.nn.Dropout(??)\n    torch.nn.Linear(??,??), ## &lt;-- í•™ìŠµí•´ì•¼í•  ê°€ì¤‘ì¹˜ê°€ ìˆëŠ” ì¸µ\n    torch.nn.ReLU()\n    torch.nn.Dropout(??)\n    torch.nn.Linear(??,??), ## &lt;-- í•™ìŠµí•´ì•¼í•  ê°€ì¤‘ì¹˜ê°€ ìˆëŠ” ì¸µ  \n    torch.nn.Sigmoid()\n) \n\n\n\n\n\n\n\n\nImportant\n\n\n\në¬¸í—Œì— ë”°ë¼ì„œ ë ˆì´ì–´ë¥¼ ì„¸ëŠ” ê°œë…ì´ ì œê°€ ì„¤ëª…í•œ ë°©ì‹ê³¼ ë‹¤ë¥¸ê²½ìš°ê°€ ìˆìŠµë‹ˆë‹¤. ì œê°€ ì„¤ëª…í•œ ë°©ì‹ë³´ë‹¤ 1ì”© ë”í•´ì„œ ì…‰ë‹ˆë‹¤. ì¦‰ ì•„ë˜ì˜ ê²½ìš° ë ˆì´ì–´ë¥¼ 3ê°œë¡œ ì¹´ìš´íŠ¸í•©ë‹ˆë‹¤.\n## ì˜ˆì‹œ1 -- ë¬¸í—Œì— ë”°ë¼ 3ì¸µìœ¼ë¡œ ì„¸ëŠ” ê²½ìš°ê°€ ìˆìŒ (íˆë“ ë ˆì´ì–´ëŠ” 1ì¸µ)\ntorch.nn.Sequential(\n    torch.nn.Linear(??,??), ## &lt;-- í•™ìŠµí•´ì•¼í•  ê°€ì¤‘ì¹˜ê°€ ìˆëŠ” ì¸µ\n    torch.nn.ReLU(),\n    torch.nn.Linear(??,??), ## &lt;-- í•™ìŠµí•´ì•¼í•  ê°€ì¤‘ì¹˜ê°€ ìˆëŠ” ì¸µ\n    torch.nn.Sigmoid()\n)\nì˜ˆë¥¼ ë“¤ì–´ ì—¬ê¸°ì—ì„œëŠ” ìœ„ì˜ ê²½ìš° ë ˆì´ì–´ëŠ” 3ê°œë¼ê³  ì„¤ëª…í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì¹´ìš´íŒ…ì€ â€œë¬´ì‹œâ€í•˜ì„¸ìš”. ì œê°€ ì„¤ëª…í•œ ë°©ì‹ì´ ë§ì•„ìš”. ì´ ë§í¬ ì˜ëª»(?) ë‚˜ì™€ìˆëŠ” ì´ìœ ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.\n- ì§„ì§œ ì˜ˆì „ì— MLPë¥¼ ì†Œê°œí•  ì´ˆì°½ê¸°ì—ì„œëŠ” ìœ„ì˜ ê²½ìš° Layerë¥¼ 3ê°œë¡œ ì…ŒìŒ. [@rosenblatt1962principles]\n- ê·¸ëŸ°ë° ìš”ì¦˜ì€ ê·¸ë ‡ê²Œ ì•ˆì…ˆ.. (ê·¸ë¦¬ê³  ì• ì´ˆì— MLPë¼ëŠ” ìš©ì–´ë„ ì˜ ì•ˆì“°ì£ ..)\nì°¸ê³ ë¡œ íˆë“ ë ˆì´ì–´ì˜ ìˆ˜ëŠ” ì˜ˆì „ë°©ì‹ì´ë‚˜ ì§€ê¸ˆë°©ì‹ì´ë‚˜ ë™ì¼í•˜ê²Œ ì¹´ìš´íŠ¸í•˜ë¯€ë¡œ íˆë“ ë ˆì´ì–´ë§Œ ì„¸ë©´ í˜¼ëˆì´ ì—†ìŠµë‹ˆë‹¤.\n\n\n\nğŸ—£ï¸\n\nìš”ì¦˜ì€ MLPë³´ë‹¤ëŠ” DNN ìš©ì–´ ì‚¬ìš©\nìš”ì¦˜ì€ Dropoutë³´ë‹¤ Batch Normalizationì„ ì‚¬ìš©\n\nBatch Normalizationì€ í•™ìŠµ parameterê°€ ìˆìŒ\nê·¸ëŸ¬ë‚˜ layerë¡œ ì„¸ì§€ëŠ” X\n\nìš”ì¦˜ì€ layerê°€ ëª‡ ì¸µì¸ì§€ êµ³ì´ ë”°ì§€ì§€ëŠ” X\n\n\n* nodeì˜ ê°œë…: \\(u\\to v\\)ë¡œ ê°€ëŠ” ìŒì„ ê°„ë‹¨íˆ ë…¸ë“œë¼ëŠ” ê°œë…ì„ ì´ìš©í•˜ì—¬ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŒ.\n(ë…¸ë“œì˜ ê°œë…ì´ í¬í•¨ëœ ê·¸ë¦¼)\n\nì—¬ê¸°ì—ì„œ nodeì˜ ìˆ«ì = featureì˜ ìˆ«ìì™€ ê°™ì´ ì´í•´í•  ìˆ˜ ìˆë‹¤. ì¦‰ ì•„ë˜ì™€ ê°™ì´ ì´í•´í•  ìˆ˜ ìˆë‹¤.\n(â€œnumber of nodes = number of featuresâ€ë¡œ ì´í•´í•œ ê·¸ë¦¼)\n\n\në‹¤ì´ì–´ê·¸ë¨ì˜ í‘œí˜„ë°©ì‹ì€ êµì¬ë§ˆë‹¤ ë‹¬ë¼ì„œ ëª¨ë“  ì˜ˆì‹œë¥¼ ë‹¬ë‹¬ ì™¸ìš¸ í•„ìš”ëŠ” ì—†ìŠµë‹ˆë‹¤. ë‹¤ë§Œ ì„ì˜ì˜ ë‹¤ì´ì–´ê·¸ë¨ì„ ë³´ê³  ëŒ€ì‘í•˜ëŠ” ë„¤íŠ¸ì›Œí¬ë¥¼ pytorchë¡œ êµ¬í˜„í•˜ëŠ” ëŠ¥ë ¥ì€ ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/05wk-2.html#c.-mnist",
    "href": "posts/05wk-2.html#c.-mnist",
    "title": "05wk-2: (ì‹ ê²½ë§) â€“ ì‹ ê²½ë§ì˜ í‘œí˜„, GPUì‚¬ìš©ë²•, í™•ë¥ ì ê²½ì‚¬í•˜ê°•ë²•",
    "section": "C. MNIST",
    "text": "C. MNIST\n\\[\\underset{(n,784)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,32)}{\\boldsymbol u^{(1)}} \\overset{relu}{\\to} \\underset{(n,32)}{\\boldsymbol v^{(1)}} \\overset{l_2}{\\to} \\underset{(n,1)}{\\boldsymbol u^{(2)}} \\overset{sig}{\\to} \\underset{(n,1)}{\\boldsymbol v^{(2)}}=\\underset{(n,1)}{\\hat{\\boldsymbol y}}\\]\n(ë‹¤ì´ì–´ê·¸ë¨í‘œí˜„)\n\nğŸ—£ï¸ í•™ìŠµ ê°€ëŠ¥í•œ parameter ìˆ˜: 784*32 (\\(X_{(n,784)}@w_{?} = (n,32)\\))\n\nLayer0,1,2 ëŒ€ì‹ ì— Input Layer, Hidden Layer, Output Layerë¡œ í‘œí˜„í•¨\n\n- ìœ„ì˜ ë‹¤ì´ì–´ê·¸ë¨ì— ëŒ€ì‘í•˜ëŠ” ì½”ë“œ\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=28*28*1,out_features=32),\n    torch.nn.ReLU(),\n    torch.nn.Linear(in_features=32,out_features=1),\n    torch.nn.Sigmoid() \n)"
  },
  {
    "objectID": "posts/05wk-2.html#a.-gpu-ì‚¬ìš©ë°©ë²•",
    "href": "posts/05wk-2.html#a.-gpu-ì‚¬ìš©ë°©ë²•",
    "title": "05wk-2: (ì‹ ê²½ë§) â€“ ì‹ ê²½ë§ì˜ í‘œí˜„, GPUì‚¬ìš©ë²•, í™•ë¥ ì ê²½ì‚¬í•˜ê°•ë²•",
    "section": "A. GPU ì‚¬ìš©ë°©ë²•",
    "text": "A. GPU ì‚¬ìš©ë°©ë²•\n- cpu ì—°ì‚°ì´ ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ì— ë°ì´í„° ì €ì¥\n\ntorch.manual_seed(43052)\nx_cpu = torch.tensor([0.0,0.1,0.2]).reshape(-1,1) \ny_cpu = torch.tensor([0.0,0.2,0.4]).reshape(-1,1) \nnet_cpu = torch.nn.Linear(1,1) \n\n\nnet_cpu(x_cpu)\n\ntensor([[-0.8470],\n        [-0.8817],\n        [-0.9164]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\n\nx_cpu\n\ntensor([[0.0000],\n        [0.1000],\n        [0.2000]])\n\n\n- gpu ì—°ì‚°ì´ ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ì— ë°ì´í„° ì €ì¥\n\n!nvidia-smi # before\n\nMon Apr  7 09:48:42 2025       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.230.02             Driver Version: 535.230.02   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  NVIDIA GeForce RTX 3090        Off | 00000000:09:00.0 Off |                  N/A |\n|  0%   29C    P8              27W / 420W |     26MiB / 24576MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|    0   N/A  N/A      1152      G   /usr/lib/xorg/Xorg                            9MiB |\n|    0   N/A  N/A      1471      G   /usr/bin/gnome-shell                          8MiB |\n+---------------------------------------------------------------------------------------+\n\n\nğŸ”¬ ?\n\n!nvidia-smi # before\n\nTue May  6 15:07:19 2025       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  NVIDIA A100-SXM4-80GB          On  | 00000000:47:00.0 Off |                   On |\n| N/A   67C    P0             197W / 275W |                  N/A |     N/A      Default |\n|                                         |                      |              Enabled |\n+-----------------------------------------+----------------------+----------------------+\n\n+---------------------------------------------------------------------------------------+\n| MIG devices:                                                                          |\n+------------------+--------------------------------+-----------+-----------------------+\n| GPU  GI  CI  MIG |                   Memory-Usage |        Vol|      Shared           |\n|      ID  ID  Dev |                     BAR1-Usage | SM     Unc| CE ENC DEC OFA JPG    |\n|                  |                                |        ECC|                       |\n|==================+================================+===========+=======================|\n|  0    0   0   0  |           49841MiB / 81050MiB  | 98      0 |  7   0    5    1    1 |\n|                  |              15MiB / 131072MiB |           |                       |\n+------------------+--------------------------------+-----------+-----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n+---------------------------------------------------------------------------------------+\n\n\nğŸ—£ï¸(\n\nx_cpu\n\ntensor([[0.0000],\n        [0.1000],\n        [0.2000]])\n\n\n\nx_cpu.to(\"cuda:0\")\n\ntensor([[0.0000],\n        [0.1000],\n        [0.2000]], device='cuda:0')\n\n\n\ndevice=â€˜cuda:0â€™\n\n)ğŸ—£ï¸\nğŸ—£ï¸ :0 =&gt; GPU ID (ì—¬ëŸ¬ê°œì¸ ê²½ìš°)\n\ntorch.manual_seed(43052)\nx_gpu = x_cpu.to(\"cuda:0\")\ny_gpu = y_cpu.to(\"cuda:0\")\nnet_gpu = torch.nn.Linear(1,1).to(\"cuda:0\") \n\nğŸ—£ï¸ ì¼ë°˜ì ìœ¼ë¡œëŠ” ë©”ëª¨ë¦¬ì— ì €ì¥ë˜ì§€ë§Œ, to(â€œcuda:0â€)ë¥¼ í•˜ë©´ GPU ë©”ëª¨ë¦¬ì— ì €ì¥ë¨\n\n!nvidia-smi\n\nMon Apr  7 09:48:43 2025       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.230.02             Driver Version: 535.230.02   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  NVIDIA GeForce RTX 3090        Off | 00000000:09:00.0 Off |                  N/A |\n|  0%   34C    P2              65W / 420W |    287MiB / 24576MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|    0   N/A  N/A      1152      G   /usr/lib/xorg/Xorg                            9MiB |\n|    0   N/A  N/A      1471      G   /usr/bin/gnome-shell                          8MiB |\n|    0   N/A  N/A    140211      C   ...b3/anaconda3/envs/dl2025/bin/python      256MiB |\n+---------------------------------------------------------------------------------------+\n\n\nğŸ”¬ ?\n\n!nvidia-smi\n\nTue May  6 15:11:18 2025       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  NVIDIA A100-SXM4-80GB          On  | 00000000:47:00.0 Off |                   On |\n| N/A   68C    P0             186W / 275W |                  N/A |     N/A      Default |\n|                                         |                      |              Enabled |\n+-----------------------------------------+----------------------+----------------------+\n\n+---------------------------------------------------------------------------------------+\n| MIG devices:                                                                          |\n+------------------+--------------------------------+-----------+-----------------------+\n| GPU  GI  CI  MIG |                   Memory-Usage |        Vol|      Shared           |\n|      ID  ID  Dev |                     BAR1-Usage | SM     Unc| CE ENC DEC OFA JPG    |\n|                  |                                |        ECC|                       |\n|==================+================================+===========+=======================|\n|  0    0   0   0  |           50298MiB / 81050MiB  | 98      0 |  7   0    5    1    1 |\n|                  |              17MiB / 131072MiB |           |                       |\n+------------------+--------------------------------+-----------+-----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n+---------------------------------------------------------------------------------------+\n\n\n\nGPUì— ë©”ëª¨ë¦¬ë¥¼ ì˜¬ë¦¬ë©´ GPUë©”ëª¨ë¦¬ê°€ ì ìœ ëœë‹¤! (26MiB -&gt; 287MiB)\n\n- cpu í˜¹ì€ gpu ì—°ì‚°ì´ ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ì— ì €ì¥ëœ ê°’ë“¤ì„ í™•ì¸\n\nx_cpu, y_cpu, net_cpu.weight, net_cpu.bias\n\n(tensor([[0.0000],\n         [0.1000],\n         [0.2000]]),\n tensor([[0.0000],\n         [0.2000],\n         [0.4000]]),\n Parameter containing:\n tensor([[-0.3467]], requires_grad=True),\n Parameter containing:\n tensor([-0.8470], requires_grad=True))\n\n\n\nx_gpu, y_gpu, net_gpu.weight, net_gpu.bias\n\n(tensor([[0.0000],\n         [0.1000],\n         [0.2000]], device='cuda:0'),\n tensor([[0.0000],\n         [0.2000],\n         [0.4000]], device='cuda:0'),\n Parameter containing:\n tensor([[-0.3467]], device='cuda:0', requires_grad=True),\n Parameter containing:\n tensor([-0.8470], device='cuda:0', requires_grad=True))\n\n\n- gpuëŠ” gpuë¼ë¦¬ ì—°ì‚°ê°€ëŠ¥í•˜ê³  cpuëŠ” cpuë¼ë¦¬ ì—°ì‚°ê°€ëŠ¥í•¨\n(ì˜ˆì‹œ1)\n\nnet_cpu(x_cpu) \n\ntensor([[-0.8470],\n        [-0.8817],\n        [-0.9164]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\n(ì˜ˆì‹œ2)\n\nnet_gpu(x_gpu) \n\ntensor([[-0.8470],\n        [-0.8817],\n        [-0.9164]], device='cuda:0', grad_fn=&lt;AddmmBackward0&gt;)\n\n\n(ì˜ˆì‹œ3)\n\nnet_cpu(x_gpu) \n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[15], line 1\n----&gt; 1 net_cpu(x_gpu) \n\nFile ~/anaconda3/envs/dl2025/lib/python3.9/site-packages/torch/nn/modules/module.py:1739, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1737     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1738 else:\n-&gt; 1739     return self._call_impl(*args, **kwargs)\n\nFile ~/anaconda3/envs/dl2025/lib/python3.9/site-packages/torch/nn/modules/module.py:1750, in Module._call_impl(self, *args, **kwargs)\n   1745 # If we don't have any hooks, we want to skip the rest of the logic in\n   1746 # this function, and just call forward.\n   1747 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1748         or _global_backward_pre_hooks or _global_backward_hooks\n   1749         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1750     return forward_call(*args, **kwargs)\n   1752 result = None\n   1753 called_always_called_hooks = set()\n\nFile ~/anaconda3/envs/dl2025/lib/python3.9/site-packages/torch/nn/modules/linear.py:125, in Linear.forward(self, input)\n    124 def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 125     return F.linear(input, self.weight, self.bias)\n\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n\n\n\n(ì˜ˆì‹œ4)\n\nnet_gpu(x_cpu)\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[16], line 1\n----&gt; 1 net_gpu(x_cpu)\n\nFile ~/anaconda3/envs/dl2025/lib/python3.9/site-packages/torch/nn/modules/module.py:1739, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1737     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1738 else:\n-&gt; 1739     return self._call_impl(*args, **kwargs)\n\nFile ~/anaconda3/envs/dl2025/lib/python3.9/site-packages/torch/nn/modules/module.py:1750, in Module._call_impl(self, *args, **kwargs)\n   1745 # If we don't have any hooks, we want to skip the rest of the logic in\n   1746 # this function, and just call forward.\n   1747 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1748         or _global_backward_pre_hooks or _global_backward_hooks\n   1749         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1750     return forward_call(*args, **kwargs)\n   1752 result = None\n   1753 called_always_called_hooks = set()\n\nFile ~/anaconda3/envs/dl2025/lib/python3.9/site-packages/torch/nn/modules/linear.py:125, in Linear.forward(self, input)\n    124 def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 125     return F.linear(input, self.weight, self.bias)\n\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n\n\n\n(ì˜ˆì‹œ5)\n\ntorch.mean((y_cpu-net_cpu(x_cpu))**2)\n\ntensor(1.2068, grad_fn=&lt;MeanBackward0&gt;)\n\n\n(ì˜ˆì‹œ6)\n\ntorch.mean((y_gpu-net_gpu(x_gpu))**2)\n\ntensor(1.2068, device='cuda:0', grad_fn=&lt;MeanBackward0&gt;)\n\n\n(ì˜ˆì‹œ7)\n\ntorch.mean((y_gpu-net_cpu(x_cpu))**2)\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[19], line 1\n----&gt; 1 torch.mean((y_gpu-net_cpu(x_cpu))**2)\n\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\n\n\n\n(ì˜ˆì‹œ8)\n\ntorch.mean((y_cpu-net_gpu(x_gpu))**2)\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[20], line 1\n----&gt; 1 torch.mean((y_cpu-net_gpu(x_gpu))**2)\n\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
  },
  {
    "objectID": "posts/05wk-2.html#b.-ì‹œê°„ì¸¡ì •-ì˜ˆë¹„í•™ìŠµ",
    "href": "posts/05wk-2.html#b.-ì‹œê°„ì¸¡ì •-ì˜ˆë¹„í•™ìŠµ",
    "title": "05wk-2: (ì‹ ê²½ë§) â€“ ì‹ ê²½ë§ì˜ í‘œí˜„, GPUì‚¬ìš©ë²•, í™•ë¥ ì ê²½ì‚¬í•˜ê°•ë²•",
    "section": "B. ì‹œê°„ì¸¡ì • (ì˜ˆë¹„í•™ìŠµ)",
    "text": "B. ì‹œê°„ì¸¡ì • (ì˜ˆë¹„í•™ìŠµ)\n\nimport time \n\n\ntime.time()\n\n1746512391.3237932\n\n\nğŸ—£ï¸ ë­”ì§€ëŠ” ëª¨ë¥´ê² ì§€ë§Œ ì°¨ì´ëŠ” ì•Œ ìˆ˜ ìˆìŒ\n\nt1 = time.time()\n\n\nt2 = time.time()\n\n\nt2-t1\n\n4.3513031005859375"
  },
  {
    "objectID": "posts/05wk-2.html#c.-cpu-vs-gpu-500-nodes",
    "href": "posts/05wk-2.html#c.-cpu-vs-gpu-500-nodes",
    "title": "05wk-2: (ì‹ ê²½ë§) â€“ ì‹ ê²½ë§ì˜ í‘œí˜„, GPUì‚¬ìš©ë²•, í™•ë¥ ì ê²½ì‚¬í•˜ê°•ë²•",
    "section": "C. CPU vs GPU (500 nodes)",
    "text": "C. CPU vs GPU (500 nodes)\n- CPU (500 nodes)\n\ntorch.manual_seed(5) \nx=torch.linspace(0,1,100).reshape(-1,1)\ny=torch.randn(100).reshape(-1,1)*0.01\n#---#\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,500),\n    torch.nn.ReLU(),\n    torch.nn.Linear(500,1)\n)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n#---#\nt1 = time.time()\nfor epoc in range(1000):\n    # 1 \n    yhat = net(x)\n    # 2 \n    loss = loss_fn(yhat,y)\n    # 3 \n    loss.backward()\n    # 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2-t1\n\n0.36923766136169434\n\n\nğŸ”¬\n\ntorch.manual_seed(5) \nx=torch.linspace(0,1,100).reshape(-1,1)\ny=torch.randn(100).reshape(-1,1)*0.01\n#---#\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,500),\n    torch.nn.ReLU(),\n    torch.nn.Linear(500,1)\n)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n#---#\nt1 = time.time()\nfor epoc in range(1000):\n    # 1 \n    yhat = net(x)\n    # 2 \n    loss = loss_fn(yhat,y)\n    # 3 \n    loss.backward()\n    # 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2-t1\n\n0.8697936534881592\n\n\n- GPU (500 nodes)\n\ntorch.manual_seed(5) \nx=torch.linspace(0,1,100).reshape(-1,1).to(\"cuda:0\")\ny=(torch.randn(100).reshape(-1,1)*0.01).to(\"cuda:0\")\n#---#\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,500),\n    torch.nn.ReLU(),\n    torch.nn.Linear(500,1)\n).to(\"cuda:0\")\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n#---#\nt1 = time.time()\nfor epoc in range(1000):\n    # 1 \n    yhat = net(x)\n    # 2 \n    loss = loss_fn(yhat,y)\n    # 3 \n    loss.backward()\n    # 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2-t1\n\n0.5803208351135254\n\n\nğŸ”¬\n\ntorch.manual_seed(5) \nx=torch.linspace(0,1,100).reshape(-1,1).to(\"cuda:0\")\ny=(torch.randn(100).reshape(-1,1)*0.01).to(\"cuda:0\")\n#---#\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,500),\n    torch.nn.ReLU(),\n    torch.nn.Linear(500,1)\n).to(\"cuda:0\")\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n#---#\nt1 = time.time()\nfor epoc in range(1000):\n    # 1 \n    yhat = net(x)\n    # 2 \n    loss = loss_fn(yhat,y)\n    # 3 \n    loss.backward()\n    # 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2-t1\n\n0.8882694244384766\n\n\n\nCPUê°€ ë” ë¹ ë¥´ë‹¤??"
  },
  {
    "objectID": "posts/05wk-2.html#d.-cpu-vs-gpu-200000-nodes",
    "href": "posts/05wk-2.html#d.-cpu-vs-gpu-200000-nodes",
    "title": "05wk-2: (ì‹ ê²½ë§) â€“ ì‹ ê²½ë§ì˜ í‘œí˜„, GPUì‚¬ìš©ë²•, í™•ë¥ ì ê²½ì‚¬í•˜ê°•ë²•",
    "section": "D. CPU vs GPU (200,000 nodes)",
    "text": "D. CPU vs GPU (200,000 nodes)\n- CPU (200,000)\n\ntorch.manual_seed(5) \nx=torch.linspace(0,1,100).reshape(-1,1)\ny=torch.randn(100).reshape(-1,1)*0.01\n#---#\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,200000),\n    torch.nn.ReLU(),\n    torch.nn.Linear(200000,1)\n)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n#---#\nt1 = time.time()\nfor epoc in range(1000):\n    # 1 \n    yhat = net(x)\n    # 2 \n    loss = loss_fn(yhat,y)\n    # 3 \n    loss.backward()\n    # 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2-t1\n\n84.05620455741882\n\n\nğŸ”¬\n\ntorch.manual_seed(5) \nx=torch.linspace(0,1,100).reshape(-1,1)\ny=torch.randn(100).reshape(-1,1)*0.01\n#---#\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,200000),\n    torch.nn.ReLU(),\n    torch.nn.Linear(200000,1)\n)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n#---#\nt1 = time.time()\nfor epoc in range(1000):\n    # 1 \n    yhat = net(x)\n    # 2 \n    loss = loss_fn(yhat,y)\n    # 3 \n    loss.backward()\n    # 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2-t1\n\n83.98482537269592\n\n\n- GPU (204,800)\n\ntorch.manual_seed(5) \nx=torch.linspace(0,1,100).reshape(-1,1).to(\"cuda:0\")\ny=(torch.randn(100).reshape(-1,1)*0.01).to(\"cuda:0\")\n#---#\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,200000),\n    torch.nn.ReLU(),\n    torch.nn.Linear(200000,1)\n).to(\"cuda:0\")\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n#---#\nt1 = time.time()\nfor epoc in range(1000):\n    # 1 \n    yhat = net(x)\n    # 2 \n    loss = loss_fn(yhat,y)\n    # 3 \n    loss.backward()\n    # 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2-t1\n\n1.373826026916504\n\n\nğŸ”¬\n\ntorch.manual_seed(5) \nx=torch.linspace(0,1,100).reshape(-1,1).to(\"cuda:0\")\ny=(torch.randn(100).reshape(-1,1)*0.01).to(\"cuda:0\")\n#---#\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,200000),\n    torch.nn.ReLU(),\n    torch.nn.Linear(200000,1)\n).to(\"cuda:0\")\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n#---#\nt1 = time.time()\nfor epoc in range(1000):\n    # 1 \n    yhat = net(x)\n    # 2 \n    loss = loss_fn(yhat,y)\n    # 3 \n    loss.backward()\n    # 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2-t1\n\n2.6831233501434326\n\n\n\nğŸ—£ï¸\n\ní•­ìƒ GPUê°€ ë¹ ë¥¸ ê²ƒì€ ì•„ë‹˜ (nodeê°€ ì»¤ì§€ë©´ GPUê°€ ìœ ë¦¬)\nCPUëŠ” ì½”ì–´ê°€ ë§ì•„ì•¼ 60ì—¬ê°œ, GPUëŠ” ì½”ì–´ê°€ ë§Œ ê°œ ë‹¨ìœ„\n\nì™œ ì´ëŸ° ì°¨ì´ê°€ ë‚˜ëŠ”ê°€?\nì—°ì‚°ì„ í•˜ëŠ” ì£¼ì²´ëŠ” ì½”ì–´ì¸ë° CPUëŠ” ìˆ˜ëŠ” ì ì§€ë§Œ ì¼ì„ ì˜í•˜ëŠ” ì½”ì–´ë“¤ì„ ê°€ì§€ê³  ìˆê³  GPUëŠ” ì¼ì€ ëª»í•˜ì§€ë§Œ ë‹¤ìˆ˜ì˜ ì½”ì–´ë¥¼ ê°€ì§€ê³  ìˆê¸° ë•Œë¬¸"
  },
  {
    "objectID": "posts/05wk-2.html#e.-ì£¼ì˜ì ",
    "href": "posts/05wk-2.html#e.-ì£¼ì˜ì ",
    "title": "05wk-2: (ì‹ ê²½ë§) â€“ ì‹ ê²½ë§ì˜ í‘œí˜„, GPUì‚¬ìš©ë²•, í™•ë¥ ì ê²½ì‚¬í•˜ê°•ë²•",
    "section": "E. ì£¼ì˜ì ",
    "text": "E. ì£¼ì˜ì \n- tensor ì¼ ê²½ìš°\n\nx = torch.tensor([1,2,3])\nx.to(\"cuda:0\"), x\n\n(tensor([1, 2, 3], device='cuda:0'), tensor([1, 2, 3]))\n\n\n- netì¼ ê²½ìš°\n\nnet = torch.nn.Linear(1,1).to(\"cuda:0\")\nnet.weight, net.bias\n\n(Parameter containing:\n tensor([[-0.0084]], device='cuda:0', requires_grad=True),\n Parameter containing:\n tensor([-0.6216], device='cuda:0', requires_grad=True))\n\n\nğŸ—£ï¸(\n\nnetì€ ê°’ ìì²´ê°€ í†µì§¸ë¡œ cudaë¡œ ê°\n\n\nnet_cpu = torch.nn.Linear(1,1)\nnet_cpu\n\nLinear(in_features=1, out_features=1, bias=True)\n\n\n\nnet_gpu = net_cpu.to(\"cuda:0\") # ì´ê±°ë¥¼ ì‹¤í–‰í•˜ëŠ” ìˆœê°„ cudaë¡œ ê°€ê¸° ë•Œë¬¸ì—\n\n\nnet_cpu.weight # ë” ì´ìƒ cpuë¡œ ë¶€ë¥¼ ìˆ˜ ì—†ìŒ\n\nParameter containing:\ntensor([[0.1766]], device='cuda:0', requires_grad=True)\n\n\n\në¹„êµ\n\n\nx_cpu = torch.tensor([1,2,3])\nx_cpu\n\ntensor([1, 2, 3])\n\n\n\nx_gpu = x_cpu.to(\"cuda:0\")\n\n\nx_cpu\n\ntensor([1, 2, 3])\n\n\n\nx_gpu\n\ntensor([1, 2, 3], device='cuda:0')\n\n\n)ğŸ—£ï¸"
  },
  {
    "objectID": "posts/05wk-2.html#a.-ì˜ë¬¸-ì¢€-ì´ìƒí•˜ì§€-ì•Šì•„ìš”",
    "href": "posts/05wk-2.html#a.-ì˜ë¬¸-ì¢€-ì´ìƒí•˜ì§€-ì•Šì•„ìš”",
    "title": "05wk-2: (ì‹ ê²½ë§) â€“ ì‹ ê²½ë§ì˜ í‘œí˜„, GPUì‚¬ìš©ë²•, í™•ë¥ ì ê²½ì‚¬í•˜ê°•ë²•",
    "section": "A. ì˜ë¬¸: ì¢€ ì´ìƒí•˜ì§€ ì•Šì•„ìš”?",
    "text": "A. ì˜ë¬¸: ì¢€ ì´ìƒí•˜ì§€ ì•Šì•„ìš”?\n- êµ­ë¯¼ìƒì‹: GPU ë¹„ì‹¸ìš”.. https://bbs.ruliweb.com/community/board/300143/read/61066881\n\nGPU ë©”ëª¨ë¦¬ ë§ì•„ë´ì•¼ 24GB, ê·¸ë˜ë„ ë¹„ì‹¸ìš”.. http://shop.danawa.com/virtualestimate/?controller=estimateMain&methods=index&marketPlaceSeq=16\nGPU ë©”ëª¨ë¦¬ê°€ 80GBì¼ ê²½ìš° ê°€ê²©: https://prod.danawa.com/info/?pcode=21458333\n\n- ìš°ë¦¬ê°€ ë¶„ì„í•˜ëŠ” ë°ì´í„°\n\nx = torch.linspace(-10,10,100000).reshape(-1,1)\neps = torch.randn(100000).reshape(-1,1)\ny = x*2 + eps \n\n\nplt.plot(x,y,'.',alpha=0.05)\nplt.plot(x,2*x,'--')\n\n\n\n\n\n\n\n\n\nlen(x)\n\n100000\n\n\n- ë°ì´í„°ì˜ í¬ê¸°ê°€ ì»¤ì§€ëŠ” ìˆœê°„ x.to(\"cuda:0\"), y.to(\"cuda:0\") ì“°ë©´ ë‚œë¦¬ë‚˜ê² ëŠ”ê±¸? \\(\\to\\) ì´ëŸ°ì‹ì´ë©´ GPUë¥¼ ì´ìš©í•˜ì—¬ ì•„ë¬´ëŸ° ë¶„ì„ë„ ëª»í• ê²ƒ ê°™ì€ë°?? ë­”ê°€ ì¢€ ì´ìƒí•œë°??\n- ì•„ì´ë””ì–´: ë°ì´í„°ë¥¼ 100ê°œì¤‘ì— 1ê°œ ê¼´ë¡œë§Œ ì“°ë©´ ì–´ë–¨ê¹Œ?\n\nx[::2].shape\n\ntorch.Size([50000, 1])\n\n\n\nx[::100].shape\n\ntorch.Size([1000, 1])\n\n\n\nplt.plot(x[::100],y[::100],'o',alpha=0.05)\nplt.plot(x,2*x,'--')\n\n\n\n\n\n\n\n\n\nëŒ€ì¶© ì´ê±°ë§Œ ê°€ì§€ê³  ì í•©í•´ë„ ì¶©ë¶„íˆ ì •í™•í• ê²ƒ ê°™ì€ë°?"
  },
  {
    "objectID": "posts/05wk-2.html#b.-xy-ë°ì´í„°ë¥¼-êµ³ì´-ëª¨ë‘-gpuì—-ë„˜ê²¨ì•¼-í•˜ëŠ”ê°€",
    "href": "posts/05wk-2.html#b.-xy-ë°ì´í„°ë¥¼-êµ³ì´-ëª¨ë‘-gpuì—-ë„˜ê²¨ì•¼-í•˜ëŠ”ê°€",
    "title": "05wk-2: (ì‹ ê²½ë§) â€“ ì‹ ê²½ë§ì˜ í‘œí˜„, GPUì‚¬ìš©ë²•, í™•ë¥ ì ê²½ì‚¬í•˜ê°•ë²•",
    "section": "B. X,y ë°ì´í„°ë¥¼ êµ³ì´ ëª¨ë‘ GPUì— ë„˜ê²¨ì•¼ í•˜ëŠ”ê°€?",
    "text": "B. X,y ë°ì´í„°ë¥¼ êµ³ì´ ëª¨ë‘ GPUì— ë„˜ê²¨ì•¼ í•˜ëŠ”ê°€?\n- ë°ì´í„°ì…‹ì„ ì§í™€ë¡œ ë‚˜ëˆ„ì–´ì„œ ë²ˆê°ˆì•„ê°€ë©´ì„œ GPUì— ì˜¬ë ¸ë‹¤ ë‚´ë ¸ë‹¤í•˜ë©´ ì•ˆë˜ë‚˜?\n- ì•„ë˜ì˜ ì•Œê³ ë¦¬ì¦˜ì„ ìƒê°í•´ë³´ì.\n\në°ì´í„°ë¥¼ ë°˜ìœ¼ë¡œ ë‚˜ëˆˆë‹¤.\nì§ìˆ˜obsì˜ x,y ê·¸ë¦¬ê³  netì˜ ëª¨ë“  íŒŒë¼ë©”í„°ë¥¼ GPUì— ì˜¬ë¦°ë‹¤.\nyhat, loss, grad, update ìˆ˜í–‰\nì§ìˆ˜obsì˜ x,yë¥¼ GPUë©”ëª¨ë¦¬ì—ì„œ ë‚´ë¦°ë‹¤. ê·¸ë¦¬ê³  í™€ìˆ˜obsì˜ x,yë¥¼ GPUë©”ëª¨ë¦¬ì— ì˜¬ë¦°ë‹¤.\nyhat, loss, grad, update ìˆ˜í–‰\ní™€ìˆ˜obsì˜ x,yë¥¼ GPUë©”ëª¨ë¦¬ì—ì„œ ë‚´ë¦°ë‹¤. ê·¸ë¦¬ê³  ì§ìˆ˜obsì˜ x,yë¥¼ GPUë©”ëª¨ë¦¬ì— ì˜¬ë¦°ë‹¤.\në°˜ë³µ\n\n\nì´ëŸ¬ë©´ ë˜ëŠ”ê±°ì•„ë‹ˆì•¼???? â€”&gt; ë§ì•„ìš”\n\nğŸ—£ï¸ =&gt; í™•ë¥ ì ê²½ì‚¬í•˜ê°•ë²•"
  },
  {
    "objectID": "posts/05wk-2.html#c.-ê²½ì‚¬í•˜ê°•ë²•-í™•ë¥ ì ê²½ì‚¬í•˜ê°•ë²•-ë¯¸ë‹ˆë°°ì¹˜-ê²½ì‚¬í•˜ê°•ë²•",
    "href": "posts/05wk-2.html#c.-ê²½ì‚¬í•˜ê°•ë²•-í™•ë¥ ì ê²½ì‚¬í•˜ê°•ë²•-ë¯¸ë‹ˆë°°ì¹˜-ê²½ì‚¬í•˜ê°•ë²•",
    "title": "05wk-2: (ì‹ ê²½ë§) â€“ ì‹ ê²½ë§ì˜ í‘œí˜„, GPUì‚¬ìš©ë²•, í™•ë¥ ì ê²½ì‚¬í•˜ê°•ë²•",
    "section": "C. ê²½ì‚¬í•˜ê°•ë²•, í™•ë¥ ì ê²½ì‚¬í•˜ê°•ë²•, ë¯¸ë‹ˆë°°ì¹˜ ê²½ì‚¬í•˜ê°•ë²•",
    "text": "C. ê²½ì‚¬í•˜ê°•ë²•, í™•ë¥ ì ê²½ì‚¬í•˜ê°•ë²•, ë¯¸ë‹ˆë°°ì¹˜ ê²½ì‚¬í•˜ê°•ë²•\nğŸ—£ï¸ ì„ì˜ì˜ ë¬¶ìŒ: ë¯¸ë‹ˆë°°ì¹˜\n10ê°œì˜ ìƒ˜í”Œì´ ìˆë‹¤ê³  ê°€ì •. \\(\\{(x_i,y_i)\\}_{i=1}^{10}\\)\n# ver1 â€“ ëª¨ë“  ìƒ˜í”Œì„ ì´ìš©í•˜ì—¬ slope ê³„ì‚°\n(epoch 1) \\(loss=\\sum_{i=1}^{10}(y_i-\\hat{w}_0-\\hat{w}_1x_i)^2 \\to slope  \\to update\\)\n(epoch 2) \\(loss=\\sum_{i=1}^{10}(y_i-\\hat{w}_0-\\hat{w}_1x_i)^2 \\to slope  \\to update\\)\nğŸ—£ï¸ lossê°€ SSE\nâ€¦\n\nìš°ë¦¬ê°€ í•­ìƒ ì´ë ‡ê²Œ í–ˆì£ !\n\n# ver2 â€“ í•˜ë‚˜ì˜ ìƒ˜í”Œë§Œì„ ì´ìš©í•˜ì—¬ slope ê³„ì‚°\n(epoch 1)\n\n\\(loss=(y_1-\\hat{w}_0-\\hat{w}_1x_1)^2 \\to slope \\to update\\)\n\\(loss=(y_2-\\hat{w}_0-\\hat{w}_1x_2)^2 \\to slope \\to update\\)\nâ€¦\n\\(loss=(y_{10}-\\hat{w}_0-\\hat{w}_1x_{10})^2  \\to  slope  \\to  update\\)\n\n(epoch 2)\n\n\\(loss=(y_1-\\hat{w}_0-\\hat{w}_1x_1)^2  \\to slope  \\to  update\\)\n\\(loss=(y_2-\\hat{w}_0-\\hat{w}_1x_2)^2  \\to slope  \\to  update\\)\nâ€¦\n\\(loss=(y_{10}-\\hat{w}_0-\\hat{w}_1x_{10})^2  \\to  slope  \\to  update\\)\n\nğŸ—£ï¸ epoch 2 í›„ 20ë²ˆ update\nâ€¦\n# ver3 â€“ \\(m (\\leq n)\\) ê°œì˜ ìƒ˜í”Œì„ ì´ìš©í•˜ì—¬ slope ê³„ì‚°\nğŸ—£ï¸ ë¯¸ë‹ˆë°°ì¹˜ ì‚¬ì´ì¦ˆ = 3\n\\(m=3\\)ì´ë¼ê³  í•˜ì.\n(epoch 1)\n\n\\(loss=\\sum_{i=1}^{3}(y_i-\\hat{w}_0-\\hat{w}_1x_i)^2  \\to  slope  \\to  update\\)\n\\(loss=\\sum_{i=4}^{6}(y_i-\\hat{w}_0-\\hat{w}_1x_i)^2  \\to  slope  \\to  update\\)\n\\(loss=\\sum_{i=7}^{9}(y_i-\\hat{w}_0-\\hat{w}_1x_i)^2  \\to  slope  \\to  update\\)\n\\(loss=(y_{10}-\\hat{w}_0-\\hat{w}_1x_{10})^2  \\to  slope  \\to  update\\)\n\n(epoch 2)\n\n\\(loss=\\sum_{i=1}^{3}(y_i-\\hat{w}_0-\\hat{w}_1x_i)^2  \\to  slope  \\to  update\\)\n\\(loss=\\sum_{i=4}^{6}(y_i-\\hat{w}_0-\\hat{w}_1x_i)^2  \\to  slope  \\to  update\\)\n\\(loss=\\sum_{i=7}^{9}(y_i-\\hat{w}_0-\\hat{w}_1x_i)^2  \\to  slope  \\to  update\\)\n\\(loss=(y_{10}-\\hat{w}_0-\\hat{w}_1x_{10})^2  \\to  slope  \\to  update\\)\n\nğŸ—£ï¸ 10ì€ ë‹¨ë…ìœ¼ë¡œ update\nâ€¦"
  },
  {
    "objectID": "posts/05wk-2.html#d.-ìš©ì–´ì˜-ì •ë¦¬",
    "href": "posts/05wk-2.html#d.-ìš©ì–´ì˜-ì •ë¦¬",
    "title": "05wk-2: (ì‹ ê²½ë§) â€“ ì‹ ê²½ë§ì˜ í‘œí˜„, GPUì‚¬ìš©ë²•, í™•ë¥ ì ê²½ì‚¬í•˜ê°•ë²•",
    "section": "D. ìš©ì–´ì˜ ì •ë¦¬",
    "text": "D. ìš©ì–´ì˜ ì •ë¦¬\nì˜›ë‚ \n- ver1(ëª¨ë“ ): gradient descent, batch gradient descent\n- ver2(í•˜ë‚˜ë§Œ): stochastic gradient descent\n- ver3(ëª‡ê°œë§Œ): mini-batch gradient descent, mini-batch stochastic gradient descent\nğŸ—£ï¸ stochastic: ì°¨ë¡€ëŒ€ë¡œ í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ëœë¤ìœ¼ë¡œ ê³„ì† ë½‘ëŠ” ë°©ì‹ë„ ìˆì–´ì„œ ì´ê²ƒì— ê¸°ì›ì„ ë‘  / batch: ì „ì²´ data\nğŸ—£ï¸ ver3: ìœ„ ê·¸ë¦¼ì—ì„œ y10ì˜ ê²½ìš° ê°€ì¤‘ì¹˜ê°€ ê³„ì† ìˆëŠ” ê²ƒì´ ì‹«ì–´ì„œ ëœë¤ìœ¼ë¡œ 3ê°œì”© ë½‘ì„ ìˆ˜ë„ ìˆìŒ -&gt; stochastic\nìš”ì¦˜\n- ver1(ëª¨ë“ ): gradient descent\n- ver2(í•˜ë‚˜ë§Œ): stochastic gradient descent with batch size = 1\n- ver3(ëª‡ê°œë§Œ): stochastic gradient descent - https://www.deeplearningbook.org/contents/optimization.html, ì•Œê³ ë¦¬ì¦˜ 8-1 ì°¸ê³ .\nğŸ—£ï¸ ver3ì„ ì œì¼ ë§ì´ ì”€"
  },
  {
    "objectID": "posts/05wk-2.html#e.-datasetds-dataloaderdl",
    "href": "posts/05wk-2.html#e.-datasetds-dataloaderdl",
    "title": "05wk-2: (ì‹ ê²½ë§) â€“ ì‹ ê²½ë§ì˜ í‘œí˜„, GPUì‚¬ìš©ë²•, í™•ë¥ ì ê²½ì‚¬í•˜ê°•ë²•",
    "section": "E. Dataset(ds), DataLoader(dl)",
    "text": "E. Dataset(ds), DataLoader(dl)\n\nì·¨ì§€ëŠ” ì•Œê² ìœ¼ë‚˜, Cì˜ ê³¼ì •ì„ ì‹¤ì œ êµ¬í˜„í•˜ë ¤ë©´ ì§„ì§œ ì–´ë ¤ì›€.. (ì…ì½”ë”©ê³¼ ì†ì½”ë”©ì˜ ì°¨ì´) â€“&gt; ì´ê±¸ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ íŒŒì´í† ì¹˜ì—ì„œëŠ” DataLoaderë¼ëŠ” ì˜¤ë¸Œì íŠ¸ë¥¼ ì¤€ë¹„í–ˆìŒ!\n\n- ë°ì´í„°\n\nx=torch.tensor(range(10))\ny=torch.tensor([1.0]*5+[0.0]*5).reshape(-1,1)\n\n\nx, y\n\n(tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n tensor([[1.],\n         [1.],\n         [1.],\n         [1.],\n         [1.],\n         [0.],\n         [0.],\n         [0.],\n         [0.],\n         [0.]]))\n\n\n\nx.shape, y.shape # ëª‡ê°œì”© ìˆëŠ”ì§€ê°€ ì¤‘ìš”\n\n(torch.Size([10]), torch.Size([10]))\n\n\n\nx=torch.tensor(range(10)).float().reshape(-1,1)\ny=torch.tensor([1.0]*5+[0.0]*5).reshape(-1,1)\ntorch.concat([x,y],axis=1)\n\ntensor([[0., 1.],\n        [1., 1.],\n        [2., 1.],\n        [3., 1.],\n        [4., 1.],\n        [5., 0.],\n        [6., 0.],\n        [7., 0.],\n        [8., 0.],\n        [9., 0.]])\n\n\n- dsì˜¤ë¸Œì íŠ¸\n\nds = torch.utils.data.TensorDataset(x,y)\nds\n\n&lt;torch.utils.data.dataset.TensorDataset at 0x7f5c8eed5fa0&gt;\n\n\nğŸ—£ï¸ ë­”ê°€ ë§Œë“¤ì–´ì§ (ds: dataset)\n\nds.tensors \n# ìƒê¸´ê±´ ds.tensors = (x,y) ì„\n\n(tensor([[0.],\n         [1.],\n         [2.],\n         [3.],\n         [4.],\n         [5.],\n         [6.],\n         [7.],\n         [8.],\n         [9.]]),\n tensor([[1.],\n         [1.],\n         [1.],\n         [1.],\n         [1.],\n         [0.],\n         [0.],\n         [0.],\n         [0.],\n         [0.]]))\n\n\n\nds[0],(x,y)[0] # (x,y) íŠœí”Œìì²´ëŠ” ì•„ë‹˜.. ì¸ë±ì‹±ì´ ë‹¤ë¥´ê²Œ ë™ì‘\n\n((tensor([0.]), tensor([1.])),\n tensor([[0.],\n         [1.],\n         [2.],\n         [3.],\n         [4.],\n         [5.],\n         [6.],\n         [7.],\n         [8.],\n         [9.]]))\n\n\nğŸ—£ï¸(\n\n(x,y)[1] # y\n\ntensor([[1.],\n        [1.],\n        [1.],\n        [1.],\n        [1.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.]])\n\n\n\n(x,y)[2] # error\n\n\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nCell In[60], line 1\n----&gt; 1 (x,y)[2] # error\n\nIndexError: tuple index out of range\n\n\n\n\nds[:3] # ì¸ë±ì‹±ì´ í¸í•¨\n\n(tensor([[0.],\n         [1.],\n         [2.]]),\n tensor([[1.],\n         [1.],\n         [1.]]))\n\n\n)ğŸ—£ï¸\n- dl ì˜¤ë¸Œì íŠ¸\n\ndl = torch.utils.data.DataLoader(ds, batch_size=3)\n\nğŸ—£ï¸(\n\nbatch_size: ì›í•˜ëŠ” ê°’ìœ¼ë¡œ ì…ë ¥\n\n\ndl = torch.utils.data.DataLoader(ds, batch_size=5)\n\n\nfor _ in dl:\n    print(_)\n\n[tensor([[0.],\n        [1.],\n        [2.],\n        [3.],\n        [4.]]), tensor([[1.],\n        [1.],\n        [1.],\n        [1.],\n        [1.]])]\n[tensor([[5.],\n        [6.],\n        [7.],\n        [8.],\n        [9.]]), tensor([[0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.]])]\n\n\n\nfor x,y in dl:\n    print(x)\n\ntensor([[0.],\n        [1.],\n        [2.],\n        [3.],\n        [4.]])\ntensor([[5.],\n        [6.],\n        [7.],\n        [8.],\n        [9.]])\n\n\n\ndl = torch.utils.data.DataLoader(ds, batch_size=3)\n\n\nfor x,y in dl:\n    print(x)\n\ntensor([[0.],\n        [1.],\n        [2.]])\ntensor([[3.],\n        [4.],\n        [5.]])\ntensor([[6.],\n        [7.],\n        [8.]])\ntensor([[9.]])\n\n\n)ğŸ—£ï¸\n\nfor x_mbatch,y_mbatch in dl:\n    print(f\"x_mini_batch:{x_mbatch.tolist()} \\t y_mini_batch:{y_mbatch.tolist()}\")\n\nx_mini_batch:[[0.0], [1.0], [2.0]]   y_mini_batch:[[1.0], [1.0], [1.0]]\nx_mini_batch:[[3.0], [4.0], [5.0]]   y_mini_batch:[[1.0], [1.0], [0.0]]\nx_mini_batch:[[6.0], [7.0], [8.0]]   y_mini_batch:[[0.0], [0.0], [0.0]]\nx_mini_batch:[[9.0]]     y_mini_batch:[[0.0]]\n\n\n- ë§ˆì§€ë§‰ê´€ì¸¡ì¹˜ëŠ” ë­”ë° ë‹¨ë…ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•˜ëƒ?? â€“&gt; shuffle True ê°™ì´ ìì˜í•œ ì˜µì…˜ë„ ìˆìŒ..\n\ndl = torch.utils.data.DataLoader(ds,batch_size=3,shuffle=True)\nfor x_mbatch,y_mbatch in dl:\n    print(f\"x_mini_batch:{x_mbatch.tolist()} \\t y_mini_batch:{y_mbatch.tolist()}\")\n\nx_mini_batch:[[1.0], [5.0], [6.0]]   y_mini_batch:[[1.0], [0.0], [0.0]]\nx_mini_batch:[[7.0], [0.0], [9.0]]   y_mini_batch:[[0.0], [1.0], [0.0]]\nx_mini_batch:[[2.0], [8.0], [3.0]]   y_mini_batch:[[1.0], [0.0], [1.0]]\nx_mini_batch:[[4.0]]     y_mini_batch:[[1.0]]\n\n\nğŸ—£ï¸ ëŒë¦´ ë•Œë§ˆë‹¤ ë‹¬ë¼ì§"
  },
  {
    "objectID": "posts/05wk-2.html#f.-ì„±ëŠ¥ì²´í¬",
    "href": "posts/05wk-2.html#f.-ì„±ëŠ¥ì²´í¬",
    "title": "05wk-2: (ì‹ ê²½ë§) â€“ ì‹ ê²½ë§ì˜ í‘œí˜„, GPUì‚¬ìš©ë²•, í™•ë¥ ì ê²½ì‚¬í•˜ê°•ë²•",
    "section": "F. ì„±ëŠ¥ì²´í¬",
    "text": "F. ì„±ëŠ¥ì²´í¬\n- ëª©í‘œ: í™•ë¥ ì ê²½ì‚¬í•˜ê°•ë²•ê³¼ ê·¸ëƒ¥ ê²½ì‚¬í•˜ê°•ë²•ì˜ ì„±ëŠ¥ì„ â€œë™ì¼ ë°˜ë³µíšŸìˆ˜â€ë¡œ ë¹„êµí•´ë³´ì.\nğŸ—£ï¸(\n10 4 -&gt; 10, 10, 10, 10\n10 4 -&gt; 3, 3, 3, 1\n\nìœ„ê°€ ì¢‹ì„ ê²ƒ ê°™ì§€ë§Œ ë³„ ì°¨ì´ ì—†ìŒ\nbatch sizeë¥¼ ì˜ ì •í•˜ë©´ ë°‘ì´ ì˜¤íˆë ¤ ì¢‹ì„ ìˆ˜ë„ ìˆìŒ\n\n)ğŸ—£ï¸\n- MNISTìë£Œë¥¼ ê·¸ëƒ¥ ê²½ì‚¬í•˜ê°•ë²•ìœ¼ë¡œ ì í•©í•´ë³´ì.\n\nimport torchvision\n\n\ntrain_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True)\nto_tensor = torchvision.transforms.ToTensor()\nX0 = torch.stack([to_tensor(Xi) for Xi, yi in train_dataset if yi==0])\nX1 = torch.stack([to_tensor(Xi) for Xi, yi in train_dataset if yi==1])\nX = torch.concat([X0,X1],axis=0).reshape(-1,784)\ny = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1)\n\n\nX.shape, y.shape\n\n(torch.Size([12665, 784]), torch.Size([12665, 1]))\n\n\n\ntorch.manual_seed(1)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(784,32),\n    torch.nn.ReLU(),\n    torch.nn.Linear(32,1),\n    torch.nn.Sigmoid()\n)\nloss_fn = torch.nn.BCELoss()\noptimizr = torch.optim.SGD(net.parameters())\n\n\nfor epoc in range(700):\n    # step1 \n    yhat = net(X)\n    # step2 \n    loss = loss_fn(yhat,y)\n    # step3     \n    loss.backward()\n    # step4 \n    optimizr.step()\n    optimizr.zero_grad()    \n\n\n((yhat &gt; 0.5) ==  y).float().mean()\n\ntensor(0.9953)\n\n\n- MNISTìë£Œë¥¼ í™•ë¥ ì  ê²½ì‚¬í•˜ê°•ë²•ìœ¼ë¡œ ì í•©í•´ë³´ì. â€“ ë¯¸ë‹ˆë°°ì¹˜ ì“°ëŠ” í•™ìŠµ\n\n# train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True)\n# to_tensor = torchvision.transforms.ToTensor()\n# X0 = torch.stack([to_tensor(Xi) for Xi, yi in train_dataset if yi==0])\n# X1 = torch.stack([to_tensor(Xi) for Xi, yi in train_dataset if yi==1])\n# X = torch.concat([X0,X1],axis=0).reshape(-1,784)\n# y = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1)\nds = torch.utils.data.TensorDataset(X,y)\ndl = torch.utils.data.DataLoader(ds,batch_size=2048)\n\n\nlen(X)/2048\n\n6.18408203125\n\n\nğŸ—£ï¸ ë§ˆì§€ë§‰ ë©ì–´ë¦¬ëŠ” ì‘ìŒ\n\në”°ë¼ì„œ (mini) batchsize ê°€ 2048 ì´ë¼ë©´ í•œ epochë‹¹ 7íšŒ update\n\n\ntorch.manual_seed(1)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(784,32),\n    torch.nn.ReLU(),\n    torch.nn.Linear(32,1),\n    torch.nn.Sigmoid()\n)\nloss_fn = torch.nn.BCELoss()\noptimizr = torch.optim.SGD(net.parameters())\n\n\nfor epoc in range(100): \n    for xm,ym in dl:        \n        # step1 \n        ym_hat = net(xm)\n        # step2 \n        loss = loss_fn(ym_hat,ym)\n        # step3     \n        loss.backward()\n        # step4 \n        optimizr.step()\n        optimizr.zero_grad()\n\n\nğŸ—£ï¸\n\nì´ update ë°˜ë³µ ìˆ˜ëŠ” ë™ì¼ (7ë²ˆ * 100ë²ˆ)\nxm = (2048,784), ym = (2048,1)\n\n\n\n((net(X) &gt; 0.5) ==  y).float().mean()\n\ntensor(0.9931)"
  }
]